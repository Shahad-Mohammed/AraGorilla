{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ØªØ³Ù…ÙŠØ§Øª Ù„Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ². Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. This includes AutoTokenizer and AutoModelForTokenClassification.\\\\n2. Use the from_pretrained method of the AutoTokenizer class to load the tokenizer with the model 'Davlan/distilbert-base-multilingual-cased-ner-hrl'.\\\\n3. Also, use the from_pretrained method of the AutoModelForTokenClassification class to load the named entity recognition model for the same multilingual model.\\\\n4. Create a pipeline with 'ner' as the task, and pass the model and tokenizer as arguments.\\\\n5. Pass an example text to the pipeline, which will recognize and classify named entities in the text.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Davlan/distilbert-base-multilingual-cased-ner-hrl\\\\')\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'Davlan/distilbert-base-multilingual-cased-ner-hrl\\\\')\\\\nnlp = pipeline(\\\\'ner\\\\', model=model, tokenizer=tokenizer)\\\\nexample = \\\"Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\\\"\\\\nner_results = nlp(example)\\\\nprint(ner_results)', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilbert-base-multilingual-cased-ner-hrl', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl')\\\", 'api_arguments': {'model': 'Davlan/distilbert-base-multilingual-cased-ner-hrl', 'tokenizer': 'Davlan/distilbert-base-multilingual-cased-ner-hrl'}, 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoTokenizer, AutoModelForTokenClassification\\\\nfrom transformers import pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\\\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\\\nexample = Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\\\\nner_results = nlp(example)\\\\nprint(ner_results)', 'performance': {'dataset': [{'name': 'ANERcorp', 'language': 'Arabic'}, {'name': 'conll 2003', 'language': 'German'}, {'name': 'conll 2003', 'language': 'English'}, {'name': 'conll 2002', 'language': 'Spanish'}, {'name': 'Europeana Newspapers', 'language': 'French'}, {'name': 'Italian I-CAB', 'language': 'Italian'}, {'name': 'Latvian NER', 'language': 'Latvian'}, {'name': 'conll 2002', 'language': 'Dutch'}, {'name': 'Paramopama + Second Harem', 'language': 'Portuguese'}, {'name': 'MSRA', 'language': 'Chinese'}], 'accuracy': 'Not specified'}, 'description': 'distilbert-base-multilingual-cased-ner-hrl is a Named Entity Recognition model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned Distiled BERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER).' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ù„Ø¬ÙˆØ§Ø¨ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØµÙˆØ± ÙˆÙ…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ.? [/INST]{'domain': 'Multimodal Document Question Answer', 'api_call': \\\"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the pipeline function from the transformers library provided by Hugging Face.', 'code': \\\"from transformers import pipeline\\\\ndoc_vqa = pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\\nresult = doc_vqa(image_path='path/to/image.jpg', question='What is the total cost?')\\\", 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'vision-encoder-decoder', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': \\\"from transformers import pipeline\\\\ndoc_vqa = pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\\nresult = doc_vqa(image_path='path/to/image.jpg', question='What is the total cost?')\\\", 'api_arguments': 'image_path, question', 'python_environment_requirements': 'transformers', 'example_code': \\\"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\\\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØµÙ…ÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø¬Ø¯ÙˆÙ„ÙŠ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"pipeline('text-generation', model='distilgpt2')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': 'from transformers import pipeline, set_seed\\\\nset_seed(42)\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'distilgpt2\\\\')\\\\nprompts = [\\\"Revolutionize your daily routine with\\\", \\\"Elevate your lifestyle by\\\"]\\\\nresults = []\\\\nfor prompt in prompts:\\\\n    generated_text = generator(prompt, max_length=30, num_return_sequences=1)\\\\n    results.append(generated_text[0][\\\\'generated_text\\\\'])\\\\nresults', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Transformers', 'functionality': 'Text Generation', 'api_name': 'distilgpt2', 'api_call': 'from transformers import pipeline, set_seed\\\\nset_seed(42)\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'distilgpt2\\\\')\\\\nprompts = [\\\"Revolutionize your daily routine with\\\", \\\"Elevate your lifestyle by\\\"]\\\\nresults = []\\\\nfor prompt in prompts:\\\\n    generated_text = generator(prompt, max_length=30, num_return_sequences=1)\\\\n    results.append(generated_text[0][\\\\'generated_text\\\\'])\\\\nresults', 'api_arguments': ['model'], 'python_environment_requirements': ['from transformers import pipeline, set_seed'], 'example_code': 'set_seed(42)\\\\ngenerator(Hello, Iâ€™m a language model, max_length=20, num_return_sequences=5)', 'performance': {'dataset': 'WikiText-103', 'accuracy': '21.100'}, 'description': 'DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. With 82 million parameters, it was developed using knowledge distillation and designed to be a faster, lighter version of GPT-2. It can be used for text generation, writing assistance, creative writing, entertainment, and more.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ´ÙŠØ± Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø­ÙˆÙ„ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¥Ù„Ù‰ ØµÙˆØ±Ø© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø¥Ù„Ù‰ Ø£Ù†Ù‡ ÙŠÙ…ÙƒÙ† ØªÙ†Ø¸ÙŠÙ… Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø¥ÙØªØ±Ø§Ø¶ÙŠØ©. Ù†ÙˆØµÙŠ Ø¨Ø¥Ø¶Ø§ÙØ© Ø£Ø¯Ø§Ø© ØªÙ…ÙƒÙ†Ù‡Ù… Ù…Ù† Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ ÙˆØ§Ù„Ø¥Ø³ØªÙØ§Ø¯Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary 'pipeline' function from the transformers library provided by Hugging Face.\\\\n2. Use the 'pipeline' function to load the 'Helsinki-NLP/opus-mt-sv-en' model, which is a Swedish to English translation model trained on the OPUS dataset.\\\\n3. The loaded model will be used for translating your Swedish text to English.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntranslator = pipeline('translation_sv_to_en', model='Helsinki-NLP/opus-mt-sv-en')\\\\nswedish_text = 'Svensk text hÃ¤r...'\\\\nenglish_translation = translator(swedish_text, max_length=250)[0]['translation_text']\\\", 'api_call': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-sv-en', 'api_call': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", 'api_arguments': ['inputs'], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'Tatoeba.sv.en', 'accuracy': 'BLEU: 64.5, chr-F: 0.763'}, 'description': 'A Swedish to English translation model trained on the OPUS dataset using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ù†Ø·Ù‚ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø·Ù‚ Ø¢Ù„ÙŠ Ù„Ù„ÙƒÙ„Ø§Ù…: \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. In order to transcribe the podcast audio, begin by importing the necessary classes from the transformers package. This includes Wav2Vec2Processor for audio preprocessing and Wav2Vec2ForCTC for the automatic speech recognition model.\\\\n2. Load the pre-trained model 'facebook/wav2vec2-base-960h' using the Wav2Vec2ForCTC class's from_pretrained method. This model has been specifically trained for transcription tasks.\\\\n3. Convert the audio file into a format that can be processed by the model, making sure it is sampled at 16kHz. This can be done using the Wav2Vec2Processor.\\\\n4. Use the model to generate logits, then select the most probable sequence of characters for each chunk of audio.\\\\n5. Lastly, decode the transcription to produce human-readable text.\\\", '<<<code>>>': \\\"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nimport torch\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\\\\n\\\\n# Load the audio file here\\\\ninput_values = processor(audio_array, return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/wav2vec2-base-960h', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\\\", 'api_arguments': ['input_values'], 'python_environment_requirements': [['transformers', 'datasets', 'torch', 'jiwer']], 'example_code': \\\"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\\\\nds = load_dataset('patrickvonplaten/librispeech_asr_dummy', 'clean', split='validation')\\\\ninput_values = processor(ds[0]['audio']['array'], return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'performance': {'dataset': 'LibriSpeech', 'accuracy': {'clean': 3.4, 'other': 8.6}}, 'description': \\\"Facebook's Wav2Vec2 base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It is designed for automatic speech recognition and can transcribe audio files.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'financial-summarization-pegasus', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabiaâ€™s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Sambaâ€™s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf regionâ€™s third-largest lender. The entityâ€™s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle Eastâ€™s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))', 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 35.206, 'ROUGE-2': 16.569, 'ROUGE-L': 30.128, 'ROUGE-LSUM': 30.171}}, 'description': 'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.' </s>\"\n"}
{"text": "<s>[INST]Ø³ØªÙ‚ÙˆÙ… Ø§Ù„Ø´Ø±ÙƒØ© Ø¨ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù†ØµÙˆØµ. ÙŠØ±ØºØ¨ Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ù†ÙˆØ¹ Ø¥Ù„Ù‰ Ù†ÙˆØ¹ Ø¢Ø®Ø± Ø¨Ø´ÙƒÙ„ Ø¢Ù„ÙŠ ÙˆØ¯Ù‚ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"pipeline('text2text-generation', model='google/t5-v1_1-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a text2text generation model, specifying the model 'google/t5-v1_1-base'.\\\\n3. This model is a state-of-the-art text-to-text transformer trained on various NLP tasks, including summarization.\\\\n4. By providing a text in the target language of the user, the model can produce a summary, aiding the user in learning the language more efficiently.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nt5 = pipeline('text2text-generation', model='google/t5-v1_1-base')\\\\nsummary = t5('summarize: <Text in the target language here>')\\\", 'api_call': \\\"pipeline('text2text-generation', model='google/t5-v1_1-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/t5-v1_1-base', 'api_call': \\\"pipeline('text2text-generation', model='google/t5-v1_1-base')\\\", 'api_arguments': {'model': 'google/t5-v1_1-base'}, 'python_environment_requirements': [{'transformers': '>=4.0.0'}], 'example_code': \\\"from transformers import pipeline\\\\n\\\\nt5 = pipeline('text2text-generation', model='google/t5-v1_1-base')\\\\nt5('translate English to French: Hugging Face is a great company')\\\", 'performance': {'dataset': 'c4', 'accuracy': 'Not provided'}, 'description': \\\"Google's T5 Version 1.1 is a state-of-the-art text-to-text transformer model that achieves high performance on various NLP tasks such as summarization, question answering, and text classification. It is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on downstream tasks.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ ØªÙˆÙ„ÙŠØ¯ÙŠ ÙŠØ³ØªØ®Ø¯Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù†ØµÙˆØµØŒ ÙˆØ§Ø¨Ø¯Ø£ Ø¨Ù€ \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"pipeline('translation_en_to_de', model='sshleifer/tiny-marian-en-de')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library function, which is 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'sshleifer/tiny-marian-en-de'.\\\\n3. The loaded model will be used for English to German translation.\\\\n4. The model will translate the given sentence from English to German.\\\\n5. The translated output will be returned.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_en_to_de\\\\', model=\\\\'sshleifer/tiny-marian-en-de\\\\')\\\\nsentence = \\\"A beautiful sunset faded into the dark night.\\\"\\\\ntranslated_sentence = translator(sentence)[0][\\\\'translation_text\\\\']', 'api_call': \\\"pipeline('translation_en_to_de', model='sshleifer/tiny-marian-en-de')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sshleifer/tiny-marian-en-de', 'api_call': \\\"pipeline('translation_en_to_de', model='sshleifer/tiny-marian-en-de')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny English to German translation model using the Marian framework in Hugging Face Transformers.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ù„ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø¯Ø§Ù„Ø©. ÙŠÙ‡Ø¯Ù Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¥Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØµÙˆØ± ÙˆØ¨Ø§Ù„Ø¹ÙƒØ³. Ø¹Ù„ÙŠÙ†Ø§ ØªÙˆÙÙŠØ± ÙˆØ³ÙŠÙ„Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù„ØªØ­Ù‚ÙŠÙ‚ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨ÙƒÙ„ Ø³Ù‡ÙˆÙ„Ø© ÙˆÙŠØ³Ø±.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library: AutoTokenizer, AutoModelForTokenClassification, and pipeline.\\\\n2. Use the from_pretrained method of the AutoTokenizer to create a tokenizer for the 'Babelscape/wikineural-multilingual-ner' model.\\\\n3. Similarly, use the from_pretrained method of the AutoModelForTokenClassification to create a token classification model appropriate for named entity recognition.\\\\n4. Create an NER pipeline using the pipeline function, passing in the NER model and tokenizer created earlier.\\\\n5. Finally, pass the Spanish text as input to the NER pipeline.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Babelscape/wikineural-multilingual-ner\\\\')\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'Babelscape/wikineural-multilingual-ner\\\\')\\\\nnlp = pipeline(\\\\'ner\\\\', model=model, tokenizer=tokenizer)\\\\ntext_in_spanish = \\\"Me llamo Pablo y vivo en Madrid.\\\"\\\\nner_results = nlp(text_in_spanish)\\\\nprint(ner_results)', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Babelscape/wikineural-multilingual-ner', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\\\", 'api_arguments': {'model': 'AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)', 'tokenizer': 'AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)'}, 'python_environment_requirements': [{'transformers': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline'}], 'example_code': 'tokenizer = AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\\\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\\\nexample = My name is Wolfgang and I live in Berlin\\\\nner_results = nlp(example)\\\\nprint(ner_results)', 'performance': {'dataset': 'Babelscape/wikineural-multilingual-ner', 'accuracy': 'span-based F1-score up to 6 points over previous state-of-the-art systems for data creation'}, 'description': 'A multilingual Named Entity Recognition (NER) model fine-tuned on the WikiNEuRal dataset, supporting 9 languages (de, en, es, fr, it, nl, pl, pt, ru). It is based on the mBERT architecture and trained on all 9 languages jointly. The model can be used with the Hugging Face Transformers pipeline for NER tasks.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ØªØ¨Ø¯Ø£ Ø¨Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\" ÙˆØ§Ø³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\\\", '<<<api_provider>>>': 'PyTorch Transformers', '<<<explanation>>>': 'e are an international business company, and one of our Spanish customers shares their document in Spanish. Translate it into English.\\\\n###Input: \\\"Hola, necesitamos una respuesta rÃ¡pidamente sobre nuestra consulta en relaciÃ³n con el Ãºltimo negocio que realizamos juntos. Por favor, avÃ­senos lo antes posible. Gracias.\\\"\\\\n###Output:\\\\n<<<domain>>>: Natural Language Processing Translation\\\\n<<<api_call>>>: pipeline(\\\\'translation_xx_to_yy\\\\', model=\\\\'facebook/nllb-200-distilled-600M\\\\')\\\\n<<<api_provider>>>: PyTorch Transformers\\\\n\\\\n<<<translation>>>: \\\"Hello, we need a quick response to our inquiry regarding the last business we did together. Please let us know as soon as possible. Thank you.\\\"', '<<<code>>>': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_es_to_en\\\\', model=\\\\'facebook/nllb-200-distilled-600M\\\\')\\\\ntranslated_text = translator(\\\"Hola, necesitamos una respuesta rÃ¡pidamente sobre nuestra consulta en relaciÃ³n con el Ãºltimo negocio que realizamos juntos. Por favor, avÃ­senos lo antes posible. Gracias.\\\")', 'api_call': \\\"pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\\\", 'provider': 'PyTorch Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/nllb-200-distilled-600M', 'api_call': \\\"pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\\\", 'api_arguments': ['model', 'text'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline; translator = pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M'); translator('Hello World')\\\", 'performance': {'dataset': 'Flores-200', 'accuracy': 'BLEU, spBLEU, chrF++'}, 'description': 'NLLB-200 is a machine translation model primarily intended for research in machine translation, especially for low-resource languages. It allows for single sentence translation among 200 languages. The model was trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙŠØ±Ø¬Ù‰ ØªÙˆÙÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ÙŠØ© ØªØªØ¶Ù…Ù† Ø£Ø²ÙˆØ§Ø¬ Ù…Ù† Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø© ÙˆØ§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…Ø®Ø±Ø¬Ø© Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªÙˆÙ„ÙŠÙ Ø§Ù„Ù„ÙˆØ§Ø¦ÙŠ Ø£Ùˆ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„ØªØ±ÙƒÙŠØ¨ÙŠØ© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ù‡Ø¯Ù. Ø«Ù… ÙŠÙ…ÙƒÙ† ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…Ø«Ù„ BLEU ÙˆROUGE.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"pipeline('text2text-generation', model='google/t5-v1_1-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and functions, namely, 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function to create a text2text generation model, in this case 'google/t5-v1_1-base', which is capable of translating text automatically.\\\\n3. To use this model for translating English text to French, provide the input text in English as the input with the prefix 'translate English to French:'.\\\\n4. The model will automatically translate the given text to French.\\\", '<<<code>>>': 'from transformers import pipeline\\\\n\\\\nt5 = pipeline(\\\\'text2text-generation\\\\', model=\\\\'google/t5-v1_1-base\\\\')\\\\n\\\\nbreaking_news = \\\"Hugging Face releases the most advanced language model to date.\\\"\\\\ntranslated_news = t5(f\\\"translate English to French: {breaking_news}\\\")[0][\\\\'generated_text\\\\']', 'api_call': \\\"pipeline('text2text-generation', model='google/t5-v1_1-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/t5-v1_1-base', 'api_call': \\\"pipeline('text2text-generation', model='google/t5-v1_1-base')\\\", 'api_arguments': {'model': 'google/t5-v1_1-base'}, 'python_environment_requirements': [{'transformers': '>=4.0.0'}], 'example_code': \\\"from transformers import pipeline\\\\n\\\\nt5 = pipeline('text2text-generation', model='google/t5-v1_1-base')\\\\nt5('translate English to French: Hugging Face is a great company')\\\", 'performance': {'dataset': 'c4', 'accuracy': 'Not provided'}, 'description': \\\"Google's T5 Version 1.1 is a state-of-the-art text-to-text transformer model that achieves high performance on various NLP tasks such as summarization, question answering, and text classification. It is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on downstream tasks.\\\" </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆØ¯ ØªÙˆÙ„ÙŠØ¯ Ù†Øµ ØªØµÙ†ÙŠÙÙŠ Ù„Ù…Ù„Ø®Øµ ØªØµÙ…ÙŠÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø®ØµÙˆØµ Ø§Ù„ÙØ¦Ø§Øª ÙˆØ§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù…ÙØ­ØªÙ…Ù„Ø© Ù„Ù„Ù†ØµÙˆØµ. Ù‡Ù„ ØªØ³ØªØ·ÙŠØ¹ ØªÙ†ÙÙŠØ° Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø±ØŸ? [/INST]{'domain': 'Natural Language Processing Text Classification', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\\\", 'api_provider': 'Transformers', 'explanation': '1. We first import the tokenizer and model from the transformers library. The tokenizer will be used to tokenize the text input, and the model will be used for classification.', 'code': 'from transformers import AutoModelForSequenceClassification, AutoTokenizer\\\\nimport numpy as np\\\\nfrom scipy.special import softmax\\\\n\\\\ndef preprocess(text):\\\\n    new_text = []\\\\n    for t in text.split(\\\\' \\\\'):\\\\n        t = \\\\'@user\\\\' if t.startswith(\\\\'@\\\\') and len(t) > 1 else t\\\\n        t = \\\\'http\\\\' if t.startswith(\\\\'http\\\\') else t\\\\n        new_text.append(t)\\\\n    return \\\\' \\\\'.join(new_text)\\\\n\\\\nmodel_name = \\\\'cardiffnlp/twitter-roberta-base-sentiment\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\\\\n\\\\nreview_text = \\\"The hotel was clean, staff were friendly, and the location was perfect. Definitely coming back!\\\"\\\\npreprocessed_text = preprocess(review_text)\\\\nencoded_input = tokenizer(preprocessed_text, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**encoded_input)\\\\nscores = output[0][0].detach().numpy()\\\\nscores = softmax(scores)\\\\n\\\\nlabels = [\\\\'negative\\\\', \\\\'neutral\\\\', \\\\'positive\\\\']\\\\nranking = np.argsort(scores)[::-1]\\\\n\\\\nresult = [(labels[index], round(score, 4)) for index, score in enumerate(scores[ranking])]', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Transformers', 'functionality': 'Sentiment Analysis', 'api_name': 'cardiffnlp/twitter-roberta-base-sentiment', 'api_call': 'from transformers import AutoModelForSequenceClassification, AutoTokenizer\\\\nimport numpy as np\\\\nfrom scipy.special import softmax\\\\n\\\\ndef preprocess(text):\\\\n    new_text = []\\\\n    for t in text.split(\\\\' \\\\'):\\\\n        t = \\\\'@user\\\\' if t.startswith(\\\\'@\\\\') and len(t) > 1 else t\\\\n        t = \\\\'http\\\\' if t.startswith(\\\\'http\\\\') else t\\\\n        new_text.append(t)\\\\n    return \\\\' \\\\'.join(new_text)\\\\n\\\\nmodel_name = \\\\'cardiffnlp/twitter-roberta-base-sentiment\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\\\\n\\\\nreview_text = \\\"The hotel was clean, staff were friendly, and the location was perfect. Definitely coming back!\\\"\\\\npreprocessed_text = preprocess(review_text)\\\\nencoded_input = tokenizer(preprocessed_text, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**encoded_input)\\\\nscores = output[0][0].detach().numpy()\\\\nscores = softmax(scores)\\\\n\\\\nlabels = [\\\\'negative\\\\', \\\\'neutral\\\\', \\\\'positive\\\\']\\\\nranking = np.argsort(scores)[::-1]\\\\n\\\\nresult = [(labels[index], round(score, 4)) for index, score in enumerate(scores[ranking])]', 'api_arguments': ['MODEL'], 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import AutoModelForSequenceClassification\\\\nfrom transformers import TFAutoModelForSequenceClassification\\\\nfrom transformers import AutoTokenizer\\\\nimport numpy as np\\\\nfrom scipy.special import softmax\\\\nimport csv\\\\nimport urllib.request\\\\n\\\\ndef preprocess(text):\\\\n new_text = []\\\\nfor t in text.split( ):\\\\n t = '@user' if t.startswith('@') and len(t) > 1 else t\\\\n t = 'http' if t.startswith('http') else t\\\\n new_text.append(t)\\\\nreturn  .join(new_text)\\\\n\\\\ntask='sentiment'\\\\nMODEL = fcardiffnlp/twitter-roberta-base-{task}\\\\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\\\\n\\\\nlabels=[]\\\\nmapping_link = fhttps://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\\\\nwith urllib.request.urlopen(mapping_link) as f:\\\\n html = f.read().decode('utf-8').split(\\\\\\\\n)\\\\n csvreader = csv.reader(html, delimiter='\\\\\\\\t')\\\\nlabels = [row[1] for row in csvreader if len(row) > 1]\\\\n\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\\\\nmodel.save_pretrained(MODEL)\\\\ntext = Good night ğŸ˜Š\\\\ntext = preprocess(text)\\\\nencoded_input = tokenizer(text, return_tensors='pt')\\\\noutput = model(**encoded_input)\\\\nscores = output[0][0].detach().numpy()\\\\nscores = softmax(scores)\\\\n\\\\nranking = np.argsort(scores)\\\\nranking = ranking[::-1]\\\\nfor i in range(scores.shape[0]):\\\\n l = labels[ranking[i]]\\\\n s = scores[ranking[i]]\\\\n print(f{i+1}) {l} {np.round(float(s), 4)})\\\", 'performance': {'dataset': 'tweet_eval', 'accuracy': 'Not provided'}, 'description': 'Twitter-roBERTa-base for Sentiment Analysis. This is a roBERTa-base model trained on ~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark. This model is suitable for English.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ø§Ù‡Ø¯ ØªØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ·ÙˆÙŠØ± ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù…Ù„ØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"BertModel.from_pretrained('setu4993/LaBSE')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To perform the sentence similarity comparison, we first import the BertModel and BertTokenizerFast from the transformers library provided by Hugging Face.\\\\n2. We load the pre-trained Language-agnostic BERT Sentence Encoder (LaBSE) with BertModel.from_pretrained('setu4993/LaBSE') and the corresponding tokenizer with BertTokenizerFast.from_pretrained('setu4993/LaBSE').\\\\n3. LaBSE is a BERT-based model trained on embedding sentences in 109 languages, making it suitable for multilingual sentence similarity analysis.\\\\n4. Tokenize your sentences using the LaBSE tokenizer and obtain their embeddings from the LaBSE model.\\\\n5. The embeddings can be used to compute the distance or similarity between the sentences in multiple languages.\\\", '<<<code>>>': 'import torch\\\\nfrom transformers import BertModel, BertTokenizerFast\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\\\'setu4993/LaBSE\\\\')\\\\nmodel = BertModel.from_pretrained(\\\\'setu4993/LaBSE\\\\')\\\\nmodel = model.eval()\\\\nenglish_sentences = [\\\\'dog\\\\', \\\\'Puppies are nice.\\\\', \\\\'I enjoy taking long walks along the beach with my dog.\\\\']\\\\nitalian_sentences = [\\\"cane\\\", \\\"I cuccioli sono carini.\\\", \\\"Mi piace fare lunghe passeggiate sulla spiaggia con il mio cane.\\\"]\\\\njapanese_sentences = [\\\"çŠ¬\\\", \\\"å­çŠ¬ã¯ã„ã„ã§ã™ã€‚\\\", \\\"ç§ã¯çŠ¬ã¨ä¸€ç·’ã«ãƒ“ãƒ¼ãƒã§é•·ã„æ•£æ­©ã‚’æ¥½ã—ã‚€ã€‚\\\"]\\\\nenglish_inputs = tokenizer(english_sentences, return_tensors=\\\\'pt\\\\', padding=True)\\\\nitalian_inputs = tokenizer(italian_sentences, return_tensors=\\\\'pt\\\\', padding=True)\\\\njapanese_inputs = tokenizer(japanese_sentences, return_tensors=\\\\'pt\\\\', padding=True)\\\\nwith torch.no_grad():\\\\n    english_outputs = model(**english_inputs)\\\\n    italian_outputs = model(**italian_inputs)\\\\n    japanese_outputs = model(**japanese_inputs)\\\\nenglish_embeddings = english_outputs.pooler_output\\\\nitalian_embeddings = italian_outputs.pooler_output\\\\njapanese_embeddings = japanese_outputs.pooler_output', 'api_call': \\\"BertModel.from_pretrained('setu4993/LaBSE')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \\\"BertModel.from_pretrained('setu4993/LaBSE')\\\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': [['torch', 'transformers']], 'example_code': \\\"import torch\\\\nfrom transformers import BertModel, BertTokenizerFast\\\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\\\nmodel = model.eval()\\\\nenglish_sentences = [\\\\n 'dog',\\\\n 'Puppies are nice.',\\\\n 'I enjoy taking long walks along the beach with my dog.',\\\\n]\\\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\\\nwith torch.no_grad():\\\\n english_outputs = model(**english_inputs)\\\\nenglish_embeddings = english_outputs.pooler_output\\\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a depth estimation model.\\\\n3. Specify the model 'sayakpaul/glpn-nyu-finetuned-diode' to be loaded, which was trained on the diode-subset dataset for depth estimation.\\\\n4. Provide an image with desired dimensions as an input, and the loaded model will estimate the depth information for each pixel in the image.\\\\n5. This depth information can be further used to adapt the properties of window blinds by estimating the distance of objects from the windows.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimport cv2\\\\ndepth_estimator = pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\\\\n\\\\n# Read the image using OpenCV\\\\nimage = cv2.imread('path/to/image.jpg')\\\\n\\\\n# Get the depth estimation for each pixel in the image\\\\ndepth_map = depth_estimator(image)\\\", 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode', 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4359, 'Rmse': 0.4276}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ¹ÙŠÙ† Ø¹Ù„ÙŠÙƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø³Ù…ÙˆØ¹. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ø¯ÙŠØ¯ Ù„ØºØ© Ø§Ù„Ù†Øµ ÙˆØ§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ø¨Ø±Ø© ÙˆØ§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù†Ø·Ù‚.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the SpeechRecognitionModel from the huggingsound library provided by Hugging Face. \\\\n2. Create an instance of the model by specifying 'jonatasgrosman/wav2vec2-large-xlsr-53-japanese' as the model to be loaded. \\\\n3. This model has been trained on a large number of Japanese speech data, allowing it to recognize spoken words and transcribe audio files to text.\\\\n4. Pass the audio file paths to the model's 'transcribe' method to obtain the text summary of the spoken content.\\\", '<<<code>>>': \\\"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')\\\\n\\\\naudio_paths = ['/path/to/audio1.wav', '/path/to/audio2.mp3']\\\\ntranscriptions = model.transcribe(audio_paths)\\\", 'api_call': \\\"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'jonatasgrosman/wav2vec2-large-xlsr-53-japanese', 'api_call': \\\"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')\\\", 'api_arguments': ['audio_paths'], 'python_environment_requirements': [['huggingsound', 'torch', 'librosa', 'datasets', 'transformers']], 'example_code': 'from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel(jonatasgrosman/wav2vec2-large-xlsr-53-japanese)\\\\naudio_paths = [/path/to/file.mp3, /path/to/another_file.wav]\\\\ntranscriptions = model.transcribe(audio_paths)', 'performance': {'dataset': 'common_voice', 'accuracy': {'WER': 81.8, 'CER': 20.16}}, 'description': 'Fine-tuned XLSR-53 large model for speech recognition in Japanese. Trained on Common Voice 6.1, CSS10, and JSUT datasets. Make sure your speech input is sampled at 16kHz.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØºÙˆÙŠ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ­Ø¯Ø« ÙˆØ§Ù„ØªÙØ§Ø¹Ù„.? [/INST]{'domain': 'Natural Language Processing Conversational', 'api_call': \\\"pipeline('conversational', model='mywateriswet/ShuanBot')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\nchatbot = pipeline(\\\\'conversational\\\\', model=\\\\'mywateriswet/ShuanBot\\\\')\\\\nmessage = \\\"What do you think of artificial intelligence?\\\"\\\\nresponse = chatbot(message)\\\\nprint(response)', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Generation', 'api_name': 'mywateriswet/ShuanBot', 'api_call': 'from transformers import pipeline\\\\nchatbot = pipeline(\\\\'conversational\\\\', model=\\\\'mywateriswet/ShuanBot\\\\')\\\\nmessage = \\\"What do you think of artificial intelligence?\\\"\\\\nresponse = chatbot(message)\\\\nprint(response)', 'api_arguments': 'message', 'python_environment_requirements': 'transformers', 'example_code': \\\"response = chatbot('What is your name?')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© Ø§Ù„ØªÙŠ ØªØ´Ù…Ù„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØµÙˆØ± ÙˆØ§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© Ø¨Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ø¯Ø§Ø¡ Ù…ØªÙ…ÙŠØ² ÙÙŠ ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©.? [/INST]'<<<domain>>>': 'Multimodal Graph Machine Learning', '<<<api_call>>>': \\\"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class from the transformers package, in this case, the AutoModel class.\\\\n2. We then use the from_pretrained method of the AutoModel class to load the pre-trained Graphormer model 'graphormer-base-pcqm4mv1'. This model is specifically designed for graph classification tasks and has been pretrained on the PCQM4M-LSC dataset, which is relevant for molecule modeling.\\\\n3. This Graphormer-based model can be applied to drug discovery projects by providing input data related to molecular graphs. The model can be used for graph classification, representation, or for fine-tuning on specific drug discovery tasks.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\ngraphormer_model = AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\\\\n# Apply the Graphormer model to the drug discovery task using the input data and following the Graph Classification with Transformers tutorial instructions.\\\", 'api_call': \\\"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': \\\"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers']], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers and decord packages. This includes VideoMAEFeatureExtractor for processing input videos, VideoMAEForVideoClassification for the video classification model, and VideoReader for decoding video files.\\\\n2. Load the VideoMAE model from the Hugging Face model hub using the from_pretrained method, specifying 'nateraw/videomae-base-finetuned-ucf101' as the model name. This model is designed for video action recognition and is fine-tuned on the UCF101 dataset.\\\\n3. Define a function `sample_frame_indices` to randomly select a desired number of equally spaced frames from the input video.\\\\n4. Use the VideoReader class from the decord package to load the video file and retrieve the selected frames.\\\\n5. Create a feature extractor to process the video frames and transform them into the appropriate format for the model.\\\\n6. Use the VideoMAEForVideoClassification model to classify the actions in the video.\\\\n7. Obtain the predicted action label and print the corresponding action name.\\\", '<<<code>>>': \\\"from decord import VideoReader, cpu\\\\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\\\nfile_path = 'video_file_path.mp4'\\\\n# replace 'video_file_path.mp4' with path to your video\\\\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\\\\nvideoreader.seek(0)\\\\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\\\\nvideo = videoreader.get_batch(indices).asnumpy()\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\\ninputs = feature_extractor(list(video), return_tensors='pt')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\\noutputs = model(**inputs)\\\\npredicted_label = outputs.logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'videomae-base-finetuned-ucf101', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/videomae-base-finetuned-ucf101'}, 'python_environment_requirements': [['transformers', 'decord', 'huggingface_hub']], 'example_code': 'from decord import VideoReader, cpu\\\\nimport torch\\\\nimport numpy as np\\\\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\\\nfrom huggingface_hub import hf_hub_download\\\\nnp.random.seed(0)\\\\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\\\\n converted_len = int(clip_len * frame_sample_rate)\\\\n end_idx = np.random.randint(converted_len, seg_len)\\\\n start_idx = end_idx - converted_len\\\\n indices = np.linspace(start_idx, end_idx, num=clip_len)\\\\n indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\\\\n return indices\\\\nfile_path = hf_hub_download(\\\\n repo_id=nateraw/dino-clips, filename=archery.mp4, repo_type=space\\\\n)\\\\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\\\\nvideoreader.seek(0)\\\\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\\\\nvideo = videoreader.get_batch(indices).asnumpy()\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\\\\ninputs = feature_extractor(list(video), return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'UCF101', 'accuracy': 0.758209764957428}, 'description': 'VideoMAE Base model fine tuned on UCF101 for Video Action Recognition' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ Ø¥Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ Ø¢Ø®Ø±. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, including the SpeechT5Processor and SpeechT5ForSpeechToSpeech classes as well as the required dataset-processing modules.\\\\n2. Load the speech dataset and select the example speech audio.\\\\n3. Create a processor by loading the pretrained 'microsoft/speecht5_vc' model.\\\\n4. Use the loaded model, processor, and vocoder to convert the original speech audio into the desired voice output.\\\\n5. Save the new audio file in wav format.\\\", '<<<code>>>': \\\"from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\nimport numpy as np\\\\nimport soundfile as sf\\\\n\\\\ndataset = load_dataset('hf-internal-testing/librispeech_asr_demo', 'clean', split='validation')\\\\nsampling_rate = dataset.features['audio'].sampling_rate\\\\nexample_speech = dataset[0]['audio']['array']\\\\n\\\\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_vc')\\\\nmodel = SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\\\\n\\\\ninputs = processor(audio=example_speech, sampling_rate=sampling_rate, return_tensors='pt')\\\\nspeaker_embeddings = np.load('xvector_speaker_embedding.npy')\\\\nspeaker_embeddings = torch.tensor(speaker_embeddings).unsqueeze(0)\\\\n\\\\nspeech = model.generate_speech(inputs['input_values'], speaker_embeddings, vocoder=vocoder)\\\\n\\\\nsf.write('speech.wav', speech.numpy(), samplerate=16000)\\\", 'api_call': \\\"SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/speecht5_vc', 'api_call': \\\"SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\", 'api_arguments': {'audio': 'example_speech', 'sampling_rate': 'sampling_rate', 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'datasets', 'numpy', 'torch', 'soundfile']], 'example_code': \\\"from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset('hf-internal-testing/librispeech_asr_demo', 'clean', split='validation')\\\\ndataset = dataset.sort('id')\\\\nsampling_rate = dataset.features['audio'].sampling_rate\\\\nexample_speech = dataset[0]['audio']['array']\\\\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_vc')\\\\nmodel = SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\\\\ninputs = processor(audio=example_speech, sampling_rate=sampling_rate, return_tensors='pt')\\\\nimport numpy as np\\\\nimport torch\\\\nspeaker_embeddings = np.load('xvector_speaker_embedding.npy')\\\\nspeaker_embeddings = torch.tensor(speaker_embeddings).unsqueeze(0)\\\\nspeech = model.generate_speech(inputs['input_values'], speaker_embeddings, vocoder=vocoder)\\\\nimport soundfile as sf\\\\nsf.write('speech.wav', speech.numpy(), samplerate=16000)\\\", 'performance': {'dataset': 'CMU ARCTIC', 'accuracy': 'Not specified'}, 'description': 'SpeechT5 model fine-tuned for voice conversion (speech-to-speech) on CMU ARCTIC. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. It is designed to improve the modeling capability for both speech and text. This model can be used for speech conversion tasks.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¨Ø±Ù…Ø¬ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]{'domain': 'Natural Language Processing Token Classification', 'api_call': \\\"SequenceTagger.load('flair/ner-english-ontonotes-fast')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load('flair/ner-english-ontonotes-fast')\\\\nsentence = Sentence('On September 1st George Washington won 1 dollar.')\\\\ntagger.predict(sentence)\\\\nentities = sentence.get_spans('ner')\\\\nfor entity in entities:\\\\n    print(entity)\\\", 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Named Entity Recognition', 'api_name': 'flair/ner-english-ontonotes-fast', 'api_call': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load('flair/ner-english-ontonotes-fast')\\\\nsentence = Sentence('On September 1st George Washington won 1 dollar.')\\\\ntagger.predict(sentence)\\\\nentities = sentence.get_spans('ner')\\\\nfor entity in entities:\\\\n    print(entity)\\\", 'api_arguments': ['sentence'], 'python_environment_requirements': ['flair'], 'example_code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load(flair/ner-english-ontonotes-fast)\\\\nsentence = Sentence(On September 1st George Washington won 1 dollar.)\\\\ntagger.predict(sentence)\\\\nprint(sentence)\\\\nfor entity in sentence.get_spans('ner'):\\\\n  print(entity)\\\", 'performance': {'dataset': 'Ontonotes', 'accuracy': 'F1-Score: 89.3'}, 'description': 'This is the fast version of the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on Flair embeddings and LSTM-CRF.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ÙˆÙƒØ´Ù Ø§Ù„Ø£Ø¬Ø³Ø§Ù…. Ù†Ø­ØªØ§Ø¬ Ù„Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ ØªÙ…ÙŠÙŠØ² Ø§Ù„Ø£Ø´ÙƒØ§Ù„ ÙˆØ§Ù„Ø£Ù†Ù…Ø§Ø· ÙÙŠ Ø§Ù„ØµÙˆØ± Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø¬Ø³Ø§Ù… ÙˆØªØ¹Ø±ÙŠÙÙ‡Ø§.? [/INST]{'domain': 'Computer Vision Object Detection', 'api_call': \\\"yolov5.load('keremberke/yolov5s-license-plate')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import and install the required packages such as yolov5 from Hugging Face.', 'code': \\\"import yolov5\\\\nmodel = yolov5.load('keremberke/yolov5s-license-plate')\\\\nmodel.conf = 0.25\\\\nmodel.iou = 0.45\\\\nmodel.agnostic = False\\\\nmodel.multi_label = False\\\\nmodel.max_det = 1000\\\\nimg = 'https://example.com/car_image.jpg'\\\\nresults = model(img, size=640, augment=True)\\\\npredictions = results.pred[0]\\\\nboxes = predictions[:, :4]\\\\nscores = predictions[:, 4]\\\\ncategories = predictions[:, 5]\\\\nresults.show()\\\\nrecognized_plates = len(predictions)\\\\ntotal_plates = len(car_images)\\\\npercentage_recognized_plates = (recognized_plates / total_plates) * 100\\\", 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5s-license-plate', 'api_call': \\\"import yolov5\\\\nmodel = yolov5.load('keremberke/yolov5s-license-plate')\\\\nmodel.conf = 0.25\\\\nmodel.iou = 0.45\\\\nmodel.agnostic = False\\\\nmodel.multi_label = False\\\\nmodel.max_det = 1000\\\\nimg = 'https://example.com/car_image.jpg'\\\\nresults = model(img, size=640, augment=True)\\\\npredictions = results.pred[0]\\\\nboxes = predictions[:, :4]\\\\nscores = predictions[:, 4]\\\\ncategories = predictions[:, 5]\\\\nresults.show()\\\\nrecognized_plates = len(predictions)\\\\ntotal_plates = len(car_images)\\\\npercentage_recognized_plates = (recognized_plates / total_plates) * 100\\\", 'api_arguments': {'img': 'image url or path', 'size': 'image resize dimensions', 'augment': 'optional, test time augmentation'}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \\\"model = yolov5.load('keremberke/yolov5s-license-plate')\\\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \\\"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \\\"results.save(save_dir='results/')\\\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.985}, 'description': 'A YOLOv5 based license plate detection model trained on a custom dataset.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ Ù…Ù‚Ø±ÙˆØ¡ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"pipeline('summarization', model='it5/it5-base-news-summarization')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a summarization model.\\\\n3. Specify the model 'it5/it5-base-news-summarization' to be loaded. This model is trained to automatically summarize news articles in Italian.\\\\n4. Use the created summarizer to generate a brief summary of the given news article content.\\\", '<<<code>>>': 'from transformers import pipeline\\\\narticle_summarizer = pipeline(\\\\'summarization\\\\', model=\\\\'it5/it5-base-news-summarization\\\\')\\\\nnews_article = \\\"Dal 31 maggio Ã¨ infine partita la piattaforma ITsART...\\\"\\\\nsummary = article_summarizer(news_article)', 'api_call': \\\"pipeline('summarization', model='it5/it5-base-news-summarization')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'it5-base-news-summarization', 'api_call': \\\"pipeline('summarization', model='it5/it5-base-news-summarization')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': 'newsum(Dal 31 maggio Ã¨ infine partita la piattaforma ITsART, a piÃ¹ di un anno da quando â€“ durante il primo lockdown â€“ il ministro della Cultura Dario Franceschini ne aveva parlato come di Â«una sorta di Netflix della culturaÂ», pensata per Â«offrire a tutto il mondo la cultura italiana a pagamentoÂ». Ãˆ presto per dare giudizi definitivi sulla piattaforma, e di certo sarÃ  difficile farlo anche piÃ¹ avanti senza numeri precisi. Al momento, lâ€™unica cosa che si puÃ² fare Ã¨ guardare comâ€™Ã¨ fatto il sito, contare quanti contenuti ci sono (circa 700 â€œtitoliâ€, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro varietÃ . Intanto, una cosa notata da piÃ¹ parti Ã¨ che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.)', 'performance': {'dataset': 'NewsSum-IT', 'accuracy': {'Rouge1': 0.339, 'Rouge2': 0.16, 'RougeL': 0.263}}, 'description': 'IT5 Base model fine-tuned on news summarization on the Fanpage and Il Post corpora for Italian Language Understanding and Generation.' </s>\"\n"}
{"text": "<s>[INST]Ø¨Ù†Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ±Ø¬Ù…Ø©ØŒ ÙŠØªØ·Ù„Ø¨ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ ÙˆØªÙˆÙ„ÙŠØ¯ ØªØ±Ø¬Ù…Ø§Øª ÙØ¹Ø§Ù„Ø© ÙˆØ¯Ù‚ÙŠÙ‚Ø©.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"ORTModelForSeq2SeqLM.from_pretrained('optimum/t5-small')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the required libraries: the pipeline and AutoTokenizer from the transformers package, and ORTModelForSeq2SeqLM from the optimum.onnxruntime package.', 'code': 'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'optimum/t5-small\\\\')\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(\\\\'optimum/t5-small\\\\')\\\\ntranslator = pipeline(\\\\'translation_en_to_fr\\\\', model=model, tokenizer=tokenizer)\\\\nresults = translator(\\\"My name is Eustache and I have a pet raccoon.\\\")\\\\nprint(results)', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Transformers', 'functionality': 'Text-to-Text Generation', 'api_name': 'optimum/t5-small', 'api_call': 'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'optimum/t5-small\\\\')\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(\\\\'optimum/t5-small\\\\')\\\\ntranslator = pipeline(\\\\'translation_en_to_fr\\\\', model=model, tokenizer=tokenizer)\\\\nresults = translator(\\\"My name is Eustache and I have a pet raccoon.\\\")\\\\nprint(results)', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers', 'optimum.onnxruntime'], 'example_code': 'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\\\nresults = translator(My name is Eustache and I have a pet raccoon)\\\\nprint(results)', 'performance': {'dataset': 'c4', 'accuracy': 'N/A'}, 'description': 'T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ø¥Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ ØªÙ†Ø¨Ø¤ÙŠ ÙŠØ³ØªØ·ÙŠØ¹ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø­ØµØ§Ø¦ÙŠ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. To use the model for predicting carbon emissions, first import necessary libraries: joblib, json, and pandas.\\\\n2. Load the trained model from the file 'model.joblib' using joblib.load() function.\\\\n3. Load the configuration file, which contains the feature names for the dataset, using json.load() function.\\\\n4. Read the input data using pandas' read_csv() function, and filter the data based on the features listed in the configuration file.\\\\n5. Rename the columns of filtered data with the prefix 'feat_'.\\\\n6. Use the model's predict() function to obtain predictions for carbon emissions based on the input data.\\\", '<<<code>>>': \\\"import joblib\\\\nimport json\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kochetkovIT/autotrain-ironhack-49741119788', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['json', 'joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'kochetkovIT/autotrain-data-ironhack', 'accuracy': {'Loss': 2.603, 'R2': 0.013, 'MSE': 6.776, 'MAE': 1.666, 'RMSLE': 0.502}}, 'description': 'A tabular regression model trained using AutoTrain to predict carbon emissions (in grams) with an R2 score of 0.013.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ­ØªØ§Ø¬ Ø¹Ù…ÙŠÙ„Ù†Ø§ Ø§Ù„Ù…ØªØ®ØµØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù‚Ø§Ø¨Ù„ Ù„Ù„ÙÙ‡Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ ØªØ·Ø¨ÙŠÙ‚Ù‡.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model='deepset/roberta-large-squad2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the necessary class 'pipeline' from the transformers package. This will allow us to load the model and set it up to answer questions.\\\\n2. Use the 'pipeline' function to load the pre-trained RoBERTa question answering model 'deepset/roberta-large-squad2'. This model is specifically trained to answer questions based on a given context and is suitable for answering readers' questions based on your medical articles.\\\\n3. With the loaded model, you can pass the reader's question and the article's text to the model to produce an answer for the reader.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=\\\\'deepset/roberta-large-squad2\\\\')\\\\nquestion = \\\"What is the best treatment for diabetes?\\\"\\\\narticle_context = \\\"The best treatment for diabetes includes a combination of healthy eating, regular physical activity, and medications as needed.\\\"\\\\nanswer = nlp({\\\"question\\\": question, \\\"context\\\": article_context})', 'api_call': \\\"pipeline('question-answering', model='deepset/roberta-large-squad2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'deepset/roberta-large-squad2', 'api_call': \\\"pipeline('question-answering', model='deepset/roberta-large-squad2')\\\", 'api_arguments': ['question', 'context'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; nlp = pipeline('question-answering', model='deepset/roberta-large-squad2'); nlp({'question': 'What is the capital of Germany?', 'context': 'Berlin is the capital of Germany.'})\\\", 'performance': {'dataset': 'squad_v2', 'accuracy': 'Not provided'}, 'description': 'A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ø³Ø¤Ø§Ù„Ù‹Ø§ Ù‚ØµÙŠØ±Ù‹Ø§ Ø­ÙˆÙ„ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù…Ù„ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Install the required Python package using 'pip install -U sentence-transformers'.\\\", 'code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = ['This is an example sentence.', 'Each sentence is converted.']\\\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Sentence Transformers', 'api_name': 'sentence-transformers/all-MiniLM-L6-v2', 'api_call': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = ['This is an example sentence.', 'Each sentence is converted.']\\\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': '1B sentence pairs dataset', 'accuracy': 'https://seb.sbert.net'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£ÙˆØ¬Ø¯ Ù„ÙŠ ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. We first import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. We then create a text generation pipeline by specifying the model \\\\'sshleifer/tiny-gpt2\\\\'. This is a tiny GPT-2 model that is suitable for generating text efficiently.\\\\n3. With this pipeline, we can pass in the prompt \\\"It was a dark and stormy night.\\\" as input and generate a continuation to the given text.', '<<<code>>>': \\\"from transformers import pipeline\\\\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\\\\nresult = nlp('It was a dark and stormy night.')\\\", 'api_call': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sshleifer/tiny-gpt2', 'api_call': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", 'api_arguments': {'model': 'sshleifer/tiny-gpt2'}, 'python_environment_requirements': [{'huggingface_transformers': '>=4.0.0'}], 'example_code': \\\"from transformers import pipeline\\\\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\\\\nresult = nlp('Once upon a time')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A tiny GPT-2 model for text generation, suitable for low-resource environments and faster inference. This model is part of the Hugging Face Transformers library and can be used for generating text given a prompt.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù„ØºÙˆÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ·ÙŠØ¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ø§Ù„Ù†ØµØŒ Ù…Ø«Ù„ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø®Ø§ØµØ© ÙˆØ§Ù„ØªÙˆØ§Ø±ÙŠØ® ÙˆØ§Ù„Ù…ÙƒØ§Ù†ØŒ ÙˆØ°Ù„Ùƒ Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚Ø§ØªÙ†Ø§.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers library, which includes BertTokenizerFast for tokenizing the text and AutoModel to build the Part-of-speech (POS) tagging model.\\\\n2. Initialize the tokenizer with the pre-trained 'bert-base-chinese' tokenizer using the from_pretrained method of the BertTokenizerFast class.\\\\n3. Initialize the POS tagging model with the pre-trained 'ckiplab/bert-base-chinese-pos' using the from_pretrained method of the AutoModel class.\\\\n4. Tokenize the Chinese email text using the tokenizer.\\\\n5. Input the tokenized text to the model to get the POS tags for each word in the text.\\\", '<<<code>>>': \\\"from transformers import BertTokenizerFast, AutoModel\\\\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\\\\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\\ninputs = tokenizer(email_text, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\npredicted_pos_tags = outputs.logits.argmax(-1).tolist()\\\", 'api_call': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ckiplab/bert-base-chinese-pos', 'api_call': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'api_arguments': {'tokenizer': \\\"BertTokenizerFast.from_pretrained('bert-base-chinese')\\\"}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import (\\\\n  BertTokenizerFast,\\\\n  AutoModel,\\\\n)\\\\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\\\\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù†Øµ. ÙŠØ±Ø¬Ù‰ Ø¶Ù…Ø§Ù† Ø¯Ù‚Ø© Ø§Ù„ØªØµÙ†ÙŠÙ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø±Ù…ÙˆØ² Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"XLMRobertaForSequenceClassification.from_pretrained('vicgalle/xlm-roberta-large-xnli-anli')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import XLMRobertaForSequenceClassification\\\\nmodel = XLMRobertaForSequenceClassification.from_pretrained('vicgalle/xlm-roberta-large-xnli-anli')\\\\ndocument_classifier = pipeline('zero-shot-classification', model=model)\\\\ncategories = ['travel', 'cooking', 'dancing']\\\\nclassification_results = document_classifier(document_text, candidate_labels=categories)\\\", 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Classification', 'api_name': 'vicgalle/xlm-roberta-large-xnli-anli', 'api_call': \\\"from transformers import XLMRobertaForSequenceClassification\\\\nmodel = XLMRobertaForSequenceClassification.from_pretrained('vicgalle/xlm-roberta-large-xnli-anli')\\\\ndocument_classifier = pipeline('zero-shot-classification', model=model)\\\\ncategories = ['travel', 'cooking', 'dancing']\\\\nclassification_results = document_classifier(document_text, candidate_labels=categories)\\\", 'api_arguments': ['sequence_to_classify', 'candidate_labels'], 'python_environment_requirements': ['transformers'], 'example_code': \\\"sequence_to_classify = AlgÃºn dÃ­a irÃ© a ver el mundo\\\\ncandidate_labels = ['viaje', 'cocina', 'danza']\\\\nclassifier(sequence_to_classify, candidate_labels)\\\", 'performance': {'dataset': [{'name': 'XNLI-es', 'accuracy': '93.7%'}, {'name': 'XNLI-fr', 'accuracy': '93.2%'}, {'name': 'ANLI-R1', 'accuracy': '68.5%'}, {'name': 'ANLI-R2', 'accuracy': '53.6%'}, {'name': 'ANLI-R3', 'accuracy': '49.0%'}]}, 'description': 'XLM-RoBERTa-large model finetunned over several NLI datasets, ready to use for zero-shot classification.'}} </s>\"\n"}
{"text": "<s>[INST]Ù„Ø·Ø§Ø² Ù†Ø¯Ø¹ÙˆÙƒ Ù„Ø­Ø¶ÙˆØ± ÙˆØ±Ø´Ø© Ø¹Ù…Ù„ Ø­ÙˆÙ„ Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø¬Ù…Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ø³ØªØªØ¶Ù…Ù† Ø§Ù„ÙˆØ±Ø´Ø© ØªØ¯Ø±ÙŠØ¨Ø§Øª Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„ØªÙ†Ø§Ø³Ù‚ ÙˆØ§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©.? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"INSTRUCTOR('hkunlp/instructor-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from InstructorEmbedding import INSTRUCTOR\\\\nmodel = INSTRUCTOR(\\\\'hkunlp/instructor-base\\\\')\\\\ninstruction = \\\"Find books with a similar description.\\\"\\\\n\\\\nbook_descriptions = [\\\"Book 1 description\\\", \\\"Book 2 description\\\", \\\"Book 3 description\\\"]\\\\nembeddings = [model.encode([[instruction, description]]) for description in book_descriptions]\\\\n\\\\n# Compute similarities between each pair of embeddings and find the most similar pairs.', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Sentence Transformers', 'api_name': 'hkunlp/instructor-base', 'api_call': 'from InstructorEmbedding import INSTRUCTOR\\\\nmodel = INSTRUCTOR(\\\\'hkunlp/instructor-base\\\\')\\\\ninstruction = \\\"Find books with a similar description.\\\"\\\\n\\\\nbook_descriptions = [\\\"Book 1 description\\\", \\\"Book 2 description\\\", \\\"Book 3 description\\\"]\\\\nembeddings = [model.encode([[instruction, description]]) for description in book_descriptions]\\\\n\\\\n# Compute similarities between each pair of embeddings and find the most similar pairs.', 'api_arguments': {'instruction': 'string', 'sentence': 'string'}, 'python_environment_requirements': 'pip install InstructorEmbedding', 'example_code': \\\"from InstructorEmbedding import INSTRUCTOR\\\\nmodel = INSTRUCTOR('hkunlp/instructor-base')\\\\nsentence = 3D ActionSLAM: wearable person tracking in multi-floor environments\\\\ninstruction = Represent the Science title:\\\\nembeddings = model.encode([[instruction,sentence]])\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'MTEB AmazonCounterfactualClassification (en)', 'accuracy': 86.209}, 'description': 'Instructor is an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor achieves state-of-the-art performance on 70 diverse embedding tasks.'}} </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ±ØºØ¨ ÙÙŠ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„ØªØ­ÙˆÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„Ø§Øª ØµÙˆØªÙŠØ© Ø£Ø®Ø±Ù‰. ÙŠØ¬Ø¨ ØªØµÙ…ÙŠÙ… Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­ÙˆÙŠÙ„ ØµÙŠØº Ø§Ù„ØµÙˆØª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø¨Ø¯Ù‚Ø© ÙˆØ³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ© Ù„ØªÙ„Ø¨ÙŠØ© Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"hf_hub_download(repo_id='JorisCos/ConvTasNet_Libri2Mix_sepclean_8k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is 'hf_hub_download' from huggingface_hub.\\\\n2. Download the ConvTasNet_Libri2Mix_sepclean_8k model from the Hugging Face Hub using the hf_hub_download function with the repo_id 'JorisCos/ConvTasNet_Libri2Mix_sepclean_8k'. This model has been trained on the Libri2Mix dataset to separate speakers in an audio recording.\\\\n3. Load the downloaded model using the appropriate library (e.g., Asteroid) to perform source separation on the noisy audio input.\\\\n4. Pass the noisy audio input to the model for processing. The output will be a version of the input audio with the speaker's voices separated.\\\", '<<<code>>>': \\\"from huggingface_hub import hf_hub_download\\\\nrepo_id = 'JorisCos/ConvTasNet_Libri2Mix_sepclean_8k'\\\\nmodel_file = hf_hub_download(repo_id=repo_id)\\\\n# Load the model using the appropriate library (e.g., Asteroid) and perform source separation on the input audio.\\\", 'api_call': \\\"hf_hub_download(repo_id='JorisCos/ConvTasNet_Libri2Mix_sepclean_8k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ConvTasNet_Libri2Mix_sepclean_8k', 'api_call': \\\"hf_hub_download(repo_id='JorisCos/ConvTasNet_Libri2Mix_sepclean_8k')\\\", 'api_arguments': ['repo_id', 'filename'], 'python_environment_requirements': [['huggingface_hub']], 'example_code': '', 'performance': {'dataset': 'Libri2Mix', 'accuracy': {'si_sdr': 14.764543634468069, 'si_sdr_imp': 14.764029375607246, 'sdr': 15.29337970745095, 'sdr_imp': 15.114146605113111, 'sir': 24.092904661115366, 'sir_imp': 23.913669683141528, 'sar': 16.06055906916849, 'sar_imp': -51.980784441287454, 'stoi': 0.9311142440593033, 'stoi_imp': 0.21817376142710482}}, 'description': 'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"pipeline('table-question-answering', model='Meena/table-question-answering-tapas')\\\", '<<<api_provider>>>': 'PyTorch Transformers', '<<<explanation>>>': '1. Import the pipeline function from the transformers library provided by PyTorch Transformers\\\\n2. The pipeline function is used to create a table question answering (TQA) model, which is capable of understanding and answering questions about structured tabular data.\\\\n3. We specify the model \\\\'Meena/table-question-answering-tapas\\\\' to be loaded. TAPAS is a BERT-based model that has been pre-trained on millions of tables from English Wikipedia and can answer questions about tabular data.\\\\n4. The created TQA model can be used to process the tabular information and answer questions about the data contained in the table, such as \\\"What is the price of the DVD with the highest resolution?\\\"', '<<<code>>>': 'from transformers import pipeline\\\\ntqa = pipeline(\\\\'table-question-answering\\\\', model=\\\\'Meena/table-question-answering-tapas\\\\')\\\\nquestion = \\\"What is the price of the DVD with the highest resolution?\\\"\\\\n# Replace \\\\'table_data\\\\' with the actual table containing DVD features and prices\\\\nresult = tqa(question=question, table=table_data)', 'api_call': \\\"pipeline('table-question-answering', model='Meena/table-question-answering-tapas')\\\", 'provider': 'PyTorch Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'table-question-answering-tapas', 'api_call': \\\"pipeline('table-question-answering', model='Meena/table-question-answering-tapas')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': [{'name': 'SQA (Sequential Question Answering by Microsoft)', 'accuracy': None}, {'name': 'WTQ (Wiki Table Questions by Stanford University)', 'accuracy': None}, {'name': 'WikiSQL (by Salesforce)', 'accuracy': None}]}, 'description': 'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆØ¯ÙŠÙ„ ÙŠØ³ØªØ·ÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù…Ø«Ù„ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ. Ø¶Ø¹ Ø®Ø·Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ© ÙˆØ¶Ø¨Ø·Ù‡ Ù„Ø£Ø¯Ø§Ø¡ Ù…Ø«Ø§Ù„ÙŠ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"AutoModel.from_pretrained('microsoft/git-base-textvqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and functions, specifically AutoModel and the tokenizer from transformers.\\\\n2. Load the 'microsoft/git-base-textvqa' model using the AutoModel.from_pretrained() method. This model is designed to generate text conditioned on both image and text tokens.\\\\n3. Write a function that takes an image file path and the text question as input. This function should tokenize the text and preprocess the image.\\\\n4. Utilize the model to generate text tokens conditioned on the image tokens and text tokens of the input image and question. Then, convert the generated text tokens back to text.\\\\n5. The resulting text will provide a description of the actions happening within the image and answer any questions regarding the contents of the image.\\\", '<<<code>>>': \\\"from transformers import AutoModel, AutoTokenizer\\\\ngit_model = AutoModel.from_pretrained('microsoft/git-base-textvqa')\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/git-base-textvqa')\\\\n\\\\ndef generate_description_and_answer(image_path, question):\\\\n    with open(image_path, 'rb') as image_file:\\\\n        encoded_image = tokenizer(image_file.read(), return_tensors='pt')\\\\n\\\\n    input_text = tokenizer(question, return_tensors='pt')\\\\n    input_tokens = {key: val + input_text[key] for key, val in encoded_image.items()}\\\\n    generated = git_model.generate(**input_tokens)\\\\n    description_and_answer = tokenizer.decode(generated[0])\\\\n    return description_and_answer\\\\n\\\\nresult = generate_description_and_answer('path/to/image.jpg', 'What is happening in the image?')\\\", 'api_call': \\\"AutoModel.from_pretrained('microsoft/git-base-textvqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/git-base-textvqa', 'api_call': \\\"AutoModel.from_pretrained('microsoft/git-base-textvqa')\\\", 'api_arguments': 'image, question', 'python_environment_requirements': ['transformers'], 'example_code': \\\"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\\\", 'performance': {'dataset': 'TextVQA', 'accuracy': 'Refer to the paper'}, 'description': \\\"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\\\" </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ¹ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‚Ø·Ø¹ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ÙŠØµÙ Ù…Ù‡Ù…Ø© Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø¯ÙˆÙ† Ø¯Ø¹Ù… Ø³Ø§Ø¨Ù‚.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"AutoModel.from_pretrained('deepset/bert-medium-squad2-distilled')\\\", 'api_provider': 'PyTorch Transformers', 'explanation': '1. We start by importing the necessary modules from the transformers libraryâ€”AutoModel and AutoTokenizerâ€”for working with the pre-trained model.', 'code': \\\"from transformers import AutoModel, AutoTokenizer\\\\nmodel = AutoModel.from_pretrained('deepset/bert-medium-squad2-distilled')\\\\ntokenizer = AutoTokenizer.from_pretrained('deepset/bert-medium-squad2-distilled')\\\\ninputs = tokenizer.encode_plus(question, context, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nanswer_start = outputs[0].argmax().item()\\\\nanswer_end = outputs[1].argmax().item()\\\\nanswer = tokenizer.decode(inputs['input_ids'][0][answer_start:answer_end + 1])\\\", 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'PyTorch Transformers', 'functionality': 'Question Answering', 'api_name': 'deepset/bert-medium-squad2-distilled', 'api_call': \\\"from transformers import AutoModel, AutoTokenizer\\\\nmodel = AutoModel.from_pretrained('deepset/bert-medium-squad2-distilled')\\\\ntokenizer = AutoTokenizer.from_pretrained('deepset/bert-medium-squad2-distilled')\\\\ninputs = tokenizer.encode_plus(question, context, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nanswer_start = outputs[0].argmax().item()\\\\nanswer_end = outputs[1].argmax().item()\\\\nanswer = tokenizer.decode(inputs['input_ids'][0][answer_start:answer_end + 1])\\\", 'api_arguments': {'model': 'deepset/bert-medium-squad2-distilled'}, 'python_environment_requirements': {'transformers': '>=4.0.0'}, 'example_code': \\\"qa({'context': 'This is an example context.', 'question': 'What is this example about?'})\\\", 'performance': {'dataset': 'squad_v2', 'exact': 68.6431398972458, 'f1': 72.7637083790805}, 'description': 'This model is a distilled version of deepset/bert-large-uncased-whole-word-masking-squad2, trained on the SQuAD 2.0 dataset for question answering tasks. It is based on the BERT-medium architecture and uses the Hugging Face Transformers library.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…ØªØ®ØµØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù…Ù† ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø£Ø®Ø±Ù‰. Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ ØµÙˆØ± Ø£Ø®Ø±Ù‰ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠØŸ? [/INST]{'domain': 'Computer Vision Image-to-Image', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': \\\"from diffusers import ControlNetModel, StableDiffusionControlNetPipeline, UniPCMultistepScheduler\\\\nfrom PIL import Image\\\\nimport torch\\\\nfrom diffusers.utils import load_image\\\\n\\\\nimage = load_image('input_image.jpg').convert('RGB')\\\\n\\\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained('fusing/stable-diffusion-v1-5-controlnet-normal', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\n\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\n\\\\nnormal_map_image = pipe('image_description', image, num_inference_steps=20).images[0]\\\\nnormal_map_image.save('normal_map_output.png')\\\", 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Normal Map Estimation', 'api_name': 'lllyasviel/sd-controlnet-normal', 'api_call': \\\"from diffusers import ControlNetModel, StableDiffusionControlNetPipeline, UniPCMultistepScheduler\\\\nfrom PIL import Image\\\\nimport torch\\\\nfrom diffusers.utils import load_image\\\\n\\\\nimage = load_image('input_image.jpg').convert('RGB')\\\\n\\\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained('fusing/stable-diffusion-v1-5-controlnet-normal', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\n\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\n\\\\nnormal_map_image = pipe('image_description', image, num_inference_steps=20).images[0]\\\\nnormal_map_image.save('normal_map_output.png')\\\", 'api_arguments': ['image', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \\\"from PIL import Image\\\\nfrom transformers import pipeline\\\\nimport numpy as np\\\\nimport cv2\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom diffusers.utils import load_image\\\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png).convert(RGB)\\\\ndepth_estimator = pipeline(depth-estimation, model =Intel/dpt-hybrid-midas )\\\\nimage = depth_estimator(image)['predicted_depth'][0]\\\\nimage = image.numpy()\\\\nimage_depth = image.copy()\\\\nimage_depth -= np.min(image_depth)\\\\nimage_depth /= np.max(image_depth)\\\\nbg_threhold = 0.4\\\\nx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\\\\nx[image_depth &lt; bg_threhold] = 0\\\\ny = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\\\\ny[image_depth &lt; bg_threhold] = 0\\\\nz = np.ones_like(x) * np.pi * 2.0\\\\nimage = np.stack([x, y, z], axis=2)\\\\nimage /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\\\\nimage = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\\\\nimage = Image.fromarray(image)\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\n fusing/stable-diffusion-v1-5-controlnet-normal, torch_dtype=torch.float16\\\\n)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\nimage = pipe(cute toy, image, num_inference_steps=20).images[0]\\\\nimage.save('images/toy_normal_out.png')\\\", 'performance': {'dataset': 'DIODE', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø§Ù„Ø´Ø±ÙƒØ© Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠØ³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ.? [/INST]{'domain': 'Computer Vision Depth Estimation', 'api_call': 'AutoModel.from_pretrained(\\\"sayakpaul/glpn-nyu-finetuned-diode-221215-095508\\\")', 'api_provider': 'Hugging Face Transformers', 'explanation': '1. First, import the AutoModel class from the transformers library, which is provided by Hugging Face.', 'code': 'from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained(\\\"sayakpaul/glpn-nyu-finetuned-diode-221215-095508\\\")', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-095508', 'api_call': 'from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained(\\\"sayakpaul/glpn-nyu-finetuned-diode-221215-095508\\\")', 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': 'DIODE', 'accuracy': None}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. The pipeline function is used to create a text-to-image model capable of generating images based on text descriptions, specifically the 'gsdf/Counterfeit-V2.5' model.\\\\n3. This model is designed for generating anime-style images, which is suitable for creating custom trading card game characters, such as a mystical creature.\\\\n4. We can provide a text description of the mystical creature, and the model will generate an image based on it. The generated image can then be used as an illustration for the trading card game.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nimage_generator = pipeline(\\\\'text-to-image\\\\', model=\\\\'gsdf/Counterfeit-V2.5\\\\')\\\\nmystical_creature_description = \\\"A mystical creature with dragon wings, a unicorn horn, and a lion\\\\'s body, surrounded by enchanted forest\\\"\\\\nresult = image_generator(mystical_creature_description)', 'api_call': \\\"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \\\"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… ÙØ±ÙŠÙ‚ Ø§Ù„Ø¨Ø­Ø« Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù‚ÙŠØ§Ø³ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„ Ø¨ÙŠÙ† Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„ÙƒÙØ§Ø¡Ø© ÙÙŠ ØªÙ‚Ø¯ÙŠØ± ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„.? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the SentenceTransformer from the sentence_transformers library.', 'code': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/paraphrase-MiniLM-L3-v2\\\\')\\\\nquestion = \\\"How do I reset my password?\\\"\\\\narticles = [\\\"How to change your password\\\", \\\"Updating your email address\\\", \\\"Resetting your account password\\\"]\\\\nembeddings = model.encode([question] + articles)\\\\nquestion_embedding, *article_embeddings = embeddings\\\\nsimilarity_scores = cosine_similarity([question_embedding], article_embeddings)\\\\nrecommended_articles = sorted(articles, key=lambda x: -similarity_scores[0][articles.index(x)])\\\\nprint(\\\"Most relevant articles are: \\\", recommended_articles[:3])', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Sentence Embeddings', 'api_name': 'sentence-transformers/paraphrase-MiniLM-L3-v2', 'api_call': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/paraphrase-MiniLM-L3-v2\\\\')\\\\nquestion = \\\"How do I reset my password?\\\"\\\\narticles = [\\\"How to change your password\\\", \\\"Updating your email address\\\", \\\"Resetting your account password\\\"]\\\\nembeddings = model.encode([question] + articles)\\\\nquestion_embedding, *article_embeddings = embeddings\\\\nsimilarity_scores = cosine_similarity([question_embedding], article_embeddings)\\\\nrecommended_articles = sorted(articles, key=lambda x: -similarity_scores[0][articles.index(x)])\\\\nprint(\\\"Most relevant articles are: \\\", recommended_articles[:3])', 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'snli, multi_nli, ms_marco', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¬Ù…Ù„ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØµÙÙŠØªÙ‡Ø§ Ø«Ù… ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ù…Ù‚Ø§Ø±Ù†Ø© Ø¬Ù…Ù„. Ù‚Ù… Ø¨Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¶Ù…Ø§Ù† Ø¯Ù‚ØªÙ‡. Ø§Ø­ØªÙØ¸ Ø¨ØªØ­Ø¯ÙŠØ«Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ‚Ù… Ø¨Ù…Ø´Ø§Ø±ÙƒØªÙ‡Ø§ Ù…Ø¹ ÙØ±ÙŠÙ‚Ùƒ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ø§Ø­Ø¸Ø§Øª ÙˆØªØ¹Ø¯ÙŠÙ„Ø§Øª.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the SentenceTransformer class from the sentence_transformers module.\\\\n2. We load the pre-trained model 'sentence-transformers/nli-mpnet-base-v2', which has been trained to produce meaningful embeddings for natural language texts.\\\\n3. We encode the input sentences using the model.encode() method to compute embeddings for each sentence.\\\\n4. We calculate the pairwise cosine similarity between the embeddings of the input sentences to measure their similarity.\\\\n5. We display the pairs with highest similarity, representing the most similar variations to the original sentence.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nimport numpy as np\\\\n\\\\nsentences = [\\\"The perfect sale for the holiday season\\\", \\\"Amazing holiday sale awaits you\\\", \\\"Holiday season special: incredible sales\\\", \\\"Huge discounts are close, don\\\\'t miss them\\\", \\\"Celebrate the holidays with our exclusive sale\\\"]\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/nli-mpnet-base-v2\\\\')\\\\nembeddings = model.encode(sentences)\\\\nsimilarity_matrix = cosine_similarity(embeddings)\\\\nsimilar_pairs = np.argwhere(similarity_matrix > 0.8)\\\\nresult = [tuple(sentences[i] for i in pair) for pair in similar_pairs if pair[0] != pair[1]]\\\\nprint(result)', 'api_call': \\\"SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/nli-mpnet-base-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯ÙˆÙ† ØªØµÙ†ÙŠÙ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ Ù„Ù… ØªØªÙ… ØªØµÙ†ÙŠÙÙ‡Ø§ Ø³Ø§Ø¨Ù‚Ù‹Ø§ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯Ù‚Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"CLIPModel.from_pretrained('patrickjohncyh/fashion-clip')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. This includes CLIPProcessor for pre-processing the image and text data and CLIPModel for the zero-shot image classification model.\\\\n2. Load the pre-trained model 'patrickjohncyh/fashion-clip' using the from_pretrained method of the CLIPModel class. This model is specifically trained for fashion industry applications.\\\\n3. Load the processor that matches the model using the from_pretrained method of the CLIPProcessor class.\\\\n4. Prepare the inputs to the model, which includes the image and text data. The text data should include names of the clothing categories you wish to classify the image.\\\\n5. Use the model to get logits_per_image, which represent the probability distribution of the image belonging to each of the categories mentioned earlier. The softmax function is used to convert logits into probabilities.\\\", '<<<code>>>': 'from transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(\\\\'patrickjohncyh/fashion-clip\\\\')\\\\nprocessor = CLIPProcessor.from_pretrained(\\\\'patrickjohncyh/fashion-clip\\\\')\\\\nimage_path = \\\"path/to/image.jpg\\\" # replace with the path to the customer\\\\'s image\\\\nimage = Image.open(image_path)\\\\nclass_names = \\\"t-shirt, dress, shoes, pants, hat\\\"\\\\ninputs = processor(text=class_names, images=image, return_tensors=\\\\'pt\\\\', padding=True)\\\\nlogits_per_image = model(**inputs).logits_per_image\\\\nprobs = logits_per_image.softmax(dim=-1).tolist()[0]', 'api_call': \\\"CLIPModel.from_pretrained('patrickjohncyh/fashion-clip')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'patrickjohncyh/fashion-clip', 'api_call': \\\"CLIPModel.from_pretrained('patrickjohncyh/fashion-clip')\\\", 'api_arguments': {'image': 'File', 'class_names': 'String (comma-separated)'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained('patrickjohncyh/fashion-clip'); processor = CLIPProcessor.from_pretrained('patrickjohncyh/fashion-clip'); inputs = processor(text='blue shoes', images=image, return_tensors='pt', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\\\", 'performance': {'dataset': [{'name': 'FMNIST', 'accuracy': 0.83}, {'name': 'KAGL', 'accuracy': 0.73}, {'name': 'DEEP', 'accuracy': 0.62}]}, 'description': 'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙˆÙ†ÙŠØ© Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ Ù…Ù„ØµÙ‚Ø© Ù„ØªÙ†ÙÙŠØ° Ù…Ù‡Ù…Ø© ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries such as PIL, transformers, and requests.\\\\n2. Set up the API processor using AutoImageProcessor.from_pretrained() with the model 'facebook/mask2former-swin-large-coco-panoptic'.\\\\n3. Load the image from your shipping yard ['url'] and preprocess it using the processor.\\\\n4. Predict the universal segmentation using the trained Mask2Former model.\\\\n5. Post-process the output prediction to generate a panoptic segmentation map, which will have different sections and types of objects segmented and labeled in the image.\\\", '<<<code>>>': \\\"from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\\n\\\\nurl = 'your_shipping_yard_image_url_here'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n\\\\npredicted_panoptic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]['segmentation']\\\", 'api_call': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/mask2former-swin-large-coco-panoptic', 'api_call': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\", 'api_arguments': ['image'], 'python_environment_requirements': [['requests', 'torch', 'PIL', 'transformers']], 'example_code': 'processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result[segmentation]', 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØµÙˆØ± Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ø§Ù„Ù…ØµÙˆØ±Ø© ÙÙŠ Ø§Ù„ØµÙˆØ±ØŒ Ø®Ø§ØµØ© ÙÙŠ ØªÙ‚Ù†ÙŠØ§Øª ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± ÙÙŠ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­ÙˆØ§Ø³ÙŠØ¨.? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the necessary classes for image processing and the Mask2FormerForUniversalSegmentation class from the provided libraries.', 'code': \\\"from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nurl = 'http://your_surveillance_camera_image_url'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-cityscapes-semantic', 'api_call': \\\"from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nurl = 'http://your_surveillance_camera_image_url'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-large-cityscapes-semantic'}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \\\"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø§Ù„ØµÙØ±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… ØªØµÙ†ÙŠÙÙ‡Ø§.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Initialize the image classification model by specifying the 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k' model.\\\\n3. This model is a part of the CLIP architecture, which is designed for zero-shot image classification. So, even if the model is not trained on a specific animal, it can recognize the animal from a given image.\\\\n4. Use the created classifier to determine whether the image contains a cat or a dog by providing the image file and possible class names (in this case, 'cat' and 'dog').\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\\\\nresult = image_classifier(image_file, possible_class_names=['cat', 'dog'])\\\", 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k', 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\\\", 'api_arguments': ['image', 'possible_class_names'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'); classifier(image, possible_class_names=['cat', 'dog'])\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '80.1'}, 'description': 'A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. The model is intended for research purposes and enables researchers to better understand and explore zero-shot, arbitrary image classification. It can be used for interdisciplinary studies of the potential impact of such models. The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù„ØªØ¹Ø±Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„ØµÙˆØªÙŠØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¬Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the necessary classes from the transformers library, which are WhisperProcessor and WhisperForConditionalGeneration.\\\\n2. Load the whisper-large pretrained model using the from_pretrained method of the WhisperForConditionalGeneration class.\\\\n3. Load the processor (openai/whisper-large) using the from_pretrained method of the WhisperProcessor class.\\\\n4. Pass the audio file to the processor and obtain the input features.\\\\n5. Pass the input features to the whisper model using the generate method to get the predicted transcription.\\\\n6. Decode the predicted transcription to obtain the final text.', '<<<code>>>': \\\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-large')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large')\\\\n\\\\naudio_sample = '<audio_sample_representation>'\\\\nsampling_rate = '<sample_rate>'\\\\ninput_features = processor(audio_sample, sampling_rate=sampling_rate, return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\\\", 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai/whisper-large', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large')\\\", 'api_arguments': ['audio', 'sampling_rate'], 'python_environment_requirements': [['transformers', 'datasets']], 'example_code': 'from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\n\\\\nprocessor = WhisperProcessor.from_pretrained(openai/whisper-large)\\\\nmodel = WhisperForConditionalGeneration.from_pretrained(openai/whisper-large)\\\\n\\\\nmodel.config.forced_decoder_ids = None\\\\n\\\\nds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\\\\nsample = ds[0][audio]\\\\ninput_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\\\\n\\\\npredicted_ids = model.generate(input_features)\\\\n\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=False)', 'performance': {'dataset': [{'name': 'LibriSpeech (clean)', 'accuracy': 3.0}, {'name': 'LibriSpeech (other)', 'accuracy': 5.4}, {'name': 'Common Voice 11.0', 'accuracy': 54.8}]}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªÙ†ÙÙŠØ° Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ Ù„ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©: 1. Ø§ÙØªØ­ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø£Ùˆ Ø§Ù„Ù…Ù†ØµØ© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù„Ø¹Ø¨Ø© Ø§Ù„ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. 2. Ø§Ø®ØªØ± Ø®ÙŠØ§Ø± Ø§Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ. 3. Ù‚Ù… Ø¨ØªØ­Ø¯ÙŠØ¯ Ø§Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù„Ø¹Ø¨Ø© ÙˆØ§Ø¨Ø¯Ø£ Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø©. 4. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø§Ø´Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø£Ùˆ Ø§Ù„Ø¨ØµØ±ÙŠØ© Ù„Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ. 5. Ø§ØªØ¨Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø´Ø© Ù„Ù„Ø¹Ø¨ Ø¨ÙƒÙØ§Ø¡Ø© ÙˆØ§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„ÙŠ. Ø­Ø¸Ù‹Ø§ Ø³Ø¹ÙŠØ¯Ù‹Ø§ ÙˆØ§Ø³ØªÙ…ØªØ¹ Ø¨Ø§Ù„Ù„Ø¹Ø¨!? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries and classes from the transformers package. This includes AutoModelForDocumentQuestionAnswering for the question answering model.\\\\n2. Load the pre-trained 'tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa' document question answering model using the from_pretrained method.\\\\n3. Prepare the documents you wish to analyze and convert them into plain text format. This can be achieved by reading the document or converting it using appropriate libraries (e.g. for PDFs, you can use the PyPDF2 library).\\\\n4. Use the loaded model to analyze the plain text documents and answer questions based on the information found within the document.\\\", '<<<code>>>': 'from transformers import AutoModelForDocumentQuestionAnswering, AutoTokenizer\\\\n\\\\nmodel_name = \\\"tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa\\\"\\\\ndocument_model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_name)\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\n\\\\n# Replace \\\\'your_question\\\\' and \\\\'your_text_document\\\\' with the actual question and document text\\\\nquestion = \\\"your_question\\\"\\\\ncontext = \\\"your_text_document\\\"\\\\n\\\\ninputs = tokenizer(question, context, return_tensors=\\\"pt\\\")\\\\noutputs = document_model(**inputs)\\\\n# Extract answer\\\\nstart_location = torch.argmax(outputs[\\\\'start_logits\\\\'])\\\\nend_location = torch.argmax(outputs[\\\\'end_logits\\\\']) + 1\\\\nanswer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\\\\'input_ids\\\\'][0][start_location:end_location]))', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')\\\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers==4.12.3, Pytorch==1.8.0+cu101, Datasets==1.15.1, Tokenizers==0.10.3'], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.2207}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙƒØªÙˆØ¨Ø© ÙÙŠ ØªØ±Ø¬Ù…Ø© Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨ØµÙˆØª ÙˆØ§Ø¶Ø­ ÙˆØ¯Ù‚ÙŠÙ‚ Ù„Ø¯Ø¹Ù… Ù…Ø³ØªØ®Ø¯Ù…ÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù…ÙŠÙ„Ù†Ø§ Ø§Ù„Ø°ÙŠÙ† ÙŠØ¹Ø§Ù†ÙˆÙ† Ù…Ù† Ø¥Ø¹Ø§Ù‚Ø© Ø¨ØµØ±ÙŠØ©ØŒ Ø®Ø§ØµØ© Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© ÙÙŠ Ø¬Ø¯Ø§ÙˆÙ„. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„ØµÙˆØª ÙˆØ§Ø¶Ø­Ù‹Ø§ ÙˆØ³Ù‡Ù„ Ø§Ù„ÙÙ‡Ù… Ù„ÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© ÙÙŠ ÙÙ‡Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the Hugging Face Transformers library.\\\\n2. Use the pipeline function to create an automatic translation model for Dutch to English translation tasks.\\\\n3. Specify the model 'Helsinki-NLP/opus-mt-nl-en' to be loaded. This model is a transformer-align architecture trained on the OPUS dataset with normalization and SentencePiece pre-processing.\\\\n4. With the created translation pipeline, you can translate Dutch texts into English automatically.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntranslator = pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\\ntranslated_text = translator('Hallo, hoe gaat het met je?')[0]['translation_text']\\\", 'api_call': \\\"pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-nl-en', 'api_call': \\\"pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\\ntranslated_text = translation('Hallo, hoe gaat het met je?')[0]['translation_text']\\\", 'performance': {'dataset': 'Tatoeba.nl.en', 'accuracy': {'BLEU': 60.9, 'chr-F': 0.749}}, 'description': 'A Dutch to English translation model based on the OPUS dataset, using a transformer-align architecture with normalization and SentencePiece pre-processing.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries YOLO and render_result from the ultralyticsplus module.\\\\n2. Create a YOLO model instance by specifying the model name 'keremberke/yolov8s-csgo-player-detection'.\\\\n3. Set the model override parameters like confidence threshold, intersection over union, and maximum number of detections.\\\\n4. Use the model.predict() method with the image input to produce detection results.\\\\n5. To visualize the detection results, use the render_result() function.\\\\n6. Display the rendered result by calling the show() method.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://path.to/csgo_image.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \\\"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\\\" </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ·Ù„Ø¨ Ø¹Ù…ÙŠÙ„Ù†Ø§ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„ØªØ­Ø¯Ø«ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ·Ø¨ÙŠÙ‚ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the 'pipeline' function from the transformers package.\\\\n2. We use the 'pipeline' function, specifying the task 'text-generation' and the model 'PygmalionAI/pygmalion-1.3b', to create a conversation generator.\\\\n3. We provide an input message that follows the format required by the model. The input should include the character persona, dialogue history, and user input message.\\\\n4. Using the generator, we generate a dialogue response appropriate for a wise old wizard character.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ngenerator = pipeline(\\\\'text-generation\\\\', \\\\'PygmalionAI/pygmalion-1.3b\\\\')\\\\ncharacter_persona = \\\"Wise Old Wizard Persona: A wise old wizard who has spent centuries studying ancient texts and possesses vast knowledge of the magical realm. He is patient, kind, and always willing to impart wisdom to those who seek it.\\\"\\\\nuser_input = \\\"You: Tell me, wise wizard, what is the secret to mastering the arcane arts?\\\"\\\\ninput_prompt = f\\\"{character_persona}\\\\\\\\n{user_input}\\\\\\\\nWizard:\\\"\\\\ngenerated_text = generator(input_prompt)\\\\nwizard_response = generated_text[0][\\\\'generated_text\\\\']', 'api_call': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pygmalion-1.3b', 'api_call': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", 'api_arguments': 'input_prompt', 'python_environment_requirements': [['transformers']], 'example_code': \\\"[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\\\", 'performance': {'dataset': '56MB of dialogue data', 'accuracy': 'Not provided'}, 'description': \\\"Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\\\" </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"pipeline('image-to-text', model='microsoft/git-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required library, which is the 'pipeline' function from the transformers library provided by Hugging Face.\\\\n2. Create a pipeline object that uses the microsoft/git-base model for the image-to-text task.\\\\n3. Provide the input image to the pipeline, and it will use the model to generate a caption for the image.\\\\n4. GIT (GenerativeImage2Text) model is used for generating captions for images, and it is based on the concept of predicting the next text token, given previous tokens and image tokens.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ncaption_pipeline = pipeline('image-to-text', model='microsoft/git-base')\\\\ncaption = caption_pipeline(image)\\\", 'api_call': \\\"pipeline('image-to-text', model='microsoft/git-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/git-base', 'api_call': \\\"pipeline('image-to-text', model='microsoft/git-base')\\\", 'api_arguments': 'image', 'python_environment_requirements': ['transformers'], 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \\\"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\\\" </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØµÙˆØ± Ù„Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù…Ø¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆÙ†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØµÙ…ÙŠÙ… Ù†Ù…Ø§Ø°Ø¬ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³Ù…Ø§Øª Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© ÙƒØ§Ù„ØµÙˆØª ÙˆØ§Ù„ØµÙˆØ± ÙˆØ§Ù„Ù†ØµÙˆØµ ÙˆØºÙŠØ±Ù‡Ø§ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØªØ­Ù‚ÙŠÙ‚ Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.? [/INST]{'domain': 'Multimodal Text-to-Image', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion', torch_dtype=torch.float32)\\\", 'api_provider': 'Hugging Face', 'explanation': '1. To generate an image, we first import the required libraries, including torch and autocast from torch, and StableDiffusionPipeline from diffusers.', 'code': 'import torch\\\\nfrom torch import autocast\\\\nfrom diffusers import StableDiffusionPipeline\\\\npipe = StableDiffusionPipeline.from_pretrained(\\\\'hakurei/waifu-diffusion\\\\', torch_dtype=torch.float32).to(\\\\'cuda\\\\')\\\\nprompt = \\\"a garden full of mythical creatures\\\"\\\\nwith autocast(cuda=True):\\\\n    image = pipe(prompt, guidance_scale=6)[0]\\\\nimage.save(\\\\'garden_mythical_creatures.png\\\\')', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': 'import torch\\\\nfrom torch import autocast\\\\nfrom diffusers import StableDiffusionPipeline\\\\npipe = StableDiffusionPipeline.from_pretrained(\\\\'hakurei/waifu-diffusion\\\\', torch_dtype=torch.float32).to(\\\\'cuda\\\\')\\\\nprompt = \\\"a garden full of mythical creatures\\\"\\\\nwith autocast(cuda=True):\\\\n    image = pipe(prompt, guidance_scale=6)[0]\\\\nimage.save(\\\\'garden_mythical_creatures.png\\\\')', 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \\\"import torch\\\\nfrom torch import autocast\\\\nfrom diffusers import StableDiffusionPipeline\\\\npipe = StableDiffusionPipeline.from_pretrained(\\\\n 'hakurei/waifu-diffusion',\\\\n torch_dtype=torch.float32\\\\n).to('cuda')\\\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\\\nwith autocast(cuda):\\\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\\\nimage.save(test.png)\\\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØµÙ…ÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import MarianMTModel from the transformers library by Hugging Face to facilitate translation tasks.\\\\n2. We use the from_pretrained method of MarianMTModel to load the pre-trained model 'Helsinki-NLP/opus-mt-ROMANCE-en'. This model is designed for translating Romance languages, such as French, Spanish, Portuguese, Italian, and Romanian to English.\\\\n3. With the model in place, we can process user comments and translate them from various Romance languages to English so that website moderators or administrators can understand and evaluate the comments.\\\", '<<<code>>>': 'from transformers import MarianMTModel, MarianTokenizer\\\\nmodel = MarianMTModel.from_pretrained(\\\\'Helsinki-NLP/opus-mt-ROMANCE-en\\\\')\\\\ntokenizer = MarianTokenizer.from_pretrained(\\\\'Helsinki-NLP/opus-mt-ROMANCE-en\\\\')\\\\ntext = \\\"User comment in Romance language, e.g., French, Spanish, Portuguese, Italian, or Romanian.\\\"\\\\ninput_tokens = tokenizer.encode(text, return_tensors=\\\"pt\\\")\\\\ntranslated_tokens = model.generate(input_tokens)\\\\ntranslated_text = tokenizer.decode(translated_tokens[0])', 'api_call': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-ROMANCE-en', 'api_call': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", 'api_arguments': ['source languages', 'target languages'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': '', 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': 62.2, 'chr-F': 0.75}}, 'description': 'A model for translating Romance languages to English, trained on the OPUS dataset. It supports multiple source languages such as French, Spanish, Portuguese, Italian, and Romanian, among others. The model is based on the transformer architecture and uses normalization and SentencePiece for pre-processing.' </s>\"\n"}
{"text": "<s>[INST]Ù…Ø¬Ø§Ù„ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØª ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ ÙˆÙÙ‡Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ©ØŒ ÙˆÙ‚Ø¯Ø±Ø©Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø±Ù ÙˆØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø¨Ø¯Ù‚Ø© ÙˆÙØ¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the necessary dependencies from the transformers library. This will include the AutoModelForAudioClassification class, which is needed for loading the pre-trained model.\\\\n2. Use the from_pretrained method of the AutoModelForAudioClassification class to load the 'MIT/ast-finetuned-speech-commands-v2' model. This model is specifically fine-tuned for classifying speech commands in audio files.\\\\n3. Import the audio file's data and decode it accordingly.\\\\n4. Use the created audio classification model to classify different types of speech commands in the provided audio file.\\\", '<<<code>>>': \\\"from transformers import AutoModelForAudioClassification\\\\naudio_classifier = AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\\nresult = audio_classifier('path/to/audio/file.wav')\\\\n# Replace 'path/to/audio/file.wav' with the path to your audio file\\\", 'api_call': \\\"AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ast-finetuned-speech-commands-v2', 'api_call': \\\"AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\", 'api_arguments': 'audio file', 'python_environment_requirements': ['transformers library'], 'example_code': \\\"result = audio_classifier('path/to/audio/file.wav')\\\", 'performance': {'dataset': 'Speech Commands v2', 'accuracy': '98.120'}, 'description': 'Audio Spectrogram Transformer (AST) model fine-tuned on Speech Commands v2. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªÙˆÙÙŠØ± ØµÙˆØ±Ø© Ù…Ù† Ù†Ù‡Ø± ÙˆØ§Ø·Ù„Ø¨ Ù…Ù† ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ØªÙˆÙ„ÙŠØ¯ ÙˆØµÙ Ù†ØµÙŠ Ø¯Ù‚ÙŠÙ‚ Ù„Ù„ØµÙˆØ±Ø© Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ù…ÙŠØ§Ù‡ ÙˆØ§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ù…Ø­ÙŠØ·Ø©.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import necessary libraries, including MgpstrProcessor and MgpstrForSceneTextRecognition classes from the transformers package.\\\\n2. Instantiate the MgpstrProcessor and MgpstrForSceneTextRecognition classes using the from_pretrained() method with the model name 'alibaba-damo/mgp-str-base'. This pre-trained MGP-STR model is designed for OCR tasks and can read text in images.\\\\n3. Load your image file into a PIL.Image object and convert it to the RGB format.\\\\n4. Preprocess the image using the MgpstrProcessor instance, and pass the pixel values of the image to the MgpstrForSceneTextRecognition model.\\\\n5. Decode the output logits using the MgpstrProcessor's batch_decode() method to obtain the recognized text.\\\", '<<<code>>>': \\\"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\\\nfrom PIL import Image\\\\nimport requests\\\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\\\nurl = 'https://i.postimg.cc/ZKwLg2Gw/367-14.png'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\noutputs = model(pixel_values)\\\", 'api_call': \\\"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mgp-str', 'api_call': \\\"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\\", 'api_arguments': {'model_name': 'alibaba-damo/mgp-str-base'}, 'python_environment_requirements': [{'packages': ['transformers']}], 'example_code': \\\"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\\\nimport requests\\\\nfrom PIL import Image\\\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\\\noutputs = model(pixel_values)\\\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\\\", 'performance': {'dataset': 'MJSynth and SynthText', 'accuracy': None}, 'description': 'MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ù…Ø³ØªÙ‚Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the YOLO class from the 'ultralyticsplus' package.\\\\n2. Create a YOLO object using the 'keremberke/yolov8n-pcb-defect-segmentation' model.\\\\n3. Set the model overrides for 'conf', 'iou', 'agnostic_nms', and 'max_det'.\\\\n4. Specify the path or URL of the target PCB image for the detection.\\\\n5. Use the 'predict' method to get the results of the detection and segmentation.\\\\n6. Extract the box coordinates and mask information from the results.\\\\n7. Visualize the rendered result on the PCB image.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\npcb_image = 'path/to/pcb_image.jpg'\\\\nresults = model.predict(pcb_image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=pcb_image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8n-pcb-defect-segmentation', 'api_call': \\\"YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.512, 'mAP@0.5(mask)': 0.517}}, 'description': 'A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨ÙˆØ§Ø³Ø·Ø© ØªÙ‚Ù†ÙŠØ§Øª ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± ÙÙŠ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline class from the transformers package for easy API usage.\\\\n2. Create an image classification pipeline using the provided model 'martinezomg/vit-base-patch16-224-diabetic-retinopathy'. This model is specifically designed for diabetic retinopathy detection and classifying it into severity categories: none, mild, moderate, severe, and proliferative.\\\\n3. Given an image of a patient's retina, the created classifier can be used to predict the severity of diabetic retinopathy. For example, you can provide the path to an image and the classifier will return the severity class.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\\nresult = image_classifier('path/to/image.jpg')\\\\n# replace 'path/to/image.jpg' with the image path\\\", 'api_call': \\\"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy', 'api_call': \\\"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\", 'api_arguments': {'model_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy'}, 'python_environment_requirements': [{'transformers': '4.28.1', 'pytorch': '2.0.0+cu118', 'datasets': '2.11.0', 'tokenizers': '0.13.3'}], 'example_code': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\\nresult = image_classifier('path/to/image.jpg')\\\", 'performance': {'dataset': 'None', 'accuracy': 0.7744}, 'description': 'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…ÙŠÙ„ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø¯ÙˆÙ† Ø´Ø±ÙˆØ· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨. ÙŠØªØ·Ù„Ø¨ Ø°Ù„Ùƒ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ ØµÙˆØ± Ø¬Ø¯ÙŠØ¯Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ ÙˆØ¨Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ù„ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ù…Ø­Ø¯Ø¯Ø©.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. To generate a piece of art inspired by WikiArt, first, import the DDPMPipeline class from the 'diffusers' package.\\\", 'code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\\\ngenerated_image = pipeline().images[0]\\\\ngenerated_image.save('generated_wikiart_inspired_image.png')\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'johnowhitaker/sd-class-wikiart-from-bedrooms', 'api_call': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\\\ngenerated_image = pipeline().images[0]\\\\ngenerated_image.save('generated_wikiart_inspired_image.png')\\\", 'api_arguments': '', 'python_environment_requirements': 'diffusers', 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': 'https://huggingface.co/datasets/huggan/wikiart', 'accuracy': 'Not provided'}, 'description': 'This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­Ø¯ÙŠØ¯ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"BertModel.from_pretrained('setu4993/LaBSE')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers and torch packages. We will need the BertModel class to create the language-agnostic sentence embeddings and the BertTokenizerFast class to tokenize the sentences in all three languages.\\\\n2. We then use the from_pretrained method of the BertModel to load the pre-trained model 'setu4993/LaBSE'. This model has been trained to generate multilingual sentence embeddings.\\\\n3. Tokenize and encode the sentences using the tokenizer's method called tokenizer.\\\\n4. Finally, compute the sentence embeddings by feeding the tokenized and encoded sentences to the model.\\\\n5. The output embeddings can be compared to determine the similarity among the sentences using an appropriate similarity metric, such as cosine similarity or L2 distance.\\\", '<<<code>>>': \\\"import torch\\\\nfrom transformers import BertModel, BertTokenizerFast\\\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\\\nenglish_sentences = ['english_query1', 'english_query2']\\\\nitalian_sentences = ['italian_query1', 'italian_query2']\\\\njapanese_sentences = ['japanese_query1', 'japanese_query2']\\\\nall_sentences = english_sentences + italian_sentences + japanese_sentences\\\\ninputs = tokenizer(all_sentences, return_tensors='pt', padding=True)\\\\nmodel = model.eval()\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nsentence_embeddings = outputs.pooler_output\\\", 'api_call': \\\"BertModel.from_pretrained('setu4993/LaBSE')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \\\"BertModel.from_pretrained('setu4993/LaBSE')\\\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': [['torch', 'transformers']], 'example_code': \\\"import torch\\\\nfrom transformers import BertModel, BertTokenizerFast\\\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\\\nmodel = model.eval()\\\\nenglish_sentences = [\\\\n 'dog',\\\\n 'Puppies are nice.',\\\\n 'I enjoy taking long walks along the beach with my dog.',\\\\n]\\\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\\\nwith torch.no_grad():\\\\n english_outputs = model(**english_inputs)\\\\nenglish_embeddings = english_outputs.pooler_output\\\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>\"\n"}
{"text": "<s>[INST]Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªÙ…ÙŠÙŠØ² Ø£ØµÙˆØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹Ø© Ø¹Ù† Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ø¨Ø´Ø±ÙŠØ©. Ù‚Ù… Ø¨ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø¨ØªØ³Ù…ÙŠØ§Øª Ù…Ø¹Ø¨Ø±Ø© Ù„Ù„ØµÙÙ‚Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ù…ØªÙ†ÙˆØ¹Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-ar-cv7')\\\", '<<<api_provider>>>': 'Fairseq', '<<<explanation>>>': \\\"1. We import the necessary functions from the fairseq and IPython libraries.\\\\n2. We load the pre-trained model 'facebook/tts_transformer-ar-cv7' using the load_model_ensemble_and_task_from_hf_hub function. This Transformer text-to-speech model is trained for Arabic language with a single-speaker male voice.\\\\n3. To generate speech from text, we first update the model configuration based on the available data.\\\\n4. Then, we build a generator object, which is used to produce the speech output.\\\\n5. Finally, we provide the input text, generate the audio waveform, and play the audio using the IPython.Audio function.\\\", '<<<code>>>': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\'facebook/tts_transformer-ar-cv7\\\\', arg_overrides={\\\\'vocoder\\\\': \\\\'hifigan\\\\', \\\\'fp16\\\\': False})\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\ntext = \\\"Ù…Ø±Ø­Ø¨Ø§ØŒ Ù‡Ø°Ø§ Ø§Ø®ØªØ¨Ø§Ø± ØªØ´ØºÙŠÙ„.\\\"\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-ar-cv7')\\\", 'provider': 'Fairseq', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tts_transformer-ar-cv7', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-ar-cv7')\\\", 'api_arguments': {'text': 'input text'}, 'python_environment_requirements': [['fairseq', 'IPython']], 'example_code': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n facebook/tts_transformer-ar-cv7,\\\\n arg_overrides={vocoder: hifigan, fp16: False}\\\\n)\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\ntext = Ù…Ø±Ø­Ø¨Ù‹Ø§ ØŒ Ù‡Ø°Ø§ Ø§Ø®ØªØ¨Ø§Ø± ØªØ´ØºÙŠÙ„.\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)', 'performance': {'dataset': 'common_voice', 'accuracy': 'Not specified'}, 'description': 'Transformer text-to-speech model for Arabic language with a single-speaker male voice, trained on Common Voice v7 dataset.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† ØªØµÙ†ÙŠÙ (Zero-Shot Classification) ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙˆÙ†ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø·Ø¨Ù‚Ø§Øª.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"mDeBERTaForSequenceClassification.from_pretrained('MoritzLaurer/mDeBERTa-v3-base-mnli-xnli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the mDeBERTaForSequenceClassification model from the transformers library provided by Hugging Face.\\\\n2. Use the 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli' model, which has been trained on the XNLI dataset and is suitable for multilingual zero-shot classification tasks.\\\\n3. Load the model by using the from_pretrained method.\\\\n4. This model can be used to classify text into different categories in multiple languages. Simply pass in the text to be analyzed and the candidate category labels in the desired language.\\\", '<<<code>>>': \\\"from transformers import mDeBERTaForSequenceClassification\\\\nmultilingual_classifier = mDeBERTaForSequenceClassification.from_pretrained('MoritzLaurer/mDeBERTa-v3-base-mnli-xnli')\\\", 'api_call': \\\"mDeBERTaForSequenceClassification.from_pretrained('MoritzLaurer/mDeBERTa-v3-base-mnli-xnli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli', 'api_call': \\\"mDeBERTaForSequenceClassification.from_pretrained('MoritzLaurer/mDeBERTa-v3-base-mnli-xnli')\\\", 'api_arguments': {'model': 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli'}, 'python_environment_requirements': [{'transformers': '>=4.13'}], 'example_code': {'simple_zero_shot_classification_pipeline': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=MoritzLaurer/mDeBERTa-v3-base-mnli-xnli)\\\\nsequence_to_classify = Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\\\\ncandidate_labels = [politics, economy, entertainment, environment]\\\\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\\\\nprint(output)'}, 'performance': {'dataset': {'average': 0.808, 'ar': 0.802, 'bg': 0.829, 'de': 0.825, 'el': 0.826, 'en': 0.883, 'es': 0.845, 'fr': 0.834, 'hi': 0.771, 'ru': 0.813, 'sw': 0.748, 'th': 0.793, 'tr': 0.807, 'ur': 0.74, 'vi': 0.795, 'zh': 0.8116}, 'accuracy': '0.808'}, 'description': 'This multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual zero-shot classification. The underlying model was pre-trained by Microsoft on the CC100 multilingual dataset. It was then fine-tuned on the XNLI dataset, which contains hypothesis-premise pairs from 15 languages, as well as the English MNLI dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ± Ø¨Ø¯ÙˆÙ† Ø´Ø±ÙˆØ· ÙÙŠ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('Apocalypse-19/shoe-generator')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. In order to create a generated shoe design, we start by importing the DDPMPipeline from the diffusers package.\\\\n2. The DDPMPipeline is used to create a model that is capable of generating high-quality images of shoes. The pretrained model 'Apocalypse-19/shoe-generator' is a diffusion model trained to generate shoes at 128x128 resolution.\\\\n3. This model can then be used to generate new, unique shoe designs. It can be utilized to provide creative inspiration, or combined with other designs to create original shoe models.\\\\n4. Finally, the generated image can be saved for further refinement and design purposes.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('Apocalypse-19/shoe-generator')\\\\nimage = pipeline().images[0]\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('Apocalypse-19/shoe-generator')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Apocalypse-19/shoe-generator', 'api_call': \\\"DDPMPipeline.from_pretrained('Apocalypse-19/shoe-generator')\\\", 'api_arguments': [], 'python_environment_requirements': [['diffusers']], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('Apocalypse-19/shoe-generator')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': 'custom dataset', 'accuracy': '128x128 resolution'}, 'description': 'This model is a diffusion model for unconditional image generation of shoes trained on a custom dataset at 128x128 resolution.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…Ø´Ø±ÙˆØ·ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù„ØµÙˆØ± Ø§Ù„Ø­Ø§Ù„ÙŠØ©ØŒ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·.? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\nmanga_ocr = pipeline('ocr', model='kha-white/manga-ocr-base')\\\\nrecognized_text = manga_ocr(image_file_path)\\\", 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \\\"from transformers import pipeline\\\\nmanga_ocr = pipeline('ocr', model='kha-white/manga-ocr-base')\\\\nrecognized_text = manga_ocr(image_file_path)\\\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.'}} </s>\"\n"}
{"text": "<s>[INST]Ù„ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø¬Ø¯ÙˆÙ„ÙŠ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ ØªÙ†Ø¨Ø¤Ø§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØŒ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ÙÙ‡Ù… Ù…ØªØ¹Ù…Ù‚ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª ÙˆØ§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ©.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"load('path_to_folder/example.pkl')\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': \\\"1. The first step is to download and store the pre-trained model's weights in a folder using the download function from the skops.hub_utils.\\\\n2. Next, import the load function from the skops.io package to access the pre-trained GradientBoostingRegressor model.\\\\n3. Use the load() function to load the model saved in the specified folder.\\\\n4. This model can be used to make predictions for the fish data provided by the fishery company. By providing the features of the fish (length, height, etc.), the model can estimate the weight of the fish.\\\", '<<<code>>>': \\\"from skops.hub_utils import download\\\\nfrom skops.io import load\\\\ndownload('brendenc/Fish-Weight', 'path_to_folder')\\\\nmodel = load('path_to_folder/example.pkl')\\\\nfish_weight_prediction = model.predict(fish_data_features)\\\", 'api_call': \\\"load('path_to_folder/example.pkl')\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Fish-Weight', 'api_call': \\\"load('path_to_folder/example.pkl')\\\", 'api_arguments': {'model_path': 'path_to_folder/example.pkl'}, 'python_environment_requirements': [{'skops.hub_utils': 'download', 'skops.io': 'load'}], 'example_code': \\\"from skops.hub_utils import download\\\\nfrom skops.io import load\\\\ndownload('brendenc/Fish-Weight', 'path_to_folder')\\\\nmodel = load('path_to_folder/example.pkl')\\\", 'performance': {'dataset': 'Fish dataset', 'accuracy': 'Not provided'}, 'description': 'This is a GradientBoostingRegressor on a fish dataset. This model is intended for educational purposes.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØªØµÙ†ÙŠÙÙ‡Ø§ ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('shahrukhx01/question-vs-statement-classifier')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, which include AutoTokenizer and AutoModelForSequenceClassification from the transformers package.\\\\n2. Load the tokenizer and model using the from_pretrained method using the provided model name: 'shahrukhx01/question-vs-statement-classifier'.\\\\n3. Use the tokenizer to convert user input (queries) into tokens that can be processed by the model.\\\\n4. Pass these tokens to the model, and it will output the probabilities of the input being a question or a statement. The model is trained on the Haystack dataset specifically for question/statement classification tasks.\\\\n5. According to the highest probability, you can classify the input as either a question or a statement.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'shahrukhx01/question-vs-statement-classifier\\\\')\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\\\'shahrukhx01/question-vs-statement-classifier\\\\')\\\\nuser_query = \\\"Input query here...\\\"\\\\ntokens = tokenizer(user_query, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**tokens)\\\\nprediction = output.logits.argmax(dim=-1).item()\\\\nclassification = \\\\'question\\\\' if prediction == 0 else \\\\'statement\\\\'', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('shahrukhx01/question-vs-statement-classifier')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'shahrukhx01/question-vs-statement-classifier', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('shahrukhx01/question-vs-statement-classifier')\\\", 'api_arguments': {'tokenizer': 'AutoTokenizer.from_pretrained(shahrukhx01/question-vs-statement-classifier)'}, 'python_environment_requirements': [{'transformers': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification'}], 'example_code': 'tokenizer = AutoTokenizer.from_pretrained(shahrukhx01/question-vs-statement-classifier)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(shahrukhx01/question-vs-statement-classifier)', 'performance': {'dataset': 'Haystack', 'accuracy': 'Not provided'}, 'description': 'Trained to add the feature for classifying queries between Question Query vs Statement Query using classification in Haystack' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø§Ø¯Ø±Ù‹Ø§ Ø¹Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØªØ¹ÙŠÙŠÙ† Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ø§Ù„ØµÙˆØ± Ø¨Ø¯Ù‚Ø©. ÙŠØ¬Ø¨ Ø£Ù† ÙŠØªØ¶Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø®Ø·ÙˆØ§Øª Ù„ØªØ­Ø¯ÙŠØ¯ Ù…ÙƒØ§Ù† ÙˆÙ†ÙˆØ¹ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙÙŠ Ø§Ù„ØµÙˆØ±.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8n-csgo-player-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first install ultralyticsplus and ultralytics to access the YOLO model. Then, we import YOLO and render_result from ultralyticsplus library.\\\\n2. Next, we load the pre-trained YOLOv8 model 'keremberke/yolov8n-csgo-player-detection' specifically trained for detecting CS:GO players. This model can recognize the following labels: ['ct', 'cthead', 't', 'thead'].\\\\n3. Configure the required confidences, inter-cell overlap, and maximum detected objects allowed by the model.\\\\n4. Provide the image URL or local path containing CS:GO players and make the predictions using the predict method.\\\\n5. To render and visualize results, use the render_result function with the model, image, and detected results.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path_to_your_csgo_image.jpg'\\\\n# replace 'path_to_your_csgo_image.jpg' with your image URL or local path\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8n-csgo-player-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8n-csgo-player-detection', 'api_call': \\\"YOLO('keremberke/yolov8n-csgo-player-detection')\\\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\\\n\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\n\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\n\\\\nresults = model.predict(image)\\\\n\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.844}, 'description': \\\"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\\\" </s>\"\n"}
{"text": "<s>[INST]Ø§Ù„Ù…Ø·ÙˆØ± ÙŠÙ‚ÙˆÙ… Ø¨ØªØµÙ…ÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØ±Ø©. ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù„ØºØ§Øª ÙˆØ§Ù„Ø£Ø³Ø§Ù„ÙŠØ¨ Ù„Ø¶Ù…Ø§Ù† Ø§Ø³ØªÙŠØ¹Ø§Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We import the necessary function which is the pipeline' from the 'transformers' package provided by Hugging Face.\\\\n2. We initialize a 'text-to-image' pipeline with the pre-trained model 'gsdf/Counterfeit-V2.5'. This model generates anime-style images based on text prompts, which is suitable for creating animal characters.\\\\n3. We can provide text prompts describing the animal character we want the model to generate. The model will generate an image representation of the character based on the given text prompt.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntext_to_image = pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\\\\ntext_prompt = 'cute fox, standing, looking happy, wearing a hat'\\\\ngenerated_image = text_to_image(text_prompt)\\\", 'api_call': \\\"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \\\"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ†Ø¨Ø¤ Ù‚ÙŠÙ… Ø±Ù‚Ù…ÙŠØ© Ù…Ø³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø£Ø³Ø§Ù„ÙŠØ¨ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¶Ø¨Ø· Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡ Ù…Ù…ÙƒÙ†.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We start by importing the necessary libraries, which include joblib for loading the trained model, and pandas for working with tabular data.\\\\n2. We load the pre-trained model using `joblib.load('model.joblib')`.\\\\n3. Next, we load our dataset (data.csv) into a pandas DataFrame and extract only the columns specified in the configuration file (config.json) as features for the prediction.\\\\n4. We then use the loaded model to predict CO2 emissions for each instance in the data.\\\\n5. The output can be used by your application to provide insights on the eco-friendliness of the cars.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\n\\\\ndata = pd.read_csv('data.csv')  # replace 'data.csv' with path to your dataset\\\\n# process data according to config\\\\npredictions = model.predict(data)\\\\n\\\\n# predictions now contain CO2 emissions for each instance in the data\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': '45473113800', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'samvelkoch/autotrain-data-prknsn-2', 'accuracy': {'Loss': 5.079, 'R2': 0.109, 'MSE': 25.795, 'MAE': 3.78, 'RMSLE': 0.849}}, 'description': 'A tabular regression model trained with AutoTrain for predicting carbon emissions.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ù„ØºÙˆÙŠ ÙŠÙ…ÙƒÙ†Ù‡ Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ§Øª ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙˆØ¯Ù‚ÙŠÙ‚. ÙŠÙ†Ø¨ØºÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‚Ø§Ø¯Ø±Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ ÙˆÙ…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ§Øª ÙÙŠÙ‡ Ø¨ÙƒÙ„Ù…Ø§Øª Ù…Ù†Ø§Ø³Ø¨Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­ÙŠØ·.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We first import the necessary library, which is 'pipeline' from transformers.\\\\n2. We use the 'pipeline' function from transformers to load the model, which in this case is 'distilroberta-base'. \\\\n3. This model is designed to complete sentences where certain words or phrases have been omitted, represented by the special token <mask>.\\\\n4. The model will generate an appropriate word or phrase to fill the <mask> token.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'distilroberta-base\\\\')\\\\nsentence = \\\"Science is constantly <mask> new discoveries and challenging our understanding of the universe.\\\"\\\\nresult = unmasker(sentence)\\\\nbest_completion = result[0][\\\\'token_str\\\\']', 'api_call': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilroberta-base', 'api_call': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"unmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': 'openwebtext', 'accuracy': 'Not provided'}, 'description': \\\"DistilRoBERTa is a distilled version of the RoBERTa-base model, designed to be smaller, faster, and lighter. It is a Transformer-based language model trained on the OpenWebTextCorpus, which is a reproduction of OpenAI's WebText dataset. The model has 6 layers, 768 dimensions, and 12 heads, totaling 82M parameters. It is primarily intended for fine-tuning on downstream tasks such as sequence classification, token classification, or question answering.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø´ÙƒÙ„ Ø¥Ù„Ù‰ Ø¢Ø®Ø±ØŒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-small')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the required libraries from transformers, specifically T5Tokenizer and T5ForConditionalGeneration.\\\\n2. Load the pre-trained model \\\\'google/flan-t5-small\\\\' using the T5ForConditionalGeneration.from_pretrained() method.\\\\n3. Load the corresponding tokenizer using T5Tokenizer.from_pretrained(\\\\'google/flan-t5-small\\\\'\\\\').\\\\n4. Prepare the input text with the translation instruction, e.g., \\\"translate German to English: <German text>\\\".\\\\n5. Tokenize the input text using the loaded tokenizer and convert it to input_ids.\\\\n6. Generate the translated text by calling model.generate() with the input_ids.\\\\n7. Decode the generated output using the tokenizer\\\\'s decode() method to obtain the translated text in English.', '<<<code>>>': \\\"from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained('google/flan-t5-small')\\\\nmodel = T5ForConditionalGeneration.from_pretrained('google/flan-t5-small')\\\\ninput_text = 'translate German to English: Wie alt bist du?'\\\\ninput_ids = tokenizer(input_text, return_tensors='pt').input_ids\\\\noutputs = model.generate(input_ids)\\\\ntranslated_text = tokenizer.decode(outputs[0])\\\", 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-small')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/flan-t5-small', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-small')\\\", 'api_arguments': ['input_text'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-small)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-small)\\\\ninput_text = translate English to German: How old are you?\\\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))', 'performance': {'dataset': [{'name': 'MMLU', 'accuracy': '75.2%'}]}, 'description': 'FLAN-T5 small is a fine-tuned version of T5 language model on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. The model is designed for research on language models, including zero-shot and few-shot NLP tasks, reasoning, question answering, fairness, and safety research. It has not been tested in real-world applications and should not be used directly in any application without prior assessment of safety and fairness concerns specific to the application.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ·Ù„Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨. Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‚Ø§Ø¯Ø±Ø§Ù‹ Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯ÙˆÙ† Ø­Ø§Ø¬Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ØŒ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø­Ù„Ø§Ù‹ ÙØ¹Ø§Ù„Ø§Ù‹ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„.? [/INST]{'domain': 'Computer Vision Image Classification', 'api_call': \\\"ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import ViTForImageClassification\\\\nmodel = ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\\\\n# Now, fine-tune the model further on specific dataset of city life and nature landscapes\\\", 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_base_patch16_224.augreg2_in21k_ft_in1k', 'api_call': \\\"from transformers import ViTForImageClassification\\\\nmodel = ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\\\\n# Now, fine-tune the model further on specific dataset of city life and nature landscapes\\\", 'api_arguments': {'pretrained_model_name_or_path': 'timm/vit_base_patch16_224.augreg2_in21k_ft_in1k'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù…Ù† Ø®Ù„Ø§Ù„ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ø¥Ù†ØªØ§Ø¬ ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ø§Ù‡Ø¯ ØªØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ Ø¨Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('bigcode/santacoder', trust_remote_code=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We will first import the necessary classes from the transformers library. This includes AutoModelForCausalLM for text generation and AutoTokenizer for tokenizing the input code.\\\\n2. Then, we load the 'bigcode/santacoder' model using the from_pretrained method. This model is designed for generating code based on an existing code snippet, which makes it suitable for generating your area_coverage method.\\\\n3. We provide the input code snippet - the Python class and a brief description of the area_coverage method you are looking for.\\\\n4. The model generates the code for the area_coverage method which can be incorporated into your existing Python class.\\\", '<<<code>>>': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained('bigcode/santacoder')\\\\nmodel = AutoModelForCausalLM.from_pretrained('bigcode/santacoder', trust_remote_code=True)\\\\ninput_code = 'class MyClass:\\\\\\\\n  # Add method for calculating area_coverage'\\\\ninputs = tokenizer.encode(input_code, return_tensors='pt')\\\\noutputs = model.generate(inputs)\\\\ngenerated_method = tokenizer.decode(outputs[0])\\\", 'api_call': \\\"AutoModelForCausalLM.from_pretrained('bigcode/santacoder', trust_remote_code=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bigcode/santacoder', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('bigcode/santacoder', trust_remote_code=True)\\\", 'api_arguments': ['inputs'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ncheckpoint = bigcode/santacoder\\\\ndevice = cuda # for GPU usage or cpu for CPU usage\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\\\\ninputs = tokenizer.encode(def print_hello_world():, return_tensors=pt).to(device)\\\\noutputs = model.generate(inputs)\\\\nprint(tokenizer.decode(outputs[0]))', 'performance': {'dataset': 'bigcode/the-stack', 'accuracy': {'pass@1 on MultiPL HumanEval (Python)': 0.18, 'pass@10 on MultiPL HumanEval (Python)': 0.29, 'pass@100 on MultiPL HumanEval (Python)': 0.49, 'pass@1 on MultiPL MBPP (Python)': 0.35, 'pass@10 on MultiPL MBPP (Python)': 0.58, 'pass@100 on MultiPL MBPP (Python)': 0.77, 'pass@1 on MultiPL HumanEval (JavaScript)': 0.16, 'pass@10 on MultiPL HumanEval (JavaScript)': 0.27, 'pass@100 on MultiPL HumanEval (JavaScript)': 0.47, 'pass@1 on MultiPL MBPP (Javascript)': 0.28, 'pass@10 on MultiPL MBPP (Javascript)': 0.51, 'pass@100 on MultiPL MBPP (Javascript)': 0.7, 'pass@1 on MultiPL HumanEval (Java)': 0.15, 'pass@10 on MultiPL HumanEval (Java)': 0.26, 'pass@100 on MultiPL HumanEval (Java)': 0.41, 'pass@1 on MultiPL MBPP (Java)': 0.28, 'pass@10 on MultiPL MBPP (Java)': 0.44, 'pass@100 on MultiPL MBPP (Java)': 0.59, 'single_line on HumanEval FIM (Python)': 0.44, 'single_line on MultiPL HumanEval FIM (Java)': 0.62, 'single_line on MultiPL HumanEval FIM (JavaScript)': 0.6, 'BLEU on CodeXGLUE code-to-text (Python)': 18.13}}, 'description': 'The SantaCoder models are a series of 1.1B parameter models trained on the Python, Java, and JavaScript subset of The Stack (v1.1) (which excluded opt-out requests). The main model uses Multi Query Attention, was trained using near-deduplication and comment-to-code ratio as filtering criteria and using the Fill-in-the-Middle objective. In addition there are several models that were trained on datasets with different filter parameters and with architecture and objective variations.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨ÙˆØ¬ÙˆØ¯ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙˆØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ§Ù‚Ø¹Ù‡Ø§ ÙÙŠ Ø§Ù„ØµÙˆØ± Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes YOLO and render_result from the ultralyticsplus package.\\\\n2. Use the YOLO class to load the pre-trained model 'keremberke/yolov8n-table-extraction' which is designed for table extraction in documents.\\\\n3. Set the model's overrides for 'conf', 'iou', 'agnostic_nms', and 'max_det' to achieve the right level of confidence threshold, intersection over union threshold, and maximum detections.\\\\n4. Provide the URL of the image containing the scanned document to the model.\\\\n5. Pass the image to the model's predict method to detect tables in the image.\\\\n6. Render and display the detected tables in the image.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-table-extraction')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8n-table-extraction', 'api_call': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8n-table-extraction')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'table-extraction', 'accuracy': 0.967}, 'description': \\\"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù‚Ø§Ù… Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¨ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø´Ø±ÙŠØ§Ù‹ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¢Ù„ÙŠØ© Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"MarianMTModel.from_pretrained('pytorch-models/opus-mt-tc-big-en-pt')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package, in this case, MarianMTModel and MarianTokenizer.\\\\n2. Define the model name 'pytorch-models/opus-mt-tc-big-en-pt', which specializes in translating from English to Portuguese.\\\\n3. Create the tokenizer and model objects using the from_pretrained method with the specified model name.\\\\n4. For each participant's speech input in the conference call, tokenize the input text and use the model to generate real-time translations.\\\\n5. Decode the translated text and display or send it to the other participants in the conference call.\\\", '<<<code>>>': \\\"from transformers import MarianMTModel, MarianTokenizer\\\\n\\\\nsrc_text = ['>por< INPUT_SPEECH']\\\\nmodel_name = 'pytorch-models/opus-mt-tc-big-en-pt'\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\n\\\\ntranslated = model.generate(**tokenizer(src_text, return_tensors='pt', padding=True))\\\\ntranslation_output = tokenizer.decode(translated[0], skip_special_tokens=True)\\\", 'api_call': \\\"MarianMTModel.from_pretrained('pytorch-models/opus-mt-tc-big-en-pt')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-tc-big-en-pt', 'api_call': \\\"MarianMTModel.from_pretrained('pytorch-models/opus-mt-tc-big-en-pt')\\\", 'api_arguments': {'src_text': 'list of text strings with language tokens'}, 'python_environment_requirements': [{'transformers': '4.16.2'}], 'example_code': 'from transformers import MarianMTModel, MarianTokenizer\\\\nsrc_text = [\\\\n &gt;&gt;por&lt;&lt; Tom tried to stab me.,\\\\n &gt;&gt;por&lt;&lt; He has been to Hawaii several times.\\\\n]\\\\nmodel_name = pytorch-models/opus-mt-tc-big-en-pt\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\ntranslated = model.generate(**tokenizer(src_text, return_tensors=pt, padding=True))\\\\nfor t in translated:\\\\n print( tokenizer.decode(t, skip_special_tokens=True) )', 'performance': {'dataset': [{'name': 'flores101-devtest', 'accuracy': 50.4}, {'name': 'tatoeba-test-v2021-08-07', 'accuracy': 49.6}]}, 'description': 'Neural machine translation model for translating from English (en) to Portuguese (pt). This model is part of the OPUS-MT project, an effort to make neural machine translation models widely available and accessible for many languages in the world.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†Øµ Ù„ØªØ·Ø¨ÙŠÙ‚Ù†Ø§ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø±Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„ØµÙˆØ± ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ Ù†Øµ Ù‚Ø§Ø¨Ù„ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø© Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary packages, including TrOCRProcessor from transformers and Image from PIL.\\\\n2. Instantiate the TrOCRProcessor with the pre-trained 'microsoft/trocr-large-handwritten' model.\\\\n3. Load the VisionEncoderDecoderModel using the from_pretrained method and the 'microsoft/trocr-large-handwritten' model.\\\\n4. Load the image of the handwritten text using the Image class from the PIL library. Convert the image to RGB format if needed.\\\\n5. Process the image with the TrOCRProcessor to obtain pixel values.\\\\n6. Use the VisionEncoderDecoderModel to generate an ID representation of the transcribed text.\\\\n7. Use the TrOCRProcessor to decode the generated IDs into text.\\\", '<<<code>>>': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\n\\\\nimage = Image.open('handwritten_image.jpg').convert('RGB')\\\\n# replace 'handwritten_image.jpg' with the path to your image\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\\n\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ntranscribed_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': [{'packages': ['transformers', 'PIL', 'requests']}], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>\"\n"}
{"text": "<s>[INST]ØªØ´Ù…Ù„ Ø£Ø¹Ù…Ø§Ù„ Ø´Ø±ÙƒØªÙ†Ø§ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµØŒ Ø­ÙŠØ« Ù†Ø±ØºØ¨ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"AutoModel.from_pretrained('deepset/roberta-base-squad2-distilled')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import 'pipeline' from the transformers library provided by Hugging Face.\\\\n2. Create the question answering pipeline using the 'pipeline' function and the specified model 'deepset/roberta-base-squad2-distilled'. This model is pre-trained on the SQuAD 2.0 dataset and is designed for question answering tasks.\\\\n3. To find the answer to your question from the paragraph, call the pipeline with the context and question as input.\\\\n4. The output 'result' will contain the answer to your question and the associated confidence score for the answer.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='deepset/roberta-base-squad2-distilled')\\\\ncontext = 'This is a paragraph with the information necessary to answer your question.'\\\\nquestion = 'What is the answer to my question?'\\\\nresult = qa_pipeline({'context': context, 'question': question})\\\\nprint(result)\\\", 'api_call': \\\"AutoModel.from_pretrained('deepset/roberta-base-squad2-distilled')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'deepset/roberta-base-squad2-distilled', 'api_call': \\\"AutoModel.from_pretrained('deepset/roberta-base-squad2-distilled')\\\", 'api_arguments': {'context': 'string', 'question': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='deepset/roberta-base-squad2-distilled')\\\\nresult = qa_pipeline({'context': 'This is a context.', 'question': 'What is this?'})\\\\nprint(result)\\\", 'performance': {'dataset': 'squad_v2', 'exact': 79.8366040596311, 'f1': 83.916407079888}, 'description': \\\"This model is a distilled version of deepset/roberta-large-squad2, trained on SQuAD 2.0 dataset for question answering tasks. It is based on the Roberta architecture and has been fine-tuned using Haystack's distillation feature.\\\" </s>\"\n"}
{"text": "<s>[INST]ØªØ¹Ù…Ù„ Ø´Ø±ÙƒØ© ØªØ·ÙˆÙŠØ± Ø¹Ù…ÙŠÙ„Ù†Ø§ Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙŠØ³ØªÙ‡Ø¯Ù Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø¨Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„ØªØ¹Ø²ÙŠØ² Ø§Ù„ØªØ¹Ù„Ù… ÙŠÙ…ÙƒÙ†Ù‡ Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ§Øª ÙÙŠ Ø§Ù„Ù†Øµ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ù„ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-hopper-medium')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, which are 'AutoModel' from transformers.\\\\n2. Use 'AutoModel.from_pretrained()' with argument 'edbeeching/decision-transformer-gym-hopper-medium' to load the trained decision transformer model for the Gym Hopper environment.\\\\n3. Once the model is loaded, enable evaluation mode by using 'model.eval()' to disable dropout and other noise that are only used during training.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('edbeeching/decision-transformer-gym-hopper-medium')\\\\nmodel.eval()\\\", 'api_call': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-hopper-medium')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'decision-transformer-gym-hopper-medium', 'api_call': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-hopper-medium')\\\", 'api_arguments': {'mean': [1.311279, -0.08469521, -0.5382719, -0.07201576, 0.04932366, 2.1066856, -0.15017354, 0.00878345, -0.2848186, -0.18540096, -0.28461286], 'std': [0.17790751, 0.05444621, 0.21297139, 0.14530419, 0.6124444, 0.85174465, 1.4515252, 0.6751696, 1.536239, 1.6160746, 5.6072536]}, 'python_environment_requirements': [{'transformers': 'latest'}], 'example_code': 'See our Blog Post, Colab notebook or Example Script for usage.', 'performance': {'dataset': 'Gym Hopper environment', 'accuracy': 'Not provided'}, 'description': 'Decision Transformer model trained on medium trajectories sampled from the Gym Hopper environment.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‡Ø¯Ù Ø´Ø±ÙƒØªÙ†Ø§ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ± ØºÙŠØ± Ø§Ù„Ù…Ø´Ø±ÙˆØ·Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ø§Ù„ØªÙŠ ØªØªÙŠØ­ Ø¥Ù†ØªØ§Ø¬ ØµÙˆØ± ØªØ´Ø¨Ù‡ Ø§Ù„ØµÙˆØ± Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ© Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù‚ÙŠØ¯ Ø§Ù„ØªØµÙ…ÙŠÙ…ØŒ ÙˆÙ†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ù„Ø¨Ù†Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ ØªÙ†Ø¨Ø¤ ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± ÙˆØ¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¬Ø¯ÙŠØ¯Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the 'diffusers' package and import the DDPMPipeline class provided by the diffusers library.\\\\n2. Use the DDPMPipeline.from_pretrained() method to load the 'myunus1/diffmodels_galaxies_scratchbook' model.\\\\n3. Call the pipeline() method to generate an image according to the model's specifications.\\\\n4. Access the image from pipeline's `images` attribute and use your preferred method to display the generated image.\\\", '<<<code>>>': 'from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\\"myunus1/diffmodels_galaxies_scratchbook\\\")\\\\ngenerated_image_data = pipeline().images[0]\\\\n# Use any image display library to show the generated image, e.g. PIL, matplotlib, or OpenCV', 'api_call': \\\"DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'myunus1/diffmodels_galaxies_scratchbook', 'api_call': \\\"DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", 'api_arguments': {'from_pretrained': 'myunus1/diffmodels_galaxies_scratchbook'}, 'python_environment_requirements': [{'package': 'diffusers', 'import': 'from diffusers import DDPMPipeline'}], 'example_code': {'initialize_pipeline': \\\"pipeline = DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", 'generate_image': 'image = pipeline().images[0]', 'display_image': 'image'}, 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'This model is a diffusion model for unconditional image generation of cute ğŸ¦‹.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø­ÙˆØ§Ø± Ø§Ù„Ø°ÙƒÙŠØŒ Ø­ÙŠØ« Ù†Ø­ØªØ§Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© ÙˆØªØ­Ø¯ÙŠØ¯ ÙØ¦ØªÙ‡Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.? [/INST]{'domain': 'Natural Language Processing Conversational', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('ToddGoldfarb/Cadet-Tiny', low_cpu_mem_usage=True)\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import necessary libraries such as torch, transformers, and the CadetTinyAgent class, which is a conversational model trained on the SODA dataset.', 'code': 'import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nfrom CadetTinyAgent_class import CadetTinyAgent\\\\n\\\\ncadet_tiny_agent = CadetTinyAgent()\\\\nuser_input = \\\"What is your favorite color?\\\"\\\\ncadet_response = cadet_tiny_agent.generate(\\\"\\\", \\\"\\\", user_input)\\\\nprint(\\\"Cadet-Tiny response:\\\", cadet_response)', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'ToddGoldfarb/Cadet-Tiny', 'api_call': 'import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nfrom CadetTinyAgent_class import CadetTinyAgent\\\\n\\\\ncadet_tiny_agent = CadetTinyAgent()\\\\nuser_input = \\\"What is your favorite color?\\\"\\\\ncadet_response = cadet_tiny_agent.generate(\\\"\\\", \\\"\\\", user_input)\\\\nprint(\\\"Cadet-Tiny response:\\\", cadet_response)', 'api_arguments': {'pretrained_model': 't5-small', 'model_max_length': 512}, 'python_environment_requirements': {'torch': '', 'transformers': '', 'colorful': ''}, 'example_code': \\\"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style('monokai')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == '<strong>main</strong>':\\\\n main()\\\", 'performance': {'dataset': 'allenai/soda', 'accuracy': ''}, 'description': 'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµØŒ Ø­ÙŠØ« Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨ÙˆÙ„Ø© Ø§Ù„ØªÙˆÙ‚Ø¹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='openai-gpt')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function and set_seed function from the transformers library provided by Hugging Face.\\\\n2. Set a random seed for reproducible results.\\\\n3. Use the pipeline function to create a text generation model.\\\\n4. Specify the model 'openai-gpt' to be loaded. This model is a transformer-based language model trained on a large corpus of text data, making it suitable for generating creative responses to user inquiries.\\\\n5. The created generator can be used to generate a variety of responses to user inputs.\\\", '<<<code>>>': 'from transformers import pipeline, set_seed\\\\nset_seed(42)\\\\nchatbot_generator = pipeline(\\\\'text-generation\\\\', model=\\\\'openai-gpt\\\\')\\\\nuser_input = \\\"How can I cook the perfect steak?\\\"\\\\nresponses = chatbot_generator(user_input, max_length=30, num_return_sequences=5)', 'api_call': \\\"pipeline('text-generation', model='openai-gpt')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai-gpt', 'api_call': \\\"pipeline('text-generation', model='openai-gpt')\\\", 'api_arguments': {'model': 'openai-gpt', 'max_length': 30, 'num_return_sequences': 5}, 'python_environment_requirements': [{'transformers': 'from transformers import pipeline, set_seed'}], 'example_code': \\\"from transformers import pipeline, set_seed\\\\n\\\\nset_seed(42)\\\\ngenerator = pipeline('text-generation', model='openai-gpt')\\\\n\\\\ngenerator(Hello, I'm a language model,, max_length=30, num_return_sequences=5)\\\", 'performance': {'dataset': [{'name': 'SNLI', 'accuracy': 89.9}, {'name': 'MNLI Matched', 'accuracy': 82.1}, {'name': 'MNLI Mismatched', 'accuracy': 81.4}, {'name': 'SciTail', 'accuracy': 88.3}, {'name': 'QNLI', 'accuracy': 88.1}, {'name': 'RTE', 'accuracy': 56.0}, {'name': 'STS-B', 'accuracy': 82.0}, {'name': 'QQP', 'accuracy': 70.3}, {'name': 'MPRC', 'accuracy': 82.3}, {'name': 'RACE', 'accuracy': 59.0}, {'name': 'ROCStories', 'accuracy': 86.5}, {'name': 'COPA', 'accuracy': 78.6}, {'name': 'SST-2', 'accuracy': 91.3}, {'name': 'CoLA', 'accuracy': 45.4}, {'name': 'GLUE', 'accuracy': 72.8}]}, 'description': 'openai-gpt is a transformer-based language model created and released by OpenAI. The model is a causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long-range dependencies.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ø¬Ø¯ÙˆÙ„ ØªØµÙ†ÙŠÙÙŠ ÙŠØªØ¶Ù…Ù† Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„ØªØ§Ù„ÙŠØ©: Ø§Ù„ÙØ¦Ø©ØŒ Ø§Ù„Ø³Ù…Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ØŒ Ø§Ù„Ø³Ù…Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© ÙˆØ§Ù„ØªØµÙ†ÙŠÙØŒ ÙˆØ§Ù…Ù„Ø£ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø¨Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¹Ø±Ø¶ ØªØµÙ†ÙŠÙ Ù…Ø¹ÙŠÙ† Ù„Ø¹Ù†Ø§ØµØ± Ù…Ø®ØªÙ„ÙØ©.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': '1. Import required libraries: joblib, json, and pandas.\\\\n2. Load the pre-trained carbon emissions classification model using joblib.load().\\\\n3. Open the configuration file for the model (config.json) and extract required features.\\\\n4. Preprocess the input dataset by applying the necessary transformations and selecting only required features.\\\\n5. Use the model.predict() method to get the category predictions for the given dataset entries.\\\\n6. As a result, you will get the category predictions for the carbon emissions of the given dataset.', '<<<code>>>': \\\"import joblib\\\\nimport json\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tejas23/autotrain-amx2-1702259729', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'json', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'tejas23/autotrain-data-amx2', 'accuracy': 0.824}, 'description': 'A multi-class classification model for predicting carbon emissions.' </s>\"\n"}
{"text": "<s>[INST]Ù„ØªÙˆÙ„ÙŠØ¯ ØªØ³Ø¬ÙŠÙ„ Ù…Ø³Ù…ÙˆØ¹ Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙƒØªÙˆØ¨ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…. ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… API Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ. ÙˆÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„ÙÙ†ÙŠØ© ÙˆØ§Ù„ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ù„Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"pipeline('automatic-speech-recognition', model='facebook/wav2vec2-base-960h')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. Start by importing the necessary libraries. In this case, you need to import the 'pipeline' function from the 'transformers' library.\\\", 'code': \\\"from transformers import pipeline\\\\nasr = pipeline('automatic-speech-recognition', model='facebook/wav2vec2-base-960h')\\\\ntranscription = asr(audio_file_path)\\\", 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Transformers', 'functionality': 'Transcription', 'api_name': 'facebook/wav2vec2-base-960h', 'api_call': \\\"from transformers import pipeline\\\\nasr = pipeline('automatic-speech-recognition', model='facebook/wav2vec2-base-960h')\\\\ntranscription = asr(audio_file_path)\\\", 'api_arguments': ['input_values'], 'python_environment_requirements': ['transformers', 'datasets', 'torch', 'jiwer'], 'example_code': \\\"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\\\\nds = load_dataset('patrickvonplaten/librispeech_asr_dummy', 'clean', split='validation')\\\\ninput_values = processor(ds[0]['audio']['array'], return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'performance': {'dataset': 'LibriSpeech', 'accuracy': {'clean': 3.4, 'other': 8.6}}, 'description': \\\"Facebook's Wav2Vec2 base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It is designed for automatic speech recognition and can transcribe audio files.\\\"}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries. In this case, we use the 'pipeline' function from the transformers package.\\\\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'moussaKam/barthez-orangesum-abstract'.\\\\n3. The loaded model will be used for the summarization task, which generates a concise and informative summary of the given lengthy article.\\\\n4. The model will process the input text (the long article in French) and produce a summary as output.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'moussaKam/barthez-orangesum-abstract\\\\')\\\\nfrench_article = \\\"Long French article goes here...\\\"\\\\nsummary = summarizer(french_article)', 'api_call': \\\"BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'moussaKam/barthez-orangesum-abstract', 'api_call': \\\"BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'orangeSum', 'accuracy': ''}, 'description': 'Barthez model finetuned on orangeSum for abstract generation in French language' </s>\"\n"}
{"text": "<s>[INST]Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ù…ØªØ¹Ù„Ù‚ Ø¨ØªÙ‚Ù†ÙŠØ© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© NLP ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ© Fill-Mask. ÙŠØ¬Ø¨ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„ØªÙˆÙ‚Ø¹ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ.? [/INST]{'domain': 'Natural Language Processing Fill-Mask', 'api_call': \\\"pipeline('fill-mask', model='microsoft/deberta-v3-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\nfill_mask = pipeline('fill-mask', model='microsoft/deberta-v3-base')\\\\nsentence_with_mask = 'The quick [MASK] fox jumps over the lazy dog.'\\\\nresult = fill_mask(sentence_with_mask)\\\", 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Fill-Mask', 'api_name': 'microsoft/deberta-v3-base', 'api_call': \\\"from transformers import pipeline\\\\nfill_mask = pipeline('fill-mask', model='microsoft/deberta-v3-base')\\\\nsentence_with_mask = 'The quick [MASK] fox jumps over the lazy dog.'\\\\nresult = fill_mask(sentence_with_mask)\\\", 'api_arguments': ['model_name_or_path'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \\\"from transformers import pipeline\\\\nfill_mask = pipeline('fill-mask', model='microsoft/deberta-v3-base')\\\\nresult = fill_mask('Hugging Face is a [MASK] company.')\\\", 'performance': {'dataset': {'SQuAD 2.0': {'F1': 88.4, 'EM': 85.4}, 'MNLI-m/mm': {'ACC': '90.6/90.7'}}}, 'description': 'DeBERTa V3 improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It further improves the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. The DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer. This model was trained using the 160GB data as DeBERTa V2.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨Ø¬Ù‡Ø§Øª Ù…Ù† ØºÙŠØ± Ø§Ù„Ù…Ù†Ø¶Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØµÙØ± Ø§Ù„Ø´Ø§Ù…Ù„Ø©ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨ØªØµÙ†ÙŠÙ Ù†Øµ Ù…Ø¹ÙŠÙ† Ø¥Ù„Ù‰ ÙØ¦Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Use the pipeline function from the transformers library provided by Hugging Face. \\\\n2. Specify the task 'zero-shot-classification' and the model 'cross-encoder/nli-deberta-v3-xsmall' to create a zero-shot classification model.\\\\n3. This model is trained for natural language inference tasks, and it can be used for zero-shot classification tasks as well.\\\\n4. The created classifier can be used to analyze the text of news articles and categorize them into topics like technology, sports, or politics without the need for labeled training data.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'cross-encoder/nli-deberta-v3-xsmall\\\\')\\\\nsent = \\\"Apple just announced the newest iPhone X\\\"\\\\ncandidate_labels = [\\\"technology\\\", \\\"sports\\\", \\\"politics\\\"]\\\\nres = classifier(sent, candidate_labels)\\\\nprint(res)', 'api_call': \\\"pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cross-encoder/nli-deberta-v3-xsmall', 'api_call': \\\"pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\\\", 'api_arguments': ['sent', 'candidate_labels'], 'python_environment_requirements': [['sentence_transformers', 'transformers']], 'example_code': \\\"from transformers import pipeline\\\\nclassifier = pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\\\\nsent = 'Apple just announced the newest iPhone X'\\\\ncandidate_labels = ['technology', 'sports', 'politics']\\\\nres = classifier(sent, candidate_labels)\\\\nprint(res)\\\", 'performance': {'dataset': {'SNLI-test': '91.64', 'MNLI_mismatched': '87.77'}}, 'description': 'This model is a Cross-Encoder for Natural Language Inference, trained on the SNLI and MultiNLI datasets. It can be used for zero-shot classification tasks.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø¹Ù„ÙˆÙŠ Ù…ÙØ®ØµØµ Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„Ø£Ø´ÙƒØ§Ù„ Ø§Ù„Ù…Ø®ØªØµØ© Ø¨Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§ØªØŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªØ¬Ø§Ù‡Ù„ Ø£ÙŠ Ø±Ù…ÙˆØ² Ø£Ùˆ Ù…Ù„ØµÙ‚Ø§Øª Ù„Ø§ ØªØ¨Ø¯Ùˆ ÙˆØ§Ø¶Ø­Ø© ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø£Ùˆ Ù„Ø§ ØªØ³Ù‡Ù… ÙÙŠ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§ØªØŒ Ø¨Ø¹Ø¯ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ù†ØµØŒ ÙŠØ±Ø¬Ù‰ ÙƒØªØ§Ø¨Ø© Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ø¬Ù…Ù„Ø© \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\" Ø¨Ù…Ø¹Ø§ÙˆÙ†Ø© ØªØ¹Ø¨ÙŠØ± Ø§Ù„Ù…Ù„Ø¡.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To accomplish this task, we first import the necessary classes from the ultralyticsplus package. This includes YOLO, which is an object detection model.\\\\n2. We then use the YOLO function to load the pre-trained model 'keremberke/yolov8n-table-extraction'. This model has been trained for table extraction tasks and is capable of identifying both bordered and borderless tables.\\\\n3. We can then use this model to analyze an image of a form and extract the tables present in it. To do that, we provide several parameters: confidence threshold, IoU threshold, Non-Maximum Suppression setting, and maximum number of detections.\\\\n4. We use the predict function to obtain the table's bounding boxes and display the image with the detected tables using the render_result function.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-table-extraction')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path_to_your_image.jpg'\\\\n# replace 'path_to_your_image.jpg' with the path to your form image\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8n-table-extraction', 'api_call': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8n-table-extraction')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'table-extraction', 'accuracy': 0.967}, 'description': \\\"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\\\" </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ ÙˆÙ†Ø³Ø¹Ù‰ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© ÙˆØ§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø¯Ù‚Ø© Ø£ÙØ¶Ù„ ÙÙŠ Ø§Ù„ØªØµÙ†ÙŠÙ.? [/INST]{'domain': 'Multimodal Feature Extraction', 'api_call': \\\"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary libraries, which are 'HubertModel' from transformers.\\\", 'code': \\\"from transformers import HubertModel\\\\nhubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\\n# Process the input speech and obtain the speech embeddings\\\\nspeech_embeddings = hubert(speech_input)\\\", 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \\\"from transformers import HubertModel\\\\nhubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\\n# Process the input speech and obtain the speech embeddings\\\\nspeech_embeddings = hubert(speech_input)\\\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \\\"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§ØªÙŠØ©ØŒ ÙˆÙ†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ØªØ£Ø®Ø° ÙÙŠ Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø± ÙƒÙŠÙÙŠØ© Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø£Ø³Ø§Ù„ÙŠØ¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"pipeline('conversational', model='mywateriswet/ShuanBot')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' method from transformers.\\\\n2. Use the 'pipeline' function to load the 'mywateriswet/ShuanBot' model, which is a conversational chatbot based on GPT-2 architecture.\\\\n3. The model can be used to generate human-like responses during a chat. For this task, we can prompt the model with a question or request related to movie script ideas.\\\\n4. The model is expected to produce a creative response containing a storyline.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nchatbot = pipeline(\\\\'conversational\\\\', model=\\\\'mywateriswet/ShuanBot\\\\')\\\\nprompt = \\\"Provide me with a movie script idea about a thrilling adventure.\\\"\\\\nresponse = chatbot(prompt)\\\\nprint(response[0][\\\\'generated_text\\\\'])', 'api_call': \\\"pipeline('conversational', model='mywateriswet/ShuanBot')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mywateriswet/ShuanBot', 'api_call': \\\"pipeline('conversational', model='mywateriswet/ShuanBot')\\\", 'api_arguments': 'message', 'python_environment_requirements': ['transformers'], 'example_code': \\\"response = chatbot('What is your name?')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.' </s>\"\n"}
{"text": "<s>[INST]ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØŒ ÙŠÙØ·Ù„Ø¨ Ù…Ù† Ø§Ù„Ø¨Ø§Ø­Ø«ÙŠÙ† ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªÙ…ÙŠÙŠØ² ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©. ÙŠÙØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„.? [/INST]{'domain': 'Audio Audio Classification', 'api_call': \\\"pipeline('audio-classification', model=Wav2Vec2ForCTC.from_pretrained('anton-l/wav2vec2-random-tiny-classifier'))\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import Wav2Vec2ForCTC, pipeline\\\\ncommercial_classifier = pipeline('audio-classification', model=Wav2Vec2ForCTC.from_pretrained('anton-l/wav2vec2-random-tiny-classifier'))\\\\nclassification_result = commercial_classifier(podcast_audio_path)\\\", 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'wav2vec2-random-tiny-classifier', 'api_call': \\\"from transformers import Wav2Vec2ForCTC, pipeline\\\\ncommercial_classifier = pipeline('audio-classification', model=Wav2Vec2ForCTC.from_pretrained('anton-l/wav2vec2-random-tiny-classifier'))\\\\nclassification_result = commercial_classifier(podcast_audio_path)\\\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'An audio classification model based on wav2vec2.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø¬ÙˆØ§Ù†Ø¨ Ø¹Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†Øµ. Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªÙ†ÙˆØ¹ ÙÙŠ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ù†ØµÙˆØµ Ù„Ø¶Ù…Ø§Ù† ÙƒÙØ§Ø¡Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø§Ù„ØªØ­ÙˆÙŠÙ„.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes and functions from the torch, requests, PIL, and transformers packages.\\\\n2. We are using the pre-trained 'ydshieh/vit-gpt2-coco-en' model, which is a VisionEncoderDecoderModel - this combines an image processing model (ViT) and a language modeling model (GPT-2) to generate captions.\\\\n3. Load the model and set it to evaluation mode.\\\\n4. Define a function (predict) that takes an image as input, extracts features from the image using the ViTFeatureExtractor, generates text based on the image features using the language model, and returns a list of generated captions.\\\\n5. To generate a caption for an image, call the predict function with the image as the argument.\\\", '<<<code>>>': \\\"from torch import no_grad\\\\nfrom PIL import Image\\\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\\\n\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\\\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('ydshieh/vit-gpt2-coco-en')\\\\ntokenizer = AutoTokenizer.from_pretrained('ydshieh/vit-gpt2-coco-en')\\\\nmodel.eval()\\\\n\\\\ndef generate_caption(image):\\\\n    pixel_values = feature_extractor(images=image, return_tensors='pt').pixel_values\\\\n    with no_grad():\\\\n        output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\\\n    captions = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\\\n    return captions\\\\n\\\\nimage = Image.open('path/to/image') # replace 'path/to/image' with the path to your image\\\\ncaptions = generate_caption(image)\\\\nprint(captions)\\\", 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\\\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': [['torch', 'requests', 'PIL', 'transformers']], 'example_code': 'import torch\\\\nimport requests\\\\nfrom PIL import Image\\\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\\\nloc = ydshieh/vit-gpt2-coco-en\\\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\\\nmodel.eval()\\\\ndef predict(image):\\\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\\\n with torch.no_grad():\\\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\\\n preds = [pred.strip() for pred in preds]\\\\n return preds\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\\\n preds = predict(image)\\\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙˆÙŠØ±ÙŠ ÙŠØ¹ÙƒØ³ Ù†Øµ Ù‚ØµÙŠØ± Ù…Ù† Ù…Ø¬Ø§Ù„ ØªØ±Ø¬Ù…Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary packages, including the MarianMTModel and MarianTokenizer from the transformers library.\\\\n2. Instantiate the pretrained model and tokenizer for translating from Romance languages to English using 'Helsinki-NLP/opus-mt-ROMANCE-en'.\\\\n3. Convert the input phrase from a Romance language to English by first tokenizing it with the tokenizer and then feeding it to the model for translation. Finally, use the decode() method to convert the tokens back to a human-readable English string.\\\", '<<<code>>>': 'from transformers import MarianMTModel, MarianTokenizer\\\\nmodel = MarianMTModel.from_pretrained(\\\\'Helsinki-NLP/opus-mt-ROMANCE-en\\\\')\\\\ntokenizer = MarianTokenizer.from_pretrained(\\\\'Helsinki-NLP/opus-mt-ROMANCE-en\\\\')\\\\nsrc_text = \\\\'Phrase in a romance language here...\\\\'\\\\ninput_tokens = tokenizer(src_text, return_tensors=\\\"pt\\\")\\\\ntranslated_tokens = model.generate(**input_tokens)\\\\nresult = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)', 'api_call': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-ROMANCE-en', 'api_call': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", 'api_arguments': ['source languages', 'target languages'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': '', 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': 62.2, 'chr-F': 0.75}}, 'description': 'A model for translating Romance languages to English, trained on the OPUS dataset. It supports multiple source languages such as French, Spanish, Portuguese, Italian, and Romanian, among others. The model is based on the transformer architecture and uses normalization and SentencePiece for pre-processing.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… ÙŠØ³ØªØ·ÙŠØ¹ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙˆØ·Ø¨ÙŠØ¹ÙŠ. Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ¥Ù†Ø´Ø§Ø¡ Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ Ù…Ø¨ØªÙƒØ± ÙˆØ°ÙƒÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='facebook/opt-125m')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a text generation model.\\\\n3. Specify the model \\\\'facebook/opt-125m\\\\' to be loaded. This model is a decoder-only pre-trained transformer that can generate text.\\\\n4. Use the created text generation model to generate a piece of writing starting with the prompt \\\"It was a dark and stormy night...\\\".', '<<<code>>>': 'from transformers import pipeline\\\\ntext_generator = pipeline(\\\\'text-generation\\\\', model=\\\\'facebook/opt-125m\\\\')\\\\ninitial_sentence = \\\"It was a dark and stormy night...\\\"\\\\ngenerated_text = text_generator(initial_sentence, do_sample=True, max_length=100)\\\\nstory = generated_text[0][\\\\'generated_text\\\\']', 'api_call': \\\"pipeline('text-generation', model='facebook/opt-125m')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/opt-125m', 'api_call': \\\"pipeline('text-generation', model='facebook/opt-125m')\\\", 'api_arguments': {'do_sample': 'True'}, 'python_environment_requirements': ['from transformers import pipeline, set_seed'], 'example_code': \\\"generator(Hello, I'm am conscious and)\\\", 'performance': {'dataset': 'Various', 'accuracy': 'Roughly matches GPT-3 performance'}, 'description': 'OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. It was predominantly pretrained with English text, but a small amount of non-English data is present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective. OPT can be used for prompting for evaluation of downstream tasks as well as text generation.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ².? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"SequenceTagger.load('flair/ner-english-large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the flair library, such as Sentence and SequenceTagger.\\\\n2. We load the pre-trained SequenceTagger model 'flair/ner-english-large', which can recognize named entities, such as person names, organization names, and location names.\\\\n3. We create a Sentence object from the given text using the Sentence class.\\\\n4. The tagger.predict method is used to predict the named entities in the sentence.\\\\n5. We print the detected named entities by iterating over the sentence's spans and filtering for organizations specifically.\\\", '<<<code>>>': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\ntagger = SequenceTagger.load('flair/ner-english-large')\\\\n\\\\nsentence = Sentence(text)\\\\ntagger.predict(sentence)\\\\n\\\\norganization_entities = [entity for entity in sentence.get_spans('ner') if entity.tag == 'ORG']\\\\n\\\\nfor entity in organization_entities:\\\\n    print(entity.text)\\\", 'api_call': \\\"SequenceTagger.load('flair/ner-english-large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'flair/ner-english-large', 'api_call': \\\"SequenceTagger.load('flair/ner-english-large')\\\", 'api_arguments': 'sentence', 'python_environment_requirements': ['Flair (pip install flair)'], 'example_code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\n# load tagger\\\\ntagger = SequenceTagger.load(flair/ner-english-large)\\\\n\\\\n# make example sentence\\\\nsentence = Sentence(George Washington went to Washington)\\\\n\\\\n# predict NER tags\\\\ntagger.predict(sentence)\\\\n\\\\n# print sentence\\\\nprint(sentence)\\\\n\\\\n# print predicted NER spans\\\\nprint('The following NER tags are found:')\\\\n\\\\n# iterate over entities and print\\\\nfor entity in sentence.get_spans('ner'):\\\\n print(entity)\\\", 'performance': {'dataset': 'conll2003', 'accuracy': '94.36'}, 'description': 'This is the large 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on document-level XLM-R embeddings and FLERT.' </s>\"\n"}
{"text": "<s>[INST]Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªØ¹Ù„Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ÙŠØªÙ… Ø¥Ø¯Ø®Ø§Ù„Ù‡Ø§ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©ØŒ Ù…Ø«Ù„ Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨ ÙˆØ§Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª Ø§Ù„Ù…ØªØ­Ø±ÙƒØ©.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-halfcheetah-expert')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary AutoModel class from the transformers package.\\\\n2. Use the from_pretrained method of the AutoModel class to load the pre-trained model 'edbeeching/decision-transformer-gym-halfcheetah-expert'. This model has been trained on expert trajectories sampled from the Gym HalfCheetah environment, which can be useful to study the movement of a robotic cheetah.\\\\n3. Utilize the model's predictions to analyze the movements of the robotic cheetah and apply them to your own robot's legs to improve its flexibility and movement.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('edbeeching/decision-transformer-gym-halfcheetah-expert')\\\", 'api_call': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-halfcheetah-expert')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'edbeeching/decision-transformer-gym-halfcheetah-expert', 'api_call': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-halfcheetah-expert')\\\", 'api_arguments': {'mean': [-0.04489148, 0.03232588, 0.06034835, -0.17081226, -0.19480659, -0.05751596, 0.09701628, 0.03239211, 11.047426, -0.07997331, -0.32363534, 0.36297753, 0.42322603, 0.40836546, 1.1085187, -0.4874403, -0.0737481], 'std': [0.04002118, 0.4107858, 0.54217845, 0.41522816, 0.23796624, 0.62036866, 0.30100912, 0.21737163, 2.2105937, 0.572586, 1.7255033, 11.844218, 12.06324, 7.0495934, 13.499867, 7.195647, 5.0264325]}, 'python_environment_requirements': [['transformers']], 'example_code': 'See our Blog Post, Colab notebook or Example Script for usage.', 'performance': {'dataset': 'Gym HalfCheetah environment', 'accuracy': 'Not specified'}, 'description': 'Decision Transformer model trained on expert trajectories sampled from the Gym HalfCheetah environment' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØºØ© Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ù†Øµ ÙŠØ¨Ø¯Ø£ Ø¨Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='bigscience/bloom-560m')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We can use a text generation model to come up with an introduction paragraph.\\\\n2. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n3. Use the pipeline function to create a text generation model.\\\\n4. Specify the model 'bigscience/bloom-560m' to be loaded. This is a large-scale text generation model trained on multiple languages, making it suitable for generating content for an article in your language of choice.\\\\n5. Now, use the created model to generate an introductory paragraph based on the given input.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntext_generator = pipeline('text-generation', model='bigscience/bloom-560m')\\\\nprompt = 'The history of artificial intelligence'\\\\ngenerated_text = text_generator(prompt)\\\\nprint(generated_text[0]['generated_text'])\\\", 'api_call': \\\"pipeline('text-generation', model='bigscience/bloom-560m')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bigscience/bloom-560m', 'api_call': \\\"pipeline('text-generation', model='bigscience/bloom-560m')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline\\\\n\\\\nmodel_name = 'bigscience/bloom-560m'\\\\napi = pipeline('text-generation', model=model_name)\\\\n\\\\ntext = 'The history of artificial intelligence began in the '\\\\noutput = api(text)\\\\nprint(output[0]['generated_text'])\\\", 'performance': {'dataset': 'Validation', 'accuracy': {'Training Loss': 2.0, 'Validation Loss': 2.2, 'Perplexity': 8.9}}, 'description': 'BLOOM LM is a large open-science, open-access multilingual language model developed by BigScience. It is a transformer-based language model trained on 45 natural languages and 12 programming languages. The model has 559,214,592 parameters, 24 layers, and 16 attention heads.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import M2M100ForConditionalGeneration and M2M100Tokenizer classes from transformers.\\\\n2. Load the pre-trained M2M100 model and its corresponding tokenizer using the from_pretrained method and 'facebook/m2m100_418M' as the model identifier.\\\\n3. Set the source language in the tokenizer by assigning the language code (i.e., 'fr' for French, 'de' for German, etc.) to the src_lang attribute of the tokenizer.\\\\n4. Tokenize the input text using the tokenizer, and include the return_tensors parameter to produce a PyTorch tensor.\\\\n5. Use the generate method on the loaded model, forcing the first generated token to be the target language code (i.e., 'en' for English) using forced_bos_token_id parameter.\\\\n6. Decode the generated tokens using the batch_decode method of the tokenizer, setting skip_special_tokens parameter to True. This will provide the English-translated text.\\\", '<<<code>>>': 'from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\\\nsource_text = \\\"User comment text in their native language\\\"\\\\ntokenizer = M2M100Tokenizer.from_pretrained(\\\\'facebook/m2m100_418M\\\\')\\\\nmodel = M2M100ForConditionalGeneration.from_pretrained(\\\\'facebook/m2m100_418M\\\\')\\\\ntokenizer.src_lang = \\\"native_language_code\\\" # replace with the appropriate code of the native language\\\\nencoded_input = tokenizer(source_text, return_tensors=\\\\'pt\\\\')\\\\ngenerated_tokens = model.generate(**encoded_input, forced_bos_token_id=tokenizer.get_lang_id(\\\\'en\\\\'))\\\\ntranslated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)', 'api_call': \\\"M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/m2m100_418M', 'api_call': \\\"M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')\\\", 'api_arguments': {'encoded_input': 'Encoded input text', 'target_lang': 'Target language code'}, 'python_environment_requirements': [['transformers', 'sentencepiece']], 'example_code': ['from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer', 'hi_text = ', 'chinese_text = ', 'model = M2M100ForConditionalGeneration.from_pretrained(facebook/m2m100_418M)', 'tokenizer = M2M100Tokenizer.from_pretrained(facebook/m2m100_418M)', 'tokenizer.src_lang = hi', 'encoded_hi = tokenizer(hi_text, return_tensors=pt)', 'generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(fr))', 'tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)'], 'performance': {'dataset': 'WMT', 'accuracy': 'Not provided'}, 'description': 'M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© ØªØµÙ†ÙŠÙÙŠØ© ØªØªØ¹Ù„Ù‚ Ø¨Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ØŒ ÙˆØ¶Ø¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø­ÙŠØ« ØªØ³Ø§Ø¹Ø¯ ÙÙŠ ÙÙ‡Ù… ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒÙ…ÙŠØ© ÙˆØ§Ù„Ù†ØµÙˆØµ Ø§Ù„ÙˆØ§Ø±Ø¯Ø© ÙÙŠ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ ÙˆÙØ¹Ø§Ù„.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"load_model(cached_download(hf_hub_url('danupurnomo/dummy-titanic', 'titanic_model.h5')))\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': \\\"1. Import the required libraries: huggingface_hub for loading the model, joblib for deserializing the saved model, pandas and numpy for data manipulation, and tensorflow for using the model.\\\\n2. Use the REPO_ID 'danupurnomo/dummy-titanic' to specify the model repository and PIPELINE_FILENAME and TF_FILENAME to specify the model file names.\\\\n3. Load the preprocessing pipeline and model from the Hugging Face model hub using hf_hub_url, cached_download, and load_model functions.\\\\n4. With the loaded model and pipeline, pass in new data as a pandas DataFrame. It needs to contain features such as passenger class, age, sex, fare, and others as specified during model training.\\\\n5. Use the preprocessing pipeline to transform the new data and obtain predictions using the TensorFlow model.\\\", '<<<code>>>': \\\"from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nfrom tensorflow.keras.models import load_model\\\\nREPO_ID = 'danupurnomo/dummy-titanic'\\\\nPIPELINE_FILENAME = 'final_pipeline.pkl'\\\\nTF_FILENAME = 'titanic_model.h5'\\\\nmodel_pipeline = joblib.load(cached_download(\\\\n    hf_hub_url(REPO_ID, PIPELINE_FILENAME)\\\\n))\\\\nmodel_seq = load_model(cached_download(\\\\n    hf_hub_url(REPO_ID, TF_FILENAME)\\\\n))\\\\nnew_data = pd.DataFrame(new_data, columns=['Pclass', 'Age', 'Sex', 'Fare',...])\\\\ntransformed_data = model_pipeline.transform(new_data)\\\\npredictions = model_seq.predict(transformed_data)\\\", 'api_call': \\\"load_model(cached_download(hf_hub_url('danupurnomo/dummy-titanic', 'titanic_model.h5')))\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'danupurnomo/dummy-titanic', 'api_call': \\\"load_model(cached_download(hf_hub_url('danupurnomo/dummy-titanic', 'titanic_model.h5')))\\\", 'api_arguments': ['new_data'], 'python_environment_requirements': [['huggingface_hub', 'joblib', 'pandas', 'numpy', 'tensorflow']], 'example_code': \\\"from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nfrom tensorflow.keras.models import load_model\\\\nREPO_ID = 'danupurnomo/dummy-titanic'\\\\nPIPELINE_FILENAME = 'final_pipeline.pkl'\\\\nTF_FILENAME = 'titanic_model.h5'\\\\nmodel_pipeline = joblib.load(cached_download(\\\\n hf_hub_url(REPO_ID, PIPELINE_FILENAME)\\\\n))\\\\nmodel_seq = load_model(cached_download(\\\\n hf_hub_url(REPO_ID, TF_FILENAME)\\\\n))\\\", 'performance': {'dataset': 'Titanic', 'accuracy': 'Not provided'}, 'description': 'This model is a binary classifier for predicting whether a passenger on the Titanic survived or not, based on features such as passenger class, age, sex, fare, and more.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª Ø¨ÙŠØ¦Ø© ØªØ·ÙˆÙŠØ± Ù…Ø´ØªØ±ÙƒØ© Ù„ØªØ´ØºÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¹Ù…Ù‚ Ø¨Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆÙ‚Ù… Ø¨ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ù…Ù‚ Ø§Ù„Ù…Ø³ØªÙ†ØªØ¬. Ù‚Ù… Ø¨Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù…Ø¹ Ù…Ø´Ø§Ø±ÙƒÙŠ Ø§Ù„Ù†Ø§Ø¯ÙŠ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ¹Ù„ÙŠÙ‚Ø§Øª ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To achieve depth estimation, we will use a pre-trained model from the Hugging Face Model Hub, specifically 'sayakpaul/glpn-nyu-finetuned-diode-221116-104421'.\\\\n2. We first import the required package AutoModel from transformers and load the pre-trained depth estimation model using the AutoModel.from_pretrained() method.\\\\n3. Once you have the model, you can feed the required input image (acquired from the camera feed) into the model, which will then process the image and provide an output depth map.\\\\n4. This depth map can be used to determine the distance of various objects in the scene from the camera.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\\\\n# Process the input image and get the depth map prediction\\\\n# depth_map = model(<input_image_prepared>)\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221116-104421', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3736, 'Mae': 0.3079, 'Rmse': 0.4321, 'Abs Rel': 0.3666, 'Log Mae': 0.1288, 'Log Rmse': 0.1794, 'Delta1': 0.4929, 'Delta2': 0.7934, 'Delta3': 0.9234}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø©. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø§Ø¯Ø±Ø§Ù‹ Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù„ØºÙˆÙŠØ© Ø¨Ø¯Ù‚Ø© Ù„Ø¶Ù…Ø§Ù† ÙƒÙØ§Ø¡Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙˆØ¯Ù‚ØªÙ‡Ø§.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"AutoModel.from_pretrained('deepset/bert-medium-squad2-distilled')\\\", 'api_provider': 'PyTorch Transformers', 'explanation': '', 'code': \\\"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\\\\ntokenizer = AutoTokenizer.from_pretrained('deepset/bert-medium-squad2-distilled')\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained('deepset/bert-medium-squad2-distilled')\\\\ninputs = tokenizer(question, context, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nanswer_start = outputs.start_logits.argmax().item()\\\\nanswer_end = outputs.end_logits.argmax().item() + 1\\\\nanswer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\\\", 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'PyTorch Transformers', 'functionality': 'Question Answering', 'api_name': 'deepset/bert-medium-squad2-distilled', 'api_call': \\\"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\\\\ntokenizer = AutoTokenizer.from_pretrained('deepset/bert-medium-squad2-distilled')\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained('deepset/bert-medium-squad2-distilled')\\\\ninputs = tokenizer(question, context, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nanswer_start = outputs.start_logits.argmax().item()\\\\nanswer_end = outputs.end_logits.argmax().item() + 1\\\\nanswer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\\\", 'api_arguments': {'model': 'deepset/bert-medium-squad2-distilled'}, 'python_environment_requirements': {'transformers': '>=4.0.0'}, 'example_code': \\\"qa({'context': 'This is an example context.', 'question': 'What is this example about?'})\\\", 'performance': {'dataset': 'squad_v2', 'exact': 68.6431398972458, 'f1': 72.7637083790805}, 'description': 'This model is a distilled version of deepset/bert-large-uncased-whole-word-masking-squad2, trained on the SQuAD 2.0 dataset for question answering tasks. It is based on the BERT-medium architecture and uses the Hugging Face Transformers library.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø¹Ù…Ù„ÙŠØ© ØªØ­Ù„ÙŠÙ„ ØªØ§Ø¨Ø¹Ù‡ Ù„ØªÙˆÙ‚Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ø­ØªØ³Ø§Ø¨ Ø§Ù„ØªØ§Ø¨Ø¹.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': \\\"1. We first import the necessary function from the Scikit-learn package. This includes pipeline, a function that allows us to quickly create machine learning pipelines.\\\\n2. We then use the pipeline function to create a tabular regression pipeline. We specify the type of pipeline as 'tabular-regression' and load the pre-trained model 'merve/tips9y0jvt5q-tip-regression', which has been trained on tips data for regression tasks.\\\\n3. This pipeline can then be used to perform regression on the customer tipping data, helping us better understand the patterns present in the data.\\\\n4. You will need to install the `dabl` package if it's not already present.\\\", '<<<code>>>': \\\"from sklearn.pipeline import pipeline\\\\nimport dabl\\\\n\\\\nmodel = pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\\\\n# You can then use model.fit(X_train, y_train) and model.predict(X_test) to train and predict, respectively.\\\", 'api_call': \\\"pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'merve/tips9y0jvt5q-tip-regression', 'api_call': \\\"pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['dabl'], 'example_code': 'N/A', 'performance': {'dataset': 'tips9y0jvt5q', 'accuracy': {'r2': 0.41524, 'neg_mean_squared_error': -1.098792}}, 'description': 'Baseline Model trained on tips9y0jvt5q to apply regression on tip. The model uses Ridge(alpha=10) and is trained with dabl library as a baseline. For better results, use AutoTrain.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠØ±Ø¬Ù‰ ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø¬Ù‡ÙˆØ¯ Ù†Ø­Ùˆ Ø¬Ù…Ø¹ ÙˆØªØµÙ†ÙŠÙ ÙƒÙ…ÙŠØ© ÙƒØ¨ÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØŒ Ø«Ù… ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ø¥Ù„Ù‰ ØªØ¬Ù…Ø¹Ø§Øª ØŒ ÙˆØ¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯Ù‚Ø©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„ÙƒØŸ? [/INST]{'domain': 'Natural Language Processing Text Classification', 'api_call': \\\"pipeline('text-classification', model='roberta-base-openai-detector')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the pipeline function from the transformers library.', 'code': 'from transformers import pipeline\\\\ndetector = pipeline(\\\\'text-classification\\\\', model=\\\\'roberta-base-openai-detector\\\\')\\\\nresult = detector(\\\"Hello world! Is this content AI-generated?\\\")', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Transformers', 'functionality': 'Detect GPT-2 generated text', 'api_name': 'roberta-base-openai-detector', 'api_call': 'from transformers import pipeline\\\\ndetector = pipeline(\\\\'text-classification\\\\', model=\\\\'roberta-base-openai-detector\\\\')\\\\nresult = detector(\\\"Hello world! Is this content AI-generated?\\\")', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import pipeline\\\\npipe = pipeline(text-classification, model=roberta-base-openai-detector)\\\\nprint(pipe(Hello world! Is this content AI-generated?))', 'performance': {'dataset': 'WebText', 'accuracy': '95%'}, 'description': 'RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ù…Ù†Ùƒ Ù„ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©. Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªÙŠ ØªØ¹ØªØ²Ù… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙÙŠ Ø§Ù„Ù…Ù‡Ù…Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"pipeline('summarization', model='google/pegasus-xsum')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. The pipeline function is used to create a summarization model which is capable of summarizing large text.\\\\n3. We specify the model 'google/pegasus-xsum' to be loaded. This is a state-of-the-art model for abstractive text summarization based on the Transformer architecture.\\\\n4. Using the created summarizer, we can input the lengthy news article and get a summarized version of the content in a brief format.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nsummarizer = pipeline('summarization', model='google/pegasus-xsum')\\\\nsummary = summarizer(long_news_article, max_length=100, min_length=25, do_sample=False)[0]['summary_text']\\\", 'api_call': \\\"pipeline('summarization', model='google/pegasus-xsum')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/pegasus-xsum', 'api_call': \\\"pipeline('summarization', model='google/pegasus-xsum')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': [{'name': 'xsum', 'accuracy': {'ROUGE-1': 46.862, 'ROUGE-2': 24.453, 'ROUGE-L': 39.055, 'ROUGE-LSUM': 39.099}}, {'name': 'cnn_dailymail', 'accuracy': {'ROUGE-1': 22.206, 'ROUGE-2': 7.67, 'ROUGE-L': 15.405, 'ROUGE-LSUM': 19.218}}, {'name': 'samsum', 'accuracy': {'ROUGE-1': 21.81, 'ROUGE-2': 4.253, 'ROUGE-L': 17.447, 'ROUGE-LSUM': 18.891}}]}, 'description': 'PEGASUS is a pre-trained model for abstractive summarization, developed by Google. It is based on the Transformer architecture and trained on both C4 and HugeNews datasets. The model is designed to extract gap sentences and generate summaries by stochastically sampling important sentences.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ù†ØµÙŠØ© Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©ØŒ Ù…Ø¹ Ø¥Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†ØµÙˆØµ ØªÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø¨Ø¯Ù‚Ø© ÙˆØ¥Ø¨Ø¯Ø§Ø¹? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the Hugging Face Transformers library.\\\\n2. Use the pipeline function to create an optical character recognition (OCR) model for converting the Japanese manga image to text.\\\\n3. Specify the model 'kha-white/manga-ocr-base'. This model is specifically designed for Japanese text in manga, making it a perfect fit for the publisher's requirements.\\\\n4. The OCR pipeline can be used to process the manga page image, extract the Japanese text, and create its digital representation. This could then be used for their online release.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nocr_pipeline = pipeline('ocr', model='kha-white/manga-ocr-base')\\\\nmanga_text = ocr_pipeline(manga_page_image)\\\", 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'api_arguments': 'image', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ±ÙƒØ² ØªØ·Ø¨ÙŠÙ‚Ù†Ø§ Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØµÙˆØ± Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶Ø© ÙÙŠ Ø§Ù„ØµÙˆØ± Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© Ù„Ù‡Ø§.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting', revision=fp16, torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting', revision=fp16, torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \\\"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting', revision=fp16, torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': [{'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}], 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ÙˆØµÙ Ù†ØµÙŠ Ù„Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶Ø© ÙŠØ£Ø®Ø° ÙÙŠ Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¨ØµØ±ÙŠ ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬Ù‡Ø§ Ù…Ù†Ù‡Ø§.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries from the transformers and PIL packages, including TrOCRProcessor and VisionEncoderDecoderModel for OCR processing, and Image for image processing.\\\\n2. Load the pre-trained model 'microsoft/trocr-small-printed' using the from_pretrained method of the VisionEncoderDecoderModel class. This model is designed for optical character recognition tasks.\\\\n3. Use the TrOCRProcessor.from_pretrained method to load the OCR processor and convert an input image to a format that the model can understand.\\\\n4. Use the model to generate text from the processed image.\\\\n5. Decode the generated text to a human-readable format using the processor's batch_decode method.\\\", '<<<code>>>': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimage = Image.open('path_to_image.jpg')\\\\n# replace 'path_to_image.jpg' with the path to your image\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\\nprint(generated_text)\\\", 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/trocr-small-printed', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\\", 'api_arguments': {'image': \\\"Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\", 'processor': \\\"TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\\"}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù‡ÙˆÙŠØ© ØªØ·Ø¨ÙŠÙ‚Ù†Ø§ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø§Ù„Ø®Ø§Øµ Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary libraries from Hugging Face Transformers, which are AutoModelForCausalLM and AutoTokenizer.\\\\n2. Load the tokenizer and model 'microsoft/DialoGPT-small', which is a pre-trained model designed for conversational applications.\\\\n3. When receiving a user input, tokenize it and generate a response using the model.\\\\n4. Decode the generated tokens to obtain the model's response as text.\\\\n5. Use the generated response to answer users' questions on the support page.\\\", '<<<code>>>': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'microsoft/DialoGPT-small\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'microsoft/DialoGPT-small\\\\')\\\\n\\\\nuser_input = \\\"User question or input here...\\\"\\\\ninput_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\\\\'pt\\\\')\\\\ngenerated_ids = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.eos_token_id)\\\\nresponse = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\\\\n\\\\nprint(\\\"Support response:\\\", response)', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/DialoGPT-small', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')\\\", 'api_arguments': ['bot_input_ids', 'max_length', 'pad_token_id'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-small)\\\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-small)\\\\nfor step in range(5):\\\\n new_user_input_ids = tokenizer.encode(input(>> User:) + tokenizer.eos_token, return_tensors='pt')\\\\n bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\\\\n chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n print(DialoGPT: {}.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\\\", 'performance': {'dataset': 'Reddit discussion thread', 'accuracy': 'Comparable to human response quality under a single-turn conversation Turing test'}, 'description': 'DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multiturn conversations. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ ÙˆÙ†Ø´Ø¬Ø¹ Ø¹Ù„Ù‰ ØªÙˆÙÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ³Ø§Ø¦Ø· Ù…ØªØ¹Ø¯Ø¯Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary classes from the transformers library provided by Hugging Face: TrOCRProcessor for image processing, and VisionEncoderDecoderModel for the pretrained model.\\\\n2. We load the pre-trained model, called 'microsoft/trocr-large-printed', with VisionEncoderDecoderModel.from_pretrained method.\\\\n3. Using the Image class from the PIL library, we read the image data and convert the image to the RGB color space.\\\\n4. We preprocess the image using TrOCRProcessor.from_pretrained to create pixel_values for the model.\\\\n5. The model takes the pixel_values as input and generates the corresponding ids for the text.\\\\n6. Finally, processor.batch_decode is used to decode the generated ids, converting the handwritten text in the images to digital text.\\\", '<<<code>>>': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\n\\\\nimage = Image.open('your_image_path.jpg').convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/trocr-large-printed', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\\", 'api_arguments': {'TrOCRProcessor': \\\"from_pretrained('microsoft/trocr-large-printed')\\\", 'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': [{'transformers': 'pip install transformers', 'PIL': 'pip install pillow', 'requests': 'pip install requests'}], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆÙ…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆÙÙŠØ± Ø¯Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ø£Ùˆ ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ø¹Ù…Ù„ÙŠØ© Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ©ØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='roberta-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the necessary \\\\'pipeline\\\\' function from the transformers library provided by Hugging Face.\\\\n2. Create an instance of the \\\"fill-mask\\\" pipeline with the pre-trained model \\\\'roberta-base\\\\'.\\\\n3. This model is based on the RoBERTa architecture, which is an advanced masked language model that helps in filling missing words in the given text, completing the sentences in context.\\\\n4. You can use this pipeline to fill in missing words (represented by <mask>) in an input text, making it easier to complete sentences while writing articles.', '<<<code>>>': 'from transformers import pipeline\\\\nfill_mask = pipeline(\\\\'fill-mask\\\\', model=\\\\'roberta-base\\\\')\\\\ninput_text = \\\"Artificial intelligence is a rapidly growing field that has the potential to <mask> many industries.\\\"\\\\nresult = fill_mask(input_text)', 'api_call': \\\"pipeline('fill-mask', model='roberta-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'roberta-base', 'api_call': \\\"pipeline('fill-mask', model='roberta-base')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='roberta-base')\\\\nunmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': [{'name': 'MNLI', 'accuracy': 87.6}, {'name': 'QQP', 'accuracy': 91.9}, {'name': 'QNLI', 'accuracy': 92.8}, {'name': 'SST-2', 'accuracy': 94.8}, {'name': 'CoLA', 'accuracy': 63.6}, {'name': 'STS-B', 'accuracy': 91.2}, {'name': 'MRPC', 'accuracy': 90.2}, {'name': 'RTE', 'accuracy': 78.7}]}, 'description': 'RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using the Masked language modeling (MLM) objective. This model is case-sensitive and can be fine-tuned on a downstream task.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model='deepset/roberta-large-squad2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary libraries, which include the 'pipeline' function from the Transformers library provided by Hugging Face.\\\\n2. We specify the RoBERTa model 'deepset/roberta-large-squad2' to be used for question-answering purposes.\\\\n3. We can provide questions related to programming languages, and specific contexts containing the desired information to be extracted.\\\\n4. The model will be able to answer the questions based on the given context, providing brief descriptions and examples of programming languages.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nnlp_qa = pipeline(\\\\'question-answering\\\\', model=\\\\'deepset/roberta-large-squad2\\\\')\\\\nquestion = \\\\'What is the main use of JavaScript?\\\\'\\\\ncontext = \\\"JavaScript is a programming language that is primarily used for web development. It allows for dynamic content updates and interactions in web browsers.\\\"\\\\nanswer = nlp_qa({\\\\'question\\\\': question, \\\\'context\\\\': context})', 'api_call': \\\"pipeline('question-answering', model='deepset/roberta-large-squad2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'deepset/roberta-large-squad2', 'api_call': \\\"pipeline('question-answering', model='deepset/roberta-large-squad2')\\\", 'api_arguments': ['question', 'context'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; nlp = pipeline('question-answering', model='deepset/roberta-large-squad2'); nlp({'question': 'What is the capital of Germany?', 'context': 'Berlin is the capital of Germany.'})\\\", 'performance': {'dataset': 'squad_v2', 'accuracy': 'Not provided'}, 'description': 'A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ØªØµÙ†ÙŠÙ Ù…Ù…ÙŠØ² Ù„Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ ÙˆØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ù†Ø´Ø£Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import AutoModel and BertTokenizerFast from the transformers library provided by Hugging Face.\\\\n2. Load the tokenizer 'bert-base-chinese' and the model 'ckiplab/bert-base-chinese-pos'. The tokenizer is used to tokenize the traditional Chinese text, and the model is used to identify the part-of-speech tags associated with each token.\\\\n3. Create a function that takes a traditional Chinese text as input, tokenizes it using the BertTokenizerFast, and then generates part-of-speech tags using the AutoModel.\\\\n4. The output should be the original tokenized text along with its associated part-of-speech tags.\\\", '<<<code>>>': \\\"from transformers import BertTokenizerFast, AutoModel\\\\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\\\\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\\n\\\\ndef tag_POS(text):\\\\n    inputs = tokenizer(text, return_tensors='pt')\\\\n    outputs = model(**inputs)\\\\n    predictions = outputs.logits.argmax(axis=-1).squeeze().tolist()\\\\n    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze())\\\\n    pos_tags = [tokenizer.decode([pred], skip_special_tokens=True) for pred in predictions]\\\\n    return list(zip(tokens, pos_tags))\\\", 'api_call': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ckiplab/bert-base-chinese-pos', 'api_call': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'api_arguments': {'tokenizer': \\\"BertTokenizerFast.from_pretrained('bert-base-chinese')\\\"}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import (\\\\n  BertTokenizerFast,\\\\n  AutoModel,\\\\n)\\\\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\\\\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).' </s>\"\n"}
{"text": "<s>[INST]ØªØ¹Ù…Ù„ Ø´Ø±ÙƒØ© ØªØ·ÙˆÙŠØ± Ø¹Ù…ÙŠÙ„Ù†Ø§ Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙŠØ³ØªÙ‡Ø¯Ù Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø¨Ù…Ø¬Ø§Ù„ Vision Computer Object Detection. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ§Øª ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries and classes: YolosFeatureExtractor and YolosForObjectDetection from transformers, Image from PIL, and requests for downloading the image from a URL.\\\\n2. Define the source of the image to be processed. In this case, it's a URL containing a sample image.\\\\n3. Open the image using the Image library and get the model ready. To do that, first load the feature_extractor from YolosFeatureExtractor.from_pretrained() and the model from YolosForObjectDetection.from_pretrained().\\\\n4. Process the image with the feature extractor and pass it to the model to get the object detection results in the form of bounding boxes (pred_boxes) and corresponding logits.\\\\n5. The resulting bounding boxes and logits can be used to identify and track different object types in the image, which can be useful for a self-navigating robotics framework.\\\", '<<<code>>>': \\\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\n\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\", 'api_call': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hustvl/yolos-tiny', 'api_call': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': \\\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '28.7 AP'}, 'description': 'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†ØµÙˆØµØŒ Ø­ÙŠØ« Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©. ÙŠÙØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ Ù„ÙØ±ÙŠÙ‚ Ø§Ù„Ø¹Ù…Ù„ Ù„ØªØµÙ…ÙŠÙ… Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ ÙˆÙØ¹Ø§Ù„.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the pipeline function from Hugging Face's transformers library.\\\", 'code': \\\"from transformers import pipeline\\\\ntranslator = pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\\nenglish_product_description = 'A high-quality and affordable gaming chair with ergonomic design...'\\\\narabic_product_description = translator(english_product_description)[0]['translation_text']\\\", 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Translation', 'api_name': 'Helsinki-NLP/opus-mt-en-ar', 'api_call': \\\"from transformers import pipeline\\\\ntranslator = pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\\nenglish_product_description = 'A high-quality and affordable gaming chair with ergonomic design...'\\\\narabic_product_description = translator(english_product_description)[0]['translation_text']\\\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\\ntranslated_text = translation('Hello World')\\\", 'performance': {'dataset': 'Tatoeba-test.eng.ara', 'accuracy': {'BLEU': 14.0, 'chr-F': 0.437}}, 'description': \\\"A Hugging Face Transformers model for English to Arabic translation, trained on the Tatoeba dataset. It uses a transformer architecture and requires a sentence initial language token in the form of '>>id<<' (id = valid target language ID).\\\"}} </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ±ØºØ¨ ÙÙŠ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ØªÙ‚Ù†ÙŠØ© Ø¬Ø¯ÙˆÙ„ÙŠØ© Ù„ØªØ­Ù‚ÙŠÙ‚ ØªÙˆÙ‚Ø¹Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·Ø¨ÙŠÙ‚ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© ÙˆØªØ­Ù‚ÙŠÙ‚ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. First, we import the necessary libraries, including joblib and pandas.\\\\n2. Load the pre-trained model by calling joblib.load('model.joblib').\\\\n3. Load the configuration file 'config.json' to get the names of the input features.\\\\n4. Load the input data in a CSV file (e.g., 'data.csv') as a pandas DataFrame, selecting only the specified input features and renaming the columns as 'feat_' + the original column name.\\\\n5. Use the model.predict() method to obtain predictions for carbon emissions for each row of input data.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nimport json\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\n\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\n\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pcoloc/autotrain-mikrotik-7-7-1860563588', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['json', 'joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'pcoloc/autotrain-data-mikrotik-7-7', 'accuracy': {'Loss': 48.213, 'R2': 0.654, 'MSE': 2324.518, 'MAE': 32.634, 'RMSLE': 0.586}}, 'description': 'A tabular regression model trained with AutoTrain to predict carbon emissions based on input features.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ·Ù„Ø¨ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ø§Ù„ÙŠØ¨ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ø¥Ù†ØªØ§Ø¬ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªÙ‚ÙˆÙ… Ø¨Ø°Ù„Ùƒ.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"PegasusForConditionalGeneration.from_pretrained('tuner007/pegasus_paraphrase')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import PegasusForConditionalGeneration and PegasusTokenizer from the transformers library provided by Hugging Face.\\\\n2. Load the 'tuner007/pegasus_paraphrase' model, which is a PEGASUS fine-tuned for paraphrasing tasks such as rewriting paragraphs.\\\\n3. Create a function that takes input_text, num_return_sequences, and num_beams as arguments. The input_text is the paragraph you want to rewrite, num_return_sequences is the number of different paraphrases you want to receive, and num_beams is a parameter that affects the quality and diversity of generated paraphrases.\\\\n4. Tokenize the input text and use the model to generate paraphrased output using generate() method.\\\\n5. Decode the generated output to receive readable paraphrased text.\\\", '<<<code>>>': 'from transformers import PegasusForConditionalGeneration, PegasusTokenizer\\\\nmodel_name = \\\\'tuner007/pegasus_paraphrase\\\\'\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\n\\\\ndef paraphrase_text(input_text, num_return_sequences=1, num_beams=5):\\\\n    batch = tokenizer([input_text], truncation=True, padding=\\\\'longest\\\\', max_length=60, return_tensors=\\\\'pt\\\\')\\\\n    translated = model.generate(**batch, max_length=60, num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\\\\n    paraphrased_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\\\\n    return paraphrased_text\\\\n\\\\nparaphrased_text = paraphrase_text(\\\"an_original_paragraph\\\", num_return_sequences=1, num_beams=5)', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('tuner007/pegasus_paraphrase')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tuner007/pegasus_paraphrase', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('tuner007/pegasus_paraphrase')\\\", 'api_arguments': {'input_text': 'string', 'num_return_sequences': 'integer', 'num_beams': 'integer'}, 'python_environment_requirements': [{'torch': 'any', 'transformers': 'any'}], 'example_code': \\\"import torch\\\\nfrom transformers import PegasusForConditionalGeneration, PegasusTokenizer\\\\nmodel_name = 'tuner007/pegasus_paraphrase'\\\\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\\\\ndef get_response(input_text,num_return_sequences,num_beams):\\\\n batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=pt).to(torch_device)\\\\n translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\\\\n tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\\\\n return tgt_text\\\", 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'PEGASUS fine-tuned for paraphrasing' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We need to import AutoTokenizer and AutoModelForSequenceClassification from the transformers library provided by Hugging Face.\\\\n2. Then, we load the pre-trained model 'MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary', which is proficient in natural language inference (NLI) tasks.\\\\n3. We create the input by encoding the given news headline as a hypothesis and a suitable predefined premise for whether the headline is clickbait or not.\\\\n4. The model then outputs probabilities for both entailment and not_entailment cases.\\\\n5. The final step is to compute and print the confidence levels for both clickbait and not_clickbait labels based on these output probabilities.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nimport torch\\\\n\\\\nmodel_name = \\\\'MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\\\\n\\\\nheadline = \\\"You won\\\\'t believe what happened when this celebrity was spotted in Beverly Hills!\\\"\\\\npremise_clickbait = \\\"The headline is clickbait.\\\"\\\\npremise_not_clickbait = \\\"The headline is not clickbait.\\\"\\\\ninput_clickbait = tokenizer(premise_clickbait, headline, truncation=True, return_tensors=\\\\'pt\\\\')\\\\ninput_not_clickbait = tokenizer(premise_not_clickbait, headline, truncation=True, return_tensors=\\\\'pt\\\\')\\\\n\\\\noutput_clickbait = model(**input_clickbait)\\\\noutput_not_clickbait = model(**input_not_clickbait)\\\\n\\\\npredictions = {\\\\n    \\\\'clickbait\\\\': torch.softmax(output_clickbait.logits, -1)[0][0].item() * 100,\\\\n    \\\\'not_clickbait\\\\': torch.softmax(output_not_clickbait.logits, -1)[0][0].item() * 100\\\\n}\\\\n\\\\nprint(predictions)', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary')\\\", 'api_arguments': {'premise': 'string', 'hypothesis': 'string'}, 'python_environment_requirements': [{'transformers': '>=4.13', 'torch': 'any'}], 'example_code': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nimport torch\\\\ndevice = torch.device(cuda) if torch.cuda.is_available() else torch.device(cpu)\\\\nmodel_name = MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\\\\npremise = I first thought that I liked the movie, but upon second thought it was actually disappointing.\\\\nhypothesis = The movie was good.\\\\ninput = tokenizer(premise, hypothesis, truncation=True, return_tensors=pt)\\\\noutput = model(input[input_ids].to(device))\\\\nprediction = torch.softmax(output[logits][0], -1).tolist()\\\\nlabel_names = [entailment, not_entailment]\\\\nprediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\\\\nprint(prediction)', 'performance': {'dataset': {'mnli-m-2c': {'accuracy': 0.925}, 'mnli-mm-2c': {'accuracy': 0.922}, 'fever-nli-2c': {'accuracy': 0.892}, 'anli-all-2c': {'accuracy': 0.676}, 'anli-r3-2c': {'accuracy': 0.665}, 'lingnli-2c': {'accuracy': 0.888}}}, 'description': 'This model was trained on 782 357 hypothesis-premise pairs from 4 NLI datasets: MultiNLI, Fever-NLI, LingNLI and ANLI. The base model is DeBERTa-v3-xsmall from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ±.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries such as OneFormerProcessor, OneFormerForUniversalSegmentation from transformers, Image from PIL, and requests.\\\\n2. Load the image from the warehouse surveillance camera.\\\\n3. Initialize the OneFormerProcessor and OneFormerForUniversalSegmentation with the 'shi-labs/oneformer_ade20k_swin_tiny' model.\\\\n4. Perform multi-task segmentation of the image by preparing the required task inputs like 'semantic', 'instance', and 'panoptic'. Pass these as task_inputs to the processor and then to the model.\\\\n5. Obtain the segmented outputs for each task and use the processor's post-processing functions to generate the final segmentation maps for each task separately.\\\", '<<<code>>>': \\\"from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://path_to_warehouse_image.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nprocessor = OneFormerProcessor.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\\n\\\\nsemantic_inputs = processor(images=image, task_inputs=['semantic'], return_tensors='pt')\\\\nsemantic_outputs = model(**semantic_inputs)\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(semantic_outputs, target_sizes=[image.size[::-1]])[0]\\\\n\\\\ninstance_inputs = processor(images=image, task_inputs=['instance'], return_tensors='pt')\\\\ninstance_outputs = model(**instance_inputs)\\\\npredicted_instance_map = processor.post_process_instance_segmentation(instance_outputs, target_sizes=[image.size[::-1]])[0]['segmentation']\\\\n\\\\npanoptic_inputs = processor(images=image, task_inputs=['panoptic'], return_tensors='pt')\\\\npanoptic_outputs = model(**panoptic_inputs)\\\\npredicted_panoptic_map = processor.post_process_panoptic_segmentation(panoptic_outputs, target_sizes=[image.size[::-1]])[0]['segmentation']\\\", 'api_call': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'shi-labs/oneformer_ade20k_swin_tiny', 'api_call': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\\\n\\\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\\\nsemantic_outputs = model(**semantic_inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\n\\\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\\\ninstance_outputs = model(**instance_inputs)\\\\n\\\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\\\n\\\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\\\npanoptic_outputs = model(**panoptic_inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ø§Ø¯Ø«Ø© ÙŠØ³ØªØ®Ø¯Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø­ÙˆÙ„ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ´Ù…Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø±Ø¯ Ø¨Ø¬Ù…Ù„Ø© \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"TapasForCovid.from_pretrained('lysandre/tiny-tapas-random-sqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary libraries and classes from the transformers package, including AutoTokenizer and TapasForSequenceClassification.\\\\n2. We then use the from_pretrained method of the TapasForSequenceClassification class to load the pre-trained model 'lysandre/tiny-tapas-random-sqa'. This model is specifically designed for table question answering tasks, making it suitable for the hotel reservation system.\\\\n3. To use the model, we will first need a tokenizer to convert the input text (questions and table) into a format that the model can understand. For this, we use the AutoTokenizer.from_pretrained function with the same model name.\\\\n4. We can then use the tokenized input to make predictions using the model, allowing us to answer customers' questions about hotel prices and room availability.\\\", '<<<code>>>': \\\"from transformers import TapasForSequenceClassification, AutoTokenizer\\\\nmodel = TapasForSequenceClassification.from_pretrained('lysandre/tiny-tapas-random-sqa')\\\\ntokenizer = AutoTokenizer.from_pretrained('lysandre/tiny-tapas-random-sqa')\\\", 'api_call': \\\"TapasForCovid.from_pretrained('lysandre/tiny-tapas-random-sqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lysandre/tiny-tapas-random-sqa', 'api_call': \\\"TapasForCovid.from_pretrained('lysandre/tiny-tapas-random-sqa')\\\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'A tiny TAPAS model for table question answering tasks.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ Ø´Ø¨ÙƒØ© Ø¬Ù†Ø±Ø§ØªÙŠÙ Ù…ØªØ¯ÙÙ‚Ø© Ù„ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…Ø´Ø±ÙˆØ· ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary library, which is DDPMPipeline from diffusers.', 'code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\\ngenerated_images = pipeline(number_of_images=5)\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'sd-class-butterflies-32', 'api_call': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\\ngenerated_images = pipeline(number_of_images=5)\\\", 'api_arguments': {'model_id': 'clp/sd-class-butterflies-32'}, 'python_environment_requirements': ['diffusers'], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': None, 'accuracy': None}, 'description': 'This model is a diffusion model for unconditional image generation of cute butterflies.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ù…Ù‡ØªÙ…ÙˆÙ† Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙØ§Øª Ø¨ÙˆØ§Ø³Ø·Ø© ØªÙ‚Ù†ÙŠØ© Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ Ø­ÙŠØ« ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø±Ø¦ÙŠ ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ. Ù‡Ù„ Ù„Ø¯ÙŠÙƒ Ø§Ù„Ø®Ø¨Ø±Ø© Ø§Ù„Ù„Ø§Ø²Ù…Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± Ø¨ÙˆØ§Ø³Ø·Ø© ØªÙ‚Ù†ÙŠØ© Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŸ? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\\n\\\\nimage = Image.open('aerial_photo.jpg')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-ade-640-640', 'api_call': \\\"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\\n\\\\nimage = Image.open('aerial_photo.jpg')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20K', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠÙˆÙØ± Ù…Ø´Ø±ÙˆØ¹ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø£Ø¯Ø§Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø©. Ù‚Ù… Ø¨ØªØ®ØµÙŠØµ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© ÙˆØªÙ†ÙˆØ¹Ù‹Ø§ ÙÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary libraries, including the transformers package and its 'AutoModelForCausalLM' and 'AutoTokenizer' classes, as well as the 'set_seed' function for reproducibility.\\\", 'code': 'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\n\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'facebook/opt-66b\\\\', torch_dtype=torch.float16)\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/opt-66b\\\\', use_fast=False)\\\\n\\\\nprompt = \\\"Once upon a time in a small village,\\\"\\\\ninput_ids = tokenizer(prompt, return_tensors=\\\\'pt\\\\').input_ids\\\\nset_seed(42)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=50)\\\\n\\\\nstory_prompts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\\\\nprint(story_prompts)', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Generation', 'api_name': 'facebook/opt-66b', 'api_call': 'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\n\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'facebook/opt-66b\\\\', torch_dtype=torch.float16)\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/opt-66b\\\\', use_fast=False)\\\\n\\\\nprompt = \\\"Once upon a time in a small village,\\\"\\\\ninput_ids = tokenizer(prompt, return_tensors=\\\\'pt\\\\').input_ids\\\\nset_seed(42)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=50)\\\\n\\\\nstory_prompts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\\\\nprint(story_prompts)', 'api_arguments': ['input_ids', 'do_sample', 'num_return_sequences', 'max_length'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-66b, torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-66b, use_fast=False)\\\\nprompt = Hello, I am conscious and\\\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\\\nset_seed(32)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\\\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)', 'performance': {'dataset': 'GPT-3', 'accuracy': 'roughly matched'}, 'description': 'OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. OPT models are trained to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Øµ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary components from the Hugging Face Transformers library, such as BartForConditionalGeneration for the text summarization model.\\\\n2. Load the pre-trained model using the BartForConditionalGeneration.from_pretrained method. The 'sshleifer/distilbart-cnn-12-6' checkpoint is trained on the cnn_dailymail dataset and provides a suitable summarization model for summarizing news articles.\\\\n3. Create an instance of the model, and for a given news article text, generate a short summary.\\\\n4. The generated summary can be used to represent the main points of the news article to readers.\\\", '<<<code>>>': 'from transformers import BartForConditionalGeneration, BartTokenizer\\\\nmodel = BartForConditionalGeneration.from_pretrained(\\\\'sshleifer/distilbart-cnn-12-6\\\\')\\\\ntokenizer = BartTokenizer.from_pretrained(\\\\'sshleifer/distilbart-cnn-12-6\\\\')\\\\narticle = \\\"The news article text goes here...\\\"\\\\ninput_tokens = tokenizer(article, return_tensors=\\\\'pt\\\\')\\\\nsummary_output = model.generate(**input_tokens)\\\\nsummary = tokenizer.decode(summary_output[0], skip_special_tokens=True)', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sshleifer/distilbart-cnn-12-6', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\\\", 'api_arguments': '', 'python_environment_requirements': ['huggingface/transformers'], 'example_code': '', 'performance': {'dataset': [{'name': 'cnn_dailymail', 'accuracy': {'Rouge 2': '22.12', 'Rouge-L': '36.99'}}]}, 'description': \\\"DistilBART is a distilled version of BART, a model for text summarization. This specific checkpoint, 'sshleifer/distilbart-cnn-12-6', is trained on the cnn_dailymail dataset and provides a fast and effective way to generate summaries of text. The model can be loaded using the Hugging Face Transformers library.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø·Ù„Ø§Ø¨ÙŠØ© ØªÙ„ØªÙ‚ÙŠ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„ØªØ­Ø³ÙŠÙ† Ù…Ù‡Ø§Ø±Ø§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‚Ø¯Ù… Ù„Ù‡Ù… Ø¨Ø±Ù†Ø§Ù…Ø¬Ø§Ù‹ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ ÙØ¹Ø§Ù„Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('conversational', model='PygmalionAI/pygmalion-350m')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class 'pipeline' from the transformers package.\\\\n2. We then use the 'pipeline' function to create a conversational pipeline with the 'PygmalionAI/pygmalion-350m' model.\\\\n3. With this conversational pipeline, we can initiate a conversation with the prompt as the input and receive generated responses.\\\\n4. The assistant can provide supporting information to various discussions based on the user's input.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nconversational_pipeline = pipeline(\\\\'conversational\\\\', model=\\\\'PygmalionAI/pygmalion-350m\\\\')\\\\nuser_input = \\\"What\\\\'s the primary source of renewable energy?\\\"\\\\ngenerated_response = conversational_pipeline([user_input])\\\\nprint(generated_response)', 'api_call': \\\"pipeline('conversational', model='PygmalionAI/pygmalion-350m')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pygmalion-350m', 'api_call': \\\"pipeline('conversational', model='PygmalionAI/pygmalion-350m')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['transformers'], 'example_code': 'N/A', 'performance': {'dataset': 'The Pile', 'accuracy': 'N/A'}, 'description': \\\"This is a proof-of-concept fine-tune of Facebook's OPT-350M model optimized for dialogue, to be used as a stepping stone to higher parameter models. Disclaimer: NSFW data was included in the fine-tuning of this model. Although SFW inputs will usually result in SFW outputs, you are advised to chat at your own risk. This model is not suitable for use by minors.\\\" </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… ÙØ±ÙŠÙ‚Ù†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„ØºÙˆÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¢Ù„ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ù„ØªØ­Ø³ÙŠÙ† ÙƒÙØ§Ø¡Ø© ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯Ù‚Ø© ÙˆÙØ¹Ø§Ù„ÙŠØ© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\\\", '<<<api_provider>>>': 'PyTorch Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library by Hugging Face.\\\\n2. Use the pipeline function to create an English to French translation model.\\\\n3. Specify the model 'facebook/nllb-200-distilled-600M' to be loaded. This model has been trained to translate text between 200 languages.\\\\n4. Provide the input text to the translator function, which is the English product description, and it will return the translation in French.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_en_to_fr\\\\', model=\\\\'facebook/nllb-200-distilled-600M\\\\')\\\\ninput_text = \\\"Our state-of-the-art smartwatch offers numerous features, including GPS tracking, heart rate monitoring, sleep tracking, and smartphone notifications. With a sleek design and customizable watch faces, it is perfect for any occasion.\\\"\\\\ntranslation = translator(input_text)[0][\\\\'translation_text\\\\']', 'api_call': \\\"pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\\\", 'provider': 'PyTorch Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/nllb-200-distilled-600M', 'api_call': \\\"pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\\\", 'api_arguments': ['model', 'text'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline; translator = pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M'); translator('Hello World')\\\", 'performance': {'dataset': 'Flores-200', 'accuracy': 'BLEU, spBLEU, chrF++'}, 'description': 'NLLB-200 is a machine translation model primarily intended for research in machine translation, especially for low-resource languages. It allows for single sentence translation among 200 languages. The model was trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ¹Ø¨Ø¦Ø© Ø§Ù„ÙØ¬ÙˆØ§Øª ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª \\\"Fill-Mask\\\" Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØªØ­Ø³ÙŠÙ† ØªÙØ§Ø¹Ù„ Ø§Ù„Ø­ÙˆØ§Ø± Ù…Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….? [/INST]{'domain': 'Natural Language Processing Fill-Mask', 'api_call': \\\"pipeline('fill-mask', model='bert-large-uncased')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': '', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Transformers', 'functionality': 'Masked Language Modeling and Next Sentence Prediction', 'api_name': 'bert-large-uncased', 'api_call': '', 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='bert-large-uncased')\\\\nunmasker(Hello I'm a [MASK] model.)\\\", 'performance': {'dataset': {'SQUAD 1.1 F1/EM': '91.0/84.3', 'Multi NLI Accuracy': '86.05'}}, 'description': 'BERT large model (uncased) is a transformer model pretrained on a large corpus of English data using a masked language modeling (MLM) objective. It has 24 layers, 1024 hidden dimensions, 16 attention heads, and 336M parameters. The model is intended to be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„ØªØ­Ø³ÙŠÙ† ØªØ­Ø¯ÙŠØ¯ ÙˆØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ±ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¯Ù‚Ø© ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ Ù†Ø­Ù„Ù„Ù‡Ø§.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, such as ConvNextFeatureExtractor, ConvNextForImageClassification from transformers, torch, and datasets.\\\\n2. Load an image dataset containing living room images. In your case, you can load the image of your living room.\\\\n3. The pre-trained model 'facebook/convnext-tiny-224' is made for image classification task. Before using it, you need to preprocess the image with the ConvNextFeatureExtractor.\\\\n4. Use the preprocessed image to predict the logits using the ConvNextForImageClassification model.\\\\n5. Select the index of the highest logit value, which corresponds to the predicted label.\\\\n6. Print the predicted label.\\\", '<<<code>>>': \\\"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\\\nimport torch\\\\n# Replace the image file path with the path of your living room image\\\\nimage = 'path/to/your/image/file'\\\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\\\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\\\ninputs = feature_extractor(image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/convnext-tiny-224', 'api_call': \\\"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-tiny-224'}, 'python_environment_requirements': [['transformers', 'torch', 'datasets']], 'example_code': \\\"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset('huggingface/cats-image')\\\\nimage = dataset['test']['image'][0]\\\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\\\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\\\ninputs = feature_extractor(image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†ØµÙˆØµØŒ Ø­ÙŠØ« Ù†Ø³ØªØ®Ø¯Ù… Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„ØªÙˆØ¸ÙŠÙ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. First, import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create a translation pipeline with model \\\"Helsinki-NLP/opus-mt-en-ru\\\" to translate text from English to Russian.\\\\n3. Pass the English promotional content into the translation pipeline to obtain the Russian translation.\\\\n4. The translated Russian text can then be edited if necessary, and it can be used in your marketing materials targeting Russian audiences.', '<<<code>>>': 'from transformers import pipeline\\\\ntranslation = pipeline(\\\\'translation_en_to_ru\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-ru\\\\')\\\\nenglish_promo_content = \\\"Your English promotional content here...\\\"\\\\nrussian_translation = translation(english_promo_content)\\\\nrussian_text = russian_translation[0][\\\\'translation_text\\\\']', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-en-ru', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers', 'torch', 'tensorflow']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ru', model='Helsinki-NLP/opus-mt-en-ru')\\\\ntranslated_text = translation('Hello, how are you?')\\\", 'performance': {'dataset': 'newstest2019-enru', 'accuracy': '27.1'}, 'description': 'Helsinki-NLP/opus-mt-en-ru is a translation model trained on the OPUS dataset, which translates English text to Russian. It is based on the Marian NMT framework and can be used with Hugging Face Transformers.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†Øµ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† Ø´Ø±Ø­ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­ÙˆØ§Ø³ÙŠØ¨ Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first install the required packages 'ultralyticsplus' and 'ultralytics' from the package manager.\\\\n2. Then, we import the YOLO class from the ultralyticsplus package.\\\\n3. We load the model 'keremberke/yolov8s-csgo-player-detection' by creating an instance of the YOLO class.\\\\n4. We update the model's parameters such as confidence threshold (conf), Intersection over Union threshold (iou), agnostic_nms, and maximum detections (max_det).\\\\n5. We pass an image URL from the game Counter-Strike: Global Offensive (CS:GO) to the model for prediction.\\\\n6. The model will detect the players in the image and return the detected objects with their bounding boxes.\\\\n7. Finally, we render the result on the original image and display it with the bounding boxes drawn around the detected players.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://example.com/csgo_image.jpg'\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \\\"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\\\" </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ø³Ø¤Ø§Ù„ Ù„ØºÙˆÙŠ ÙŠØªØ¹Ù„Ù‚ Ø¨ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ Ø¨Ø¯ÙˆÙ† ØªØµÙ†ÙŠÙ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù†Ù‡.? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"pipeline('zero-shot-classification', model='BaptisteDoyen/camembert-base-xnli')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\", 'code': \\\"from transformers import pipeline\\\\nclassifier = pipeline('zero-shot-classification', model='BaptisteDoyen/camembert-base-xnli')\\\\nfrench_article_text = 'Sample French article text...'\\\\nthemes = ['sports', 'politique', 'science', 'culture', 'Ã©conomie']\\\\n# Candidate themes for articles\\\\nresult = classifier(french_article_text, themes)\\\", 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Classification', 'api_name': 'BaptisteDoyen/camembert-base-xnli', 'api_call': \\\"from transformers import pipeline\\\\nclassifier = pipeline('zero-shot-classification', model='BaptisteDoyen/camembert-base-xnli')\\\\nfrench_article_text = 'Sample French article text...'\\\\nthemes = ['sports', 'politique', 'science', 'culture', 'Ã©conomie']\\\\n# Candidate themes for articles\\\\nresult = classifier(french_article_text, themes)\\\", 'api_arguments': {'sequence': 'str', 'candidate_labels': 'List[str]', 'hypothesis_template': 'str'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"sequence = L'Ã©quipe de France joue aujourd'hui au Parc des Princes\\\\ncandidate_labels = [sport,politique,science]\\\\nhypothesis_template = Ce texte parle de {}.\\\\nclassifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)\\\", 'performance': {'dataset': 'xnli', 'accuracy': {'validation': 81.4, 'test': 81.7}}, 'description': 'Camembert-base model fine-tuned on french part of XNLI dataset. One of the few Zero-Shot classification models working on French.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø­ÙˆØ§Ø±Ø§Øª Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a text-generation pipeline with the model 'PygmalionAI/pygmalion-1.3b'.\\\\n3. This model is trained to generate conversational responses and can be used with a specific input format. The input format includes character persona, dialogue history, and user input message.\\\\n4. You can now use this text-generation pipeline to generate responses for your web app given a character persona and user input.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ndialogue_pipeline = pipeline(\\\\'text-generation\\\\', \\\\'PygmalionAI/pygmalion-1.3b\\\\')\\\\n\\\\ninput_prompt = \\\"\\\"\\\"\\\\n[CHARACTER]\\\\'s Persona: [A few sentences about the character you want the model to play]\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\\\\n\\\"\\\"\\\"\\\\n\\\\nresponse = dialogue_pipeline(input_prompt)', 'api_call': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pygmalion-1.3b', 'api_call': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", 'api_arguments': 'input_prompt', 'python_environment_requirements': [['transformers']], 'example_code': \\\"[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\\\", 'performance': {'dataset': '56MB of dialogue data', 'accuracy': 'Not provided'}, 'description': \\\"Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\\\" </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø§Ø­Ø«ÙŠÙ† ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ© Ø£Ù† ÙŠØ¹Ø±ÙÙˆØ§ ÙƒÙŠÙÙŠØ© ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„ØªØµÙ†ÙŠÙ Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙˆÙ†ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"DebertaModel.from_pretrained('microsoft/deberta-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create a fill-mask pipeline using the 'microsoft/deberta-base' model, which is an advanced pre-trained transformer specifically designed for filling in textual masks.\\\\n3. Use the pipeline to fill in the masked word in the given sentence to get the most suitable replacement for the [MASK] token based on the context.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nfill_mask = pipeline(\\\\'fill-mask\\\\', model=\\\\'microsoft/deberta-base\\\\')\\\\nsentence_with_mask = \\\"In order to succeed, one must have [MASK].\\\"\\\\ncompleted_sentence = fill_mask(sentence_with_mask)', 'api_call': \\\"DebertaModel.from_pretrained('microsoft/deberta-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/deberta-base', 'api_call': \\\"DebertaModel.from_pretrained('microsoft/deberta-base')\\\", 'api_arguments': 'text', 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline\\\\nfill_mask = pipeline('fill-mask', model='microsoft/deberta-base')\\\\nfill_mask('The capital of France is [MASK].')\\\", 'performance': {'dataset': {'SQuAD 1.1': '93.1/87.2', 'SQuAD 2.0': '86.2/83.1', 'MNLI-m': '88.8'}}, 'description': 'DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§ØªØŒ Ø§Ø¨Ø¯Ø£ Ø¨ØªØ¬Ù…ÙŠØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø«Ù… Ù‚Ù… Ø¨ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø¥Ù„Ù‰ ÙØ¦Ø§Øª Ù…Ø®ØªÙ„ÙØ©. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚ØŒ ÙˆØªØ­Ø³ÙŠÙ† Ø¯Ù‚ØªÙ‡ Ø¨Ø¶Ø¨Ø· Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª. ÙˆØ£Ø®ÙŠØ±Ù‹Ø§ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ ØµÙˆØªÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª.? [/INST]{'domain': 'Audio Audio Classification', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. First, we'll import the necessary classes from the transformers library: Wav2Vec2ForCTC and Wav2Vec2Processor.\\\", 'code': 'from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\\\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\\\')\\\\ntokenizer = Wav2Vec2Processor.from_pretrained(\\\\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\\\')\\\\n# Replace \\\\'audio_path.wav\\\\' with path to your speech sample\\\\ninput_audio = \\\"audio_path.wav\\\"\\\\ninput_values = tokenizer(file_audio_path, return_tensors=\\\"pt\\\").input_values\\\\nlogits = model(input_values).logits\\\\nprobabilities = torch.softmax(logits, dim=-1)\\\\nemotions = [\\\\'angry\\\\', \\\\'calm\\\\', \\\\'disgust\\\\', \\\\'fearful\\\\', \\\\'happy\\\\', \\\\'neutral\\\\', \\\\'sad\\\\', \\\\'surprised\\\\']\\\\nemotion_probs = {emotion: probability for emotion, probability in zip(emotions, probabilities.tolist()[0])}', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Speech Emotion Recognition', 'api_name': 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition', 'api_call': 'from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\\\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\\\')\\\\ntokenizer = Wav2Vec2Processor.from_pretrained(\\\\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\\\')\\\\n# Replace \\\\'audio_path.wav\\\\' with path to your speech sample\\\\ninput_audio = \\\"audio_path.wav\\\"\\\\ninput_values = tokenizer(file_audio_path, return_tensors=\\\"pt\\\").input_values\\\\nlogits = model(input_values).logits\\\\nprobabilities = torch.softmax(logits, dim=-1)\\\\nemotions = [\\\\'angry\\\\', \\\\'calm\\\\', \\\\'disgust\\\\', \\\\'fearful\\\\', \\\\'happy\\\\', \\\\'neutral\\\\', \\\\'sad\\\\', \\\\'surprised\\\\']\\\\nemotion_probs = {emotion: probability for emotion, probability in zip(emotions, probabilities.tolist()[0])}', 'api_arguments': 'wav2vec2, tokenizer', 'python_environment_requirements': 'transformers 4.8.2, pytorch 1.9.0+cu102, datasets 1.9.0, tokenizers 0.10.3', 'example_code': 'from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor', 'performance': {'dataset': 'RAVDESS', 'accuracy': 0.8223}, 'description': \\\"The model is a fine-tuned version of jonatasgrosman/wav2vec2-large-xlsr-53-english for a Speech Emotion Recognition (SER) task. The dataset used to fine-tune the original pre-trained model is the RAVDESS dataset. This dataset provides 1440 samples of recordings from actors performing on 8 different emotions in English, which are: emotions = ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised'].\\\"}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"AutoTokenizer.from_pretrained('Salesforce/codegen-350M-multi')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Salesforce/codegen-350M-multi\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'Salesforce/codegen-350M-multi\\\\')\\\\ntext = \\\"Create a function that returns the square of a number.\\\"\\\\ninput_ids = tokenizer(text, return_tensors=\\\\'pt\\\\').input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\ngenerated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Program Synthesis', 'api_name': 'Salesforce/codegen-350M-multi', 'api_call': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Salesforce/codegen-350M-multi\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'Salesforce/codegen-350M-multi\\\\')\\\\ntext = \\\"Create a function that returns the square of a number.\\\"\\\\ninput_ids = tokenizer(text, return_tensors=\\\\'pt\\\\').input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\ngenerated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval and MTPB', 'accuracy': 'Refer to the paper for accuracy details'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.'}} </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù†Ø¯ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¯ÙˆØ±Ø§Øª Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø­ÙˆÙ„ ØªÙ‚Ù†ÙŠØ§Øª ÙØµÙ„ Ø§Ù„ØµÙˆØ±ØŒ ÙŠÙÙ†ØµØ­ Ø¨ØªÙ†Ø¸ÙŠÙ… ÙˆØ±Ø´ Ø¹Ù…Ù„ Ø¯ÙˆØ±ÙŠØ© Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„ØªØ·Ø¨ÙŠÙ‚ Ù…Ø§ ØªØ¹Ù„Ù…ÙˆÙ‡. ÙŠÙ…ÙƒÙ† ØªÙˆÙÙŠØ± Ø£Ø¯ÙˆØ§Øª Ø¨Ø±Ù…Ø¬ÙŠØ© Ù„ØªØ³Ù‡ÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± ÙˆØªØ³Ù‡ÙŠÙ„ Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙØµÙ„.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes and functions from the ultralyticsplus package. This includes the YOLO class, and the render_result function.\\\\n2. Create a YOLO model using the specific PCB defect segmentation model, 'keremberke/yolov8s-pcb-defect-segmentation'.\\\\n3. Set model override options, such as confidence threshold, IoU threshold, agnostic NMS, and max_det.\\\\n4. Load the image of the PCB. This can be either a URL or a local path to the image.\\\\n5. Use the model to predict the defects and their locations in the image.\\\\n6. Render the results on the original image with the detected defects and locations.\\\\n7. Print the results including bounding boxes and masks.\\\\n8. Finally, show or save the rendered image with the highlighted PCB defects.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path_to_pcb_image.jpg'\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8s-pcb-defect-segmentation', 'api_call': \\\"YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\n\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.515, 'mAP@0.5(mask)': 0.491}}, 'description': 'YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ± Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØµÙˆØ±.? [/INST]{'domain': 'Multimodal Text-to-Image Generation', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler), torch_dtype=torch.float16)\\\", 'api_provider': 'Hugging Face', 'explanation': '1. First, import the necessary classes from the diffusers and transformers libraries to work with the StableDiffusion model.', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = \\\\'stabilityai/stable-diffusion-2\\\\'\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\\\\'scheduler\\\\')\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(\\\\'cuda\\\\')\\\\nprompt = \\\\'A thrilling fantasy book cover for \\\"The Magical Journey\\\" featuring a majestic castle and a mysterious figure\\\\'\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(\\\\'book_cover.png\\\\')', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = \\\\'stabilityai/stable-diffusion-2\\\\'\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\\\\'scheduler\\\\')\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(\\\\'cuda\\\\')\\\\nprompt = \\\\'A thrilling fantasy book cover for \\\"The Magical Journey\\\" featuring a majestic castle and a mysterious figure\\\\'\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(\\\\'book_cover.png\\\\')', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ±Ø¬Ù…Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©. Ù†ÙˆØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"pipeline('translation_en_to_ru', model='Helsinki-NLP/opus-mt-en-ru')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. First, import the necessary libraries, which are 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\ntranslation = pipeline(\\\\'translation_en_to_ru\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-ru\\\\')\\\\n# Example: Translate English text to Russian\\\\nenglish_text = \\\"Hello, how are you?\\\"\\\\nrussian_translation = translation(english_text)[0][\\\\'translation_text\\\\']\\\\n# Process the translated text in your chatbot and generate a response\\\\nenglish_response = \\\"I\\\\'m good. How can I help you?\\\"\\\\n# Translate the English response back to Russian\\\\nrussian_response = translation(english_response)[0][\\\\'translation_text\\\\']', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Translation', 'api_name': 'opus-mt-en-ru', 'api_call': 'from transformers import pipeline\\\\ntranslation = pipeline(\\\\'translation_en_to_ru\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-ru\\\\')\\\\n# Example: Translate English text to Russian\\\\nenglish_text = \\\"Hello, how are you?\\\"\\\\nrussian_translation = translation(english_text)[0][\\\\'translation_text\\\\']\\\\n# Process the translated text in your chatbot and generate a response\\\\nenglish_response = \\\"I\\\\'m good. How can I help you?\\\"\\\\n# Translate the English response back to Russian\\\\nrussian_response = translation(english_response)[0][\\\\'translation_text\\\\']', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers', 'torch', 'tensorflow'], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ru', model='Helsinki-NLP/opus-mt-en-ru')\\\\ntranslated_text = translation('Hello, how are you?')\\\", 'performance': {'dataset': 'newstest2019-enru', 'accuracy': '27.1'}, 'description': 'Helsinki-NLP/opus-mt-en-ru is a translation model trained on the OPUS dataset, which translates English text to Russian. It is based on the Marian NMT framework and can be used with Hugging Face Transformers.'}} </s>\"\n"}
{"text": "<s>[INST]ØªØ¹Ù…Ù„ Ø´Ø±ÙƒØ© ØªØ·ÙˆÙŠØ± Ø¹Ù…ÙŠÙ„Ù†Ø§ Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙŠØ³ØªÙ‡Ø¯Ù Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø¯ÙˆÙ† Ù‚ÙŠÙˆØ¯ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('google/ddpm-cat-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include DDPMPipeline from diffusers.\\\\n2. Use the DDPMPipeline.from_pretrained() function to load the 'google/ddpm-cat-256' model.\\\\n3. The loaded model will be used to generate random unconditional cat images.\\\\n4. You can use this model to generate a cat image and save it using the .save() method.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\nddpm = DDPMPipeline.from_pretrained('google/ddpm-cat-256')\\\\ngenerated_image = ddpm().images[0]\\\\ngenerated_image.save('cat_image.png')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-cat-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/ddpm-cat-256', 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-cat-256')\\\", 'api_arguments': ['model_id'], 'python_environment_requirements': [['diffusers']], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-cat-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ±Ø¬Ù‰ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ù‡Ø°Ø§ØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='bert-base-multilingual-cased')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We first import the pipeline function from the transformers package.\\\\n2. We configure the pipeline function for the masked language modeling task using the 'fill-mask' parameter and set the model to 'bert-base-multilingual-cased'.\\\\n3. We can then use this pipeline to provide a sentence with a missing word, represented by [MASK]. The model will predict the most likely word to fill in the gap based on the context of the sentence.\\\\n4. This feature can be integrated into our language learning app, allowing users to practice filling in missing words in sentences in a variety of languages.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'bert-base-multilingual-cased\\\\')\\\\nsentence = \\\"Hello, I\\\\'m a [MASK] model.\\\"\\\\nprediction = unmasker(sentence)', 'api_call': \\\"pipeline('fill-mask', model='bert-base-multilingual-cased')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-base-multilingual-cased', 'api_call': \\\"pipeline('fill-mask', model='bert-base-multilingual-cased')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='bert-base-multilingual-cased')\\\\nunmasker(Hello I'm a [MASK] model.)\\\", 'performance': {'dataset': 'wikipedia', 'accuracy': 'Not provided'}, 'description': 'BERT multilingual base model (cased) is pretrained on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective. The model is case sensitive and can be used for masked language modeling or next sentence prediction. It is intended to be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠØ³ØªØ®Ø¯Ù… Computer Vision Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯ÙˆÙ† ØªØµÙ†ÙŠÙ Ø³Ø§Ø¨Ù‚ØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªÙˆÙ‚Ø¹ ÙØ¦Ø© Ø§Ù„ØµÙˆØ±Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. To achieve this, you can use transformers from the Hugging Face library. First, import the necessary library 'pipeline'.\\\\n2. Run the 'pipeline()' function with the 'image-classification' task and the model 'laion/CLIP-convnex_base_w-laion_aesthetic-s13B-b82K' to configure the zero-shot image classifier.\\\\n3. Use the model to classify your input image by providing the path to the image and a list of possible class labels. The list of labels provided in the example includes 'car', 'tree', 'cat', and 'building'. Change the label list according to your requirement.\\\\n4. The classifier returns the predicted class along with the confidence score.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nclassifier = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K')\\\\nresult = classifier('path/to/image.jpg', ['car', 'tree', 'cat', 'building'])\\\", 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K', 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K')\\\", 'api_arguments': {'image': 'path to image file', 'class_names': 'list of possible class names (comma-separated)'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; model = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K'); model('path/to/image.jpg', ['cat', 'dog'])\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '70.8% to 71.7%'}, 'description': 'A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. These models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. They can be used for zero-shot image classification, image and text retrieval, and other tasks.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ù…Ù‡Ø§Ø±Ø§Øª ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include BartForConditionalGeneration and BartTokenizer from transformers.\\\\n2. Load the 'sshleifer/distilbart-cnn-6-6' model using the.from_pretrained method of the BartForConditionalGeneration class. This model has been trained for text summarization tasks on the CNN/Daily Mail and XSum datasets.\\\\n3. Load the tokenizer for the same model.\\\\n4. Use the tokenizer to encode the input text, and then feed the encoded inputs to the model to generate summarized text.\\\\n5. Use the tokenizer to decode the model output to obtain the final summary.\\\", '<<<code>>>': 'from transformers import BartForConditionalGeneration, BartTokenizer\\\\nmodel = BartForConditionalGeneration.from_pretrained(\\\\'sshleifer/distilbart-cnn-6-6\\\\')\\\\ntokenizer = BartTokenizer.from_pretrained(\\\\'sshleifer/distilbart-cnn-6-6\\\\')\\\\narticle = \\\"Insert the article text here.\\\"\\\\ninputs = tokenizer(article, return_tensors=\\\\'pt\\\\')\\\\nsummary_ids = model.generate(inputs.input_ids)\\\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sshleifer/distilbart-cnn-6-6', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': {'cnn_dailymail': {'Rouge 2': 20.17, 'Rouge-L': 29.7}, 'xsum': {'Rouge 2': 20.92, 'Rouge-L': 35.73}}}, 'description': 'DistilBART model for text summarization, trained on the CNN/Daily Mail and XSum datasets. It is a smaller and faster version of BART, suitable for summarizing English text.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¹Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‚Ø§Ø·Ø¹ ÙÙŠØ¯ÙŠÙˆ Ù…Ø¨ØªÙƒØ±Ø© ÙˆÙ…ØªÙ†ÙˆØ¹Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù… Ù„Ù†Ø¸Ø§Ù…Ù†Ø§ Ø§Ù„ØªÙ‚Ù†ÙŠ ÙˆØ¶Ù…Ø§Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬.? [/INST]{'domain': 'Multimodal Text-to-Video', 'api_call': \\\"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b-legacy', torch_dtype=torch.float16)\\\", 'api_provider': 'Hugging Face', 'explanation': '1. Import the necessary libraries, such as torch and DiffusionPipeline from diffusers.', 'code': 'import torch\\\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\\\nfrom diffusers.utils import export_to_video\\\\npipe = DiffusionPipeline.from_pretrained(\\\\'damo-vilab/text-to-video-ms-1.7b-legacy\\\\', torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\nprompt = \\\"Spiderman is surfing\\\"\\\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\\\nvideo_path = export_to_video(video_frames)', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b-legacy', 'api_call': 'import torch\\\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\\\nfrom diffusers.utils import export_to_video\\\\npipe = DiffusionPipeline.from_pretrained(\\\\'damo-vilab/text-to-video-ms-1.7b-legacy\\\\', torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\nprompt = \\\"Spiderman is surfing\\\"\\\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\\\nvideo_path = export_to_video(video_frames)', 'api_arguments': ['prompt', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': 'import torch\\\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\\\nfrom diffusers.utils import export_to_video\\\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b-legacy, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\nprompt = Spiderman is surfing\\\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': ['LAION5B', 'ImageNet', 'Webvid'], 'accuracy': 'Not provided'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ±Ø¬Ù‰ ÙƒØªØ§Ø¨Ø© Ø¹Ø¨Ø§Ø±Ø§Øª Ù†ØµÙŠØ© ØªØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØªØ´Ø¨Ù‡ ÙÙŠ Ø§Ù„Ù‡ÙŠÙƒÙ„ ÙˆØ§Ù„Ù…Ø¹Ù†Ù‰ØŒ ÙˆÙ…Ù† Ø«Ù… ØªÙˆÙ„ÙŠØ¯ Ø¬Ù…Ù„Ø© Ø£Ùˆ ÙÙ‚Ø±Ø© Ù†ØµÙŠØ© ØªÙƒÙˆÙ† Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„ØªÙ„Ùƒ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª. ÙŠØ±Ø¬Ù‰ ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙŠ ÙŠØªØ¹ÙŠÙ† Ø§ØªØ¨Ø§Ø¹Ù‡Ø§ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/all-distilroberta-v1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the SentenceTransformer class from the sentence_transformers library.\\\\n2. Instantiate the SentenceTransformer model with the pre-trained model 'sentence-transformers/all-distilroberta-v1'.\\\\n3. Encode the search query and documents as vectors using the model's 'encode()' method.\\\\n4. Calculate the similarity between the query and document vectors using a similarity measure, like cosine similarity.\\\\n5. Rank the documents based on their similarity scores and return the most relevant documents.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\n\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-distilroberta-v1\\\\')\\\\n\\\\nquery = \\\"search query\\\"\\\\ndocuments = [\\\"document1\\\", \\\"document2\\\", \\\"document3\\\"]\\\\n\\\\nquery_embedding = model.encode([query])\\\\ndoc_embeddings = model.encode(documents)\\\\n\\\\nranking = cosine_similarity(query_embedding, doc_embeddings)[0]\\\\nranking_sorted = sorted(range(len(ranking)), key=lambda i: ranking[i], reverse=True)\\\\n\\\\nmost_relevant = [documents[i] for i in ranking_sorted]', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-distilroberta-v1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/all-distilroberta-v1', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-distilroberta-v1')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': [{'name': 's2orc', 'accuracy': 'Not provided'}, {'name': 'MS Marco', 'accuracy': 'Not provided'}, {'name': 'yahoo_answers_topics', 'accuracy': 'Not provided'}]}, 'description': 'This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø±Ø¦ÙŠØ© ÙˆØ§Ù„Ø£Ø³Ø¦Ù„Ø© Ù„Ù„Ø±Ø¯ Ø¹Ù„ÙŠÙ‡Ø§ØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„ØµÙˆØ± Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ ÙˆÙØ¹Ø§Ù„.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary libraries from the transformers package. These include BlipProcessor and BlipForQuestionAnswering, which will be used for image question answering.\\\\n2. We then load the pre-trained model and processor with the from_pretrained method using the model name 'Salesforce/blip-vqa-capfilt-large'.\\\\n3. With the model and the processor in place, we can now process the raw image and a user-provided question.\\\\n4. To process the image and questions, we use the processor with the raw_image and the question text to create input tensors.\\\\n5. These input tensors are then passed to the model's generate method which outputs the answer to the question.\\\\n6. Finally, we decode and output the generated answer with the processor's decode method, which returns a user-friendly answer.\\\", '<<<code>>>': \\\"from PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\\\nprocessor = BlipProcessor.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\\nmodel = BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\\nraw_image = Image.open(image_path).convert('RGB')\\\\nquestion = user_provided_question\\\\ninputs = processor(raw_image, question, return_tensors='pt')\\\\noutput = model.generate(**inputs)\\\\nanswer = processor.decode(output[0], skip_special_tokens=True)\\\", 'api_call': \\\"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Salesforce/blip-vqa-capfilt-large', 'api_call': \\\"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\", 'api_arguments': {'raw_image': 'RGB image', 'question': 'string'}, 'python_environment_requirements': [{'transformers': 'BlipProcessor, BlipForQuestionAnswering'}], 'example_code': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\\\nquestion = how many dogs are in the picture?\\\\ninputs = processor(raw_image, question, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\\", 'performance': {'dataset': 'VQA', 'accuracy': '+1.6% in VQA score'}, 'description': 'BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªÙˆÙ„ÙŠØ¯ ØªØ¹Ù„ÙŠÙ…Ø© ØªÙØµÙŠÙ„ÙŠØ© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ø­ÙŠØ« ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯ Ù…Ù†Ø§Ø³Ø¨Ù‹Ø§ ÙˆÙ…ØªÙ‚Ù†Ù‹Ø§ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Qiliang/bart-large-cnn-samsum-ChatGPT_v3')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary libraries, which are 'AutoModelForSeq2SeqLM' and 'AutoTokenizer' from transformers.\\\\n2. Use the 'from_pretrained' method of the 'AutoModelForSeq2SeqLM' class to load the model, which in this case is 'Qiliang/bart-large-cnn-samsum-ChatGPT_v3'. This model is a fine-tuned version for text-to-text generation tasks.\\\\n3. Initialize the tokenizer with the same model name using 'AutoTokenizer.from_pretrained()'.\\\\n4. The loaded model will be used to generate responses for customer support inquiries through a conversational interface.\\\", '<<<code>>>': 'from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\\\n\\\\nmodel_name = \\\\'Qiliang/bart-large-cnn-samsum-ChatGPT_v3\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\n\\\\ndef generate_response(input_text):\\\\n    inputs = tokenizer.encode(input_text, return_tensors=\\\\'pt\\\\')\\\\n    outputs = model.generate(inputs, max_length=150, num_return_sequences=1)\\\\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n\\\\n# Example usage:\\\\n# input_text = \\\"My internet is not working properly. Can you help me?\\\"\\\\n# response = generate_response(input_text)\\\\n# print(response)', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Qiliang/bart-large-cnn-samsum-ChatGPT_v3')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bart-large-cnn-samsum-ChatGPT_v3', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Qiliang/bart-large-cnn-samsum-ChatGPT_v3')\\\", 'api_arguments': ['input_text'], 'python_environment_requirements': [['transformers==4.24.0', 'torch==1.12.1', 'datasets==2.6.1', 'tokenizers==0.13.2']], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'This model is a fine-tuned version of philschmid/bart-large-cnn-samsum on an unknown dataset.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ØªØ¬Ø§ÙˆØ¨ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„ØªÙŠ Ù†ØªØ¹Ø§Ù…Ù„ Ù…Ø¹Ù‡Ø§.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the required libraries: AutoModelForTableQuestionAnswering, AutoTokenizer, and pipeline from transformers.\\\\n2. Load the TAPAS model and tokenizer using from_pretrained method, specifying 'navteca/tapas-large-finetuned-wtq' as the pre-trained model.\\\\n3. Create a table-question-answering pipeline using the loaded model and tokenizer.\\\\n4. Use the pipeline to get the desired answer by providing the table data and a question/query about the table.\\\\n5. The result will contain the required information (in this case, the number of stars for a specific repository).\\\", '<<<code>>>': 'from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\\\\n\\\\ntapas_model = AutoModelForTableQuestionAnswering.from_pretrained(\\\\'navteca/tapas-large-finetuned-wtq\\\\')\\\\ntapas_tokenizer = AutoTokenizer.from_pretrained(\\\\'navteca/tapas-large-finetuned-wtq\\\\')\\\\n\\\\nnlp = pipeline(\\\\'table-question-answering\\\\', model=tapas_model, tokenizer=tapas_tokenizer)\\\\n\\\\ntable_data = {\\\\n\\\"Repository\\\": [\\\"Transformers\\\", \\\"Datasets\\\", \\\"Tokenizers\\\"],\\\\n\\\"Stars\\\": [\\\"36542\\\", \\\"4512\\\", \\\"3934\\\"],\\\\n\\\"Contributors\\\": [\\\"651\\\", \\\"77\\\", \\\"34\\\"],\\\\n\\\"Programming language\\\": [\\\"Python\\\", \\\"Python\\\", \\\"Rust, Python and NodeJS\\\"]\\\\n}\\\\nquery = \\\"How many stars does the transformers repository have?\\\"\\\\n\\\\nresult = nlp({\\\\'table\\\\': table_data, \\\\'query\\\\': query})\\\\nprint(result)', 'api_call': \\\"AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'navteca/tapas-large-finetuned-wtq', 'api_call': \\\"AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\", 'api_arguments': {'table': 'table_data', 'query': 'query'}, 'python_environment_requirements': [{'transformers': 'AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline'}], 'example_code': \\\"from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\\\\n\\\\n# Load model & tokenizer\\\\ntapas_model = AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\\ntapas_tokenizer = AutoTokenizer.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\\n\\\\n# Get predictions\\\\nnlp = pipeline('table-question-answering', model=tapas_model, tokenizer=tapas_tokenizer)\\\\nresult = nlp({'table': {'Repository': ['Transformers', 'Datasets', 'Tokenizers'], 'Stars': ['36542', '4512', '3934'], 'Contributors': ['651', '77', '34'], 'Programming language': ['Python', 'Python', 'Rust, Python and NodeJS']}, 'query': 'How many stars does the transformers repository have?'})\\\\nprint(result)\\\", 'performance': {'dataset': 'wikisql', 'accuracy': 'Not provided'}, 'description': 'TAPAS large model fine-tuned on WikiTable Questions (WTQ). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Text2Text GenerationØŒ Ø¨Ø­ÙŠØ« ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ø¬Ø¯ÙŠØ¯Ø© Ø§Ø³ØªÙ†Ø§Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"Parrot(model_tag='prithivida/parrot_paraphraser_on_T5', use_gpu=False)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary package and the Parrot class from the module.\\\\n2. Initialize the Parrot object with the model using the 'prithivida/parrot_paraphraser_on_T5' model for paraphrasing text.\\\\n3. A list of phrases should be available, each of which represents a possible response to a customer query.\\\\n4. Iterate through the list of phrases and use the 'parrot.augment' function to generate paraphrasable variations of the text. This can provide more diverse responses to customer queries while preserving the meaning.\\\", '<<<code>>>': 'from parrot import Parrot\\\\n\\\\nparrot = Parrot(model_tag=\\\\'prithivida/parrot_paraphraser_on_T5\\\\', use_gpu=False)\\\\nphrases = [\\\\n    \\\"Sure, I can help you with that.\\\",\\\\n    \\\"We appreciate your feedback.\\\",\\\\n]\\\\n\\\\nfor phrase in phrases:\\\\n    para_phrases = parrot.augment(input_phrase=phrase)\\\\n    for para_phrase in para_phrases:\\\\n        print(para_phrase)', 'api_call': \\\"Parrot(model_tag='prithivida/parrot_paraphraser_on_T5', use_gpu=False)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'prithivida/parrot_paraphraser_on_T5', 'api_call': \\\"Parrot(model_tag='prithivida/parrot_paraphraser_on_T5', use_gpu=False)\\\", 'api_arguments': ['input_phrase', 'diversity_ranker', 'do_diverse', 'max_return_phrases', 'max_length', 'adequacy_threshold', 'fluency_threshold'], 'python_environment_requirements': [['torch', 'transformers']], 'example_code': 'from parrot import Parrot\\\\nimport torch\\\\nimport warnings\\\\nwarnings.filterwarnings(ignore)\\\\n\\\\nparrot = Parrot(model_tag=prithivida/parrot_paraphraser_on_T5, use_gpu=False)\\\\nphrases = [Can you recommed some upscale restaurants in Newyork?,\\\\n What are the famous places we should not miss in Russia?\\\\n]\\\\nfor phrase in phrases:\\\\n print(-*100)\\\\n print(Input_phrase: , phrase)\\\\n print(-*100)\\\\n para_phrases = parrot.augment(input_phrase=phrase)\\\\n for para_phrase in para_phrases:\\\\n  print(para_phrase)', 'performance': {'dataset': 'Not mentioned', 'accuracy': 'Not mentioned'}, 'description': 'Parrot is a paraphrase based utterance augmentation framework purpose built to accelerate training NLU models. It offers knobs to control Adequacy, Fluency, and Diversity as per your needs. It mainly focuses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªØ·ÙˆÙŠØ± ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙˆØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ ØµÙˆØ± Ø£Ø®Ø±Ù‰ØŒ Ù†Ø­Ù† Ù†Ø±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©ØŒ Ù…Ø¹ Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙÙŠ Ø¹Ù…Ù„ÙŠØ© ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ±.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8m-pcb-defect-segmentation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include YOLO and render_result from the ultralyticsplus package.\\\\n2. Call the 'YOLO' function from the ultralyticsplus package to load the pre-trained model 'keremberke/yolov8m-pcb-defect-segmentation'. This model can detect and segment defects in Printed Circuit Board (PCB) images.\\\\n3. Set some model configuration options, such as confidence score threshold, IOU threshold, etc.\\\\n4. Provide the path to the image of the PCB that you want to analyze for defects.\\\\n5. Use the 'predict' method of the model to get the detection and segmentation results of the defects in the PCB image.\\\\n6. Finally, use the 'render_result' function to visualize the results.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-pcb-defect-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'URL or local path to the PCB image'\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8m-pcb-defect-segmentation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8m-pcb-defect-segmentation', 'api_call': \\\"YOLO('keremberke/yolov8m-pcb-defect-segmentation')\\\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': [['ultralyticsplus==0.0.24', 'ultralytics==8.0.23']], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8m-pcb-defect-segmentation')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'print(results[0].masks)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.568, 'mAP@0.5(mask)': 0.557}}, 'description': 'A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø­Ø§Ø¯Ø«Ø© Ø°ÙƒÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© ÙƒØ¨ÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø¶Ù…Ø§Ù† Ø¯Ù‚Ø© ÙˆÙØ¹Ø§Ù„ÙŠØ© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('ruDialoGpt3-medium-finetuned-telegram')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import AutoTokenizer and AutoModelForCausalLM from the transformers library by Hugging Face.\\\\n2. Use the pretrained model checkpoint 'ruDialoGpt3-medium-finetuned-telegram' to load both the tokenizer and the model.\\\\n3. The model is a DialoGPT trained on Russian language data and fine-tuned on a telegram chat dataset. It is designed for generating conversational responses in Russian.\\\\n4. Use the model and tokenizer to generate responses for a specific user input using the appropriate method.\\\", '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ncheckpoint = 'Kirili4ik/ruDialoGpt3-medium-finetuned-telegram'\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\\\nmodel.eval()\\\\ninput_text = 'Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚, ĞºĞ°Ğº Ğ´ĞµĞ»Ğ°?'\\\\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\\\\noutput_tokens = model.generate(input_ids, max_length=100, no_repeat_ngram_size=3, do_sample=True, top_k=50, top_p=0.9, temperature=0.6)\\\\nresponse = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\\\", 'api_call': \\\"AutoModelForCausalLM.from_pretrained('ruDialoGpt3-medium-finetuned-telegram')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ruDialoGpt3-medium-finetuned-telegram', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('ruDialoGpt3-medium-finetuned-telegram')\\\", 'api_arguments': {'num_return_sequences': 1, 'max_length': 512, 'no_repeat_ngram_size': 3, 'do_sample': True, 'top_k': 50, 'top_p': 0.9, 'temperature': 0.6, 'mask_token_id': 'tokenizer.mask_token_id', 'eos_token_id': 'tokenizer.eos_token_id', 'unk_token_id': 'tokenizer.unk_token_id', 'pad_token_id': 'tokenizer.pad_token_id', 'device': 'cpu'}, 'python_environment_requirements': [{'huggingface_transformers': 'AutoTokenizer, AutoModelForCausalLM', 'torch': 'zeros, cat'}], 'example_code': 'checkpoint = Kirili4ik/ruDialoGpt3-medium-finetuned-telegram\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\\\nmodel.eval()', 'performance': {'dataset': 'Russian forums and Telegram chat', 'accuracy': 'Not available'}, 'description': 'DialoGPT trained on Russian language and fine tuned on my telegram chat. This model was created by sberbank-ai and trained on Russian forums. It has been fine-tuned on a 30mb json file of exported telegram chat data.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªÙ„Ø®ÙŠØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ø§Ø¨Ø¯Ø£ Ø¨ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¬Ù…Ù„ ÙˆØ§Ù„Ù†ØµÙˆØµ Ø«Ù… Ø§Ø³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ·Ø¨ÙŠÙ‚Ù‡ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ù„Ù… ØªØ³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨. Ù„ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙ„Ø®ÙŠØµØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø«Ù„ ØªØ¹Ø²ÙŠØ² Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù…Ù‡Ø§Ù….? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('plguillou/t5-base-fr-sum-cnndm')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the T5Tokenizer and T5ForConditionalGeneration from the transformers package.', 'code': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'plguillou/t5-base-fr-sum-cnndm\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'plguillou/t5-base-fr-sum-cnndm\\\\')\\\\ninput_text = \\\"summarize: Hier, une banque dans le centre de Paris a Ã©tÃ© la cible d\\\\'un braquage audacieux. Trois hommes armÃ©s ont pÃ©nÃ©trÃ© dans la banque et ont pris plusieurs otages. La police est rapidement intervenue et a rÃ©ussi Ã  libÃ©rer tous les otages aprÃ¨s une courte nÃ©gociation avec les braqueurs. Les malfaiteurs ont Ã©tÃ© arrÃªtÃ©s et personne n\\\\'a Ã©tÃ© blessÃ© lors de l\\\\'incident.\\\"\\\\ninput_tokens = tokenizer.encode(input_text, return_tensors=\\\\'pt\\\\')\\\\nsummary_output = model.generate(input_tokens, num_beams=4, max_length=30, early_stopping=True)\\\\nsummary_text = tokenizer.decode(summary_output[0], skip_special_tokens=True)', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Abstractive Text Summarization', 'api_name': 'plguillou/t5-base-fr-sum-cnndm', 'api_call': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'plguillou/t5-base-fr-sum-cnndm\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'plguillou/t5-base-fr-sum-cnndm\\\\')\\\\ninput_text = \\\"summarize: Hier, une banque dans le centre de Paris a Ã©tÃ© la cible d\\\\'un braquage audacieux. Trois hommes armÃ©s ont pÃ©nÃ©trÃ© dans la banque et ont pris plusieurs otages. La police est rapidement intervenue et a rÃ©ussi Ã  libÃ©rer tous les otages aprÃ¨s une courte nÃ©gociation avec les braqueurs. Les malfaiteurs ont Ã©tÃ© arrÃªtÃ©s et personne n\\\\'a Ã©tÃ© blessÃ© lors de l\\\\'incident.\\\"\\\\ninput_tokens = tokenizer.encode(input_text, return_tensors=\\\\'pt\\\\')\\\\nsummary_output = model.generate(input_tokens, num_beams=4, max_length=30, early_stopping=True)\\\\nsummary_text = tokenizer.decode(summary_output[0], skip_special_tokens=True)', 'api_arguments': {'input_text': 'summarize: ARTICLE'}, 'python_environment_requirements': {'transformers': 'from transformers import T5Tokenizer, T5ForConditionalGeneration'}, 'example_code': 'tokenizer = T5Tokenizer.from_pretrained(plguillou/t5-base-fr-sum-cnndm)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(plguillou/t5-base-fr-sum-cnndm)', 'performance': {'dataset': 'cnn_dailymail', 'ROUGE-1': 44.5252, 'ROUGE-2': 22.652, 'ROUGE-L': 29.8866}, 'description': 'This model is a T5 Transformers model (JDBN/t5-base-fr-qg-fquad) that was fine-tuned in French for abstractive text summarization.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ Ù„Ø¹Ù„ÙˆÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ø­ÙŠØ« ÙŠØ¬ØªÙ…Ø¹ Ø§Ù„Ø£Ø¹Ø¶Ø§Ø¡ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ ØªÙˆÙ„ÙŠØ¯ÙŠ. ÙŠØ¬Ø¨ ØªØ²ÙˆÙŠØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fr-es')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To perform the translations, we need to import the necessary libraries from the Hugging Face Transformers package, specifically AutoModelForSeq2SeqLM and AutoTokenizer.\\\\n2. We then load the French to Spanish translation model, 'Helsinki-NLP/opus-mt-fr-es', using the from_pretrained method of the AutoModelForSeq2SeqLM class.\\\\n3. After loading the model, we create the tokenizer for pre-processing the text using the from_pretrained method of the AutoTokenizer class.\\\\n4. With the model and tokenizer prepared, we can now use them to translate the product description from French to Spanish.\\\", '<<<code>>>': 'from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\\\n\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'Helsinki-NLP/opus-mt-fr-es\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Helsinki-NLP/opus-mt-fr-es\\\\')\\\\n\\\\nfrench_text = \\\"Description du produit en franÃ§ais ici...\\\"\\\\n\\\\ninputs = tokenizer(french_text, return_tensors=\\\"pt\\\", padding=True, truncation=True)\\\\noutputs = model.generate(inputs[\\\"input_ids\\\"], max_length=50, num_return_sequences=1)\\\\nspanish_translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fr-es')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-fr-es', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fr-es')\\\", 'api_arguments': {'source_languages': 'fr', 'target_languages': 'es'}, 'python_environment_requirements': [{'PyTorch': '1.0.0', 'TensorFlow': '2.0', 'Transformers': '4.0.0'}], 'example_code': \\\"translation('Bonjour, comment Ã§a va?')\\\", 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': {'newssyscomb2009.fr.es': 34.3, 'news-test2008.fr.es': 32.5, 'newstest2009.fr.es': 31.6, 'newstest2010.fr.es': 36.5, 'newstest2011.fr.es': 38.3, 'newstest2012.fr.es': 38.1, 'newstest2013.fr.es': 34.0, 'Tatoeba.fr.es': 53.2}, 'chr-F': {'newssyscomb2009.fr.es': 0.601, 'news-test2008.fr.es': 0.583, 'newstest2009.fr.es': 0.586, 'newstest2010.fr.es': 0.616, 'newstest2011.fr.es': 0.622, 'newstest2012.fr.es': 0.619, 'newstest2013.fr.es': 0.587, 'Tatoeba.fr.es': 0.709}}}, 'description': 'A French to Spanish translation model trained on the OPUS dataset using the Hugging Face Transformers library. The model is based on the transformer-align architecture and uses normalization and SentencePiece for pre-processing.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ© ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø§ØªØŒ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªÙˆÙÙŠØ± Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ Ø§Ù„Ø°ÙŠ ØªØªØ¹Ø§Ù…Ù„ Ù…Ø¹Ù‡ ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù Ø¬Ù…Ø¹Ù‡Ø§. Ø³Ù†Ù‚ÙˆÙ… Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„ ÙˆØ¥ÙƒÙ…Ø§Ù„ Ø§Ù„ÙØ¬ÙˆØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"pipeline('question-answering', model='deepset/deberta-v3-base-squad2', tokenizer='deepset/deberta-v3-base-squad2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the required libraries, which are 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=\\\\'deepset/deberta-v3-base-squad2\\\\', tokenizer=\\\\'deepset/deberta-v3-base-squad2\\\\')\\\\nquestion = \\\"Your question here\\\"\\\\ncontext = \\\"Long paragraph of text or context\\\"\\\\nQA_input = {\\\\'question\\\\': question, \\\\'context\\\\': context}\\\\nresult = nlp(QA_input)', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'deepset/deberta-v3-base-squad2', 'api_call': 'from transformers import pipeline\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=\\\\'deepset/deberta-v3-base-squad2\\\\', tokenizer=\\\\'deepset/deberta-v3-base-squad2\\\\')\\\\nquestion = \\\"Your question here\\\"\\\\ncontext = \\\"Long paragraph of text or context\\\"\\\\nQA_input = {\\\\'question\\\\': question, \\\\'context\\\\': context}\\\\nresult = nlp(QA_input)', 'api_arguments': {'model_name': 'deepset/deberta-v3-base-squad2', 'tokenizer': 'deepset/deberta-v3-base-squad2'}, 'python_environment_requirements': ['transformers'], 'example_code': {'import': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline', 'initialize': [\\\"model_name = 'deepset/deberta-v3-base-squad2'\\\", \\\"nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\\\"], 'example_input': {'question': 'Why is model conversion important?', 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'}, 'example_output': 'res = nlp(QA_input)'}, 'performance': {'dataset': 'squad_v2', 'accuracy': {'Exact Match': 83.825, 'F1': 87.41}}, 'description': \\\"This is the deberta-v3-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering.\\\"}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø·ÙˆÙ‘Ø± ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¢Ù„ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ù„ØºØ© Ù…Ø¹ÙŠÙ†Ø© Ø¥Ù„Ù‰ Ù„ØºØ© Ø£Ø®Ø±Ù‰ Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„ ÙˆØ¯Ù‚ÙŠÙ‚ØŒ Ø®Ø§ØµØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·Ø¨ÙŠØ© ÙˆØªØ¹Ø¨Ø¦Ø© Ø§Ù„ÙØ±Ø§ØºØ§Øª.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library.\\\\n2. Use the pipeline function to create a fill-mask model with the 'emilyalsentzer/Bio_ClinicalBERT' model, which has been trained on clinical text and is suitable for medical-related text completion tasks.\\\\n3. Use the created model to fill in the missing word in the provided text.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nfill_mask = pipeline(\\\"fill-mask\\\", model=\\\"emilyalsentzer/Bio_ClinicalBERT\\\")\\\\ntext = \\\"The patient was recently diagnosed with <mask> disease.\\\"\\\\nresult = fill_mask(text)', 'api_call': \\\"AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'emilyalsentzer/Bio_ClinicalBERT', 'api_call': \\\"AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\\\", 'api_arguments': ['AutoTokenizer', 'AutoModel', 'from_pretrained'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\\\\nmodel = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\\\", 'performance': {'dataset': 'MIMIC III', 'accuracy': 'Not provided'}, 'description': 'Bio_ClinicalBERT is a model initialized with BioBERT and trained on all MIMIC notes. It can be used for various NLP tasks in the clinical domain, such as Named Entity Recognition (NER) and Natural Language Inference (NLI).' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·Ø¨ÙŠÙ‚ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„ØºÙˆÙŠØ© Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø´ÙƒØ§Ù„ Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ø§Ù„ØµÙˆØ±Ø©-Ø¥Ù„Ù‰-ØµÙˆØ±Ø©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙ†ÙÙŠØ° Ù‡Ø°Ø§ØŸ? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We first need to import the required classes and libraries. We will need ControlNetModel from the diffusers package. Ensure transformers and diffusers packages are also installed.\\\\n2. We load the pre-trained model 'lllyasviel/sd-controlnet-seg' by using ControlNetModel.from_pretrained().\\\\n3. This model can be used to predict segmentation maps for the images of houses. The pipeline function is used to process the input images and generate segmentation maps.\\\\n4. After obtaining the predicted segmentation maps, the images can be saved in the required format.\\\", '<<<code>>>': 'from diffusers import ControlNetModel\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/sd-controlnet-seg\\\\')\\\\nfor house_image in house_images:\\\\n    segmented_image = controlnet(house_image, num_inference_steps=20).images[0]\\\\n    segmented_image.save(f\\\"{house_image}_seg_out.png\\\")', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/sd-controlnet-seg', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", 'api_arguments': ['torch_dtype'], 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate']], 'example_code': \\\"image = pipe(house, image, num_inference_steps=20).images[0]\\\\nimage.save('./images/house_seg_out.png')\\\", 'performance': {'dataset': 'ADE20K', 'accuracy': 'Trained on 164K segmentation-image, caption pairs'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ù†Ø´Ø¦ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†ÙƒÙ… Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªØŸ? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a question-answering model using the 'tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa' model. This model has been fine-tuned for question-answering tasks and can assist in answering various questions related to different topics.\\\\n3. The created model can be used to answer questions within the given context.\\\\n4. By providing the question and context to the model, it will return the most probable answer to the question.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nqa_model = pipeline(\\\\'question-answering\\\\', model=\\\\'tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa\\\\')\\\\nquestion = \\\"What is the capital of France?\\\"\\\\ncontext = \\\"Paris is the largest city of France and it is also the capital of the country.\\\"\\\\nanswer = qa_model(question=question, context=context)', 'api_call': \\\"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa', 'api_call': \\\"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\\\", 'api_arguments': 'question, context', 'python_environment_requirements': [['transformers==4.15.0', 'torch==1.8.0+cu101', 'datasets==1.17.0', 'tokenizers==0.10.3']], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 4.3332}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø·ÙˆØ± ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ© Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", '<<<api_provider>>>': 'speechbrain', '<<<explanation>>>': '1. We import the necessary libraries: torchaudio, speechbrain\\\\'s Tacotron2, and HIFIGAN.\\\\n2. We then load the pre-trained Tacotron2 and HIFIGAN models for German texts using the \\\\'padmalcom/tts-tacotron2-german\\\\' and \\\\'padmalcom/tts-hifigan-german\\\\' sources respectively.\\\\n3. We encode the text \\\"Mary hatte ein kleines Lamm\\\" using the Tacotron2 model to produce a mel spectrogram as the output.\\\\n4. Next, we use the HIFIGAN model to decode the mel spectrogram and generate a waveform.\\\\n5. We finally save the waveform as an audio file \\\\'example_TTS.wav\\\\' in the working directory.', '<<<code>>>': 'import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2, HIFIGAN\\\\n\\\\ntacotron2 = Tacotron2.from_hparams(source=\\\\'padmalcom/tts-tacotron2-german\\\\')\\\\nhifi_gan = HIFIGAN.from_hparams(source=\\\\'padmalcom/tts-hifigan-german\\\\')\\\\ntext = \\\"Mary hatte ein kleines Lamm\\\"\\\\nmel_output, mel_length, alignment = tacotron2.encode_text(text)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save(\\\\'example_TTS.wav\\\\', waveforms.squeeze(1), 22050)', 'api_call': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", 'provider': 'speechbrain', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tts-hifigan-german', 'api_call': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", 'api_arguments': ['mel_output'], 'python_environment_requirements': [['speechbrain']], 'example_code': \\\"import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2\\\\nfrom speechbrain.pretrained import HIFIGAN\\\\ntacotron2 = Tacotron2.from_hparams(source=padmalcom/tts-tacotron2-german, savedir=tmpdir_tts)\\\\nhifi_gan = HIFIGAN.from_hparams(source=padmalcom/tts-hifigan-german, savedir=tmpdir_vocoder)\\\\nmel_output, mel_length, alignment = tacotron2.encode_text(Mary had a little lamb)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\\\", 'performance': {'dataset': 'custom German dataset', 'accuracy': 'Not specified'}, 'description': 'A HiFIGAN vocoder trained on a generated German dataset using mp3_to_training_data. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ±ØºØ¨ ÙÙŠ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¢Ù„ÙŠ Ø§Ù„Ø°ÙŠ Ù„Ø§ ÙŠØªØ·Ù„Ø¨ ØªØ¯Ø±ÙŠØ¨ Ø³Ø§Ø¨Ù‚. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ·ÙŠØ¹ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§ Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ù‚Ø§Ø¦Ù…Ø© Ù…Ø³Ø¨Ù‚Ø© Ù…Ù† Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the 'transformers' library.\\\\n2. Create an instance of the pipeline using the 'zero-shot-classification' task by specifying the pre-trained model 'valhalla/distilbart-mnli-12-6'.\\\\n3. Define the candidate labels which in this case would be ['professional', 'casual', 'negative'].\\\\n4. Use the created pipeline to pass your marketing article and the candidate labels to determine the tone of the article based on the highest probability.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'valhalla/distilbart-mnli-12-6\\\\')\\\\narticle_text = \\\"Your marketing article here...\\\"\\\\ncandidate_labels = [\\\\'professional\\\\', \\\\'casual\\\\', \\\\'negative\\\\']\\\\nresult = classifier(article_text, candidate_labels)\\\\narticle_tone = result[\\\\'labels\\\\'][0]', 'api_call': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'valhalla/distilbart-mnli-12-6', 'api_call': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')\\\", 'api_arguments': 'text, candidate_labels', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nnlp = pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')\\\\nresult = nlp('The movie was great!', ['positive', 'negative'])\\\\nprint(result)\\\", 'performance': {'dataset': 'MNLI', 'accuracy': {'matched_acc': '89.19', 'mismatched_acc': '89.01'}}, 'description': 'distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is designed for zero-shot classification tasks.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙˆÙØ± Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù†Ø§Ø¯ÙŠÙ‹Ø§ Ø·Ù„Ø§Ø¨ÙŠÙ‹Ø§ ÙŠÙØ¹Ù‚Ø¯ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù…Ù…Ø§Ø±Ø³Ø© ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„ÙŠØªÙ…ÙƒÙ†ÙˆØ§ Ù…Ù† Ø§Ù„ØªÙ†Ø§ÙØ³ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the 'pipeline' function from the transformers library provided by Hugging Face.\\\", 'code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\\nresult = qa_pipeline({'context': 'your_context_here', 'question': 'your_question_here'})\\\", 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large', 'api_call': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\\nresult = qa_pipeline({'context': 'your_context_here', 'question': 'your_question_here'})\\\", 'api_arguments': 'context, question', 'python_environment_requirements': 'transformers', 'example_code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\\nresult = qa_pipeline({'context': 'your_context_here', 'question': 'your_question_here'})\\\", 'performance': {'dataset': 'Dureader-2021', 'accuracy': '83.1'}, 'description': 'A Chinese MRC roberta_wwm_ext_large model trained on a large amount of Chinese MRC data. This model has significantly improved performance on reading comprehension and classification tasks. It has helped multiple users achieve top 5 results in the Dureader-2021 competition.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ØªØ¹Ù„Ù… ØªØ¹Ø²ÙŠØ²ÙŠ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø°ÙƒÙŠØ© Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ù…Ù‡Ø§Ù… Ù…Ø¹ÙŠÙ†Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙˆØ§Ù„Ù…ÙƒØ§ÙØ£Ø©.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"load_from_hub(repo_id='sb3/dqn-Acrobot-v1',filename='{MODEL FILENAME}.zip',)\\\", '<<<api_provider>>>': 'Stable-Baselines3', '<<<explanation>>>': \\\"1. To train your robot to balance the Acrobot-v1, we recommend using the DQN algorithm from Stable-Baselines3 library in combination with the RL Zoo framework.\\\\n2. You can load pre-trained Acrobot-v1 model using the 'load_from_hub' method with its associated parameters.\\\\n3. Utilize this API to perform training and evaluation of the robot via different training options, such as 'train.py', 'enjoy.py', and 'push_to_hub' provided in 'Stable-Baselines3'.\\\\n4. Monitor your training progress and results by storing logs in the specified logs folder.\\\\n5. Finally, you can fine-tune the model based on your specific problem if necessary.\\\", '<<<code>>>': \\\"from rl_zoo3 import load_from_hub\\\\ntrained_agent = load_from_hub(repo_id='sb3/dqn-Acrobot-v1', filename='{MODEL FILENAME}.zip')\\\", 'api_call': \\\"load_from_hub(repo_id='sb3/dqn-Acrobot-v1',filename='{MODEL FILENAME}.zip',)\\\", 'provider': 'Stable-Baselines3', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sb3/dqn-Acrobot-v1', 'api_call': \\\"load_from_hub(repo_id='sb3/dqn-Acrobot-v1',filename='{MODEL FILENAME}.zip',)\\\", 'api_arguments': {'algo': 'dqn', 'env': 'Acrobot-v1', 'logs_folder': 'logs/'}, 'python_environment_requirements': [['rl_zoo3', 'stable-baselines3', 'stable-baselines3-contrib']], 'example_code': ['python -m rl_zoo3.load_from_hub --algo dqn --env Acrobot-v1 -orga sb3 -f logs/', 'python enjoy.py --algo dqn --env Acrobot-v1 -f logs/', 'python train.py --algo dqn --env Acrobot-v1 -f logs/', 'python -m rl_zoo3.push_to_hub --algo dqn --env Acrobot-v1 -f logs/ -orga sb3'], 'performance': {'dataset': 'Acrobot-v1', 'accuracy': '-72.10 +/- 6.44'}, 'description': 'This is a trained model of a DQN agent playing Acrobot-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ù†ÙŠØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù†Øµ Ø¨Ø¯ÙˆÙ† Ø¹Ù†ØµØ± Ø§Ù„ØµÙˆØ±Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='gpt2-large')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a text generation model using the \\\\'gpt2-large\\\\' model. This is a powerful, pre-trained GPT-2 model with 774 million parameters that is excellent at generating human-like text based on the input.\\\\n3. Set a seed (optional) for deterministic results on consecutive runs.\\\\n4. Provide a prompt to start the story, such as \\\"Once upon a time in a magical forest,\\\" and set the desired maximum length and number of sequences. The model will generate text based on this prompt in the form of a new children\\\\'s story.', '<<<code>>>': 'from transformers import pipeline, set_seed\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'gpt2-large\\\\')\\\\nset_seed(42)\\\\nstory_start = \\\"Once upon a time in a magical forest,\\\"\\\\ngenerated_story = generator(story_start, max_length=100, num_return_sequences=1)', 'api_call': \\\"pipeline('text-generation', model='gpt2-large')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'gpt2-large', 'api_call': \\\"pipeline('text-generation', model='gpt2-large')\\\", 'api_arguments': {'text': \\\"Hello, I'm a language model,\\\", 'max_length': 30, 'num_return_sequences': 5}, 'python_environment_requirements': [{'transformers': 'pipeline, set_seed', 'PyTorch': 'GPT2Tokenizer, GPT2Model', 'TensorFlow': 'GPT2Tokenizer, TFGPT2Model'}], 'example_code': \\\"from transformers import pipeline, set_seed\\\\ngenerator = pipeline('text-generation', model='gpt2-large')\\\\nset_seed(42)\\\\ngenerator(Hello, I'm a language model,, max_length=30, num_return_sequences=5)\\\", 'performance': {'dataset': {'LAMBADA': {'PPL': 10.87}, 'CBT-CN': {'ACC': 93.45}, 'CBT-NE': {'ACC': 88.0}, 'WikiText2': {'PPL': 19.93}, 'PTB': {'PPL': 40.31}, 'enwiki8': {'BPB': 0.97}, 'text8': {'BPC': 1.02}, 'WikiText103': {'PPL': 22.05}, '1BW': {'PPL': 44.575}}}, 'description': 'GPT-2 Large is the 774M parameter version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚Ø¯Ù… Ø¯ÙˆØ±ÙŠÙ‹Ø§ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·. Ù‚Ù… Ø¨ØªØ¬Ù‡ÙŠØ² ÙØ±ÙŠÙ‚Ùƒ Ø¨Ø§Ù„Ù…Ø¹Ø¯Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±.? [/INST]'<<<domain>>>': 'Multimodal Feature Extraction', '<<<api_call>>>': \\\"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, i.e. 'AutoModel' and 'AutoTokenizer' from transformers.\\\\n2. Load the pretrained model 'GanjinZero/UMLSBert_ENG' using AutoModel.from_pretrained().\\\\n3. Load the tokenizer using AutoTokenizer.from_pretrained().\\\\n4. To obtain the term embeddings, tokenize the input medical terms using the loaded tokenizer.\\\\n5. Pass the tokenized input to the loaded model to get the embeddings for the medical terms.\\\\n6. These embeddings can then be used to analyze, classify, or represent medical terms in the app's glossary.\\\", '<<<code>>>': \\\"from transformers import AutoModel, AutoTokenizer\\\\nmodel = AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\\\\ntokenizer = AutoTokenizer.from_pretrained('GanjinZero/UMLSBert_ENG')\\\\nmedical_terms = ['medical term 1', 'medical term 2']\\\\ntokenized_input = tokenizer(medical_terms, return_tensors='pt', padding=True, truncation=True)\\\\nembeddings = model(**tokenized_input).last_hidden_state\\\", 'api_call': \\\"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \\\"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨ØªØ¹Ù„Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙŠ Ø«Ù… ØªÙ‚Ø¯ÙŠÙ… ØªÙ†Ø¨Ø¤Ø§Øª Ù…Ø¹Ù‚Ø¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙŠ. Ø§Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø£Ø¯Ø§Ø¡.? [/INST]{'domain': 'Natural Language Processing Text Classification', 'api_call': \\\"pipeline(model='martin-ha/toxic-comment-model')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\\\\nmodel_path = \\\\'martin-ha/toxic-comment-model\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\\\\ntoxic_comment_detector = TextClassificationPipeline(model=model, tokenizer=tokenizer)\\\\n\\\\ncomment = \\\"This is a test text.\\\"\\\\nclassification = toxic_comment_detector(comment)\\\\nif classification[0][\\\\'label\\\\'] == \\\\'LABEL_1\\\\' and classification[0][\\\\'score\\\\'] > 0.5:\\\\n    print(\\\"Toxic comment detected and removed.\\\")\\\\nelse:\\\\n    print(\\\"Non-toxic comment.\\\")', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'martin-ha/toxic-comment-model', 'api_call': 'from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\\\\nmodel_path = \\\\'martin-ha/toxic-comment-model\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\\\\ntoxic_comment_detector = TextClassificationPipeline(model=model, tokenizer=tokenizer)\\\\n\\\\ncomment = \\\"This is a test text.\\\"\\\\nclassification = toxic_comment_detector(comment)\\\\nif classification[0][\\\\'label\\\\'] == \\\\'LABEL_1\\\\' and classification[0][\\\\'score\\\\'] > 0.5:\\\\n    print(\\\"Toxic comment detected and removed.\\\")\\\\nelse:\\\\n    print(\\\"Non-toxic comment.\\\")', 'api_arguments': {'model_path': 'martin-ha/toxic-comment-model'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\\\\nmodel_path = martin-ha/toxic-comment-model\\\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\\\\npipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\\\\nprint(pipeline('This is a test text.'))\\\", 'performance': {'dataset': 'held-out test set', 'accuracy': 0.94, 'f1-score': 0.59}, 'description': 'This model is a fine-tuned version of the DistilBERT model to classify toxic comments.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØªÙ… ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø±Ø¦ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø£ÙˆØ³Ø§Ø·ØŒ ÙˆØ°Ù„Ùƒ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ù†ØµÙˆØµ Ù…Ø¹Ù‹Ø§ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ ÙˆØ°Ù„Ùƒ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØµÙˆØ± Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the necessary classes, BlipProcessor and BlipForQuestionAnswering, from the transformers library.\\\\n2. Load the pretrained model and processor by calling the from_pretrained() method on both classes with 'Salesforce/blip-vqa-capfilt-large' as the argument. The processor will handle the raw image and question, while the model is responsible for generating the answer.\\\\n3. Fetch the image using a library like requests and convert it to an RGB format with PIL.Image.\\\\n4. Pass the raw image and the user's question to the processor, which will prepare the input for the model.\\\\n5. Feed the processed input to the model and generate an output.\\\\n6. Decode the output to obtain the answer to the user's question about the image content.\\\", '<<<code>>>': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\\\nprocessor = BlipProcessor.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\\nmodel = BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\\nimg_url = 'https://example.com/image.jpg'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\\\nquestion = 'How many dogs are in the picture?'\\\\ninputs = processor(raw_image, question, return_tensors='pt')\\\\nout = model.generate(**inputs)\\\\nanswer = processor.decode(out[0], skip_special_tokens=True)\\\", 'api_call': \\\"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Salesforce/blip-vqa-capfilt-large', 'api_call': \\\"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\", 'api_arguments': {'raw_image': 'RGB image', 'question': 'string'}, 'python_environment_requirements': [{'transformers': 'BlipProcessor, BlipForQuestionAnswering'}], 'example_code': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\\\nquestion = how many dogs are in the picture?\\\\ninputs = processor(raw_image, question, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\\", 'performance': {'dataset': 'VQA', 'accuracy': '+1.6% in VQA score'}, 'description': 'BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ØªÙ†ÙÙŠØ° Ø¹Ù…Ù„ÙŠØ© ØªØµÙ†ÙŠÙ Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØµÙ†Ù Ù‚Ø¨Ù„ÙŠØŒ Ø­ÙŠØ« ØªÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨ØªØ­Ø¯ÙŠØ¯ ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø¹Ø·Ù‰.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('Recognai/bert-base-spanish-wwm-cased-xnli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library.\\\\n2. Use the pipeline function to create a zero-shot classification model, which can categorize text into predefined categories without being explicitly trained on those categories.\\\\n3. Specify the model 'Recognai/bert-base-spanish-wwm-cased-xnli', which is a fine-tuned version of the Spanish BERT model, trained on the Spanish portion of the XNLI dataset.\\\\n4. The created classifier can be used to classify the news article in Spanish into different categories, such as 'culture', 'society', 'economy', 'health', and 'sports'.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'Recognai/bert-base-spanish-wwm-cased-xnli\\\\')\\\\narticle_text = \\\"Texto del artÃ­culo en espaÃ±ol...\\\"\\\\ncandidate_labels = [\\\\'cultura\\\\', \\\\'sociedad\\\\', \\\\'economia\\\\', \\\\'salud\\\\', \\\\'deportes\\\\']\\\\nresult = classifier(article_text, candidate_labels)', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('Recognai/bert-base-spanish-wwm-cased-xnli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Recognai/bert-base-spanish-wwm-cased-xnli', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('Recognai/bert-base-spanish-wwm-cased-xnli')\\\", 'api_arguments': ['sequence', 'candidate_labels', 'hypothesis_template'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=Recognai/bert-base-spanish-wwm-cased-xnli)\\\\nclassifier(\\\\nEl autor se perfila, a los 50 aÃ±os de su muerte, como uno de los grandes de su siglo,\\\\ncandidate_labels=[cultura, sociedad, economia, salud, deportes],\\\\nhypothesis_template=Este ejemplo es {}. \\\\n)', 'performance': {'dataset': 'XNLI-es', 'accuracy': '79.9%'}, 'description': 'This model is a fine-tuned version of the spanish BERT model with the Spanish portion of the XNLI dataset. You can have a look at the training script for details of the training.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ®Ø¯Ù… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø®Ø§Øµ Ø¨Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©ØŒ ÙÙŠ Ø³ÙŠØ§Ù‚ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø£Ø¬ÙˆØ¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import required classes from transformers, such as AutoModelForDocumentQuestionAnswering and AutoTokenizer.\\\\n2. Load the pre-trained model 'tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa' using the from_pretrained() method. This model is trained on document question answering tasks, which is suitable for extracting information from historical documents.\\\\n3. Load the tokenizer using the from_pretrained() method to process the textual data.\\\\n4. Tokenize the input document and the question you want to ask.\\\\n5. Pass the tokenized input to the model and obtain the answer.\\\\n6. Decode the answer using the tokenizer.\\\", '<<<code>>>': 'from transformers import AutoModelForDocumentQuestionAnswering, AutoTokenizer\\\\nmodel = AutoModelForDocumentQuestionAnswering.from_pretrained(\\\\'tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\\\\')\\\\ndocument = \\\"Document text here...\\\"\\\\nquestion = \\\"Your question here...\\\"\\\\ninputs = tokenizer(question, document, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\\\noutputs = model(**inputs)\\\\nanswer_start = outputs.start_logits.argmax().item()\\\\nanswer_end = outputs.end_logits.argmax().item() + 1\\\\nanswer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs.input_ids[0, answer_start:answer_end])).strip()', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'layoutlmv2-base-uncased-finetuned-docvqa', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers==4.12.2', 'torch==1.8.0+cu101', 'datasets==1.14.0', 'tokenizers==0.10.3']], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 1.194}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ ØªÙ‚Ø³ÙŠÙ… ØµÙˆØ±Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ÙØµÙ„ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ÙÙ‚Ø¯Ù…Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the required libraries - SegformerFeatureExtractor and SegformerForSemanticSegmentation from transformers, Image from PIL, and requests.\\\\n2. Load the segformer-b0-finetuned-cityscapes-1024-1024 model using the SegformerFeatureExtractor and SegformerForSemanticSegmentation classes from Hugging Face transformers.\\\\n3. Load an image of a cityscape using the PIL library and download the image using the requests library.\\\\n4. Preprocess the image using the SegformerFeatureExtractor to create the input tensors.\\\\n5. Pass the input tensors to the model to obtain segmentation logits.\\\\n6. The output can later be processed to obtain the final segmentation map that can be used to analyze different objects in the cityscape image.', '<<<code>>>': \\\"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\", 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024', 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªØ­ÙˆÙ„ Ø§Ù„Ù†ØµÙŠ Ø¥Ù„Ù‰ ØµÙˆØ± Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"pipeline('text-to-image', model='prompthero/openjourney-v4')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. You will need to import the necessary library, the \\\\'pipeline\\\\' function from transformers.\\\\n2. Load the text-to-image model \\\\'prompthero/openjourney-v4\\\\' using the \\\\'pipeline\\\\' function with \\\\'text-to-image\\\\' as the task and specify the model name.\\\\n3. Provide a text description for the book cover, such as \\\"Alice in Wonderland book cover featuring Alice, the White Rabbit, and the Cheshire Cat in a colorful, whimsical setting.\\\"\\\\n4. Run the text-to-image pipeline with the input text to generate an image based on the provided instructions.\\\\n5. Save the generated image to be used as the book cover.', '<<<code>>>': 'from transformers import pipeline\\\\ntext2image = pipeline(\\\\'text-to-image\\\\', model=\\\\'prompthero/openjourney-v4\\\\')\\\\ndescription = \\\"Alice in Wonderland book cover featuring Alice, the White Rabbit, and the Cheshire Cat in a colorful, whimsical setting.\\\"\\\\ngenerated_image = text2image(description)\\\\n# Save the generated image as the book cover\\\\ngenerated_image.save(\\\\'alice_in_wonderland_cover.png\\\\')', 'api_call': \\\"pipeline('text-to-image', model='prompthero/openjourney-v4')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'prompthero/openjourney-v4', 'api_call': \\\"pipeline('text-to-image', model='prompthero/openjourney-v4')\\\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"generate_image('your text here')\\\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦Ù ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required pipeline function from the transformers package.\\\\n2. Call the pipeline function with the task 'question-answering' and the pre-trained model 'distilbert-base-cased-distilled-squad' to create a question-answer instance.\\\\n3. Provide the question and the context paragraph to the instance, which will analyze the given text and come up with the best answer.\\\\n4. Print the answer and the associated score, start, and end positions.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nquestion_answerer = pipeline(\\\\'question-answering\\\\', model=\\\\'distilbert-base-cased-distilled-squad\\\\')\\\\ncontext = \\\"\\\"\\\"Your context paragraph here...\\\"\\\"\\\"\\\\nquestion = \\\"Your question here...\\\"\\\\nresult = question_answerer(question=question, context=context)\\\\n\\\\nprint(f\\\"Answer: \\\\'{result[\\\\'answer\\\\']}\\\\', score: {round(result[\\\\'score\\\\'], 4)}, start: {result[\\\\'start\\\\']}, end: {result[\\\\'end\\\\']}\\\")', 'api_call': \\\"DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilbert-base-cased-distilled-squad', 'api_call': \\\"DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\\\", 'api_arguments': ['question', 'context'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model='distilbert-base-cased-distilled-squad')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\\\\n...)\\\", 'performance': {'dataset': 'SQuAD v1.1', 'accuracy': {'Exact Match': 79.6, 'F1': 86.996}}, 'description': \\\"DistilBERT base cased distilled SQuAD is a fine-tuned checkpoint of DistilBERT-base-cased, trained using knowledge distillation on SQuAD v1.1 dataset. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark. This model can be used for question answering.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ ÙÙŠ Ø´Ø±ÙƒØªÙ†Ø§ Ø§Ù„Ù…ØªØ®ØµØµØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØªØµÙ†ÙŠÙ Ø§Ù„ØµÙØ± Ø¥Ù„Ù‰ ØªØµÙ…ÙŠÙ… Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„Ù…ÙÙ‡ÙˆÙ… ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import necessary modules, including PIL for image processing, requests for loading images, and ChineseCLIPModel and ChineseCLIPProcessor from transformers.\\\\n2. Load the pre-trained ChineseCLIPModel using the from_pretrained method. This model is designed for zero-shot image classification, allowing it to understand the context and contents of an image without being explicitly trained for specific categories.\\\\n3. Load the ChineseCLIPProcessor, which will be used to process both the image and text data.\\\\n4. Load the image data, either from a file or through a URL using the requests library.\\\\n5. Create a list of texts that might describe the image or its elements.\\\\n6. Process the image and texts using the ChineseCLIPProcessor, and pass the resulting tensors to the ChineseCLIPModel.\\\\n7. Obtain the image and text features, and compare their similarity using the cosine similarity method.\\\\n8. The highest similarity score indicates the most likely description for the image content.', '<<<code>>>': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\n\\\\nmodel = ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\\nprocessor = ChineseCLIPProcessor.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\\n\\\\nurl = 'https://image_url.com/image.jpg'\\\\n# Replace with the URL of the image you would like to process\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\ntexts = ['Text description example 1', 'Text description example 2', 'Text description example 3']\\\\n\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nimage_features = model.get_image_features(**inputs)\\\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\\\\n\\\\ninputs = processor(text=texts, padding=True, return_tensors='pt')\\\\ntext_features = model.get_text_features(**inputs)\\\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\\\\n\\\\ncosine_scores = (text_features @ image_features.T).softmax(1)\\\\nbest_score, best_index = cosine_scores.max(dim=-1)\\\\nbest_description = texts[best_index]\\\", 'api_call': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'OFA-Sys/chinese-clip-vit-large-patch14-336px', 'api_call': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\", 'api_arguments': {'images': 'image', 'text': 'texts', 'return_tensors': 'pt', 'padding': 'True'}, 'python_environment_requirements': [['PIL', 'requests', 'transformers']], 'example_code': 'from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\\\\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\\\\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = []\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nimage_features = model.get_image_features(**inputs)\\\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\\\\ninputs = processor(text=texts, padding=True, return_tensors=pt)\\\\ntext_features = model.get_text_features(**inputs)\\\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\\\\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)', 'performance': {'dataset': {'CIFAR10': 96.0, 'CIFAR100': 79.75, 'DTD': 51.2, 'EuroSAT': 52.0, 'FER': 55.1, 'FGVC': 26.2, 'KITTI': 49.9, 'MNIST': 79.4, 'PC': 63.5, 'VOC': 84.9}, 'accuracy': 'various'}, 'description': 'Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-L/14@336px as the image encoder and RoBERTa-wwm-base as the text encoder.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙˆÙØ± Ù…Ø¬Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø£Ø¯ÙˆØ§Øª ÙˆØªÙ‚Ù†ÙŠØ§Øª Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ø³Ù…Ø§Øª Ù…Ù† ÙˆØ³Ø§Ø¦Ø· Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ø«Ù„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØµÙˆØ± ÙˆÙ…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ. Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø£Ø¯Ø§Ø© ØªØ³Ù…Ø­ Ø¨Ø¥Ø³ØªØ®Ù„Ø§Øµ Ø³Ù…Ø§Øª Ù…Ù† ÙˆØ³Ø§Ø¦Ø· Ù…ØªØ¹Ø¯Ø¯Ø©ØŒ Ù…Ø«Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ù…Ø§Øª Ù…Ù† Ù†Øµ ÙŠØµØ§Ø­Ø¨ Ø§Ù„ØµÙˆØ± Ù„Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù†Ù‡Ø§ ÙÙŠ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©.? [/INST]'<<<domain>>>': 'Multimodal Feature Extraction', '<<<api_call>>>': \\\"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, such as the HubertModel class from the transformers package.\\\\n2. Load the pre-trained model named 'facebook/hubert-large-ll60k' by using the from_pretrained() method of the HubertModel class. This model is designed to extract features from speech and has been pretrained on a large variety of speech data.\\\\n3. Stream the student's speech recording in the desired format (such as WAV, MP3, or other format) and process it accordingly.\\\\n4. Use the loaded model to extract features from the student's speech recording.\\\\n5. Based on the extracted features, analyze the vocal nuances and fluency of the students.\\\", '<<<code>>>': \\\"from transformers import HubertModel\\\\nfrom pathlib import Path\\\\nhubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\\n\\\\nfile_path = 'path/to/student_recording.wav'\\\\n# replace with the path to your audio file\\\\naudio_input = Path(file_path)\\\\nfeatures = hubert(audio_input)\\\", 'api_call': \\\"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \\\"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': ['transformers'], 'example_code': \\\"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© (NLP) Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„. ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®ØµØ§Øª Ù…ÙÙŠØ¯Ø© ÙˆÙ…ÙˆØ¬Ø²Ø© Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø¯Ø®Ù„Ø©ØŒ Ù…Ù…Ø§ ÙŠØ³Ø§Ø¹Ø¯ Ø¹Ù„Ù‰ ØªØ¨Ø³ÙŠØ· Ù‚Ø±Ø§Ø¡Ø© ÙˆÙÙ‡Ù… Ø§Ù„Ù†ØµÙˆØµ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required classes (PegasusTokenizer, PegasusForConditionalGeneration) from the transformers library provided by Hugging Face.\\\\n2. Load the pre-trained financial summarization model 'human-centered-summarization/financial-summarization-pegasus'. This model is trained using a dataset of financial news articles and is ideal for summarizing articles in the financial sector.\\\\n3. Tokenize the financial news article using the PegasusTokenizer class, which provides methods for encoding and decoding text data.\\\\n4. Generate a summary of the input text using the PegasusForConditionalGeneration class. You can control the summary's length and other parameters to suit your requirements.\\\", '<<<code>>>': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration\\\\nmodel_name = \\\\'human-centered-summarization/financial-summarization-pegasus\\\\'\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\narticle_text = \\\"Your financial news article content goes here.\\\"\\\\ninput_ids = tokenizer(article_text, return_tensors=\\\\'pt\\\\').input_ids\\\\noutput = model.generate(input_ids, max_length=100, num_beams=5, early_stopping=True)\\\\nsummary = tokenizer.decode(output[0], skip_special_tokens=True)', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'financial-summarization-pegasus', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabiaâ€™s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Sambaâ€™s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf regionâ€™s third-largest lender. The entityâ€™s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle Eastâ€™s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))', 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 35.206, 'ROUGE-2': 16.569, 'ROUGE-L': 30.128, 'ROUGE-LSUM': 30.171}}, 'description': 'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ù†Øµ ÙˆØµÙÙŠ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ ÙŠÙØµÙˆÙ‘Ø± Ù…Ø´Ø§Ù‡Ø¯ Ù„Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù†Ø§ Ù„Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-large-ntp-py')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary transformers classes, including AutoTokenizer and T5ForConditionalGeneration.\\\\n2. Load the pre-trained model 'Salesforce/codet5-large-ntp-py' using the T5ForConditionalGeneration class. This model can generate Python code based on a given text input describing the requirements.\\\\n3. Tokenize your text using the pre-trained tokenizer from the AutoTokenizer class.\\\\n4. Use the generated tokens as input and call the generate function of the model with the provided input ids and the desired output length.\\\\n5. Decode the generated token ids to get the Python code example as output.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, T5ForConditionalGeneration\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Salesforce/codet5-large-ntp-py\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'Salesforce/codet5-large-ntp-py\\\\')\\\\ntext = \\\"Create a simple python function to calculate the square of a given number\\\"\\\\ninput_ids = tokenizer(text, return_tensors=\\\\'pt\\\\').input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\ngenerated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-large-ntp-py')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'codet5-large-ntp-py', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-large-ntp-py')\\\", 'api_arguments': ['text', 'return_tensors', 'input_ids', 'max_length'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoTokenizer, T5ForConditionalGeneration\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codet5-large-ntp-py)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(Salesforce/codet5-large-ntp-py)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'APPS benchmark', 'accuracy': 'See Table 5 of the paper'}, 'description': 'CodeT5 is a family of encoder-decoder language models for code from the paper: CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation by Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. The checkpoint included in this repository is denoted as CodeT5-large-ntp-py (770M), which is introduced by the paper: CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning by Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¨Ø¯Ø¡ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¹Ø¨Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠÙØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ ØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ Ù…Ù„Ù ØµÙˆØªÙŠØŒ ÙˆØ³Ø£Ù‚ÙˆÙ… Ø¨Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ. Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ù„Ø£ÙŠ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ùˆ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©ØŸ? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the pipeline function from the transformers library.', 'code': 'from transformers import pipeline\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'EleutherAI/gpt-neo-2.7B\\\\')\\\\ntext = generator(\\\"Plastic pollution and climate change connection\\\", do_sample=True, min_length=100)[0][\\\\'generated_text\\\\']', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Generation', 'api_name': 'EleutherAI/gpt-neo-2.7B', 'api_call': 'from transformers import pipeline\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'EleutherAI/gpt-neo-2.7B\\\\')\\\\ntext = generator(\\\"Plastic pollution and climate change connection\\\", do_sample=True, min_length=100)[0][\\\\'generated_text\\\\']', 'api_arguments': ['text', 'do_sample', 'min_length'], 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n generator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\\\\n generator(EleutherAI has, do_sample=True, min_length=50)\\\", 'performance': {'dataset': 'the_pile', 'accuracy': {'Lambada_Acc': '62.22%', 'Winogrande': '56.50%', 'Hellaswag': '42.73%'}}, 'description': \\\"GPT-Neo 2.7B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. It was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model. This model is best suited for generating texts from a prompt and can be used directly with a pipeline for text generation.\\\"}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø¨Ø¯ÙˆÙ† Ø´Ø±Ø· ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆØ°Ù„Ùƒ Ù„ØªØµÙ…ÙŠÙ… ØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙˆØ£Ø¯ÙˆØ§Øª ØªØ³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙˆØ§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-ema-cat-256')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from diffusers import DDPMPipeline\\\\nddpm = DDPMPipeline.from_pretrained('google/ddpm-ema-cat-256')\\\\nimage = ddpm().images[0]\\\\nimage.save('ddpm_generated_image.png')\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-ema-cat-256', 'api_call': \\\"from diffusers import DDPMPipeline\\\\nddpm = DDPMPipeline.from_pretrained('google/ddpm-ema-cat-256')\\\\nimage = ddpm().images[0]\\\\nimage.save('ddpm_generated_image.png')\\\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['!pip install diffusers'], 'example_code': 'from diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-ema-cat-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images, and supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. On the unconditional CIFAR10 dataset, it achieves an Inception score of 9.46 and a state-of-the-art FID score of 3.17.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary libraries, which are PIL for image handling, requests for fetching image data, and various classes from transformers.', 'code': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\n\\\\nmodel = ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\\nprocessor = ChineseCLIPProcessor.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\\nimage = Image.open('user_provided_image.jpg')  # replace with user-provided image path\\\\ntexts = ['Category 1', 'Category 2', 'Category 3']  # replace with actual category descriptions\\\\n\\\\ninputs = processor(images=image, text=texts, return_tensors='pt', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\", 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'OFA-Sys/chinese-clip-vit-large-patch14-336px', 'api_call': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\n\\\\nmodel = ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\\nprocessor = ChineseCLIPProcessor.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\\nimage = Image.open('user_provided_image.jpg')  # replace with user-provided image path\\\\ntexts = ['Category 1', 'Category 2', 'Category 3']  # replace with actual category descriptions\\\\n\\\\ninputs = processor(images=image, text=texts, return_tensors='pt', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\", 'api_arguments': {'images': 'image', 'text': 'texts', 'return_tensors': 'pt', 'padding': 'True'}, 'python_environment_requirements': ['PIL', 'requests', 'transformers'], 'example_code': 'from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\\\\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\\\\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = []\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nimage_features = model.get_image_features(**inputs)\\\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\\\\ninputs = processor(text=texts, padding=True, return_tensors=pt)\\\\ntext_features = model.get_text_features(**inputs)\\\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\\\\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)', 'performance': {'dataset': {'CIFAR10': 96.0, 'CIFAR100': 79.75, 'DTD': 51.2, 'EuroSAT': 52.0, 'FER': 55.1, 'FGVC': 26.2, 'KITTI': 49.9, 'MNIST': 79.4, 'PC': 63.5, 'VOC': 84.9}, 'accuracy': 'various'}, 'description': 'Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-L/14@336px as the image encoder and RoBERTa-wwm-base as the text encoder.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We import the appropriate function from the transformers library developed by Hugging Face.\\\\n2. We create a question-answer pipeline using the pre-trained model 'distilbert-base-cased-distilled-squad', which has been fine-tuned on the SQuAD v1.1 dataset. This allows for question answering in a wide range of contexts.\\\\n3. The resulting model, named question_answerer, can understand and answer questions about popular places and attractions given an appropriate context (provided in a text format).\\\\n4. By providing the model with a question and a context (text containing the answer), the model will return the answer to the query.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nquestion_answerer = pipeline(\\\\'question-answering\\\\', model=\\\\'distilbert-base-cased-distilled-squad\\\\')\\\\ncontext = \\\"Paris is famous for its many attractions including the Eiffel Tower, the Louvre Museum, and the Notre-Dame Cathedral.\\\"\\\\nresult = question_answerer(question=\\\"What famous museum is located in Paris?\\\", context=context)', 'api_call': \\\"DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilbert-base-cased-distilled-squad', 'api_call': \\\"DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\\\", 'api_arguments': ['question', 'context'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model='distilbert-base-cased-distilled-squad')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\\\\n...)\\\", 'performance': {'dataset': 'SQuAD v1.1', 'accuracy': {'Exact Match': 79.6, 'F1': 86.996}}, 'description': \\\"DistilBERT base cased distilled SQuAD is a fine-tuned checkpoint of DistilBERT-base-cased, trained using knowledge distillation on SQuAD v1.1 dataset. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark. This model can be used for question answering.\\\" </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ© Ù…Ø±Ø¦ÙŠØ© Ù„Ù„Ø­ÙˆØ³Ø¨Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ù…Ù† ØµÙˆØ± Ø£Ø®Ø±Ù‰ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ ØªÙ‚Ø¯ÙŠÙ… ØªÙ†Ø¨Ø¤Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø¨Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ù†ØªØ¬Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø§Ø³ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We first import the pipeline class from the transformers package.\\\\n2. We then use the pipeline function to create an instance of the image-to-image model with the model identifier 'GreeneryScenery/SheepsControlV3'. This model is designed to generate images based on an input image and optional text guidance.\\\\n3. We can provide the input image and optional text guidance to the model instance, and the model will generate an advertisement image based on the given inputs.\\\\n4. The generated advertisement image can be used for the digital marketing purposes of our client.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nmodel = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\\\\nresult = model({'image': 'path/to/image.jpg', 'text_guidance': 'Create an advertisement with a vibrant background and a catchy phrase'})\\\", 'api_call': \\\"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GreeneryScenery/SheepsControlV3', 'api_call': \\\"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\\\", 'api_arguments': {'image': 'Path to image file', 'text_guidance': 'Optional text guidance for the model'}, 'python_environment_requirements': [{'transformers': 'latest', 'torch': 'latest'}], 'example_code': ['from transformers import pipeline', \\\"model = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\\\", \\\"result = model({'image': 'path/to/image.jpg', 'text_guidance': 'Optional text guidance'})\\\"], 'performance': {'dataset': 'GreeneryScenery/SheepsControlV3', 'accuracy': 'Not provided'}, 'description': 'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ø¹Ø¨Ø§Ø±Ø© Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ØªØ¨Ø¯Ø£ Ø¨Ù€ \\\"ØªØ­Ø³ÙŠÙ† ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We first import the 'pipeline' function from the transformers package.\\\\n2. We then use the pipeline function to load the pre-trained GPT-2 model 'sshleifer/tiny-gpt2'. This model is specifically designed for text generation tasks and can generate creative text-based content.\\\\n3. We provide a starting phrase for the Sci-Fi story and let the model generate a continuation of the text, which can serve as the beginning of the story.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntext_generator = pipeline(\\\\'text-generation\\\\', model=\\\\'sshleifer/tiny-gpt2\\\\')\\\\nstory_start = \\\"In a distant galaxy, a mysterious planet...\\\"\\\\ngenerated_text = text_generator(story_start, max_length=100)\\\\nsci_fi_story_start = generated_text[0][\\\\'generated_text\\\\']', 'api_call': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sshleifer/tiny-gpt2', 'api_call': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", 'api_arguments': {'model': 'sshleifer/tiny-gpt2'}, 'python_environment_requirements': [{'huggingface_transformers': '>=4.0.0'}], 'example_code': \\\"from transformers import pipeline\\\\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\\\\nresult = nlp('Once upon a time')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A tiny GPT-2 model for text generation, suitable for low-resource environments and faster inference. This model is part of the Hugging Face Transformers library and can be used for generating text given a prompt.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ù…Ù† Ø¯ÙˆÙ† Ø´Ø±ÙˆØ· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-church-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we have to install the diffusers package.\\\\n2. Next, we import the DiffusionPipeline class from the diffusers package to create the necessary model for generating unconditional images.\\\\n3. We then use the from_pretrained method of the DiffusionPipeline class to load the pre-trained model, and we pass the model_id 'google/ncsnpp-church-256' that has been trained for unconditional image synthesis tasks.\\\\n4. The model can then be used to generate an image of the church, which can be used for postcards.\\\\n5. Finally, store the generated image to a file called 'sde_ve_generated_image.png', which can be printed on the postcards.\\\", '<<<code>>>': \\\"from diffusers import DiffusionPipeline\\\\nmodel_id = 'google/ncsnpp-church-256'\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[sample]\\\\nimage[0].save('sde_ve_generated_image.png')\\\", 'api_call': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-church-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/ncsnpp-church-256', 'api_call': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-church-256')\\\", 'api_arguments': 'model_id', 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = google/ncsnpp-church-256\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[sample]\\\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and can generate high fidelity images of size 1024 x 1024.' </s>\"\n"}
{"text": "<s>[INST]Ø³ÙŠÙ‚ÙˆÙ… Ø¨ØªØµÙ…ÙŠÙ… Ù†Ø¸Ø§Ù… Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ ÙˆØ³ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙƒÙ…Ø¯Ø®Ù„Ø§Øª.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. We then use the pipeline function to create a question-answering model, specifically for Chinese text.\\\\n3. We load the 'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large' model, which is a pre-trained model designed for machine reading comprehension tasks, and has a strong performance on Chinese text.\\\\n4. The resulting model can be used to answer questions based on the given context (paragraph) in Chinese.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\\nresult = qa_pipeline({'context': 'æä¾›çš„æ–‡å­—æ®µè½', 'question': 'æœ‰å…³é—®é¢˜'})\\\", 'api_call': \\\"pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large', 'api_call': \\\"pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\", 'api_arguments': 'context, question', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\\nresult = qa_pipeline({'context': 'your_context_here', 'question': 'your_question_here'})\\\", 'performance': {'dataset': 'Dureader-2021', 'accuracy': '83.1'}, 'description': 'A Chinese MRC roberta_wwm_ext_large model trained on a large amount of Chinese MRC data. This model has significantly improved performance on reading comprehension and classification tasks. It has helped multiple users achieve top 5 results in the Dureader-2021 competition.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙØ§Ø¹Ù„ ÙˆØ§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù…ÙŠÙ„Ù†Ø§ Ù„ØªØ­Ù‚ÙŠÙ‚ ØªØ¬Ø±Ø¨Ø© Ù…Ø³ØªØ®Ø¯Ù… Ø³Ù„Ø³Ø© ÙˆÙ…Ù†Ø§Ø³Ø¨Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"pipeline('text-generation', model='PygmalionAI/pygmalion-2.7b')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We will use the Hugging Face Transformers library to load the PygmalionAI/pygmalion-2.7b model using the pipeline function for text-generation.\\\\n2. To create a chatbot for a medieval knight character, we will define the character's persona and attributes such as being brave, loyal, and following the code of chivalry.\\\\n3. For each user input message, we will format the input, including the character's persona, dialogue history, and user input message, according to the API's specifications.\\\\n4. The model will generate a response from the knight character in the context of the specified persona and the dialogue history.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nchatbot = pipeline(\\\\'text-generation\\\\', model=\\\\'PygmalionAI/pygmalion-2.7b\\\\')\\\\npersona = \\\"Knight\\\\'s Persona: A brave, loyal, and chivalrous medieval knight.\\\"\\\\ndialogue_history = \\\\'\\\\'  # Initialize an empty dialogue history\\\\nyou_message = \\\\'What do you think about honor?\\\\'\\\\nchat_input = f\\\"{persona}<START>{dialogue_history}You: {you_message}[Knight]:\\\"\\\\nresponse = chatbot(chat_input)', 'api_call': \\\"pipeline('text-generation', model='PygmalionAI/pygmalion-2.7b')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pygmalion-2.7b', 'api_call': \\\"pipeline('text-generation', model='PygmalionAI/pygmalion-2.7b')\\\", 'api_arguments': {'input_prompt': 'string'}, 'python_environment_requirements': [{'transformers': 'latest'}], 'example_code': \\\"model([CHARACTER]'s Persona: [A few sentences about the character you want the model to play]<START>[DIALOGUE HISTORY]You: [Your input message here][CHARACTER]:)\\\", 'performance': {'dataset': '56MB of dialogue data', 'accuracy': 'N/A'}, 'description': \\\"Pygmalion 2.7B is a proof-of-concept dialogue model based on EleutherAI's gpt-neo-2.7B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, including real and partially machine-generated conversations. The model is intended for use in generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\\\" </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ØªØ³ØªØ·ÙŠØ¹ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='roberta-large')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a model for masked language modeling using 'roberta-large', which is a large, pretrained model that understands the context of English language data.\\\\n3. With this 'fill-mask' model, we can provide a partially written sentence containing a special <mask> token. The model will then use its knowledge of the language to suggest the best possible completion for the masked part of the sentence.\\\\n4. The created pipeline can be passed a partially written sentence with the <mask> token, and it will return a list of possible completions ranked by likelihood.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'roberta-large\\\\')\\\\ninput_text = \\\"The quick brown fox jumps over the <mask> dog.\\\"\\\\ncompletions = unmasker(input_text)', 'api_call': \\\"pipeline('fill-mask', model='roberta-large')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'roberta-large', 'api_call': \\\"pipeline('fill-mask', model='roberta-large')\\\", 'api_arguments': {'model': 'roberta-large'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='roberta-large')\\\\nunmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': 'GLUE', 'accuracy': {'MNLI': 90.2, 'QQP': 92.2, 'QNLI': 94.7, 'SST-2': 96.4, 'CoLA': 68.0, 'STS-B': 96.4, 'MRPC': 90.9, 'RTE': 86.6}}, 'description': 'RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using the Masked language modeling (MLM) objective. It can be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‡Ø¯Ù Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø£Ø³Ø¦Ù„Ø© Ø¹Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚. Ø§Ø¨Ø¯Ø£ Ø¨ÙÙ‡Ù… Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ø«Ù… Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¯ÙˆÙ† Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ÙŠÙ†Ø©. Ù‡Ù„ ØªØ±ØºØ¨ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"LayoutLMv2ForQuestionAnswering.from_pretrained('dperales/layoutlmv2-base-uncased_finetuned_docvqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To load the pre-trained LayoutLMv2 model, first import the necessary class from the transformers package. In this case, it's LayoutLMv2ForQuestionAnswering.\\\\n2. Next, use the from_pretrained method to load the pre-trained model with the identifier 'dperales/layoutlmv2-base-uncased_finetuned_docvqa'. This is a document question answering model based on the LayoutLMv2 architecture and fine-tuned on the DocVQA dataset.\\\\n3. Once the model is loaded, you can use it to answer questions about the content of scanned documents by providing the document image and the corresponding question as input.\\\", '<<<code>>>': \\\"from transformers import LayoutLMv2ForQuestionAnswering\\\\nmodel = LayoutLMv2ForQuestionAnswering.from_pretrained('dperales/layoutlmv2-base-uncased_finetuned_docvqa')\\\", 'api_call': \\\"LayoutLMv2ForQuestionAnswering.from_pretrained('dperales/layoutlmv2-base-uncased_finetuned_docvqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dperales/layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \\\"LayoutLMv2ForQuestionAnswering.from_pretrained('dperales/layoutlmv2-base-uncased_finetuned_docvqa')\\\", 'api_arguments': {'model': 'dperales/layoutlmv2-base-uncased_finetuned_docvqa'}, 'python_environment_requirements': [{'transformers': 'latest'}], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for Document Question Answering based on the LayoutLMv2 architecture, fine-tuned on the DocVQA dataset.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ­ÙˆÙŠÙ„ ØµÙˆØ±Ø© ØºÙŠØ± Ø´Ø±Ø·ÙŠØ© Ø¥Ù„Ù‰ ØµÙˆØª Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ù…Ù‚Ø·Ø¹ ØµÙˆØªÙŠØŸ? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the required 'diffusers' package.\\\\n2. Import the DDPMPipeline class from the diffusers package.\\\\n3. Use the from_pretrained method of the DDPMPipeline class to load the pre-trained model 'google/ddpm-bedroom-256'. This model has been trained for high-quality image synthesis, specifically for generating bedroom interior designs.\\\\n4. Once the model is loaded, generate an image using the loaded model. This image represents a computer-generated bedroom interior design.\\\\n5. Save the generated image to a file, such as 'ddpm_generated_image.png'.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\nddpm = DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\\nimage = ddpm().images[0]\\\\nimage.save('ddpm_generated_image.png')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/ddpm-bedroom-256', 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\", 'api_arguments': 'None', 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. We import the necessary classes from the transformers package. This includes AutoModelForSeq2SeqLM for the translation model and AutoTokenizer for tokenizing inputs.\\\\n2. We first load the Helsinki-NLP/opus-mt-en-de model by using the from_pretrained method of the AutoModelForSeq2SeqLM class. This model has been trained on a large corpus of text and can translate English text to German.\\\\n3. We create an instance of the AutoTokenizer, specifying the same Hugging Face model.\\\\n4. We use the tokenizer to convert the input English text into a format suitable for the model, and then use the model to generate the translated German text.\\\\n5. Finally, we tokenize and reconstruct the translated German text to make it human-readable.', '<<<code>>>': 'from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-de\\\\')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-de\\\\')\\\\ninput_text = \\\"response in English\\\"\\\\ninput_tokenized = tokenizer.prepare_seq2seq_batch(src_texts=[input_text], return_tensors=\\\\'pt\\\\')\\\\noutput_tokenized = model.generate(**input_tokenized)\\\\ntranslated_text = tokenizer.batch_decode(output_tokenized, skip_special_tokens=True)[0]', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-en-de', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'Helsinki-NLP/opus-mt-en-de'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'performance': {'dataset': 'newstest2018-ende.en.de', 'accuracy': {'BLEU': 45.2, 'chr-F': 0.69}}, 'description': 'The Helsinki-NLP/opus-mt-en-de model is a translation model developed by the Language Technology Research Group at the University of Helsinki. It translates English text to German using the Hugging Face Transformers library. The model is trained on the OPUS dataset and has a BLEU score of 45.2 on the newstest2018-ende.en.de dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¥Ù„Ù‰ ØµÙˆØª: \\\"Ø£Ø±ÙŠØ¯Ùƒ Ø£Ù† ØªÙˆÙ„Ø¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨ÙˆØ§Ø³Ø·Ø© Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ù‡Ø°Ø§ØŸ\\\"? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur')\\\", 'api_provider': 'Fairseq', 'explanation': \\\"1. Import the necessary libraries, such as 'fairseq', 'hub_utils', 'huggingface_hub', 'IPython.display', and 'torchaudio'.\\\", 'code': 'from fairseq import hub_utils, checkpoint_utils\\\\nfrom fairseq.models.speech_to_text import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur\\\\')\\\\nmodel = models[0].cpu()\\\\naudio_file_path = \\\"path_to_english_speech_input.wav\\\"\\\\naudio, _ = torchaudio.load(audio_file_path)\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\n\\\\n# Get the target language code, e.g., \\\\'es\\\\' for Spanish, \\\\'fr\\\\' for French, or \\\\'it\\\\' for Italian\\\\ntarget_language_code = \\\"es\\\"\\\\nunit = S2THubInterface.get_prediction(task, model, generator, sample, target_language_code)\\\\n\\\\nvocoder_dir = snapshot_download(\\\"facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur\\\")\\\\nvocoder, vocoder_cfg = CodeHiFiGANVocoder.from_pretrained(vocoder_dir)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\ntranslated_speech, sample_rate = tts_model.get_prediction(tts_sample)', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Fairseq', 'functionality': 'Speech-to-speech translation', 'api_name': 'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur', 'api_call': 'from fairseq import hub_utils, checkpoint_utils\\\\nfrom fairseq.models.speech_to_text import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur\\\\')\\\\nmodel = models[0].cpu()\\\\naudio_file_path = \\\"path_to_english_speech_input.wav\\\"\\\\naudio, _ = torchaudio.load(audio_file_path)\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\n\\\\n# Get the target language code, e.g., \\\\'es\\\\' for Spanish, \\\\'fr\\\\' for French, or \\\\'it\\\\' for Italian\\\\ntarget_language_code = \\\"es\\\"\\\\nunit = S2THubInterface.get_prediction(task, model, generator, sample, target_language_code)\\\\n\\\\nvocoder_dir = snapshot_download(\\\"facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur\\\")\\\\nvocoder, vocoder_cfg = CodeHiFiGANVocoder.from_pretrained(vocoder_dir)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\ntranslated_speech, sample_rate = tts_model.get_prediction(tts_sample)', 'api_arguments': {'audio': '16000Hz mono channel audio'}, 'python_environment_requirements': ['fairseq', 'hub_utils', 'huggingface_hub', 'IPython.display', 'torchaudio'], 'example_code': ['import json', 'import os', 'from pathlib import Path', 'import IPython.display as ipd', 'from fairseq import hub_utils', 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub', 'from fairseq.models.speech_to_text.hub_interface import S2THubInterface', 'from fairseq.models.text_to_speech import CodeHiFiGANVocoder', 'from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface', 'from huggingface_hub import snapshot_download', 'import torchaudio', 'cache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)', 'models, cfg, task = load_model_ensemble_and_task_from_hf_hub(', 'facebook/xm_transformer_s2ut_800m-es-en-st-asr-bt_h1_2022,', 'arg_overrides={config_yaml: config.yaml, task: speech_to_text},', 'cache_dir=cache_dir,', ')', 'model = models[0].cpu()', 'cfg[task].cpu = True', 'generator = task.build_generator([model], cfg)', '# requires 16000Hz mono channel audio', 'audio, _ = torchaudio.load(/Users/lpw/git/api-inference-community/docker_images/fairseq/tests/samples/sample2.flac)', 'sample = S2THubInterface.get_model_input(task, audio)', 'unit = S2THubInterface.get_prediction(task, model, generator, sample)', 'library_name = fairseq', 'cache_dir = (', ' cache_dir or (Path.home() / .cache / library_name).as_posix()', ')', 'cache_dir = snapshot_download(', ' ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name', ')', 'x = hub_utils.from_pretrained(', ' cache_dir,', ' model.pt,', ' .,', ' archive_map=CodeHiFiGANVocoder.hub_models(),', ' config_yaml=config.json,', ' fp16=False,', ' is_vocoder=True,', ')', \\\"with open(f{x['args']['data']}/config.json) as f:\\\", ' vocoder_cfg = json.load(f)', 'assert (', ' len(x[args][model_path]) == 1', '), Too many vocoder models in the input', 'vocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)', 'tts_model = VocoderHubInterface(vocoder_cfg, vocoder)', 'tts_sample = tts_model.get_model_input(unit)', 'wav, sr = tts_model.get_prediction(tts_sample)', 'ipd.Audio(wav, rate=sr)'], 'performance': {'dataset': 'covost2', 'accuracy': None}, 'description': 'Speech-to-speech translation model from fairseq S2UT (paper/code) for Spanish-English. Trained on mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ù…ØµØ¯Ø± Ù…ÙˆØ«ÙˆÙ‚ Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø² Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Image-to-Image Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù…Ù† Ù†ÙˆØ¹ Ø¥Ù„Ù‰ Ø¢Ø®Ø±. ÙŠØ±Ø¬Ù‰ Ø¶Ø¨Ø· Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø­ÙŠØ« ØªØ­Ù‚Ù‚ Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡ Ù…Ù…ÙƒÙ†. ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©ØŒ Ù‚Ù… Ø¨ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¶Ø¨Ø·Ù‡ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©.? [/INST]{'domain': 'Computer Vision Image-to-Image', 'api_call': \\\"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV5')\\\", 'api_provider': 'Hugging Face', 'explanation': \\\"1. Import the 'pipeline' function from the transformers library, which is provided by Hugging Face.\\\", 'code': \\\"from transformers import pipeline\\\\nimage_transformer = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV5')\\\\ninput_image_path = 'path/to/image/file'\\\\noutput_image = image_transformer(input_image=input_image_path)[0]\\\", 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'GreeneryScenery/SheepsControlV5', 'api_call': \\\"from transformers import pipeline\\\\nimage_transformer = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV5')\\\\ninput_image_path = 'path/to/image/file'\\\\noutput_image = image_transformer(input_image=input_image_path)[0]\\\", 'api_arguments': {'input_image': 'path/to/image/file'}, 'python_environment_requirements': {'huggingface_hub': '>=0.0.17', 'transformers': '>=4.13.0', 'torch': '>=1.10.0'}, 'example_code': '', 'performance': {'dataset': 'poloclub/diffusiondb', 'accuracy': 'Not provided'}, 'description': 'SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.'}} </s>\"\n"}
{"text": "<s>[INST]Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ¹Ù„Ù‚ Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ. ÙŠÙ…ÙƒÙ†Ùƒ Ø§ØªØ¨Ø§Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„ÙˆØ§Ø±Ø¯Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù„ØºØ© Ø£Ø®Ø±Ù‰ØŒ Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ù†Ø´Ø§Ø¡ ØªØ³Ù„Ø³Ù„ Ù„ØªÙˆÙ„ÙŠØ¯ ØªØ±Ø¬Ù…Ø© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙØ¹Ø§Ù„Ø©. ÙŠÙØ±Ø¬Ù‰ Ø§ØªØ¨Ø§Ø¹ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ÙˆØ§Ø±Ø¯Ø© ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ù„ØªÙ†ÙÙŠØ° Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù…Ù„.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"MarianMTModel.from_pretrained('pytorch-models/opus-mt-tc-big-en-pt')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import MarianMTModel, MarianTokenizer\\\\nsrc_text = [\\\"&gt;&gt;por&lt;&lt; O produto nÃ£o estÃ¡ funcionando corretamente.\\\"] # Replace with the actual customer question\\\\nmodel_name = \\\\'pytorch-models/opus-mt-tc-big-en-pt\\\\'\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\\\\'pt\\\\', padding=True))\\\\ntranslation = tokenizer.decode(translated[0], skip_special_tokens=True)', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Neural machine translation', 'api_name': 'opus-mt-tc-big-en-pt', 'api_call': 'from transformers import MarianMTModel, MarianTokenizer\\\\nsrc_text = [\\\"&gt;&gt;por&lt;&lt; O produto nÃ£o estÃ¡ funcionando corretamente.\\\"] # Replace with the actual customer question\\\\nmodel_name = \\\\'pytorch-models/opus-mt-tc-big-en-pt\\\\'\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\\\\'pt\\\\', padding=True))\\\\ntranslation = tokenizer.decode(translated[0], skip_special_tokens=True)', 'api_arguments': {'src_text': 'list of text strings with language tokens'}, 'python_environment_requirements': {'transformers': '4.16.2'}, 'example_code': 'from transformers import MarianMTModel, MarianTokenizer\\\\nsrc_text = [\\\\n &gt;&gt;por&lt;&lt; Tom tried to stab me.,\\\\n &gt;&gt;por&lt;&lt; He has been to Hawaii several times.\\\\n]\\\\nmodel_name = pytorch-models/opus-mt-tc-big-en-pt\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\ntranslated = model.generate(**tokenizer(src_text, return_tensors=pt, padding=True))\\\\nfor t in translated:\\\\n print( tokenizer.decode(t, skip_special_tokens=True) )', 'performance': {'dataset': [{'name': 'flores101-devtest', 'accuracy': 50.4}, {'name': 'tatoeba-test-v2021-08-07', 'accuracy': 49.6}]}, 'description': 'Neural machine translation model for translating from English (en) to Portuguese (pt). This model is part of the OPUS-MT project, an effort to make neural machine translation models widely available and accessible for many languages in the world.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù†Ø¸Ù… Ø±Ø¤ÙŠØ© Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ±ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ ØªØªÙ… Ø§Ù„ØªØµÙ†ÙŠÙ Ø¹Ù„ÙŠÙ‡Ø§.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include 'AutoFeatureExtractor', 'RegNetForImageClassification', and 'load_dataset'.\\\\n2. Load the dataset containing images of kitchen appliances.\\\\n3. Use the 'AutoFeatureExtractor.from_pretrained' and 'RegNetForImageClassification.from_pretrained' methods to create a feature extractor and RegNet model.\\\\n4. Process an image of a kitchen appliance using the feature extractor and feed it into the RegNet model.\\\\n5. Retrieve the model's prediction and use it to categorize the kitchen appliance in question.\\\", '<<<code>>>': \\\"from transformers import AutoFeatureExtractor, RegNetForImageClassification\\\\nfrom PIL import Image\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset('some_dataset_with_kitchen_appliances') \\\\nimage = Image.open('kitchen_appliance_image.jpg')\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('zuppif/regnet-y-040')\\\\nmodel = RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/regnet-y-008', 'api_call': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'zuppif/regnet-y-040'}, 'python_environment_requirements': [{'transformers': 'AutoFeatureExtractor, RegNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}], 'example_code': 'from transformers import AutoFeatureExtractor, RegNetForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset(huggingface/cats-image)\\\\nimage = dataset[test][image][0]\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\\\ninputs = feature_extractor(image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n... logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¯ÙˆØ±ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨ÙŠ. Ø§Ø³ØªØ®Ø¯Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© ØªØ¹Ù„Ù… Ø¢Ù„ÙŠ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„ÙØ±Ù‚ Ø§Ù„Ø·Ù„Ø§Ø¨ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. Import joblib to load the trained model and pandas for handling data.\\\\n2. Load the pre-trained carbon emissions classification model using joblib.load() function and store it in a variable (e.g., model).\\\\n3. Read a CSV file containing the features for which the predictions are to be made using pandas. This can include factors such as the manufacturing process, materials used, and transportation variables.\\\\n4. Process the input data according to the model's configuration and extract the required features.\\\\n5. Use the trained model to predict the carbon emissions for the given product features, which will help guide the creation of more environmentally friendly products.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\ndata = pd.read_csv('product_features.csv')\\\\nprocessed_data = preprocess_data(data)  # Replace preprocess_data with the actual preprocessing function\\\\npredictions = model.predict(processed_data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tejas23/autotrain-amx2-1702259725', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': {'accuracy': 0.827}}, 'description': 'Multi-class Classification Model for Carbon Emissions' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… ÙØ±ÙŠÙ‚ Ø§Ù„Ø¨Ø­Ø« Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library for this task, which is 'pipeline' from transformers.\\\\n2. Initialize the zero-shot image classification pipeline using the 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k' model.\\\\n3. Use the initialized classifier for classifying the product images by providing the image and relevant category labels (e.g., 'shirt', 'jeans', 'dress') as input.\\\\n4. The model will return the most probable category predictions for the given product image.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\\\\nimage_path = 'path/to/fashion_product_image.jpg'\\\\ncategories = ['shirt', 'jeans', 'dress', 'shoes', 'accessories', 'jacket']\\\\n# possible fashion product categories\\\\nresult = image_classifier(image_path, possible_class_names=categories)\\\", 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k', 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\\\", 'api_arguments': ['image', 'possible_class_names'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'); classifier(image, possible_class_names=['cat', 'dog'])\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '80.1'}, 'description': 'A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. The model is intended for research purposes and enables researchers to better understand and explore zero-shot, arbitrary image classification. It can be used for interdisciplinary studies of the potential impact of such models. The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ† Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required transformer package, which includes the 'pipeline' function.\\\\n2. Load the model from the Hugging Face model hub using the 'pipeline' function, and provide the model name 'svalabs/gbert-large-zeroshot-nli'.\\\\n3. The model is designed for zero-shot text classification, which means it can classify text into labels that it has not seen before.\\\\n4. Provide the German text and the labels including the urgency labels as input to the model.\\\\n5. The model will return the classification results that indicate the theme of the text and whether it is urgent or not.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsequence = \\\"Ich habe ein Problem mit meinem Iphone das so schnell wie mÃ¶glich gelÃ¶st werden muss\\\"\\\\nlabels = [\\\"Computer\\\", \\\"Handy\\\", \\\"Tablet\\\", \\\"dringend\\\", \\\"nicht dringend\\\"]\\\\nhypothesis_template = \\\"In diesem Satz geht es um das Thema {}.\\\"\\\\nzero_shot_pipeline = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'svalabs/gbert-large-zeroshot-nli\\\\')\\\\nresult = zero_shot_pipeline(sequence, labels, hypothesis_template=hypothesis_template)', 'api_call': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'svalabs/gbert-large-zeroshot-nli', 'api_call': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", 'api_arguments': ['sequence', 'labels', 'hypothesis_template'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\nzershot_pipeline = pipeline(zero-shot-classification, model=svalabs/gbert-large-zeroshot-nli)\\\\nsequence = Ich habe ein Problem mit meinem Iphone das so schnell wie mÃ¶glich gelÃ¶st werden muss\\\\nlabels = [Computer, Handy, Tablet, dringend, nicht dringend]\\\\nhypothesis_template = In diesem Satz geht es um das Thema {}. ', 'performance': {'dataset': 'XNLI TEST-Set', 'accuracy': '85.6%'}, 'description': 'A German zeroshot classification model based on the German BERT large model from deepset.ai and finetuned for natural language inference using machine-translated nli sentence pairs from mnli, anli, and snli datasets.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§ØªØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©.? [/INST]'<<<domain>>>': 'Audio Classification', '<<<api_call>>>': \\\"HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, install the required libraries, including audio_models, transformers, torch, numpy, and pydub.\\\\n2. Load the HubertForSpeechClassification model using the from_pretrained method with the provided model name 'Rajaram1996/Hubert_emotion'.\\\\n3. Also load the feature extractor 'facebook/hubert-base-ls960' to transform the audio input into suitable format for the model.\\\\n4. Create a function to convert an audio file to numpy array with the specified sampling rate.\\\\n5. Use the feature extractor to process the audio array and generate input tensors for the model.\\\\n6. Pass the tensors into the model to get the emotion predictions along with their confidence scores.\\\\n7. You can now analyze the detected emotions from the user's voice input while interacting with the voice assistant.\\\", '<<<code>>>': 'from audio_models import HubertForSpeechClassification\\\\nfrom transformers import Wav2Vec2FeatureExtractor, AutoConfig\\\\nimport torch\\\\nimport numpy as np\\\\nfrom pydub import AudioSegment\\\\n\\\\naudio_file = \\\\'voice_input.wav\\\\' # replace this with the path to your voice input file\\\\npath = \\\\'voice_input.wav\\\\'\\\\nmodel = HubertForSpeechClassification.from_pretrained(\\\\'Rajaram1996/Hubert_emotion\\\\')\\\\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\\\\'facebook/hubert-base-ls960\\\\')\\\\n\\\\nsampling_rate = 16000\\\\n\\\\ndef speech_file_to_array(path, sampling_rate):\\\\n    sound = AudioSegment.from_file(path)\\\\n    sound = sound.set_frame_rate(sampling_rate)\\\\n    sound_array = np.array(sound.get_array_of_samples())\\\\n    return sound_array\\\\n\\\\nsound_array = speech_file_to_array(audio_file, sampling_rate)\\\\ninputs = feature_extractor(sound_array, sampling_rate=sampling_rate, return_tensors=\\\\'pt\\\\', padding=True)\\\\ninputs = {key: inputs[key].to(torch.device(\\\"cpu\\\")).float() for key in inputs}\\\\n\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\nscores = torch.nn.functional.softmax(logits, dim=1).detach().cpu().numpy()[0]', 'api_call': \\\"HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Rajaram1996/Hubert_emotion', 'api_call': \\\"HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')\\\", 'api_arguments': {'audio_file': 'string'}, 'python_environment_requirements': [['audio_models', 'transformers', 'torch', 'numpy', 'pydub']], 'example_code': \\\"def predict_emotion_hubert(audio_file):\\\\n from audio_models import HubertForSpeechClassification\\\\n from transformers import Wav2Vec2FeatureExtractor, AutoConfig\\\\n import torch.nn.functional as F\\\\n import torch\\\\n import numpy as np\\\\n from pydub import AudioSegment\\\\nmodel = HubertForSpeechClassification.from_pretrained(Rajaram1996/Hubert_emotion)\\\\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(facebook/hubert-base-ls960)\\\\nsampling_rate=16000\\\\nconfig = AutoConfig.from_pretrained(Rajaram1996/Hubert_emotion)\\\\ndef speech_file_to_array(path, sampling_rate):\\\\n sound = AudioSegment.from_file(path)\\\\n sound = sound.set_frame_rate(sampling_rate)\\\\n sound_array = np.array(sound.get_array_of_samples())\\\\n return sound_array\\\\nsound_array = speech_file_to_array(audio_file, sampling_rate)\\\\ninputs = feature_extractor(sound_array, sampling_rate=sampling_rate, return_tensors=pt, padding=True)\\\\ninputs = {key: inputs[key].to(cpu).float() for key in inputs}\\\\nwith torch.no_grad():\\\\n logits = model(**inputs).logits\\\\nscores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\\\\noutputs = [{\\\\n emo: config.id2label[i],\\\\n score: round(score * 100, 1)}\\\\n for i, score in enumerate(scores)\\\\n]\\\\nreturn [row for row in sorted(outputs, key=lambda x:x[score], reverse=True) if row['score'] != '0.0%'][:2]\\\\nresult = predict_emotion_hubert(male-crying.mp3)\\\\nresult\\\", 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'A pretrained model for predicting emotion in local audio files using Hubert.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙŠØ¯ Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\". Ø§Ù„Ø¬Ù…Ù„Ø© ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¨Ø¯Ø§Ø¹ ÙÙŠ ØªÙˆÙ„ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙˆÙÙ‡Ù… Ø³ÙŠØ§Ù‚Ù‡Ø§ Ù„Ø¥Ù†ØªØ§Ø¬ Ù†Øµ Ù…Ù…ÙŠØ² ÙˆÙ…ÙÙ‡ÙˆÙ…ØŒ Ø¨Ù…Ø§ ÙŠØªÙ†Ø§Ø³Ø¨ Ù…Ø¹ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('summarization', model='lidiya/bart-large-xsum-samsum')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required library pipeline from transformers package.\\\\n2. Initialize the summarizer with the pipeline function that uses the 'lidiya/bart-large-xsum-samsum' model for abstractive summarization.\\\\n3. Pass the conversation text into the summarizer to get the summary.\\\\n4. The summarizer produces a concise summary of the main points without losing the essence of the conversation.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'lidiya/bart-large-xsum-samsum\\\\')\\\\nconversation = \\\"Hey, how are you? Long time no see. I just moved to a new house. I really love the place. It\\\\'s bigger than my previous house and much closer to work. I get to save on commute expenses too. Last week, I got a raise at work for my excellent performance. I can finally take a vacation. I was thinking about visiting Italy during the summertime, and I even booked a flight for it. I hope everything goes well, and I can\\\\'t wait to try their delicious foods.\\\"\\\\nsummary = summarizer(conversation)\\\\nprint(summary[0][\\\\'summary_text\\\\'])', 'api_call': \\\"pipeline('summarization', model='lidiya/bart-large-xsum-samsum')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lidiya/bart-large-xsum-samsum', 'api_call': \\\"pipeline('summarization', model='lidiya/bart-large-xsum-samsum')\\\", 'api_arguments': 'conversation', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=lidiya/bart-large-xsum-samsum)\\\\nconversation = '''Hannah: Hey, do you have Betty's number?\\\\nAmanda: Lemme check\\\\nAmanda: Sorry, can't find it.\\\\nAmanda: Ask Larry\\\\nAmanda: He called her last time we were at the park together\\\\nHannah: I don't know him well\\\\nAmanda: Don't be shy, he's very nice\\\\nHannah: If you say so..\\\\nHannah: I'd rather you texted him\\\\nAmanda: Just text him ğŸ™‚\\\\nHannah: Urgh.. Alright\\\\nHannah: Bye\\\\nAmanda: Bye bye <br />\\\\n'''\\\\nsummarizer(conversation)\\\", 'performance': {'dataset': 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization', 'accuracy': {'rouge1': 53.306, 'rouge2': 28.355, 'rougeL': 44.095}}, 'description': 'This model was obtained by fine-tuning facebook/bart-large-xsum on Samsum dataset.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù…Ù†ØªØ¬ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ØŒ ÙˆÙ†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the 'jinhybr/OCR-DocVQA-Donut' model for the 'document-question-answering' task.\\\\n3. Provide an image file path and a relevant question, such as retrieving the report title.\\\\n4. The loaded model will analyze the image and generate a response based on the question.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ndoc_vqa = pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\\nimage_path = 'path/to/report_image.jpg'\\\\nquestion = 'What is the title of the report?'\\\\nanswer = doc_vqa(image_path=image_path, question=question)\\\\nprint(answer['answer'])\\\", 'api_call': \\\"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': \\\"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\", 'api_arguments': 'image_path, question', 'python_environment_requirements': ['transformers'], 'example_code': \\\"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\\\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„ØºÙˆÙŠØ© Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®ØµØ§Øª Ù‚ØµÙŠØ±Ø© Ù„Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-arxiv')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library, specifically BigBirdPegasusForConditionalGeneration for the summarization model, and AutoTokenizer for processing text input.\\\\n2. Create a tokenizer instance using the AutoTokenizer.from_pretrained() method with 'google/bigbird-pegasus-large-arxiv' as the model.\\\\n3. Load the pre-trained BigBirdPegasus model using the BigBirdPegasusForConditionalGeneration.from_pretrained() method with 'google/bigbird-pegasus-large-arxiv'.\\\\n4. Use the tokenizer to tokenize the input scientific paper text.\\\\n5. Pass the tokenized inputs to the model's generate() method to generate a summary.\\\\n6. Decode the generated summary tokens back into human-readable text.\\\", '<<<code>>>': 'from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'google/bigbird-pegasus-large-arxiv\\\\')\\\\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(\\\\'google/bigbird-pegasus-large-arxiv\\\\')\\\\ntext = \\\"<Scientific_Paper_Text>\\\"\\\\ninputs = tokenizer(text, return_tensors=\\\\'pt\\\\')\\\\nprediction = model.generate(**inputs)\\\\nsummary = tokenizer.batch_decode(prediction)[0]', 'api_call': \\\"BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-arxiv')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/bigbird-pegasus-large-arxiv', 'api_call': \\\"BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-arxiv')\\\", 'api_arguments': ['attention_type', 'block_size', 'num_random_blocks'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(google/bigbird-pegasus-large-arxiv)\\\\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(google/bigbird-pegasus-large-arxiv)\\\\ntext = Replace me by any text you'd like.\\\\ninputs = tokenizer(text, return_tensors='pt')\\\\nprediction = model.generate(**inputs)\\\\nprediction = tokenizer.batch_decode(prediction)\\\", 'performance': {'dataset': 'scientific_papers', 'accuracy': {'ROUGE-1': 36.028, 'ROUGE-2': 13.417, 'ROUGE-L': 21.961, 'ROUGE-LSUM': 29.648}}, 'description': \\\"BigBird, is a sparse-attention based transformer which extends Transformer based models, such as BERT to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle. BigBird was introduced in this paper and first released in this repository. BigBird relies on block sparse attention instead of normal attention (i.e. BERT's attention) and can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-large-ntp-py')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the AutoTokenizer and T5ForConditionalGeneration classes from the transformers package.', 'code': 'from transformers import AutoTokenizer, T5ForConditionalGeneration\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Salesforce/codet5-large-ntp-py\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'Salesforce/codet5-large-ntp-py\\\\')\\\\ntext = \\\"Generate a Python code snippet to implement iterator pattern for a simple list.\\\"\\\\ninput_ids = tokenizer(text, return_tensors=\\\\'pt\\\\').input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\ncode_snippet = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\\\\nprint(code_snippet)', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'codet5-large-ntp-py', 'api_call': 'from transformers import AutoTokenizer, T5ForConditionalGeneration\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Salesforce/codet5-large-ntp-py\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'Salesforce/codet5-large-ntp-py\\\\')\\\\ntext = \\\"Generate a Python code snippet to implement iterator pattern for a simple list.\\\"\\\\ninput_ids = tokenizer(text, return_tensors=\\\\'pt\\\\').input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\ncode_snippet = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\\\\nprint(code_snippet)', 'api_arguments': ['text', 'return_tensors', 'input_ids', 'max_length'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, T5ForConditionalGeneration\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codet5-large-ntp-py)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(Salesforce/codet5-large-ntp-py)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'APPS benchmark', 'accuracy': 'See Table 5 of the paper'}, 'description': 'CodeT5 is a family of encoder-decoder language models for code from the paper: CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation by Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. The checkpoint included in this repository is denoted as CodeT5-large-ntp-py (770M), which is introduced by the paper: CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning by Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªÙ†ÙÙŠØ° ØªØ¯Ø±ÙŠØ¨Ø§Øª Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù†Øµ Ø¨Ø¥Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ù…Ø¹Ù‹Ø§ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡. Ø¬Ø±Ø¨ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬Ùƒ Ù„ÙÙ‡Ù… ÙƒÙŠÙÙŠØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ Ù…Ø«Ù„Ù‰. Ø§Ø³ØªÙ…ØªØ¹ Ø¨ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬Ùƒ ÙˆØªØ¹Ù„Ù… ÙƒÙŠÙÙŠØ© Ø§Ù„ØªÙØ§Ø¹Ù„ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„ Ø¨ÙŠÙ† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ§Ù„Ù…ÙˆÙ„Ø¯Ø© ÙÙŠ Ù…Ù‡Ù…Ø© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary libraries, including the BlenderbotForConditionalGeneration class from transformers.\\\\n2. Utilize the from_pretrained method to load the model 'facebook/blenderbot_small-90M'.\\\\n3. The loaded model is specialized for generating conversations and will be used as the backbone of your chatbot.\\\\n4. Receive user input (questions or comments), and pass them to the chatbot.\\\\n5. The chatbot will generate appropriate responses that can be sent back to the user.\\\", '<<<code>>>': 'from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\\\\nmodel = BlenderbotForConditionalGeneration.from_pretrained(\\\\'facebook/blenderbot_small-90M\\\\')\\\\ntokenizer = BlenderbotTokenizer.from_pretrained(\\\\'facebook/blenderbot_small-90M\\\\')\\\\ndef generate_response(message):\\\\n    inputs = tokenizer([message], return_tensors=\\\\'pt\\\\')\\\\n    reply_ids = model.generate(**inputs)\\\\n    response = tokenizer.batch_decode(reply_ids)[0].replace(\\\"<s>\\\", \\\"\\\").replace(\\\"</s>\\\", \\\"\\\").strip()\\\\n    return response', 'api_call': \\\"BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/blenderbot_small-90M', 'api_call': \\\"BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')\\\", 'api_arguments': ['message'], 'python_environment_requirements': [['transformers']], 'example_code': 'Input a message to start chatting with facebook/blenderbot_small-90M.', 'performance': {'dataset': 'blended_skill_talk', 'accuracy': 'Not provided'}, 'description': 'Blenderbot is a chatbot model that provides engaging talking points and listens to their partners, both asking and answering questions, and displaying knowledge, empathy, and personality appropriately, depending on the situation.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø§Ù„Ø´Ø±ÙƒØ© Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ù†ÙŠØ© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ø£Ø®Ø±Ù‰ØŒ ÙˆØªÙ‡Ø¯Ù Ø§Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù†ØµÙˆØµ ÙˆØªØµÙ†ÙŠÙÙ‡Ø§ Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To achieve language translation for multiple languages, import the M2M100ForConditionalGeneration class from the transformers package.\\\\n2. Load the 'facebook/m2m100_1.2B' model using M2M100ForConditionalGeneration's `from_pretrained` method. This model is designed specifically for many-to-many multilingual translation, and it can directly translate between the 9,900 directions of 100 languages.\\\\n3. Also, import and load the M2M100Tokenizer from the transformers package.\\\\n4. Set the source language of the text in the tokenizer.\\\\n5. Encode the source text using the tokenizer and generate the translated tokens using the model.\\\\n6. Finally, decode the generated tokens back into human-readable form using the tokenizer, specifying the target language.\\\", '<<<code>>>': 'from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\\\nsrc_text = \\\"Enter your source text here\\\"\\\\nsrc_lang = \\\"en\\\"  # Replace with the source language code\\\\ntgt_lang = \\\"fr\\\"  # Replace with the target language code\\\\n\\\\nmodel = M2M100ForConditionalGeneration.from_pretrained(\\\\'facebook/m2m100_1.2B\\\\')\\\\ntokenizer = M2M100Tokenizer.from_pretrained(\\\\'facebook/m2m100_1.2B\\\\')\\\\ntokenizer.src_lang = src_lang\\\\n\\\\nencoded_src = tokenizer(src_text, return_tensors=\\\\'pt\\\\')\\\\ngenerated_tokens = model.generate(**encoded_src, forced_bos_token_id=tokenizer.get_lang_id(tgt_lang))\\\\ntgt_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]', 'api_call': \\\"M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/m2m100_1.2B', 'api_call': \\\"M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\\\", 'api_arguments': {'encoded_input': 'encoded text', 'forced_bos_token_id': 'target language id'}, 'python_environment_requirements': ['sentencepiece'], 'example_code': 'from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\\\nhi_text = chinese_text = model = M2M100ForConditionalGeneration.from_pretrained(facebook/m2m100_1.2B)\\\\ntokenizer = M2M100Tokenizer.from_pretrained(facebook/m2m100_1.2B)\\\\ntokenizer.src_lang = hi\\\\nencoded_hi = tokenizer(hi_text, return_tensors=pt)\\\\ngenerated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(fr))\\\\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)', 'performance': {'dataset': 'M2M100', 'accuracy': 'Not specified'}, 'description': 'M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø±Ø¦ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ¬Ø¯ÙŠØ¯ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø¨Ø¨Ù†Ø§Ø¡ Ø£Ù†Ø¸Ù…Ø© Ù„ØºÙˆÙŠØ© Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„Ø­ÙˆØ§Ø± Ø¨ÙŠÙ† Ø§Ù„Ø£ÙØ±Ø§Ø¯.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library.\\\\n2. Use the pipeline function to create a visual question answering model.\\\\n3. Load the model 'ivelin/donut-refexp-combined-v1'. This model is trained for multimodal tasks that involve both images and text, such as visual question answering.\\\\n4. Given an image and a corresponding question (e.g., about hazardous materials or their colors), the model can be used to analyze the image and provide an answer based on the visual content in the context of the question.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nvqa = pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\\\\nimage_path = 'warehouse_image.jpg'\\\\nquestion = 'What is the color of the hazardous material?'\\\\nanswer = vqa(image=image_path, question=question)\\\", 'api_call': \\\"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ivelin/donut-refexp-combined-v1', 'api_call': \\\"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\\\", 'api_arguments': 'image, question', 'python_environment_requirements': ['transformers'], 'example_code': \\\"vqa(image='path/to/image.jpg', question='What is the color of the object?')\\\", 'performance': {'dataset': 'ivelin/donut-refexp-combined-v1', 'accuracy': 'N/A'}, 'description': 'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø§Ù‡ØªÙ…Ø§Ù…ÙŠ Ø¨Ù…Ø¬Ø§Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù…ØŒ ÙˆØ§Ù„Ø±ØºØ¨Ø© ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØª Ø§Ù„Ù…ØªÙˆÙ„Ø¯ Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙƒØªÙˆØ¨ØŒ Ø£ÙˆØ¯ Ø£Ù† Ø§Ø·Ù„Ø¨ Ù…Ù†Ùƒ ØªÙˆÙ„ÙŠØ¯ ØªØ¹Ù„ÙŠÙ…Ø§Øª ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØªÙŠ Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¹ÙŠÙ†Ø§Øª ØµÙˆØªÙŠØ© ÙƒØ¨ÙŠØ±Ø©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„ÙƒØŸ? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': 'run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori', 'api_provider': 'ESPnet', 'explanation': '1. The audio text-to-speech processing is done using the ESPnet framework, which provides functionality to convert text to speech.', 'code': \\\"Install required libraries: pip install espnet2 huggingface_hub\\\\nRun the following command in your terminal: ./run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori\\\\n# Use the downloaded model and ESPnet framework for text-to-speech conversion, following ESPnet's documentation and examples.\\\", 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'ESPnet', 'functionality': 'Text-to-Speech', 'api_name': 'mio/tokiwa_midori', 'api_call': \\\"Install required libraries: pip install espnet2 huggingface_hub\\\\nRun the following command in your terminal: ./run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori\\\\n# Use the downloaded model and ESPnet framework for text-to-speech conversion, following ESPnet's documentation and examples.\\\", 'api_arguments': ['espnet', 'mio/tokiwa_midori', 'Text2Speech', 'config', 'model_path'], 'python_environment_requirements': ['espnet2', 'huggingface_hub'], 'example_code': './run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori', 'performance': {'dataset': 'amadeus', 'accuracy': 'Not provided'}, 'description': 'This model was trained by mio using amadeus recipe in espnet.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø³Ù…ÙˆØ¹ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØµÙˆØªØŒ Ø­ÙŠØ« Ù†Ù‡Ø¯Ù Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø³Ù…ÙˆØ¹ Ø¨Ø´ÙƒÙ„ Ø¢Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø©ØŒ ÙˆÙ†ØªØ·Ù„Ø¹ Ø¥Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ¬Ù…ÙŠØ¹ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªÙ†ÙˆØ¹Ø© ÙˆÙ…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\\\", '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'Text2Speech' from espnet2.bin.tts_inference and 'soundfile' for handling audio files.\\\\n2. Use the 'Text2Speech.from_pretrained()' function to load the specified pre-trained model for Chinese language text-to-speech conversion.\\\\n3. The loaded model is then used to convert the input text (usually a Chinese article content) into speech.\\\\n4. Save the generated speech as an audio file (e.g., PCM 16-bit format) with 'soundfile.write()' function.\\\\n**Note**: This model is specific to the Chinese language. Replace it with a suitable model for other languages.\\\\n5. The generated audio can then be integrated into the website for visually impaired visitors to enjoy audible content.\\\", '<<<code>>>': \\\"import soundfile\\\\nfrom espnet2.bin.tts_inference import Text2Speech\\\\n\\\\ntext2speech = Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\\\\ntext = 'Chinese article content here...'\\\\nspeech = text2speech(text)['wav']\\\\nsoundfile.write('out.wav', speech.numpy(), text2speech.fs, 'PCM_16')\\\", 'api_call': \\\"Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\\\", 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best', 'api_call': \\\"Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['torch', 'espnet_model_zoo']], 'example_code': 'import soundfile\\\\nfrom espnet2.bin.tts_inference import Text2Speech\\\\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\\\\ntext = æ˜¥æ±Ÿæ½®æ°´è¿æµ·å¹³ï¼Œæµ·ä¸Šæ˜æœˆå…±æ½®ç”Ÿ\\\\nspeech = text2speech(text)[wav]\\\\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)', 'performance': {'dataset': 'csmsc', 'accuracy': 'Not specified'}, 'description': 'A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ±ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© \\\"Computer Vision\\\" Ø¨Ø¯ÙˆÙ† ØªÙˆØ¬ÙŠÙ‡ \\\"Zero-Shot Image Classification\\\"ØŒ ÙˆØªÙ‡Ø¯Ù Ù‡Ø°Ù‡ Ø§Ù„Ø®Ø¯Ù…Ø© Ø¥Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± ÙˆØªØµÙ†ÙŠÙÙ‡Ø§ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙˆØ°Ù„Ùƒ Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ù…Ø³Ø¨Ù‚Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary function 'pipeline' from the transformers package.\\\\n2. Create an image classifier by calling the 'pipeline' function and specifying the 'image-classification' task and model name 'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft'.\\\\n3. Provide the image file path and a list of animal classes you want to recognize in the image.\\\\n4. The model will return the predicted class with its confidence percentage.\\\\n5. You can display the results in the app for kids to learn about the recognized animal.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nclassifier = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\\\\nimage_path = 'path/to/animal_image.jpg'\\\\nanimal_classes = ['elephant', 'lion', 'giraffe', 'zebra', 'rhinoceros']\\\\nresult = classifier(image_path, animal_classes)\\\", 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft', 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\\\", 'api_arguments': {'image_path': 'Path to the image file', 'class_names': 'List of comma-separated class names'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft'); classifier('path/to/image.jpg', ['class1', 'class2'])\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '75.9-76.9%'}, 'description': 'A series of CLIP ConvNeXt-Large models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models achieve between 75.9 and 76.9 top-1 zero-shot accuracy on ImageNet-1k.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„ØºÙˆÙŠØ© Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù…Ù„ ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('nikcheerla/nooks-amd-detection-realtime')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the 'sentence-transformers' library.\\\\n2. Import the 'SentenceTransformer' class from the sentence_transformers library.\\\\n3. Create a SentenceTransformer instance by specifying the model 'nikcheerla/nooks-amd-detection-realtime'.\\\\n4. Encode the given texts into 768-dimensional embeddings using the 'encode' method of the SentenceTransformer instance.\\\\n5. Compute the cosine similarity between the embeddings to measure the semantic similarity between the texts.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer(\\\\'nikcheerla/nooks-amd-detection-realtime\\\\')\\\\ntext1 = \\\"This is an example sentence.\\\"\\\\ntext2 = \\\"Each sentence is converted to a vector.\\\"\\\\nembeddings = model.encode([text1, text2])\\\\n# Calculate cosine similarity between the embeddings to measure semantic similarity', 'api_call': \\\"SentenceTransformer('nikcheerla/nooks-amd-detection-realtime')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nikcheerla/nooks-amd-detection-realtime', 'api_call': \\\"SentenceTransformer('nikcheerla/nooks-amd-detection-realtime')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('{MODEL_NAME}')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('dslim/bert-base-NER')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers package. This includes AutoTokenizer, AutoModelForTokenClassification, and pipeline function.\\\\n2. Use the from_pretrained method of the AutoTokenizer and AutoModelForTokenClassification classes to load the pre-trained tokenizer and model. The model and tokenizer are 'dslim/bert-base-NER', which is fine-tuned for Named Entity Recognition (NER) tasks.\\\\n3. Create an NER pipeline using the pipeline function, with the model and tokenizer.\\\\n4. Provide the text that describes the client's request and pass it through the pipeline.\\\\n5. The pipeline will return a list of named entities and their recognized class labels (e.g., person, location, organization, or miscellaneous).\\\\n6. Use this information to create a document summary, highlighting the key persons, locations, organizations and miscellaneous entities.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'dslim/bert-base-NER\\\\')\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'dslim/bert-base-NER\\\\')\\\\nnlp = pipeline(\\\\'ner\\\\', model=model, tokenizer=tokenizer)\\\\ntext = \\\"The client\\\\'s request describes a meeting between Bill Gates and Elon Musk at Microsoft\\\\'s headquarters in Seattle. They will discuss potential collaboration on AI development for Tesla cars.\\\"\\\\nner_results = nlp(text)\\\\nprint(ner_results)', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('dslim/bert-base-NER')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dslim/bert-base-NER', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('dslim/bert-base-NER')\\\", 'api_arguments': {'model': 'AutoModelForTokenClassification.from_pretrained(dslim/bert-base-NER)', 'tokenizer': 'AutoTokenizer.from_pretrained(dslim/bert-base-NER)'}, 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoTokenizer, AutoModelForTokenClassification\\\\nfrom transformers import pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(dslim/bert-base-NER)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(dslim/bert-base-NER)\\\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\\\nexample = My name is Wolfgang and I live in Berlin\\\\nner_results = nlp(example)\\\\nprint(ner_results)', 'performance': {'dataset': 'conll2003', 'accuracy': {'f1': 91.3, 'precision': 90.7, 'recall': 91.9}}, 'description': 'bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). Specifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø¥Ù„Ù‰ Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙÙŠ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the required libraries, which are 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\ntext_generation = pipeline(\\\\'text-generation\\\\', model=\\\\'microsoft/git-large-r-textcaps\\\\')\\\\ninput_image = \\\"path_to_the_image_file\\\"\\\\ndescription = text_generation(input_image)', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-r-textcaps', 'api_call': 'from transformers import pipeline\\\\ntext_generation = pipeline(\\\\'text-generation\\\\', model=\\\\'microsoft/git-large-r-textcaps\\\\')\\\\ninput_image = \\\"path_to_the_image_file\\\"\\\\ndescription = text_generation(input_image)', 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'TextCaps', 'accuracy': ''}, 'description': \\\"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\\\"}} </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù„ÙŠÙƒ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙŠÙ…ÙƒÙ† Ù…Ù† Ø®Ù„Ø§Ù„Ù‡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„ Ø¥Ù„Ù‰ ØµÙˆØ±Ø© Ø£Ùˆ Ø§Ø¨ØªÙƒØ§Ø± Ø±Ø³ÙˆÙ…Ø§Øª Ø¨ÙŠØ§Ù†ÙŠØ© ØªÙˆØ¶Ø­ Ø§Ù„Ù…Ø¹Ù†Ù‰. Ù‡Ø°Ø§ Ø§Ù„Ù…Ø·Ù„Ø¨ ÙŠÙÙŠØ¯ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© ÙƒØªØµÙ…ÙŠÙ… ØµÙØ­Ø§Øª Ø§Ù„ÙˆÙŠØ¨ ÙˆØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ù‡Ø§ØªÙ.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': 'StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the necessary classes and libraries, including StableDiffusionPipeline and torch from the diffusers and torch packages.\\\\n2. Set your custom prompt as a textual description of the desired image. For example: \\\"High-quality, photorealistic image of a mountain bike on a scenic forest trail, set during sunset with warm colors, and text displaying the bicycle company\\\\'s name in bold, white letters.\\\"\\\\n3. Load the pre-trained model, \\\\'darkstorm2150/Protogen_v5.8_Official_Release\\\\', using the from_pretrained method with torch_dtype set to torch.float16 for optimal performance.\\\\n4. Optionally, configure the scheduler and move the pipeline to the GPU (if available) using the provided code.\\\\n5. Generate an image based on the provided text prompt using the num_inference_steps property to control the quality and detail of the output image.\\\\n6. Save the generated image to a file for use as an advertising banner.', '<<<code>>>': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\n\\\\nprompt = \\\"High-quality, photorealistic image of a mountain bike on a scenic forest trail, set during sunset with warm colors, and text displaying the bicycle company\\\\'s name in bold, white letters.\\\"\\\\nmodel_id = \\\\'darkstorm2150/Protogen_v5.8_Official_Release\\\\'\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to(\\\\'cuda\\\\')\\\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\\\nimage.save(\\\\'./bicycle_ad_banner.jpg\\\\')', 'api_call': 'StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': 'StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': [['torch', 'diffusers']], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\\\nimport torch\\\\nprompt = (\\\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\\\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\\\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\\\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\\\n)\\\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to(cuda)\\\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø¬Ø¯ÙˆÙ„ÙŠ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ù†ØªØ§Ø¦Ø¬ Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØ¬Ø°Ø§Ø¨Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©ØŒ ÙˆÙ‚Ù… Ø¨ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø­Ù‚Ù‚Ø© Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© ÙƒØ§Ù„Ø§Ù†Ø³Ø¬Ø§Ù… ÙˆØ§Ù„Ø¥ØªÙ‚Ø§Ù† ÙˆØ§Ù„Ø¥Ø¨Ø¯Ø§Ø¹.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We first need to import the necessary libraries, such as joblib for loading the model and pandas for handling the data.\\\\n2. We load the pre-trained model 'model.joblib' using 'joblib.load'.\\\\n3. Load the input data in CSV format with relevant features into a pandas DataFrame.\\\\n4. We also load the configuration file containing the feature names.\\\\n5. We preprocess the data by selecting the required features and renaming the columns in the DataFrame according to the column naming convention used in the model.\\\\n6. Finally, we use the model to make predictions on the input data and obtain the predicted carbon emissions for the client's business.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('client_data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bibekbehera/autotrain-numeric_prediction-40376105019', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'bibekbehera/autotrain-data-numeric_prediction', 'accuracy': {'Loss': 0.152, 'R2': 0.659, 'MSE': 0.023, 'MAE': 0.062, 'RMSLE': 0.105}}, 'description': 'A tabular regression model trained with AutoTrain to predict carbon emissions based on input features.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and classes like PIL for image processing, transformers for the depth estimation model, and numpy for array manipulation.\\\\n2. Use the Global-Local Path Networks (GLPN) model from Hugging Face transformers (vinvino02/glpn-kitti), which has been trained on the KITTI dataset and tailored for monocular depth estimation.\\\\n3. Load the input image from a file or capture it in real-time using the car's camera.\\\\n4. Use the model to estimate the depth of various objects within the input image.\\\\n5. The provided depth image can be used to inform the advanced driver-assistance systems (ADAS) for better decision-making during driving.\\\", '<<<code>>>': \\\"from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimage = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with path to your image\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained('vinvino02/glpn-kitti')\\\\nmodel = GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    predicted_depth = outputs.predicted_depth\\\", 'api_call': \\\"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-kitti', 'api_call': \\\"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\\\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'KITTI', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØ¹Ù„ÙŠÙ…Ø§Øª ØµÙˆØªÙŠØ© Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØªÙŠ Ø¹Ø¨Ø± APIØŒ ØªØ¨Ø¯Ø£ Ø¨Ø¬Ù…Ù„Ø© \\\"Ø§Ø¨Ø¯Ø£ Ø§Ù„Ø±ØµØ¯ Ø§Ù„ØµÙˆØªÙŠ Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒØªØ´Ù ØµÙˆØªÙ‹Ø§\\\".? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"pipeline('voice-activity-detection', model='Eklavya/ZFF_VAD')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the required library pipeline from transformers.\\\\n2. Initialize the voice activity detection model by using the pipeline function with the model parameter set as 'Eklavya/ZFF_VAD'.\\\\n3. Now you can use the voice activity detection model to predict whether there is any human voice present in your given audio file.\\\\n4. The model will provide an output indicating the presence of a human voice in the audio.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\n\\\\nvad = pipeline('voice-activity-detection', model='Eklavya/ZFF_VAD')\\\\naudio_file = 'path/to/your/audio/file.wav'\\\\nresult = vad(audio_file)\\\", 'api_call': \\\"pipeline('voice-activity-detection', model='Eklavya/ZFF_VAD')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Eklavya/ZFF_VAD', 'api_call': \\\"pipeline('voice-activity-detection', model='Eklavya/ZFF_VAD')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['transformers, torch'], 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A Voice Activity Detection model by Eklavya, using the Hugging Face framework.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¢Ù„ÙŠØ© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ¶Ù…ÙŠÙ† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ØµØ± ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ø¨ÙˆØ¶ÙˆØ­ ÙÙŠ Ø§Ù„Ù†ØµØŒ Ù…Ø«Ù„ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª ÙˆØ§Ù„ØªÙˆØ¬ÙŠÙ‡Ø§Øª.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"translate('input_text', model='Helsinki-NLP/opus-mt-en-fr')\\\", 'api_provider': 'Hugging Face', 'explanation': '1. Import the pipeline function from the transformers library provided by Hugging Face.', 'code': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_en_to_fr\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-fr\\\\')\\\\ninput_text = \\\"Enter your English text to be translated\\\"\\\\ntranslated_text = translator(input_text)\\\\nfrench_text = translated_text[0][\\\\'translation_text\\\\']\\\\nprint(french_text)', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face', 'functionality': 'Translation', 'api_name': 'Helsinki-NLP/opus-mt-en-fr', 'api_call': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_en_to_fr\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-fr\\\\')\\\\ninput_text = \\\"Enter your English text to be translated\\\"\\\\ntranslated_text = translator(input_text)\\\\nfrench_text = translated_text[0][\\\\'translation_text\\\\']\\\\nprint(french_text)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': {'newsdiscussdev2015-enfr.en.fr': 33.8, 'newsdiscusstest2015-enfr.en.fr': 40.0, 'newssyscomb2009.en.fr': 29.8, 'news-test2008.en.fr': 27.5, 'newstest2009.en.fr': 29.4, 'newstest2010.en.fr': 32.7, 'newstest2011.en.fr': 34.3, 'newstest2012.en.fr': 31.8, 'newstest2013.en.fr': 33.2, 'Tatoeba.en.fr': 50.5}}}, 'description': 'Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ Ù„Ù†Øµ ÙŠØ¨Ø¯Ø£ Ø¨Ù€ \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\"ØŒ ÙˆØµÙ ÙÙŠÙ‡Ø§ ØµÙˆØ±Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø£ÙˆØ¬Ù‡.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", '<<<api_provider>>>': 'speechbrain', '<<<explanation>>>': '1. Import necessary libraries such as torchaudio, Tacotron2, and HIFIGAN from speechbrain.\\\\n2. Load the pre-trained Tacotron2 model for German text-to-speech using the \\\\'from_hparams\\\\' method with the appropriate source and save directory.\\\\n3. Load the pre-trained HIFIGAN vocoder model for German using the \\\\'from_hparams\\\\' method with the appropriate source and save directory.\\\\n4. Encode the input text \\\"Guten Morgen, wie geht es Ihnen heute?\\\" using the Tacotron2 model, which will return the Mel spectrogram.\\\\n5. Convert the Mel spectrogram into a waveform using the HIFIGAN vocoder\\\\'s \\\\'decode_batch\\\\' method.\\\\n6. Save the resulting waveform as an audio file using torchaudio\\\\'s \\\\'save\\\\' function.', '<<<code>>>': 'import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2\\\\nfrom speechbrain.pretrained import HIFIGAN\\\\n\\\\ntext = \\\"Guten Morgen, wie geht es Ihnen heute?\\\"\\\\ntmpdir_tts = \\\"tmp_tacotron\\\"\\\\ntmpdir_vocoder = \\\"tmp_hifigan\\\"\\\\n\\\\ntacotron2 = Tacotron2.from_hparams(source=\\\\'padmalcom/tts-tacotron2-german\\\\', savedir=tmpdir_tts)\\\\nhifi_gan = HIFIGAN.from_hparams(source=\\\\'padmalcom/tts-hifigan-german\\\\', savedir=tmpdir_vocoder)\\\\n\\\\nmel_output, mel_length, alignment = tacotron2.encode_text(text)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save(\\\\'example_TTS.wav\\\\', waveforms.squeeze(1), 22050)', 'api_call': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", 'provider': 'speechbrain', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tts-hifigan-german', 'api_call': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", 'api_arguments': ['mel_output'], 'python_environment_requirements': [['speechbrain']], 'example_code': \\\"import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2\\\\nfrom speechbrain.pretrained import HIFIGAN\\\\ntacotron2 = Tacotron2.from_hparams(source=padmalcom/tts-tacotron2-german, savedir=tmpdir_tts)\\\\nhifi_gan = HIFIGAN.from_hparams(source=padmalcom/tts-hifigan-german, savedir=tmpdir_vocoder)\\\\nmel_output, mel_length, alignment = tacotron2.encode_text(Mary had a little lamb)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\\\", 'performance': {'dataset': 'custom German dataset', 'accuracy': 'Not specified'}, 'description': 'A HiFIGAN vocoder trained on a generated German dataset using mp3_to_training_data. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ ÙˆØµÙØ§Ù‹ Ù†ØµÙŠØ§Ù‹ Ù‚ØµÙŠØ±Ø§Ù‹ ÙŠØµÙ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶Ø© Ø¨Ø´ÙƒÙ„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ±.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary packages, which are AutoImageProcessor and AutoModelForImageClassification from transformers, and Image from PIL.\\\\n2. Use the 'from_pretrained' method to load the Swin Transformer v2 model, which is 'microsoft/swinv2-tiny-patch4-window8-256', for image classification tasks.\\\\n3. Load the input image using the Image object from the PIL library.\\\\n4. Use the processor to preprocess the image, and then pass the preprocessed image to the model for classification.\\\\n5. The outputs from the model can be used to identify the clothing style present in the image.\\\", '<<<code>>>': 'from transformers import AutoModelForImageClassification, AutoImageProcessor\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = \\\\'http://example.com/path/to/image.jpg\\\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = AutoImageProcessor.from_pretrained(\\\\'microsoft/swinv2-tiny-patch4-window8-256\\\\')\\\\nmodel = AutoModelForImageClassification.from_pretrained(\\\\'microsoft/swinv2-tiny-patch4-window8-256\\\\')\\\\ninputs = processor(images=image, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/swinv2-tiny-patch4-window8-256', 'api_call': \\\"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\\\", 'api_arguments': {'image': 'http://images.cocodataset.org/val2017/000000039769.jpg'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù‘Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ²ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ¥Ù†Ø´Ø§Ø¡ Ù†ØµÙˆØµØŒ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ø¯Ø§Ø¡ Ù…Ø­Ø³Ù† ÙÙŠ Ø¥Ù†ØªØ§Ø¬ Ù†ØµÙˆØµ Ø·Ø¨ÙŠØ¹ÙŠØ© ØªØªÙ…Ø§Ø´Ù‰ Ù…Ø¹ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ù‡Ø°Ø§ØŸ? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('bigscience/bloomz-560m')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries AutoModelForCausalLM and AutoTokenizer from transformers.', 'code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ncheckpoint = \\\\'bigscience/bloomz-560m\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\\\ninputs = tokenizer.encode(\\\"Translate to English: Je tâ€™aime.\\\", return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs)\\\\ntranslated_text = tokenizer.decode(outputs[0])', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Generation', 'api_name': 'bigscience/bloomz-560m', 'api_call': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ncheckpoint = \\\\'bigscience/bloomz-560m\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\\\ninputs = tokenizer.encode(\\\"Translate to English: Je tâ€™aime.\\\", return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs)\\\\ntranslated_text = tokenizer.decode(outputs[0])', 'api_arguments': {'checkpoint': 'bigscience/bloomz-560m', 'inputs': 'Translate to English: Je tâ€™aime.'}, 'python_environment_requirements': ['transformers', 'accelerate', 'bitsandbytes'], 'example_code': {'CPU': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ncheckpoint = bigscience/bloomz-560m\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\\\ninputs = tokenizer.encode(Translate to English: Je tâ€™aime., return_tensors=pt)\\\\noutputs = model.generate(inputs)\\\\nprint(tokenizer.decode(outputs[0]))'}, 'performance': {'dataset': 'bigscience/xP3', 'accuracy': {'Winogrande XL (xl) validation set': 52.41, 'XWinograd (en) test set': 51.01, 'XWinograd (fr) test set': 51.81, 'XWinograd (jp) test set': 52.03, 'XWinograd (pt) test set': 53.99, 'XWinograd (ru) test set': 53.97, 'XWinograd (zh) test set': 54.76, 'ANLI (r1) validation set': 33.4, 'ANLI (r2) validation set': 33.4, 'ANLI (r3) validation set': 33.5}}, 'description': 'BLOOMZ & mT0 are a family of models capable of following human instructions in dozens of languages zero-shot. Finetuned on the crosslingual task mixture (xP3), these models can generalize to unseen tasks & languages. Useful for tasks expressed in natural language, such as translation, summarization, and question answering.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø·ÙˆÙ‘Ø± ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆØ°Ù„Ùƒ Ù„ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆØ§Ø¬Ø¯Ø© ÙÙŠ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙŠ ÙÙ‡Ù… ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ Ø£ÙƒØ«Ø± ÙØ¹Ø§Ù„ÙŠØ© ÙˆØ³Ù‡ÙˆÙ„Ø©.? [/INST]{'domain': 'Tabular Tabular Classification', 'api_call': \\\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv')))\\\", 'api_provider': 'Scikit-learn', 'explanation': '', 'code': \\\"from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\n\\\\nREPO_ID = 'julien-c/wine-quality'\\\\nFILENAME = 'sklearn_model.joblib'\\\\n\\\\nmodel = joblib.load(cached_download(hf_hub_url(REPO_ID, FILENAME)))\\\\ndata_file = cached_download(hf_hub_url(REPO_ID, 'winequality-red.csv'))\\\\nwinedf = pd.read_csv(data_file, sep=';')\\\\n\\\\nX = winedf.drop(['quality'], axis=1)\\\\nY = winedf['quality']\\\\n\\\\nwine_quality_labels = model.predict(X[:3])\\\", 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Scikit-learn', 'functionality': 'Wine Quality classification', 'api_name': 'julien-c/wine-quality', 'api_call': \\\"from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\n\\\\nREPO_ID = 'julien-c/wine-quality'\\\\nFILENAME = 'sklearn_model.joblib'\\\\n\\\\nmodel = joblib.load(cached_download(hf_hub_url(REPO_ID, FILENAME)))\\\\ndata_file = cached_download(hf_hub_url(REPO_ID, 'winequality-red.csv'))\\\\nwinedf = pd.read_csv(data_file, sep=';')\\\\n\\\\nX = winedf.drop(['quality'], axis=1)\\\\nY = winedf['quality']\\\\n\\\\nwine_quality_labels = model.predict(X[:3])\\\", 'api_arguments': ['X'], 'python_environment_requirements': ['huggingface_hub', 'joblib', 'pandas'], 'example_code': 'from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nREPO_ID = julien-c/wine-quality\\\\nFILENAME = sklearn_model.joblib\\\\nmodel = joblib.load(cached_download(\\\\n hf_hub_url(REPO_ID, FILENAME)\\\\n))\\\\ndata_file = cached_download(\\\\n hf_hub_url(REPO_ID, winequality-red.csv)\\\\n)\\\\nwinedf = pd.read_csv(data_file, sep=;)\\\\nX = winedf.drop([quality], axis=1)\\\\nY = winedf[quality]\\\\nprint(X[:3])\\\\nlabels = model.predict(X[:3])\\\\nmodel.score(X, Y)', 'performance': {'dataset': 'julien-c/wine-quality', 'accuracy': 0.6616635397123202}, 'description': 'A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø·ÙˆÙ‘Ø± ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† ØªØµÙ†ÙŠÙ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ù„ØºØ© Ù…Ø¹ÙŠÙ†Ø© Ø¥Ù„Ù‰ Ø£Ø®Ø±Ù‰ Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙ Ù…Ø³Ø¨Ù‚ØŒ Ø®Ø§ØµØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"CrossEncoder('cross-encoder/nli-roberta-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary library which is 'pipeline' from the transformers package.\\\\n2. Use the 'pipeline' function to create a zero-shot classification model using the 'cross-encoder/nli-roberta-base' model.\\\\n3. The model will be used for classifying news headlines into various categories like technology, sports, and politics.\\\\n4. Pass the news headline to the model along with candidate labels for classification.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'cross-encoder/nli-roberta-base\\\\')\\\\nheadline = \\\"Apple unveils its latest iPhone model with advanced features\\\"\\\\ncandidate_labels = [\\\\'technology\\\\', \\\\'sports\\\\', \\\\'politics\\\\']\\\\nresult = classifier(headline, candidate_labels)', 'api_call': \\\"CrossEncoder('cross-encoder/nli-roberta-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cross-encoder/nli-roberta-base', 'api_call': \\\"CrossEncoder('cross-encoder/nli-roberta-base')\\\", 'api_arguments': ['sentence1', 'sentence2'], 'python_environment_requirements': [['sentence_transformers', 'transformers']], 'example_code': \\\"from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model='cross-encoder/nli-roberta-base')\\\\nsent = Apple just announced the newest iPhone X\\\\ncandidate_labels = [technology, sports, politics]\\\\nres = classifier(sent, candidate_labels)\\\\nprint(res)\\\", 'performance': {'dataset': ['SNLI', 'MultiNLI'], 'accuracy': 'See SBERT.net - Pretrained Cross-Encoder'}, 'description': 'Cross-Encoder for Natural Language Inference trained on the SNLI and MultiNLI datasets. Outputs three scores corresponding to the labels: contradiction, entailment, neutral.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"pipeline('summarization', model='philschmid/distilbart-cnn-12-6-samsum')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a text summarization model.\\\\n3. Specify the model 'philschmid/distilbart-cnn-12-6-samsum' for loading. This model is trained on conversational text with the goal of generating shorter summaries.\\\\n4. The created summarizer can be used to process conversation transcripts and generate brief summaries. This can help managers in understanding key points discussed during customer interactions.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'philschmid/distilbart-cnn-12-6-samsum\\\\')\\\\nconversation = \\\"customer: I am having difficulty accessing my account. \\\\\\\\nagent: Can you please provide your email address and the error message you are receiving? \\\\\\\\ncustomer: my email is example@example.com, and the error says \\\\'invalid login credentials\\\\'. \\\\\\\\nagent: I see, it seems like you need to reset your password.\\\"\\\\nsummary = summarizer(conversation)', 'api_call': \\\"pipeline('summarization', model='philschmid/distilbart-cnn-12-6-samsum')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilbart-cnn-12-6-samsum', 'api_call': \\\"pipeline('summarization', model='philschmid/distilbart-cnn-12-6-samsum')\\\", 'api_arguments': {'model': 'philschmid/distilbart-cnn-12-6-samsum'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = '''Jeff: Can I train a ğŸ¤— Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n'''\\\\nsummarizer(conversation)\\\", 'performance': {'dataset': 'samsum', 'accuracy': {'ROUGE-1': 41.09, 'ROUGE-2': 20.746, 'ROUGE-L': 31.595, 'ROUGE-LSUM': 38.339}}, 'description': 'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ø¨ØµØ±ÙŠØ©ØŒ Ø«Ù… Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©. Ù‚Ù… Ø¨ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±ØŒ ÙˆÙ‚Ù… Ø¨ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…Ø«Ù„ Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ù…Ø«Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø­Ù‚Ù‚Ø©.? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-ssv2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary packages. In this case, we need the TimesformerForVideoClassification model from the transformers library and the numpy and torch libraries to work with the video data.', 'code': \\\"from transformers import TimesformerForVideoClassification, AutoImageProcessor\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = ...  # Load your video data here (list of 16 frames)\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/timesformer-hr-finetuned-ssv2')\\\\nmodel = TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-ssv2')\\\\ninputs = feature_extractor(images=video, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n    predicted_class_idx = logits.argmax(-1).item()\\\\n    action_label = model.config.id2label[predicted_class_idx]\\\", 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'facebook/timesformer-hr-finetuned-ssv2', 'api_call': \\\"from transformers import TimesformerForVideoClassification, AutoImageProcessor\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = ...  # Load your video data here (list of 16 frames)\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/timesformer-hr-finetuned-ssv2')\\\\nmodel = TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-ssv2')\\\\ninputs = feature_extractor(images=video, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n    predicted_class_idx = logits.argmax(-1).item()\\\\n    action_label = model.config.id2label[predicted_class_idx]\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'numpy', 'torch'], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 448, 448))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-hr-finetuned-ssv2)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-hr-finetuned-ssv2)\\\\ninputs = feature_extractor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Something Something v2', 'accuracy': 'Not provided'}, 'description': 'TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ Ø¥Ù„Ù‰ Ù…Ù„Ù ØµÙˆØªÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯Ø§Ø© Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"Asteroid('JorisCos/ConvTasNet_Libri2Mix_sepclean_16k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To extract speech from a noisy audio file, you can use the 'ConvTasNet_Libri2Mix_sepclean_16k' model from the Asteroid library hosted on Hugging Face model hub.\\\\n2. Import the Asteroid class and create an instance of the model with 'JorisCos/ConvTasNet_Libri2Mix_sepclean_16k'.\\\\n3. This model has been trained to separate clean speech from noisy audio, and it can be applied to your input audio file.\\\\n4. The output will be a clean version of the speech contained in the noisy audio file.\\\", '<<<code>>>': \\\"from asteroid import Asteroid\\\\nmodel = Asteroid('JorisCos/ConvTasNet_Libri2Mix_sepclean_16k')\\\\nclean_speech = model.separate(noisy_audio_file)\\\", 'api_call': \\\"Asteroid('JorisCos/ConvTasNet_Libri2Mix_sepclean_16k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ConvTasNet_Libri2Mix_sepclean_16k', 'api_call': \\\"Asteroid('JorisCos/ConvTasNet_Libri2Mix_sepclean_16k')\\\", 'api_arguments': [], 'python_environment_requirements': [['asteroid']], 'example_code': '', 'performance': {'dataset': 'Libri2Mix', 'accuracy': {'si_sdr': 15.243671356901526, 'si_sdr_imp': 15.243034178473609, 'sdr': 15.668108919568112, 'sdr_imp': 15.578229918028036, 'sir': 25.295100756629957, 'sir_imp': 25.205219921301754, 'sar': 16.307682590197313, 'sar_imp': -51.64989963759405, 'stoi': 0.9394951175291422, 'stoi_imp': 0.22640192740016568}}, 'description': 'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¬Ù…Ù„ Ø¨Ø§Ù„Ù‚Ø±Ø¨ Ù…Ù† \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import SentenceTransformer and util from the sentence_transformers library.\\\\n2. Create a model instance using the SentenceTransformer class and load the \\\\'sentence-transformers/multi-qa-mpnet-base-dot-v1\\\\' model. This model is designed for semantic search and can map sentences or paragraphs to a dense 768-dimensional vector space.\\\\n3. Define a query representing the information they are looking for (e.g. \\\"effects of climate change on biodiversity\\\").\\\\n4. Encode the query and a list of tweets (called \\\\'docs\\\\') using the model.\\\\n5. Calculate the similarity scores (dot product) between the query embedding and the tweet embeddings.\\\\n6. Sort the tweets by their similarity scores in descending order, and return the most relevant tweets.', '<<<code>>>': \\\"from sentence_transformers import SentenceTransformer, util\\\\nmodel = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\\\\nquery = 'effects of climate change on biodiversity'\\\\ntweets = ['Tweet1...', 'Tweet2...', 'Tweet3...']\\\\nquery_emb = model.encode(query)\\\\ntweet_emb = model.encode(tweets)\\\\nscores = util.dot_score(query_emb, tweet_emb)\\\\nsorted_tweets = sorted(zip(tweets, scores), key=lambda x: x[1], reverse=True)\\\\ntop_tweets = [tweet for tweet, score in sorted_tweets[:10]]\\\", 'api_call': \\\"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/multi-qa-mpnet-base-dot-v1', 'api_call': \\\"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\\\", 'api_arguments': ['query', 'docs'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer, util\\\\nquery = How many people live in London?\\\\ndocs = [Around 9 Million people live in London, London is known for its financial district]\\\\nmodel = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\\\\nquery_emb = model.encode(query)\\\\ndoc_emb = model.encode(docs)\\\\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\\\\ndoc_score_pairs = list(zip(docs, scores))\\\\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\\\\nfor doc, score in doc_score_pairs:\\\\n print(score, doc)\\\", 'performance': {'dataset': [{'name': 'WikiAnswers', 'accuracy': 77427422}, {'name': 'PAQ', 'accuracy': 64371441}, {'name': 'Stack Exchange', 'accuracy': 25316456}]}, 'description': 'This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªØ·ÙˆÙŠØ± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆØŒ Ø¨Ø¯ÙˆÙ† Ù‚ÙŠÙˆØ¯ØŒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ø¯Ø¯Ø©.? [/INST]{'domain': 'Multimodal Text-to-Video', 'api_call': \\\"pipeline('text-to-video', model='chavinlo/TempoFunk')\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': 'from transformers import pipeline\\\\ntext_to_video_pipeline = pipeline(\\\\'text-to-video\\\\', model=\\\\'chavinlo/TempoFunk\\\\')\\\\ninput_text = \\\"The only way to do great work is to love what you do.\\\"\\\\nvideo = text_to_video_pipeline(input_text)\\\\n# Save or display the generated video for the filmmaker', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'chavinlo/TempoFunk', 'api_call': 'from transformers import pipeline\\\\ntext_to_video_pipeline = pipeline(\\\\'text-to-video\\\\', model=\\\\'chavinlo/TempoFunk\\\\')\\\\ninput_text = \\\"The only way to do great work is to love what you do.\\\"\\\\nvideo = text_to_video_pipeline(input_text)\\\\n# Save or display the generated video for the filmmaker', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù…Ø®ØªÙ„Ù Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes and functions from the transformers package, such as VideoMAEImageProcessor, VideoMAEForPreTraining.\\\\n2. Create a video input, which in this case is a list of frames, each as a 3-channel (RGB) 224x224 image.\\\\n3. Instantiate a VideoMAEImageProcessor using the from_pretrained method, specifying the 'MCG-NJU/videomae-base' model for pre-training.\\\\n4. Create an instance of the VideoMAEForPreTraining class using the same 'MCG-NJU/videomae-base' model.\\\\n5. Preprocess the video using the processor instance, and convert it into a suitable format like tensors.\\\\n6. Perform video classification using the model, providing the tensor-formatted video input and any other necessary inputs.\\\\n7. Retrieve the classification results, and determine which sport is associated with the input video.\\\", '<<<code>>>': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base')\\\\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base')\\\\npixel_values = processor(video, return_tensors='pt').pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss\\\", 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'MCG-NJU/videomae-base', 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base')\\\", 'api_arguments': ['video'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base)\\\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base)\\\\npixel_values = processor(video, return_tensors=pt).pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss', 'performance': {'dataset': 'Kinetics-400', 'accuracy': 'To be provided'}, 'description': 'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches.' </s>\"\n"}
{"text": "<s>[INST]ØªØ±ÙƒØ² Ù…Ø¬Ø§Ù„ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ÙˆØ³Ø§Ø¦Ø· Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø¹Ù„Ù‰ Ù…Ø´ÙƒÙ„Ø© Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ§Øª ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª. Ù†Ø­Ù† Ù†Ø³Ø¹Ù‰ Ù„ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ§Øª Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ ÙˆÙØ¹Ø§Ù„. Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªÙ‚Ø¯ÙŠÙ… ØªÙ‚Ø¯ÙŠØ± Ù„Ù‚Ø¯Ø±Ø© Ù†Ù…ÙˆØ°Ø¬Ù†Ø§ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¹Ù„Ù‰ Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù…ØªØ¹Ù„Ù‚Ø© Ø¨ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ÙˆØ³Ø§Ø¦Ø· Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the Hugging Face Transformers library and load the state-of-the-art model and the tokenizer for document question answering.\\\\n2. The model, 'seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', is fine-tuned on the DocVQA dataset and specialized in processing document images and extracting answers based on the provided questions.\\\\n3. Utilize the pipeline function to create a question-answering instance using the model and tokenizer, which will receive both questions and the document context as inputs. This will identify the relevant content in the document that answers the question.\\\\n4. Use this instance to query the document by passing the required information.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ndoc_qa = pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\\\\nresult = doc_qa({'question': 'What is the total amount due?', 'context': document_content})\\\", 'api_call': \\\"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \\\"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'A document question answering model finetuned on the DocVQA dataset using LayoutLMv2-base-uncased.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¨Ø±Ù…Ø¬Ø© Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ ØªØ­ÙˆÙŠÙ„ ØµÙˆØª Ø¥Ù„Ù‰ ØµÙˆØª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ØŒ ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Ø¥Ø´Ø§Ø±Ø§Øª ØµÙˆØªÙŠØ© Ù…Ø®ØªÙ„ÙØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªØ®ÙŠÙ„ Ø§Ù„ØµÙˆØª.? [/INST]{'domain': 'Audio Audio-to-Audio', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_unity_en-hk')\\\", 'api_provider': 'Fairseq', 'explanation': '', 'code': \\\"import torchaudio\\\\nfrom fairseq import hub_utils, checkpoint_utils\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nfrom IPython.display import Audio\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_unity_en-hk')\\\\nmodel = models[0].cpu()\\\\n\\\\n# Load English audio file\\\\naudio, _ = torchaudio.load('/path/to/english/audio/file')\\\\n\\\\n# Translate English speech to Hokkien UniT text\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\\\\n\\\\ncache_dir = os.getenv('HUGGINGFACE_HUB_CACHE')\\\\ncache_dir = snapshot_download('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS', cache_dir=cache_dir)\\\\n\\\\nx = hub_utils.from_pretrained(\\\\n    cache_dir,\\\\n    'model.pt',\\\\n    '.',\\\\n    archive_map=CodeHiFiGANVocoder.hub_models(),\\\\n    config_yaml='config.json',\\\\n    fp16=False,\\\\n    is_vocoder=True\\\\n)\\\\n\\\\nvocoder = CodeHiFiGANVocoder(x['args']['model_path'][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\n\\\\n# Generate Hokkien speech from UniT text\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\n\\\\n# Play the generated Hokkien speech\\\\nAudio(wav, rate=sr)\\\", 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Fairseq', 'functionality': 'speech-to-speech-translation', 'api_name': 'xm_transformer_unity_en-hk', 'api_call': \\\"import torchaudio\\\\nfrom fairseq import hub_utils, checkpoint_utils\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nfrom IPython.display import Audio\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_unity_en-hk')\\\\nmodel = models[0].cpu()\\\\n\\\\n# Load English audio file\\\\naudio, _ = torchaudio.load('/path/to/english/audio/file')\\\\n\\\\n# Translate English speech to Hokkien UniT text\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\\\\n\\\\ncache_dir = os.getenv('HUGGINGFACE_HUB_CACHE')\\\\ncache_dir = snapshot_download('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS', cache_dir=cache_dir)\\\\n\\\\nx = hub_utils.from_pretrained(\\\\n    cache_dir,\\\\n    'model.pt',\\\\n    '.',\\\\n    archive_map=CodeHiFiGANVocoder.hub_models(),\\\\n    config_yaml='config.json',\\\\n    fp16=False,\\\\n    is_vocoder=True\\\\n)\\\\n\\\\nvocoder = CodeHiFiGANVocoder(x['args']['model_path'][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\n\\\\n# Generate Hokkien speech from UniT text\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\n\\\\n# Play the generated Hokkien speech\\\\nAudio(wav, rate=sr)\\\", 'api_arguments': {'config_yaml': 'config.yaml', 'task': 'speech_to_text', 'cache_dir': 'cache_dir'}, 'python_environment_requirements': ['fairseq', 'hub_utils', 'torchaudio', 'IPython.display', 'huggingface_hub'], 'example_code': ['import json', 'import os', 'from pathlib import Path', 'import IPython.display as ipd', 'from fairseq import hub_utils', 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub', 'from fairseq.models.speech_to_text.hub_interface import S2THubInterface', 'from fairseq.models.text_to_speech import CodeHiFiGANVocoder', 'from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface', 'from huggingface_hub import snapshot_download', 'import torchaudio', 'cache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)', 'models, cfg, task = load_model_ensemble_and_task_from_hf_hub(', ' facebook/xm_transformer_unity_en-hk,', ' arg_overrides={config_yaml: config.yaml, task: speech_to_text},', ' cache_dir=cache_dir,', ')', 'model = models[0].cpu()', 'cfg[task].cpu = True', 'generator = task.build_generator([model], cfg)', 'audio, _ = torchaudio.load(/path/to/an/audio/file)', 'sample = S2THubInterface.get_model_input(task, audio)', 'unit = S2THubInterface.get_prediction(task, model, generator, sample)', 'library_name = fairseq', 'cache_dir = (', ' cache_dir or (Path.home() / .cache / library_name).as_posix()', ')', 'cache_dir = snapshot_download(', ' ffacebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS, cache_dir=cache_dir, library_name=library_name', ')', 'x = hub_utils.from_pretrained(', ' cache_dir,', ' model.pt,', ' .,', ' archive_map=CodeHiFiGANVocoder.hub_models(),', ' config_yaml=config.json,', ' fp16=False,', ' is_vocoder=True,', ')', \\\"with open(f{x['args']['data']}/config.json) as f:\\\", ' vocoder_cfg = json.load(f)', 'assert (', ' len(x[args][model_path]) == 1', '), Too many vocoder models in the input', 'vocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)', 'tts_model = VocoderHubInterface(vocoder_cfg, vocoder)', 'tts_sample = tts_model.get_model_input(unit)', 'wav, sr = tts_model.get_prediction(tts_sample)', 'ipd.Audio(wav, rate=sr)'], 'performance': {'dataset': 'MuST-C', 'accuracy': None}, 'description': 'Speech-to-speech translation model with two-pass decoder (UnitY) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain.'}} </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ±ØºØ¨ ÙÙŠ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„ØµÙˆØªÙŠ. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨ØµÙˆØª ÙˆØ§Ø¶Ø­ ÙˆØ¯Ù‚ÙŠÙ‚ Ù„ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ø°ÙŠÙ† ÙŠØ¹Ø§Ù†ÙˆÙ† Ù…Ù† Ø¥Ø¹Ø§Ù‚Ø§Øª Ø¨ØµØ±ÙŠØ©.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"WhisperModel('large-v2')\\\", '<<<api_provider>>>': 'CTranslate2', '<<<explanation>>>': \\\"1. Import the required WhisperModel class from the faster_whisper library.\\\\n2. Instantiate the WhisperModel with the 'large-v2' model which has been trained on a dataset of 99 languages and is suitable for multilingual Automatic Speech Recognition (ASR).\\\\n3. Use the transcribe method of the WhisperModel instance to convert audio samples into text segments. The resulting segments will contain the start and end times as well as the transcriptions.\\\\n4. Iterate through the segments to obtain the text transcriptions and timestamps for further processing, such as displaying them in a customer support app.\\\", '<<<code>>>': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(\\\\'large-v2\\\\')\\\\nsegments, info = model.transcribe(\\\\'audio.mp3\\\\')\\\\nfor segment in segments:\\\\n    print(\\\"[%.2fs -> %.2fs] %s\\\" % (segment.start, segment.end, segment.text))', 'api_call': \\\"WhisperModel('large-v2')\\\", 'provider': 'CTranslate2', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': \\\"WhisperModel('large-v2')\\\", 'api_arguments': ['audio.mp3'], 'python_environment_requirements': [['faster_whisper']], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡ Ø¥Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Fill-Mask Ù„Ø¯ÙŠÙ†Ø§ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"DebertaV2ForMaskedLM.from_pretrained('microsoft/deberta-v2-xxlarge')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. We import the necessary classes from the transformers package, including DebertaV2ForMaskedLM for the fill-mask functionality.\\\\n2. We use the from_pretrained method of the DebertaV2ForMaskedLM class to load the pre-trained model \\\\'microsoft/deberta-v2-xxlarge\\\\'. This model is designed to fill in masked parts of text, which can help complete sentences or phrases.\\\\n3. We then prepare the input sentence by adding the mask token \\\"<mask>\\\" in place of the missing word in the sentence, i.e., \\\"During summer, the weather is usually very <mask>.\\\"\\\\n4. The classifier can then be used to predict the most plausible word that completes the statement.', '<<<code>>>': 'from transformers import DebertaV2ForMaskedLM, DebertaV2Tokenizer\\\\nmodel = DebertaV2ForMaskedLM.from_pretrained(\\\\'microsoft/deberta-v2-xxlarge\\\\')\\\\ntokenizer = DebertaV2Tokenizer.from_pretrained(\\\\'microsoft/deberta-v2-xxlarge\\\\')\\\\n\\\\ninput_sentence = \\\"During summer, the weather is usually very <mask>.\\\"\\\\ninputs = tokenizer.encode_plus(input_sentence, return_tensors=\\\"pt\\\")\\\\npredictions = model(**inputs)[0]\\\\npredicted_index = predictions.argmax(dim=2)[:, -2]\\\\npredicted_word = tokenizer.decode(predicted_index).strip()', 'api_call': \\\"DebertaV2ForMaskedLM.from_pretrained('microsoft/deberta-v2-xxlarge')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/deberta-v2-xxlarge', 'api_call': \\\"DebertaV2ForMaskedLM.from_pretrained('microsoft/deberta-v2-xxlarge')\\\", 'api_arguments': {'model_name_or_path': 'microsoft/deberta-v2-xxlarge'}, 'python_environment_requirements': [{'pip_install': ['datasets', 'deepspeed']}], 'example_code': 'python -m torch.distributed.launch --nproc_per_node=${num_gpus} run_glue.py --model_name_or_path microsoft/deberta-v2-xxlarge --task_name $TASK_NAME --do_train --do_eval --max_seq_length 256 --per_device_train_batch_size ${batch_size} --learning_rate 3e-6 --num_train_epochs 3 --output_dir $output_dir --overwrite_output_dir --logging_steps 10 --logging_dir $output_dir --deepspeed ds_config.json', 'performance': {'dataset': [{'name': 'SQuAD 1.1', 'accuracy': 'F1/EM: 96.1/91.4'}, {'name': 'SQuAD 2.0', 'accuracy': 'F1/EM: 92.2/89.7'}, {'name': 'MNLI-m/mm', 'accuracy': 'Acc: 91.7/91.9'}, {'name': 'SST-2', 'accuracy': 'Acc: 97.2'}, {'name': 'QNLI', 'accuracy': 'Acc: 96.0'}, {'name': 'CoLA', 'accuracy': 'MCC: 72.0'}, {'name': 'RTE', 'accuracy': 'Acc: 93.5'}, {'name': 'MRPC', 'accuracy': 'Acc/F1: 93.1/94.9'}, {'name': 'QQP', 'accuracy': 'Acc/F1: 92.7/90.3'}, {'name': 'STS-B', 'accuracy': 'P/S: 93.2/93.1'}]}, 'description': 'DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data. This is the DeBERTa V2 xxlarge model with 48 layers, 1536 hidden size. The total parameters are 1.5B and it is trained with 160GB raw data.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ·Ù„Ø¨ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØªØ­Ù„ÙŠÙ„ ÙˆØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ù…Ø¹Ù†Ù‰ ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø¯Ø®Ù„Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ø¯ÙˆÙ„ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Feature Extraction', 'api_call': \\\"pipeline('feature-extraction', model='YituTech/conv-bert-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary libraries, which are 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\nfeature_extractor = pipeline(\\\\'feature-extraction\\\\', model=\\\\'YituTech/conv-bert-base\\\\')\\\\ntext = \\\"Sample text for feature extraction.\\\"\\\\nfeatures = feature_extractor(text)', 'api_data': {'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': 'from transformers import pipeline\\\\nfeature_extractor = pipeline(\\\\'feature-extraction\\\\', model=\\\\'YituTech/conv-bert-base\\\\')\\\\ntext = \\\"Sample text for feature extraction.\\\"\\\\nfeatures = feature_extractor(text)', 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… ÙˆØ¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ ÙÙŠØ¯ÙŠÙˆØŒ ÙˆØ°Ù„Ùƒ Ù„ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Video', '<<<api_call>>>': \\\"BaseModel.from_pretrained('duncan93/video')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"BaseModel.from_pretrained('duncan93/video')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'duncan93/video', 'api_call': \\\"BaseModel.from_pretrained('duncan93/video')\\\", 'api_arguments': '', 'python_environment_requirements': ['Asteroid'], 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù…Ø¹Ø§Ù„Ø¬ Ù†ØµÙˆØµ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ Ù†Ø±ÙŠØ¯ Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† ÙˆØ«Ø§Ø¦Ù‚ Ù…ØªÙ†ÙˆØ¹Ø© Ù…Ø«Ù„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library, such as LayoutLMv3Processor and LayoutLMv3ForQuestionAnswering.\\\\n2. Instantiate the processor and model by calling the from_pretrained method with the appropriate model name: 'rubentito/layoutlmv3-base-mpdocvqa'.\\\\n3. The model can be used for multiple documents, converting images of documents to the appropriate format (RGB).\\\\n4. The system is then able to identify the specific information the legal firm is looking for by inputting relevant questions and context from each document.\\\\n5. The model finds the optimal start and end indices within the input text to answer the question, and the answer can be decoded for further analysis.\\\", '<<<code>>>': 'from transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\\\\nimport torch\\\\nprocessor = LayoutLMv3Processor.from_pretrained(\\\\'rubentito/layoutlmv3-base-mpdocvqa\\\\', apply_ocr=False)\\\\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(\\\\'rubentito/layoutlmv3-base-mpdocvqa\\\\')\\\\nquestion = \\\"Find the contract\\\\'s start date\\\"\\\\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=\\\\'pt\\\\')\\\\n\\\\noutputs = model(**document_encoding)\\\\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\\\\nend_idx = torch.argmax(outputs.end_logits, axis=1)\\\\nanswer = processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()', 'api_call': \\\"LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'layoutlmv3-base-mpdocvqa', 'api_call': \\\"LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')\\\", 'api_arguments': ['image', 'question', 'context', 'boxes'], 'python_environment_requirements': [['torch', 'transformers']], 'example_code': 'import torch\\\\nfrom transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\\\\nprocessor = LayoutLMv3Processor.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa, apply_ocr=False)\\\\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)\\\\nimage = Image.open(example.jpg).convert(RGB)\\\\nquestion = Is this a question?\\\\ncontext = [Example]\\\\nboxes = [0, 0, 1000, 1000]\\\\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=pt)\\\\noutputs = model(**document_encoding)\\\\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\\\\nend_idx = torch.argmax(outputs.end_logits, axis=1)\\\\nanswers = self.processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()', 'performance': {'dataset': 'rubentito/mp-docvqa', 'accuracy': {'ANLS': 0.4538, 'APPA': 51.9426}}, 'description': 'This is pretrained LayoutLMv3 from Microsoft hub and fine-tuned on Multipage DocVQA (MP-DocVQA) dataset. This model was used as a baseline in Hierarchical multimodal transformers for Multi-Page DocVQA.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ÙŠÙ‚ÙˆÙ… Ø¨ØªØµÙ†ÙŠÙ Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ø£Ø´ÙƒØ§Ù„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù….? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We will import the necessary classes and libraries from the Transformers package. This includes TimesformerForVideoClassification for video classification and AutoImageProcessor for pre-processing video data.', 'code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(8, 3, 224, 224))  # Replace with video frames from CCTV\\\\nprocessor = AutoImageProcessor.from_pretrained(\\\\'facebook/timesformer-base-finetuned-k600\\\\')\\\\nmodel = TimesformerForVideoClassification.from_pretrained(\\\\'facebook/timesformer-base-finetuned-k600\\\\')\\\\n\\\\ninputs = processor(images=video, return_tensors=\\\\'pt\\\\')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'facebook/timesformer-base-finetuned-k600', 'api_call': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(8, 3, 224, 224))  # Replace with video frames from CCTV\\\\nprocessor = AutoImageProcessor.from_pretrained(\\\\'facebook/timesformer-base-finetuned-k600\\\\')\\\\nmodel = TimesformerForVideoClassification.from_pretrained(\\\\'facebook/timesformer-base-finetuned-k600\\\\')\\\\n\\\\ninputs = processor(images=video, return_tensors=\\\\'pt\\\\')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_arguments': ['images'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k600)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k600)\\\\ninputs = processor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-600', 'accuracy': None}, 'description': 'TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø·ÙˆØ± ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØª Ù„ØªØ·Ø¨ÙŠÙ‚ Ø®Ø§Øµ Ø¨ØªØµÙ†ÙŠÙ Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø£ØµÙˆØ§Øª Ù…Ø®ØªÙ„ÙØ© Ø¨Ø¯Ù‚Ø© ÙˆÙØ¹Ø§Ù„ÙŠØ© Ù„ØªÙ„Ø¨ÙŠØ© Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"Wav2Vec2ForCTC.from_pretrained('ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, Wav2Vec2ForCTC and Wav2Vec2Processor, from the transformers library.\\\\n2. Load the pre-trained model 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition', which has been fine-tuned for the Speech Emotion Recognition task on the RAVDESS dataset.\\\\n3. Use the Wav2Vec2Processor to convert your input audio file into a format that the model can understand.\\\\n4. Pass the processed input to the model and obtain the output logits.\\\\n5. Decode the logits to get a predicted emotion label.\\\", '<<<code>>>': 'from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\\\nimport torchaudio\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\\\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\\\')\\\\nprocessor = Wav2Vec2Processor.from_pretrained(\\\\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\\\')\\\\naudio_file_path = \\\"path/to/your/audio/file.wav\\\"\\\\nwaveform, _ = torchaudio.load(audio_file_path)\\\\ninput_values = processor(waveform.numpy(), sampling_rate=16_000, return_tensors=\\\"pt\\\").input_values\\\\nlogits = model(input_values).logits\\\\npredicted_id = torch.argmax(logits, dim=-1)\\\\nemotion = processor.decode(predicted_id[0])', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition')\\\", 'api_arguments': 'wav2vec2, tokenizer', 'python_environment_requirements': ['transformers 4.8.2, pytorch 1.9.0+cu102, datasets 1.9.0, tokenizers 0.10.3'], 'example_code': 'from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor', 'performance': {'dataset': 'RAVDESS', 'accuracy': 0.8223}, 'description': \\\"The model is a fine-tuned version of jonatasgrosman/wav2vec2-large-xlsr-53-english for a Speech Emotion Recognition (SER) task. The dataset used to fine-tune the original pre-trained model is the RAVDESS dataset. This dataset provides 1440 samples of recordings from actors performing on 8 different emotions in English, which are: emotions = ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised'].\\\" </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯ÙˆÙ† ØªØµÙ†ÙŠÙ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·. Ø§Ø¨Ø¯Ø£ Ø¨Ø¨Ø±Ù…Ø¬Ø© Ù…Ø­Ø±Ùƒ Ù„Ù„Ø¨Ø­Ø« ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªÙ†ÙˆØ¹Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù„ØªØµÙ†ÙŠÙÙ‡Ø§ Ø¨Ø¯Ù‚Ø©.? [/INST]{'domain': 'Multimodal Zero-Shot Image Classification', 'api_call': \\\"pipeline('zero-shot-image-classification', model='microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\ncell_type_classifier = pipeline('zero-shot-image-classification', model='microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\\\\nimage_path = 'path/to/microscopy_image.png'\\\\npossible_cell_types = ['neuron', 'astrocyte', 'microglia', 'oligodendrocyte']\\\\nclassification_result = cell_type_classifier(image_path, possible_cell_types)\\\", 'api_data': {'domain': 'Multimodal Zero-Shot Image Classification', 'framework': 'Hugging Face', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224', 'api_call': \\\"from transformers import pipeline\\\\ncell_type_classifier = pipeline('zero-shot-image-classification', model='microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\\\\nimage_path = 'path/to/microscopy_image.png'\\\\npossible_cell_types = ['neuron', 'astrocyte', 'microglia', 'oligodendrocyte']\\\\nclassification_result = cell_type_classifier(image_path, possible_cell_types)\\\", 'api_arguments': 'image, possible_class_names', 'python_environment_requirements': 'transformers, torch, torchvision', 'example_code': \\\"from transformers import pipeline\\\\nclip = pipeline('zero-shot-image-classification', model='microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\\\\nimage = 'path/to/image.png'\\\\npossible_class_names = ['class1', 'class2', 'class3']\\\\nresult = clip(image, possible_class_names)\\\", 'performance': {'dataset': 'PMC-15M', 'accuracy': 'State of the art'}, 'description': 'BiomedCLIP is a biomedical vision-language foundation model pretrained on PMC-15M, a dataset of 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central, using contrastive learning. It uses PubMedBERT as the text encoder and Vision Transformer as the image encoder, with domain-specific adaptations. It can perform various vision-language processing (VLP) tasks such as cross-modal retrieval, image classification, and visual question answering.'}} </s>\"\n"}
{"text": "<s>[INST]Ø§Ù‚ØªØ±Ø­ ÙÙŠ Ø§Ø¬ØªÙ…Ø§Ø¹ Ø§Ù„Ù†Ø§Ø¯ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨ÙŠ Ø§Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯Ø§Ø© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØ­Ø³ÙŠÙ† ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ ÙˆØªÙˆÙÙŠØ± ØªØµÙ†ÙŠÙ ØºÙŠØ± Ù…Ø³Ø¨Ù‚ Ù„Ù‡Ø§.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"translation_pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_de_to_en\\\\', model=\\\\'Helsinki-NLP/opus-mt-de-en\\\\')\\\\ntranslated_text = translator(\\\"German text goes here\\\")[0][\\\\'translation_text\\\\']', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Translation', 'api_name': 'opus-mt-de-en', 'api_call': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_de_to_en\\\\', model=\\\\'Helsinki-NLP/opus-mt-de-en\\\\')\\\\ntranslated_text = translator(\\\"German text goes here\\\")[0][\\\\'translation_text\\\\']', 'api_arguments': ['source languages: de', 'target languages: en'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'opus', 'accuracy': {'newssyscomb2009.de.en': 29.4, 'news-test2008.de.en': 27.8, 'newstest2009.de.en': 26.8, 'newstest2010.de.en': 30.2, 'newstest2011.de.en': 27.4, 'newstest2012.de.en': 29.1, 'newstest2013.de.en': 32.1, 'newstest2014-deen.de.en': 34.0, 'newstest2015-ende.de.en': 34.2, 'newstest2016-ende.de.en': 40.4, 'newstest2017-ende.de.en': 35.7, 'newstest2018-ende.de.en': 43.7, 'newstest2019-deen.de.en': 40.1, 'Tatoeba.de.en': 55.4}}, 'description': 'A German to English translation model trained on the OPUS dataset using the Hugging Face Transformers library.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØªÙŠ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„ØµÙˆØª.? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"VoiceActivityDetection(segmentation='anilbs/segmentation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class from the pyannote.audio package. This includes VoiceActivityDetection for detecting segments when someone is speaking.\\\\n2. We then use the VoiceActivityDetection class with the 'anilbs/segmentation' segmentation to create a voice activity detection pipeline.\\\\n3. Set default hyperparameters like 'onset', 'offset', 'min_duration_on', and 'min_duration_off' as specified.\\\\n4. This pipeline can then be used to analyze an audio file, in your case, a recorded call, and identify the stages when someone is speaking.\\\", '<<<code>>>': \\\"from pyannote.audio.pipelines import VoiceActivityDetection\\\\npipeline = VoiceActivityDetection(segmentation='anilbs/segmentation')\\\\nHYPER_PARAMETERS = {\\\\n    'onset': 0.5,\\\\n    'offset': 0.5,\\\\n    'min_duration_on': 0.0,\\\\n    'min_duration_off': 0.0\\\\n}\\\\npipeline.instantiate(HYPER_PARAMETERS)\\\\nvad = pipeline('audio.wav')\\\\n# replace 'audio.wav' with path to your recorded call file\\\", 'api_call': \\\"VoiceActivityDetection(segmentation='anilbs/segmentation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'anilbs/segmentation', 'api_call': \\\"VoiceActivityDetection(segmentation='anilbs/segmentation')\\\", 'api_arguments': {'onset': 0.5, 'offset': 0.5, 'min_duration_on': 0.0, 'min_duration_off': 0.0}, 'python_environment_requirements': ['pyannote.audio 2.0'], 'example_code': {'voice_activity_detection': 'from pyannote.audio.pipelines import VoiceActivityDetection\\\\npipeline = VoiceActivityDetection(segmentation=anilbs/segmentation)\\\\nHYPER_PARAMETERS = {\\\\n onset: 0.5, offset: 0.5,\\\\n min_duration_on: 0.0,\\\\n min_duration_off: 0.0\\\\n}\\\\npipeline.instantiate(HYPER_PARAMETERS)\\\\nvad = pipeline(audio.wav)'}, 'performance': {'dataset': [{'name': 'AMI Mix-Headset', 'accuracy': {'onset': 0.684, 'offset': 0.577, 'min_duration_on': 0.181, 'min_duration_off': 0.037}}, {'name': 'DIHARD3', 'accuracy': {'onset': 0.767, 'offset': 0.377, 'min_duration_on': 0.136, 'min_duration_off': 0.067}}, {'name': 'VoxConverse', 'accuracy': {'onset': 0.767, 'offset': 0.713, 'min_duration_on': 0.182, 'min_duration_off': 0.501}}]}, 'description': 'Model from End-to-end speaker segmentation for overlap-aware resegmentation, by HervÃ© Bredin and Antoine Laurent. Online demo is available as a Hugging Face Space.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø§Ø­Ø«ÙŠÙ† ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¥Ù†Ø´Ø§Ø¡ Ù†ØµÙˆØµ ØªÙˆÙ„ÙŠØ¯ÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø© ØªØ³ØªØ®Ø¯Ù… Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¹Ù…Ù‚ Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ©Ø› Ù„Ø°Ø§ØŒ ÙŠÙ†Ø¨ØºÙŠ Ù„Ù‡Ù… ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ø¬ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø­ÙÙˆØ¸ ÙŠØªÙŠØ­ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ‡ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø¥Ù…ÙƒØ§Ù†ÙŠØ§Øª Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ÙˆØ§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø³ÙŠØ§Ù‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('af1tang/personaGPT')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library, including GPT2Tokenizer and GPT2LMHeadModel.\\\\n2. Load the pre-trained model 'af1tang/personaGPT' using the AutoModelForCausalLM class.\\\\n3. Create a tokenizer using AutoTokenizer.from_pretrained.\\\\n4. Feed the persona facts and action codes (e.g., description of the bot's personality and turn-level goals) into the model. These will help generate appropriate responses for a personalized conversation about a vacation.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\nimport torch\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'af1tang/personaGPT\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'af1tang/personaGPT\\\\')\\\\n\\\\nif torch.cuda.is_available():\\\\n   model = model.cuda()\\\\n\\\\npersonas = \\\\'<persona facts>\\\\'\\\\ndialog_hx = [<action codes>]\\\\n\\\\nbot_input_ids = tokenizer.encode(personas + \\\\' \\\\'.join(dialog_hx), return_tensors=\\\\'pt\\\\')\\\\nresponse = model.generate(bot_input_ids, do_sample=True, top_k=10, top_p=0.92, max_length=1000)\\\\ngenerated_response = tokenizer.decode(response[0], skip_special_tokens=True)\\\\nprint(\\\"Bot:\\\", generated_response)', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('af1tang/personaGPT')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'af1tang/personaGPT', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('af1tang/personaGPT')\\\", 'api_arguments': {'do_sample': 'True', 'top_k': '10', 'top_p': '.92', 'max_length': '1000', 'pad_token': 'tokenizer.eos_token_id'}, 'python_environment_requirements': [{'transformers': 'GPT2Tokenizer, GPT2LMHeadModel', 'torch': 'torch'}], 'example_code': {'load_model': 'tokenizer = AutoTokenizer.from_pretrained(af1tang/personaGPT)\\\\nmodel = AutoModelForCausalLM.from_pretrained(af1tang/personaGPT)\\\\nif torch.cuda.is_available():\\\\n model = model.cuda()', 'generate_response': 'bot_input_ids = to_var([personas + flatten(dialog_hx)]).long()\\\\nmsg = generate_next(bot_input_ids)\\\\ndialog_hx.append(msg)\\\\nprint(Bot: {}.format(tokenizer.decode(msg, skip_special_tokens=True)))'}, 'performance': {'dataset': 'Persona-Chat', 'accuracy': 'Not provided'}, 'description': 'PersonaGPT is an open-domain conversational agent designed to do 2 tasks: decoding personalized responses based on input personality facts (the persona profile of the bot) and incorporating turn-level goals into its responses through action codes (e.g., talk about work, ask about favorite music). It builds on the DialoGPT-medium pretrained model based on the GPT-2 architecture.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ¶Ù…ÙŠÙ† Ø­Ù‚Ù„ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙÙŠ Ø§Ù„Ù†Øµ Ù„Ø§Ø³ØªÙƒØ´Ø§Ù ØªØ¹Ø²ÙŠØ² Ø§Ù„ØªØ¹Ù„Ù… ÙˆØªÙ‚Ø¯ÙŠÙ… Ø¬Ù…Ù„Ø© Ù…Ø¹Ù†ÙˆÙŠØ© Ù…ØªÙƒØ§Ù…Ù„Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø©.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"pipeline('translation_en_to_ROMANCE', model='Helsinki-NLP/opus-mt-en-ROMANCE')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import pipeline\\\\ntranslation = pipeline(\\\\'translation_en_to_ROMANCE\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-ROMANCE\\\\')\\\\nsocial_media_content = \\\"Check out our newest product line!\\\"\\\\ntranslated_content = translation(social_media_content, tgt_lang=\\\\'it\\\\')', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Translation', 'api_name': 'opus-mt-en-ROMANCE', 'api_call': 'from transformers import pipeline\\\\ntranslation = pipeline(\\\\'translation_en_to_ROMANCE\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-ROMANCE\\\\')\\\\nsocial_media_content = \\\"Check out our newest product line!\\\"\\\\ntranslated_content = translation(social_media_content, tgt_lang=\\\\'it\\\\')', 'api_arguments': 'source languages, target languages', 'python_environment_requirements': 'transformers, torch', 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ROMANCE', model='Helsinki-NLP/opus-mt-en-ROMANCE')\\\\ntranslated_text = translation('Hello, how are you?', tgt_lang='es')\\\", 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': 50.1, 'chr-F': 0.693}}, 'description': 'A translation model trained on the OPUS dataset that supports translation between English and various Romance languages. It uses a transformer architecture and requires a sentence initial language token in the form of >>id<< (id = valid target language ID).'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø¬Ù…Ù„ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ø­ÙŠØ« ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªØ¹Ù„Ù… ÙƒÙŠÙÙŠØ© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© ÙÙŠ Ø¸Ù„ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ù…ØªØºÙŠØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ². ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ø§Ù„ØªÙŠ ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø²ÙŠØ² Ù…Ø«Ù„ Q-learning Ø£Ùˆ Deep Q Networks Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. ÙƒÙ…Ø§ ÙŠÙÙ†ØµØ­ Ø¨ØªÙ‚Ø¯ÙŠÙ… Ø¨ÙŠØ¦Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù„Ù„Ø±ÙˆØ¨ÙˆØª Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ¬Ø§Ø¨ØªÙ‡ ÙÙŠ Ù…Ù‡Ø§Ù… Ø§Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModel.from_pretrained('typeform/squeezebert-mnli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'typeform/squeezebert-mnli'.\\\\n3. The loaded model specializes in zero-shot classification, which predicts the category of an unseen example without having matching training data.\\\\n4. Provide the input text (meeting email invitation), as well as a list of candidate categories ('operations', 'marketing', 'finance', 'technology').\\\\n5. Use the loaded model to classify the given text into the appropriate category.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'typeform/squeezebert-mnli\\\\')\\\\nemail_invitation = \\\"Dear Marketing Team, Please join us for a meeting to discuss our upcoming new marketing campaign. We will review the objectives, budget, and timeline to ensure a successful launch. Looking forward to your participation. Best, [Your Name]\\\"\\\\ncategories = [\\\\'operations\\\\', \\\\'marketing\\\\', \\\\'finance\\\\', \\\\'technology\\\\']\\\\nresult = classifier(email_invitation, categories)', 'api_call': \\\"AutoModel.from_pretrained('typeform/squeezebert-mnli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'typeform/squeezebert-mnli', 'api_call': \\\"AutoModel.from_pretrained('typeform/squeezebert-mnli')\\\", 'api_arguments': 'text, candidate_labels, multi_label', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nnlp = pipeline('zero-shot-classification', model='typeform/squeezebert-mnli')\\\\nresult = nlp('The quick brown fox jumps over the lazy dog', candidate_labels=['sports', 'language', 'animals'])\\\", 'performance': {'dataset': 'mulit_nli', 'accuracy': 'not provided'}, 'description': 'SqueezeBERT is a transformer model designed for efficient inference on edge devices. This specific model, typeform/squeezebert-mnli, is fine-tuned on the MultiNLI dataset for zero-shot classification tasks.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªÙ†ÙÙŠØ° ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ± Ø«Ù… Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± ÙˆØ§Ø®ØªØ¨Ø± Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries from the transformers package, PIL, and requests. This includes AutoFeatureExtractor, SwinForImageClassification, and Image for processing image data.\\\\n2. Use the from_pretrained method of the SwinForImageClassification class to load the pre-trained model 'microsoft/swin-tiny-patch4-window7-224'. This model has been trained for image classification tasks, which is exactly what we need for detecting the main object in an image.\\\\n3. Load the image data, either from a file or by fetching it from a URL.\\\\n4. Next, use the from_pretrained method of the AutoFeatureExtractor class to load a feature extractor for the same model.\\\\n5. Use the feature extractor to preprocess the image and create input tensors.\\\\n6. Pass these input tensors to the image classification model, which outputs logits representing the confidence scores for each possible object class.\\\\n7. Find the class with the highest confidence score and print the detected object's name.\\\", '<<<code>>>': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = \\\\'http://images.cocodataset.org/val2017/000000039769.jpg\\\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\\\\'microsoft/swin-tiny-patch4-window7-224\\\\')\\\\nmodel = SwinForImageClassification.from_pretrained(\\\\'microsoft/swin-tiny-patch4-window7-224\\\\')\\\\ninputs = feature_extractor(images=image, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/swin-tiny-patch4-window7-224', 'api_call': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': [{'transformers': 'AutoFeatureExtractor', 'PIL': 'Image', 'requests': 'requests'}], 'example_code': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø¯ÙˆÙ† Ø´Ø±ÙˆØ· Ù„Ù…Ø´Ø§Ù‡Ø¯ ØªØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø´ÙƒÙ„ Ø£ÙˆØªÙˆÙ…Ø§ØªÙŠÙƒÙŠ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØµÙ…ÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ÙŠÙ‚ÙˆÙ… Ø¨ØªÙƒÙˆÙŠÙ† Ø§Ù„ØµÙˆØ±ØŸ? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import DDPMPipeline from the diffusers library.\\\\n2. Use the DDPMPipeline.from_pretrained method to load the model 'clp/sd-class-butterflies-32'. This model is designed to generate high-quality images of butterflies.\\\\n3. Call the pipeline to generate an unconditional image of a butterfly. You can access the generated image by indexing the pipeline output.\\\\n4. The resulting image can be used as a high-quality graphic for the publishing house's upcoming biology book.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\\ngenerated_butterfly_image = pipeline().images[0]\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sd-class-butterflies-32', 'api_call': \\\"DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\", 'api_arguments': {'model_id': 'clp/sd-class-butterflies-32'}, 'python_environment_requirements': [['diffusers']], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': None, 'accuracy': None}, 'description': 'This model is a diffusion model for unconditional image generation of cute butterflies.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ¹Ù„Ù‚ Ø²Ø¨ÙˆÙ†Ù†Ø§ Ø¨ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ù„Ø£Ø´Ø®Ø§Øµ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/fastspeech2-en-200_speaker-cv4',arg_overrides={'vocoder': 'hifigan', 'fp16': False})\\\", '<<<api_provider>>>': 'Fairseq', '<<<explanation>>>': \\\"1. Import the necessary packages, including fairseq and IPython.\\\\n2. Use the load_model_ensemble_and_task_from_hf_hub function to create the FastSpeech 2 text-to-speech model. The function takes the model 'facebook/fastspeech2-en-200_speaker-cv4', which is trained on English male and female voices.\\\\n3. Update the configuration with the data and build a generator.\\\\n4. Use the TTSHubInterface functions to convert a text input (e.g., the weather forecast) into an audio waveform, and play the audio using IPython.display.Audio.\\\", '<<<code>>>': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\'facebook/fastspeech2-en-200_speaker-cv4\\\\', arg_overrides={\\\\'vocoder\\\\': \\\\'hifigan\\\\', \\\\'fp16\\\\': False})\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ntext = \\\"Today\\\\'s weather is partly cloudy with a high of 75 degrees.\\\"\\\\nmodel = models[0]\\\\ngenerator = task.build_generator(model, cfg)\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\n\\\\nipd.Audio(wav, rate=rate)', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/fastspeech2-en-200_speaker-cv4',arg_overrides={'vocoder': 'hifigan', 'fp16': False})\\\", 'provider': 'Fairseq', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'fastspeech2-en-male1', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/fastspeech2-en-200_speaker-cv4',arg_overrides={'vocoder': 'hifigan', 'fp16': False})\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['fairseq', 'IPython']], 'example_code': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n facebook/fastspeech2-en-200_speaker-cv4,\\\\n arg_overrides={vocoder: hifigan, fp16: False}\\\\n)\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\ntext = Hello, this is a test run.\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)', 'performance': {'dataset': 'common_voice', 'accuracy': None}, 'description': 'FastSpeech 2 text-to-speech model from fairseq S^2. English, 200 male/female voices, trained on Common Voice v4.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯ÙˆÙ† Ø²Ø§Ø¯ Ù…Ø³ØªØ®Ø¯Ù…Ø§Ù‹ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„Ø¨ØµØ±ÙŠ. Ù†Ù‡Ø¯Ù Ø¥Ù„Ù‰ ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†ØµÙŠØ© ÙˆØ§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the required libraries and functions such as pipeline from the transformers package, PIL.Image and os.\\\\n2. Instantiate the zero-shot image classification pipeline with the model 'laion/CLIP-ViT-g-14-laion2B-s34B-b88K' from Hugging Face.\\\\n3. Load the image you want to classify into the PIL.Image object.\\\\n4. Define a list of insect categories, such as ants, bees, butterflies, etc., that you want to classify the captured images into.\\\\n5. Use the zero-shot classification pipeline to classify the image into the provided insect categories.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nfrom PIL import Image\\\\nimport os\\\\n\\\\nimage_path = \\\\'path/to/image/file\\\\'  # replace with the path to your image\\\\nimage = Image.open(image_path)\\\\nclass_names = [\\\"ant\\\", \\\"bee\\\", \\\"butterfly\\\", \\\"spider\\\", \\\"ladybug\\\", \\\"grasshopper\\\"]\\\\n\\\\nclassifier = pipeline(\\\\'zero-shot-image-classification\\\\', model=\\\\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\\\')\\\\nprediction = classifier(image, class_names)\\\\nprint(prediction)', 'api_call': \\\"pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-ViT-g-14-laion2B-s34B-b88K', 'api_call': \\\"pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\\\", 'api_arguments': {'image': 'path/to/image/file', 'class_names': 'list_of_class_names'}, 'python_environment_requirements': [{'huggingface_hub': '0.0.17', 'transformers': '4.11.3', 'torch': '1.9.0', 'torchvision': '0.10.0'}], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØµÙˆØªÙŠØ© Ø¥Ù„Ù‰ Ù†ØµÙˆØµØŒ Ø­ÙŠØ« Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØµÙˆØªÙŠØ© Ø¨Ø´ÙƒÙ„ Ø¢Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªØ¹Ø§Ù„Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØªÙ‚Ù†ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"api.load('ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS').\\\", '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. Import the necessary libraries, which in this case are 'Api' from 'huggingface_hub' and the 'espnet_model_zoo'.\\\\n2. Load the Marathi Male Text-to-Speech model using the api.load() function from huggingface_hub, passing the identifier 'ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS'.\\\\n3. After loading the pre-trained Marathi Male TTS model, you can use it to generate speech given a Marathi text input. This generated voice over can be used for advertisements or any other multimedia content.\\\", '<<<code>>>': \\\"from huggingface_hub import Api\\\\nfrom espnet_model_zoo import apimp\\\\napi = apimp.Api()\\\\ntts_model = api.load('ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS')\\\\n# text_input = Insert your Marathi text here for generating speech\\\\n# generated_speech = tts_model.generate_speech(text_input)\\\", 'api_call': \\\"api.load('ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS').\\\", 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'SYSPIN/Marathi_Male_TTS', 'api_call': \\\"api.load('ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS').\\\", 'api_arguments': [], 'python_environment_requirements': [['huggingface_hub']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Marathi Male Text-to-Speech model using ESPnet framework.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"pipeline('table-question-answering', model='dsba-lab/koreapas-finetuned-korwikitq')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' function from the transformers library.\\\\n2. Use the 'pipeline' function to create a table-question-answering pipeline, specifying the pre-trained Korean model 'dsba-lab/koreapas-finetuned-korwikitq' as the model to be loaded.\\\\n3. The created model can be used to answer queries provided in the Korean language by using 'table' and 'query' as input for the table-question-answering pipeline.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntable_qa = pipeline('table-question-answering', model='dsba-lab/koreapas-finetuned-korwikitq')\\\\nanswer = table_qa(table=table, query='ì§ˆë¬¸')\\\", 'api_call': \\\"pipeline('table-question-answering', model='dsba-lab/koreapas-finetuned-korwikitq')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dsba-lab/koreapas-finetuned-korwikitq', 'api_call': \\\"pipeline('table-question-answering', model='dsba-lab/koreapas-finetuned-korwikitq')\\\", 'api_arguments': {}, 'python_environment_requirements': [{'transformers': '>=4.0.0'}], 'example_code': \\\"from transformers import pipeline; table_qa = pipeline('table-question-answering', model='dsba-lab/koreapas-finetuned-korwikitq'); table_qa(table=table, query='ì§ˆë¬¸')\\\", 'performance': {'dataset': 'korwikitq', 'accuracy': None}, 'description': 'A Korean Table Question Answering model finetuned on the korwikitq dataset.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„ØµÙˆØªÙŠ Ù„Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªÙˆÙÙŠØ±Ù‡. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': 'TextToSpeech(NeuML/ljspeech-jets-onnx)', '<<<api_provider>>>': 'ONNX', '<<<explanation>>>': \\\"1. First, import the necessary libraries, which include soundfile, txtai.pipeline, onnxruntime, yaml, and ttstokenizer.\\\\n2. We then use the TextToSpeech function from the txtai.pipeline library to load the 'NeuML/ljspeech-jets-onnx' model. This model has been trained for text-to-speech tasks, which is exactly what we need for converting text into speech for a podcast project.\\\\n3. With the model loaded, we can provide it with the text we wish to convert into speech. The model will then generate the speech, which can be saved into a file using the soundfile.write function, with a sample rate of 22,050 Hz.\\\", '<<<code>>>': 'import soundfile as sf\\\\nfrom txtai.pipeline import TextToSpeech\\\\ntts = TextToSpeech(\\\\'NeuML/ljspeech-jets-onnx\\\\')\\\\nspeech = tts(\\\"Text to convert to speech\\\")\\\\nsf.write(\\\\'out.wav\\\\', speech, 22050)', 'api_call': 'TextToSpeech(NeuML/ljspeech-jets-onnx)', 'provider': 'ONNX', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'NeuML/ljspeech-jets-onnx', 'api_call': 'TextToSpeech(NeuML/ljspeech-jets-onnx)', 'api_arguments': ['text'], 'python_environment_requirements': [['soundfile', 'txtai.pipeline', 'onnxruntime', 'yaml', 'ttstokenizer']], 'example_code': 'import soundfile as sf\\\\nfrom txtai.pipeline import TextToSpeech\\\\n\\\\n# Build pipeline\\\\ntts = TextToSpeech(NeuML/ljspeech-jets-onnx)\\\\n\\\\n# Generate speech\\\\nspeech = tts(Say something here)\\\\n\\\\n# Write to file\\\\nsf.write(out.wav, speech, 22050)', 'performance': {'dataset': 'ljspeech', 'accuracy': None}, 'description': 'ESPnet JETS Text-to-Speech (TTS) Model for ONNX exported using the espnet_onnx library. Can be used with txtai pipeline or directly with ONNX.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ø¬Ù…Ù„Ø© Ù…ÙØªØ§Ø­ÙŠØ© ØªØ³ØªØ®Ø¯Ù… ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ØªØ¨Ø¯Ø£ Ø¨Ù€ \\\"Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ\\\" Ù„Ù…Ø¬Ø§Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù‰ Ù†Øµ.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and classes, such as TrOCRProcessor, VisionEncoderDecoderModel from transformers, Image from PIL, and requests.\\\\n2. Download the image using the supplied URL and convert it into an RGB format using the Image class from the PIL library.\\\\n3. Load the pre-trained 'microsoft/trocr-small-handwritten' model using the VisionEncoderDecoderModel.from_pretrained() and TrOCRProcessor.from_pretrained() methods.\\\\n4. Process the image using the TrOCRProcessor to obtain the pixel values.\\\\n5. Use the generated pixel values as input to the VisionEncoderDecoderModel, and use the generate() method to obtain the generated_ids.\\\\n6. Finally, decode the generated_ids using the processor's batch_decode() function to obtain the extracted text from the handwritten image.\\\", '<<<code>>>': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ±Ø¬Ù‰ ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù†Ø´Ø§Ø¡ ØµÙˆØªÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„ÙˆØµÙÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… ÙˆØ§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ù…Ø­ØªÙˆÙ‰. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆØ¬ÙŠÙ‡ÙŠ Ù†Ø­Ùˆ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„ØªÙ†ÙÙŠØ° Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŸ? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-xlsr-53-espeak-cv-ft')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. These include Wav2Vec2Processor and Wav2Vec2ForCTC.\\\\n2. We then use the from_pretrained method of the Wav2Vec2Processor and Wav2Vec2ForCTC classes to load the pre-trained model 'facebook/wav2vec2-xlsr-53-espeak-cv-ft'. This model has been trained for automatic speech recognition tasks.\\\\n3. Use the processor to process the audio clip into input values for the model.\\\\n4. Pass the input values to the model, and it will output logits.\\\\n5. Finally, get the predicted_ids by taking the argmax of logits, and use the processor to batch_decode the predicted_ids into a transcription (text).\\\", '<<<code>>>': 'from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nprocessor = Wav2Vec2Processor.from_pretrained(\\\\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\\\')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\\\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\\\')\\\\naudio_clip = \\\"path/to/audio_clip.wav\\\"\\\\ninput_values = processor(audio_clip, return_tensors=\\\"pt\\\").input_values\\\\nwith torch.no_grad():\\\\n    logits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-xlsr-53-espeak-cv-ft')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/wav2vec2-xlsr-53-espeak-cv-ft', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-xlsr-53-espeak-cv-ft')\\\", 'api_arguments': {'model_name': 'facebook/wav2vec2-xlsr-53-espeak-cv-ft'}, 'python_environment_requirements': [{'transformers': '4.13.0', 'torch': '1.10.0', 'datasets': '1.14.0'}], 'example_code': 'processor = Wav2Vec2Processor.from_pretrained(facebook/wav2vec2-xlsr-53-espeak-cv-ft)\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(facebook/wav2vec2-xlsr-53-espeak-cv-ft)\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\ninput_values = processor(ds[0][audio][array], return_tensors=pt).input_values\\\\nwith torch.no_grad():\\\\n    logits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'performance': {'dataset': 'common_voice', 'accuracy': 'Not specified'}, 'description': 'Wav2Vec2-Large-XLSR-53 finetuned on multi-lingual Common Voice for phonetic label recognition in multiple languages. The model outputs a string of phonetic labels, and a dictionary mapping phonetic labels to words has to be used to map the phonetic output labels to output words.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ±ØºØ¨ ÙÙŠ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ØµÙˆØªÙŠ Ø¥Ù„Ù‰ ØµÙˆØªÙŠ. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø´ÙƒÙ„Ù Ø£Ø³Ø§Ø³ÙŠ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØªÙ…ÙƒÙ† Ù…Ù† ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ ØµÙˆØª Ø¨Ø­ÙŠØ« ÙŠÙƒÙˆÙ† Ù…Ù„Ø§Ø¦Ù…Ø§Ù‹ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ø°ÙŠÙ† ÙŠØ¹Ø§Ù†ÙˆÙ† Ù…Ù† Ø¥Ø¹Ø§Ù‚Ø© Ø¨ØµØ±ÙŠØ©.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import pipeline function from the transformers library provided by Hugging Face.\\\\n2. Load the audio source separation model 'mpariente/DPRNNTasNet-ks2_WHAM_sepclean' using the pipeline function with the task 'audio-source-separation'.\\\\n3. Use the created pipeline to separate the sources in the input music clip.\\\\n4. Examine the output of the pipeline, which will tell you if the music clip contains multiple sources or not.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsource_separator = pipeline(\\\\'audio-source-separation\\\\', model=\\\\'mpariente/DPRNNTasNet-ks2_WHAM_sepclean\\\\')\\\\nsources = source_separator(\\\"path_to_audio_file.ogg\\\")\\\\nif len(sources) > 1:\\\\n    print(\\\"The music clip contains multiple sources.\\\")\\\\nelse:\\\\n    print(\\\"The music clip contains only a single source.\\\")', 'api_call': \\\"pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mpariente/DPRNNTasNet-ks2_WHAM_sepclean', 'api_call': \\\"pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')\\\", 'api_arguments': 'audio_file', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'WHAM!', 'si_sdr': 19.316743490695334, 'si_sdr_imp': 19.317895273889842, 'sdr': 19.68085347190952, 'sdr_imp': 19.5298092932871, 'sir': 30.362213998701232, 'sir_imp': 30.21116982007881, 'sar': 20.15553251343315, 'sar_imp': -129.02091762351188, 'stoi': 0.97772664309074, 'stoi_imp': 0.23968091518217424}, 'description': 'This model was trained by Manuel Pariente using the wham/DPRNN recipe in Asteroid. It was trained on the sep_clean task of the WHAM! dataset.' </s>\"\n"}
{"text": "<s>[INST]Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· ÙÙŠ Ù…Ø¬Ø§Ù„ Ø¥Ø¬Ø§Ø¨Ø© Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§ØªØŒ Ø­ÙŠØ« ÙŠÙ„ØªÙ‚ÙŠ Ø§Ù„Ø£Ø¹Ø¶Ø§Ø¡ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù…Ù…Ø§Ø±Ø³Ø© Ù„Ø¹Ø¨Ø© ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. ÙŠÙÙ†ØµØ­ Ø¨ØªÙˆÙÙŠØ± Ø£Ø¯Ø§Ø© ØªÙ…ÙƒÙ†Ù‡Ù… Ù…Ù† Ø§Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ø­Ù„ Ø£Ø³Ø¦Ù„Ø© Ø­ÙˆÙ„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„.? [/INST]{'domain': 'Multimodal Document Question Answer', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import AutoModelForDocumentQuestionAnswering, AutoTokenizer\\\\nfrom PIL import Image\\\\n\\\\nmodel = AutoModelForDocumentQuestionAnswering.from_pretrained(\\\\'tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\\\\')\\\\n\\\\nquestion = \\\"What is the invoice number?\\\"\\\\nimage = Image.open(\\\\'path/to/invoice/image.jpg\\\\')\\\\n\\\\ntokens = tokenizer(question, image, return_tensors=\\\\'pt\\\\')\\\\nlogits = model(**tokens)[0].squeeze()\\\\nanswer = tokenizer.decode(logits.argmax())', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': 'from transformers import AutoModelForDocumentQuestionAnswering, AutoTokenizer\\\\nfrom PIL import Image\\\\n\\\\nmodel = AutoModelForDocumentQuestionAnswering.from_pretrained(\\\\'tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\\\\')\\\\n\\\\nquestion = \\\"What is the invoice number?\\\"\\\\nimage = Image.open(\\\\'path/to/invoice/image.jpg\\\\')\\\\n\\\\ntokens = tokenizer(question, image, return_tensors=\\\\'pt\\\\')\\\\nlogits = model(**tokens)[0].squeeze()\\\\nanswer = tokenizer.decode(logits.argmax())', 'api_arguments': 'question, image', 'python_environment_requirements': 'transformers, torch, datasets, tokenizers', 'example_code': '', 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.3167}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ ÙˆØªØ±Ø¬Ù…ØªÙ‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_en_to_it', model='Helsinki-NLP/opus-mt-en-it')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"pipeline('translation_en_to_it', model='Helsinki-NLP/opus-mt-en-it')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-en-it', 'api_call': \\\"pipeline('translation_en_to_it', model='Helsinki-NLP/opus-mt-en-it')\\\", 'api_arguments': {'source_language': 'en', 'target_language': 'it'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; translator = pipeline('translation_en_to_it', model='Helsinki-NLP/opus-mt-en-it'); translator('Hello, world!')\\\", 'performance': {'dataset': 'opus', 'accuracy': {'newssyscomb2009.en.it': {'BLEU': 30.9, 'chr-F': 0.606}, 'newstest2009.en.it': {'BLEU': 31.9, 'chr-F': 0.604}, 'Tatoeba.en.it': {'BLEU': 48.2, 'chr-F': 0.695}}}, 'description': 'A Transformer-based English to Italian translation model trained on the OPUS dataset. This model can be used for translation tasks using the Hugging Face Transformers library.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ«ÙŠÙ‚Ø© Ù†ØµÙŠØ© ØªØªØ¶Ù…Ù† Ù†ØµÙˆØµ ÙˆØµÙˆØ± ÙˆÙ…Ù‚Ø§Ø·Ø¹ ØµÙˆØªÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø³Ø¨Ù‚Ø© ÙÙŠ Ø­ÙˆØ²ØªÙƒØŒ Ø«Ù… Ø£Ø¹Ø·Ù‡Ø§ Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ù‚Ø±Ø§Ø¡ØªÙ‡Ø§ ÙˆÙ…Ø´Ø§Ù‡Ø¯Ø© Ù…Ø­ØªÙˆØ§Ù‡Ø§. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ø§Ø·Ù„Ø¨ Ù…Ù†Ù‡Ù… ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© ÙˆØ¥Ø¬Ø§Ø¨ØªÙ‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯Ø§Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·.? [/INST]{'domain': 'Computer Vision Zero-Shot Image Classification', 'api_call': \\\"CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary classes and libraries, including Image from PIL, requests, CLIPProcessor, and CLIPModel from transformers.', 'code': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\n\\\\nmodel = CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')\\\\nprocessor = CLIPProcessor.from_pretrained('flax-community/clip-rsicd-v2')\\\\nimage_url = 'https://example.com/satellite_image.jpg'\\\\nimage = Image.open(requests.get(image_url, stream=True).raw)\\\\nlabels = ['a photo of a forest', 'a photo of deforestation']\\\\n\\\\ninputs = processor(text=labels, images=image, return_tensors='pt', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\\n\\\\nresult = dict(zip(labels, probs[0]))\\\\nhighest_prob_label = max(result, key=result.get)\\\", 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'flax-community/clip-rsicd-v2', 'api_call': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\n\\\\nmodel = CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')\\\\nprocessor = CLIPProcessor.from_pretrained('flax-community/clip-rsicd-v2')\\\\nimage_url = 'https://example.com/satellite_image.jpg'\\\\nimage = Image.open(requests.get(image_url, stream=True).raw)\\\\nlabels = ['a photo of a forest', 'a photo of deforestation']\\\\n\\\\ninputs = processor(text=labels, images=image, return_tensors='pt', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\\n\\\\nresult = dict(zip(labels, probs[0]))\\\\nhighest_prob_label = max(result, key=result.get)\\\", 'api_arguments': {'text': ['a photo of a residential area', 'a photo of a playground', 'a photo of a stadium', 'a photo of a forest', 'a photo of an airport'], 'images': 'image', 'return_tensors': 'pt', 'padding': 'True'}, 'python_environment_requirements': ['PIL', 'requests', 'transformers'], 'example_code': 'from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(flax-community/clip-rsicd-v2)\\\\nprocessor = CLIPProcessor.from_pretrained(flax-community/clip-rsicd-v2)\\\\nurl = https://raw.githubusercontent.com/arampacha/CLIP-rsicd/master/data/stadium_1.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nlabels = [residential area, playground, stadium, forest, airport]\\\\ninputs = processor(text=[fa photo of a {l} for l in labels], images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\\nfor l, p in zip(labels, probs[0]):\\\\n print(f{l:&lt;16} {p:.4f})', 'performance': {'dataset': {'RSICD': {'original CLIP': {'k=1': 0.572, 'k=3': 0.745, 'k=5': 0.837, 'k=10': 0.939}, 'clip-rsicd-v2 (this model)': {'k=1': 0.883, 'k=3': 0.968, 'k=5': 0.982, 'k=10': 0.998}}}}, 'description': 'This model is a fine-tuned CLIP by OpenAI. It is designed with an aim to improve zero-shot image classification, text-to-image and image-to-image retrieval specifically on remote sensing images.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚Ø¯Ù… Ù„ÙƒÙ… ØªØ·Ø¨ÙŠÙ‚Ù‹Ø§ Ù…Ø¨ØªÙƒØ±Ù‹Ø§ Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ©ØŒ ÙŠØªÙŠØ­ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ ÙˆØ§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª Ø¨Ø³Ù‡ÙˆÙ„Ø©ØŒ Ù…Ø¹ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª ÙˆÙÙ‡Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙˆØªÙŠ Ø¨Ø´ÙƒÙ„ Ø£ÙˆØ³Ø¹ ÙˆØ£Ø¹Ù…Ù‚. Ø§ÙƒØªØ´Ù ØªØ­Ø³ÙŠÙ†Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØµÙˆØªÙŠ Ù…Ø¹ ØªØ·Ø¨ÙŠÙ‚ Text-to-Speech Ø§Ù„Ø®Ø§Øµ Ø¨Ù†Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØµÙˆØªÙŠØ§Øª.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804')\\\", '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. Import the necessary libraries, including transformers and AutoModelForCausalLM.\\\\n2. Use AutoModelForCausalLM.from_pretrained() method from transformers to load the Japanese text-to-speech model 'espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804'.\\\\n3. The model can then be used to convert a given text input into natural-sounding speech in Japanese.\\\\n4. A suitable text-to-speech pipeline can be built around this model to create a speechbot for converting Japanese news articles into speech efficiently.\\\", '<<<code>>>': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nmodel_checkpoint = \\\\'espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(model_checkpoint)\\\\n\\\\ntext = \\\"Japanese news article textâ€¦\\\"\\\\ninputs = tokenizer(text, return_tensors=\\\"pt\\\")\\\\noutputs = model(**inputs)\\\\ngenerated_speech = outputs.generated_speech', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804')\\\", 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Japanese text-to-speech model trained using the ESPnet framework. It is designed to convert text input into natural-sounding speech.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØ­Ù„ÙŠÙ„ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆØ°Ù„Ùƒ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¬Ù…Ù„ Ø¥Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù‚ÙŠØ§Ø³ÙŠØ© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©.? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"BertModel.from_pretrained('setu4993/LaBSE')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the necessary classes from the torch and transformers packages. This includes BertModel for sentence similarity and BertTokenizerFast for tokenizing.', 'code': 'import torch\\\\nfrom transformers import BertModel, BertTokenizerFast\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\\\'setu4993/LaBSE\\\\')\\\\nmodel = BertModel.from_pretrained(\\\\'setu4993/LaBSE\\\\')\\\\nmodel = model.eval()\\\\nexample_speaker_interests = [\\\\'dog\\\\', \\\"Interazione uomo-cane\\\", \\\"çŠ¬ã¨ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³\\\"]\\\\ninputs = tokenizer(example_speaker_interests, return_tensors=\\\\'pt\\\\', padding=True)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nembeddings = outputs.pooler_output', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': 'import torch\\\\nfrom transformers import BertModel, BertTokenizerFast\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\\\'setu4993/LaBSE\\\\')\\\\nmodel = BertModel.from_pretrained(\\\\'setu4993/LaBSE\\\\')\\\\nmodel = model.eval()\\\\nexample_speaker_interests = [\\\\'dog\\\\', \\\"Interazione uomo-cane\\\", \\\"çŠ¬ã¨ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³\\\"]\\\\ninputs = tokenizer(example_speaker_interests, return_tensors=\\\\'pt\\\\', padding=True)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nembeddings = outputs.pooler_output', 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \\\"import torch\\\\nfrom transformers import BertModel, BertTokenizerFast\\\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\\\nmodel = model.eval()\\\\nenglish_sentences = [\\\\n 'dog',\\\\n 'Puppies are nice.',\\\\n 'I enjoy taking long walks along the beach with my dog.',\\\\n]\\\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\\\nwith torch.no_grad():\\\\n english_outputs = model(**english_inputs)\\\\nenglish_embeddings = english_outputs.pooler_output\\\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ­ØªØ§Ø¬ Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØµÙˆØª Ù…Ù† Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ ØµÙˆØª.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create an audio source separation model.\\\\n3. Specify the model 'mpariente/DPRNNTasNet-ks2_WHAM_sepclean' to be loaded. This model is trained for separating sources in audio files or recordings, specifically, it has been trained on the sep_clean task of the WHAM! dataset.\\\\n4. The created pipeline can be used to separate the different sources in the given audio file, such as background noise, voice, and speech.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nseparator = pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')\\\\nseparated_audio_sources = separator(audio_file)\\\", 'api_call': \\\"pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mpariente/DPRNNTasNet-ks2_WHAM_sepclean', 'api_call': \\\"pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')\\\", 'api_arguments': 'audio_file', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'WHAM!', 'si_sdr': 19.316743490695334, 'si_sdr_imp': 19.317895273889842, 'sdr': 19.68085347190952, 'sdr_imp': 19.5298092932871, 'sir': 30.362213998701232, 'sir_imp': 30.21116982007881, 'sar': 20.15553251343315, 'sar_imp': -129.02091762351188, 'stoi': 0.97772664309074, 'stoi_imp': 0.23968091518217424}, 'description': 'This model was trained by Manuel Pariente using the wham/DPRNN recipe in Asteroid. It was trained on the sep_clean task of the WHAM! dataset.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ø£Ø±ÙŠØ¯ ØªÙˆÙ„ÙŠØ¯ ÙˆØµÙ Ø¯Ù‚ÙŠÙ‚ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨ØªÙƒØ±Ø© ÙˆØ¯Ù‚ÙŠÙ‚Ø©ØŸ? [/INST]{'domain': 'Natural Language Processing, Text2Text Generation', 'api_call': \\\"EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import BertTokenizerFast, EncoderDecoderModel\\\\ntokenizer = BertTokenizerFast.from_pretrained('kykim/bertshared-kor-base')\\\\nmodel = EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\\\", 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kykim/bertshared-kor-base', 'api_call': \\\"from transformers import BertTokenizerFast, EncoderDecoderModel\\\\ntokenizer = BertTokenizerFast.from_pretrained('kykim/bertshared-kor-base')\\\\nmodel = EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\\\", 'api_arguments': {'tokenizer': 'BertTokenizerFast.from_pretrained(kykim/bertshared-kor-base)'}, 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import BertTokenizerFast, EncoderDecoderModel\\\\ntokenizer = BertTokenizerFast.from_pretrained(kykim/bertshared-kor-base)\\\\nmodel = EncoderDecoderModel.from_pretrained(kykim/bertshared-kor-base)', 'performance': {'dataset': '70GB Korean text dataset', 'accuracy': '42000 lower-cased subwords'}, 'description': 'Bert base model for Korean, trained on a 70GB Korean text dataset and 42000 lower-cased subwords. Can be used for Text2Text Generation tasks.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¹Ù…Ù„ Ø¹Ù…ÙŠÙ„Ù†Ø§ Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø±Ø¦ÙŠØ© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"AutoModel.from_pretrained('microsoft/git-large-vqav2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'microsoft/git-large-vqav2'.\\\\n3. The loaded model will be used for visual-question-answering, which is a task where the model answers a given text question by processing a given image.\\\\n4. The model will process an image of the room and then answer a question related to some objects or situations in it.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nvqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0)\\\\n# Replace 'path_to_image' with the path to the image in question\\\\n# Replace 'your_question' with the question you want to ask about the image\\\\nresults = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})\\\", 'api_call': \\\"AutoModel.from_pretrained('microsoft/git-large-vqav2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/git-large-vqav2', 'api_call': \\\"AutoModel.from_pretrained('microsoft/git-large-vqav2')\\\", 'api_arguments': {'model': 'microsoft/git-large-vqav2', 'task': 'visual-question-answering', 'device': 0}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; vqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0); results = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})\\\", 'performance': {'dataset': 'VQAv2', 'accuracy': 'Refer to the paper'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries from transformers, such as AutoImageProcessor and TimesformerForVideoClassification.\\\\n2. Load the pretrained model 'facebook/timesformer-base-finetuned-k600' using TimesformerForVideoClassification.from_pretrained() method.\\\\n3. Process the video frames using AutoImageProcessor and create a torch tensor of the frames.\\\\n4. Pass the processed video tensor into the model to get the logits.\\\\n5. Find the predicted class index with the highest probability in the logits.\\\\n6. Finally, use the predicted class index to obtain the label of the action that is classified by the model.\\\", '<<<code>>>': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(\\\\'facebook/timesformer-base-finetuned-k600\\\\')\\\\nmodel = TimesformerForVideoClassification.from_pretrained(\\\\'facebook/timesformer-base-finetuned-k600\\\\')\\\\ninputs = processor(images=video, return_tensors=\\\\'pt\\\\')\\\\n\\\\n with torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n predicted_class_idx = logits.argmax(-1).item()\\\\n print(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/timesformer-base-finetuned-k600', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\", 'api_arguments': ['images'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k600)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k600)\\\\ninputs = processor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-600', 'accuracy': None}, 'description': 'TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…ÙŠÙ„ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù‚ÙŠØ§Ø³ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'SentenceTransformer' from sentence_transformers.\\\\n2. Instantiate the SentenceTransformer model with the name 'sentence-transformers/bert-base-nli-mean-tokens'.\\\\n3. The loaded model is trained to convert sentences into 768-dimensional embeddings which can be used to compute their semantic similarity.\\\\n4. Use the 'encode()' method to get the embeddings of input sentences.\\\\n5. Calculate the cosine similarity between the embeddings to determine the similarity of the two sentences.\\\\n6. A higher cosine similarity score indicates that the sentences are semantically similar.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/bert-base-nli-mean-tokens\\\\')\\\\nsentence1 = \\\"This is an example sentence.\\\"\\\\nsentence2 = \\\"Each sentence is converted.\\\"\\\\nembeddings = model.encode([sentence1, sentence2])\\\\nsimilarity = cosine_similarity(embeddings[0].reshape(1, -1), embeddings[1].reshape(1, -1))[0][0]\\\\nprint(f\\\"Cosine similarity: {similarity}\\\")', 'api_call': \\\"SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/bert-base-nli-mean-tokens', 'api_call': \\\"SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„ÙƒØ´Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù† Ø§Ù„ØµÙˆØª Ø¨Ø¥Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØª Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"Model.from_pretrained('pyannote/brouhaha', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the pyannote.audio and brouhaha-vad libraries.\\\\n2. Import the Model class from pyannote.audio and use the from_pretrained method to load the 'pyannote/brouhaha' pre-trained model. Provide your Hugging Face API access token as the 'use_auth_token' parameter.\\\\n3. Create an Inference object using the imported model.\\\\n4. Use the Inference object to estimate voice activity detection, speech-to-noise ratio, and C50 room acoustics for the meeting audio.\\\\n5. The output can be further processed to identify voice activity segments, filter out noise, and provide a clean transcript of the meeting.\\\", '<<<code>>>': 'from pyannote.audio import Model\\\\nfrom pyannote.audio import Inference\\\\nmodel = Model.from_pretrained(\\\\'pyannote/brouhaha\\\\', use_auth_token=\\\\'ACCESS_TOKEN_GOES_HERE\\\\')\\\\ninference = Inference(model)\\\\noutput = inference(\\\\'audio.wav\\\\')\\\\nfor frame, (vad, snr, c50) in output:\\\\n    t = frame.middle\\\\n    print(f\\\"{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f}\\\")', 'api_call': \\\"Model.from_pretrained('pyannote/brouhaha', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pyannote/brouhaha', 'api_call': \\\"Model.from_pretrained('pyannote/brouhaha', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", 'api_arguments': ['audio.wav'], 'python_environment_requirements': [['pyannote-audio', 'brouhaha-vad']], 'example_code': ['from pyannote.audio import Model', 'model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)', 'from pyannote.audio import Inference', 'inference = Inference(model)', 'output = inference(audio.wav)', 'for frame, (vad, snr, c50) in output:', '  t = frame.middle', '  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})'], 'performance': {'dataset': 'LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene', 'accuracy': 'Not provided'}, 'description': 'Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø¹Ø¯Ù… Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø³Ø¨Ù‚ Ù„Ù„ØµÙˆØ±. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, including Image from PIL and ChineseCLIPModel and ChineseCLIPProcessor from transformers.\\\\n2. Load the pretrained Chinese-CLIP-ViT-Large-Patch14 model using the from_pretrained() method. This is a powerful model trained on a large-scale dataset of around 200 million Chinese image-text pairs.\\\\n3. Create a processor for preprocessing images and texts using the ChineseCLIPProcessor class.\\\\n4. Load the image to classify and preprocess the image using the processor.\\\\n5. Define a list of texts corresponding to the possible categories.\\\\n6. Compute image and text features and perform normalization on the computed features.\\\\n7. Use the model's output to calculate softmax probabilities for each category.\\\", '<<<code>>>': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\n\\\\nmodel = ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\\nprocessor = ChineseCLIPProcessor.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\\nurl = 'https://your-image-url.jpg'  # replace with your image URL\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\ntexts = ['category1', 'category2', 'category3']  # replace with your categories\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nimage_features = model.get_image_features(**inputs)\\\\nimage_features /= image_features.norm(p=2, dim=-1, keepdim=True)\\\\n\\\\ninputs = processor(text=texts, padding=True, return_tensors='pt')\\\\ntext_features = model.get_text_features(**inputs)\\\\ntext_features /= text_features.norm(p=2, dim=-1, keepdim=True)\\\\n\\\\ninputs = processor(text=texts, images=image, return_tensors='pt', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\", 'api_call': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'chinese-clip-vit-large-patch14', 'api_call': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\", 'api_arguments': {'model_name': 'OFA-Sys/chinese-clip-vit-large-patch14'}, 'python_environment_requirements': [{'libraries': ['transformers', 'PIL', 'requests']}], 'example_code': 'from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\\\\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\\\\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = []\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nimage_features = model.get_image_features(**inputs)\\\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True) # normalize\\\\ninputs = processor(text=texts, padding=True, return_tensors=pt)\\\\ntext_features = model.get_text_features(**inputs)\\\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True) # normalize\\\\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\\\\nprobs = logits_per_image.softmax(dim=1) # probs: [[0.0066, 0.0211, 0.0031, 0.9692]]', 'performance': {'dataset': 'MUGE Text-to-Image Retrieval, Flickr30K-CN Retrieval, COCO-CN Retrieval, CIFAR10, CIFAR100, DTD, EuroSAT, FER, FGV, KITTI, MNIST, PASCAL VOC', 'accuracy': 'Varies depending on the dataset'}, 'description': 'Chinese-CLIP-ViT-Large-Patch14 is a large version of the Chinese CLIP model, with ViT-L/14 as the image encoder and RoBERTa-wwm-base as the text encoder. Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It is designed for zero-shot image classification tasks.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¨ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø­ØµØ§Ø¦ÙŠ. Ù†Ø­ØªØ§Ø¬ Ø§Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„ØªÙ†ÙÙŠØ° ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': \\\"1. Import the 'pipeline' function from the transformers library.\\\\n2. Create a regression pipeline with the specified model 'merve/tips9y0jvt5q-tip-regression'. This model is a Ridge Regression trained on tips9y0jvt5q dataset.\\\\n3. Use the pipeline to make predictions based on the customer's total bill amount. The output will be an estimation of the tip amount.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nregression_pipeline = pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\\\\ntotal_bill = [[45.23]]\\\\npredicted_tip = regression_pipeline(total_bill)\\\", 'api_call': \\\"pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'merve/tips9y0jvt5q-tip-regression', 'api_call': \\\"pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['dabl'], 'example_code': 'N/A', 'performance': {'dataset': 'tips9y0jvt5q', 'accuracy': {'r2': 0.41524, 'neg_mean_squared_error': -1.098792}}, 'description': 'Baseline Model trained on tips9y0jvt5q to apply regression on tip. The model uses Ridge(alpha=10) and is trained with dabl library as a baseline. For better results, use AutoTrain.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù„Ù‰ ÙØ±ÙŠÙ‚ Ø§Ù„ØªØ¹Ù‚Ø¨ Ø¹Ù…Ù„ Ø¥Ù„Ù‰ ØµÙˆØ± Ù„ÙˆØ­Ø© Ù…Ø±Ø³ÙˆÙ…Ø© ÙŠØ¯ÙˆÙŠÙ‹Ø§ ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ ØµÙˆØ± Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù„ÙˆØ­Ø§Øª ÙÙ†ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, such as 'torch' and 'Swin2SRForConditionalGeneration' from the transformers library provided by Hugging Face.\\\\n2. Create an instance of the Swin2SRForConditionalGeneration class by loading the pretrained model 'condef/Swin2SR-lightweight-x2-64', which is designed for lightweight image super-resolution tasks.\\\\n3. Use the model to upscale low-resolution images by passing the input image through the model, which will generate a higher-resolution output image.\\\\n4. The higher-resolution image can then be saved or displayed, as required by the photography application.\\\", '<<<code>>>': \\\"from transformers import Swin2SRForConditionalGeneration\\\\nfrom PIL import Image\\\\nimport torch\\\\n# Load pretrained model\\\\nmodel = Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64')\\\\nmodel.eval()\\\\n# Load input image as PIL Image\\\\ninput_image = Image.open('path/to/low_resolution_image.jpg')\\\\n# Convert PIL image to tensor format\\\\ninput_tensor = torch.FloatTensor(input_image).unsqueeze(0)\\\\n# Upscale the image using the model\\\\nwith torch.no_grad():\\\\n  upscaled_tensor = model(input_tensor)\\\\n# Convert upscaled tensor back to PIL image\\\\nupscaled_image = Image.from_numpy(upscaled_tensor.squeeze(0).numpy())\\\\n# Save or display upscaled image\\\\nupscaled_image.save('path/to/upscaled_image.jpg')\\\", 'api_call': \\\"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'swin2SR-lightweight-x2-64', 'api_call': \\\"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\\\", 'api_arguments': 'feature_extractor, model', 'python_environment_requirements': ['transformers, torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ø³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ§Øª ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·Ø¨ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙˆØµÙŠÙ ÙˆÙÙ‡Ù… Ø§Ù„Ù†Øµ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='xlm-roberta-large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary pipeline function from the transformers library provided by Hugging Face.\\\\n2. We use the pipeline function to create a fill-mask model, 'xlm-roberta-large', which is capable of predicting masked words in a text.\\\\n3. This model is a multilingual version of RoBERTa pre-trained on a large corpus of text and can be fine-tuned for various NLP tasks, such as sequence classification, token classification, or question answering.\\\\n4. In our case, we'll use the model to complete missing words in essays and documents for our online educational platform.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'xlm-roberta-large\\\\')\\\\nfilled_sentence = unmasker(\\\"In the future, artificial intelligence will play a crucial role in <mask> fields.\\\")', 'api_call': \\\"pipeline('fill-mask', model='xlm-roberta-large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'xlm-roberta-large', 'api_call': \\\"pipeline('fill-mask', model='xlm-roberta-large')\\\", 'api_arguments': {'model': 'xlm-roberta-large'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='xlm-roberta-large')\\\\nunmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': 'CommonCrawl', 'accuracy': 'N/A'}, 'description': 'XLM-RoBERTa is a multilingual version of RoBERTa pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It is designed for masked language modeling and can be fine-tuned on downstream tasks such as sequence classification, token classification, or question answering.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ ØªØµÙ†ÙŠÙÙŠ Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„ØµÙˆØªÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙˆÙ‚Ù… Ø¨ØªØµÙ†ÙŠÙÙ‡Ø§ ÙˆÙÙ‚Ù‹Ø§ Ù„Ù…Ø¹Ø§ÙŠÙŠØ± ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-large-960h-lv60-self')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nmodel = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-large-960h-lv60-self')\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-large-960h-lv60-self')\\\\n\\\\ninput_values = processor(audio_data, return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'wav2vec2', 'api_name': 'facebook/wav2vec2-large-960h-lv60-self', 'api_call': \\\"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nmodel = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-large-960h-lv60-self')\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-large-960h-lv60-self')\\\\n\\\\ninput_values = processor(audio_data, return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers', 'datasets', 'torch', 'jiwer'], 'example_code': 'from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained(facebook/wav2vec2-large-960h-lv60-self)\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(facebook/wav2vec2-large-960h-lv60-self)\\\\n\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\n\\\\ninput_values = processor(ds[0][audio][array], return_tensors=pt, padding=longest).input_values\\\\n\\\\nlogits = model(input_values).logits\\\\n\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'performance': {'dataset': 'librispeech_asr', 'accuracy': {'clean': 1.9, 'other': 3.9}}, 'description': \\\"Facebook's Wav2Vec2 model pretrained and fine-tuned on 960 hours of Libri-Light and Librispeech on 16kHz sampled speech audio. The model was trained with Self-Training objective. The model is used for Automatic Speech Recognition and can be used as a standalone acoustic model.\\\"}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ ØªØ¹Ù„ÙŠÙ…Ø© Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ ÙƒØ´Ù Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØªÙŠ ØªØ¨Ø¯Ø£ Ø¨Ù€ \\\"ØªÙˆØ³ÙŠØ¹ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø£ØµÙˆØ§Øª\\\".? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"VoiceActivityDetection(segmentation='anilbs/segmentation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class from the pyannote.audio package. This includes VoiceActivityDetection for detecting segments when someone is speaking.\\\\n2. We then use the VoiceActivityDetection class with the 'anilbs/segmentation' segmentation to create a voice activity detection pipeline.\\\\n3. Set default hyperparameters like 'onset', 'offset', 'min_duration_on', and 'min_duration_off' as specified.\\\\n4. This pipeline can then be used to analyze an audio file, in your case, a recorded call, and identify the stages when someone is speaking.\\\", '<<<code>>>': \\\"from pyannote.audio.pipelines import VoiceActivityDetection\\\\npipeline = VoiceActivityDetection(segmentation='anilbs/segmentation')\\\\nHYPER_PARAMETERS = {\\\\n    'onset': 0.5,\\\\n    'offset': 0.5,\\\\n    'min_duration_on': 0.0,\\\\n    'min_duration_off': 0.0\\\\n}\\\\npipeline.instantiate(HYPER_PARAMETERS)\\\\nvad = pipeline('audio.wav')\\\\n# replace 'audio.wav' with path to your recorded call file\\\", 'api_call': \\\"VoiceActivityDetection(segmentation='anilbs/segmentation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'anilbs/segmentation', 'api_call': \\\"VoiceActivityDetection(segmentation='anilbs/segmentation')\\\", 'api_arguments': {'onset': 0.5, 'offset': 0.5, 'min_duration_on': 0.0, 'min_duration_off': 0.0}, 'python_environment_requirements': ['pyannote.audio 2.0'], 'example_code': {'voice_activity_detection': 'from pyannote.audio.pipelines import VoiceActivityDetection\\\\npipeline = VoiceActivityDetection(segmentation=anilbs/segmentation)\\\\nHYPER_PARAMETERS = {\\\\n onset: 0.5, offset: 0.5,\\\\n min_duration_on: 0.0,\\\\n min_duration_off: 0.0\\\\n}\\\\npipeline.instantiate(HYPER_PARAMETERS)\\\\nvad = pipeline(audio.wav)'}, 'performance': {'dataset': [{'name': 'AMI Mix-Headset', 'accuracy': {'onset': 0.684, 'offset': 0.577, 'min_duration_on': 0.181, 'min_duration_off': 0.037}}, {'name': 'DIHARD3', 'accuracy': {'onset': 0.767, 'offset': 0.377, 'min_duration_on': 0.136, 'min_duration_off': 0.067}}, {'name': 'VoxConverse', 'accuracy': {'onset': 0.767, 'offset': 0.713, 'min_duration_on': 0.182, 'min_duration_off': 0.501}}]}, 'description': 'Model from End-to-end speaker segmentation for overlap-aware resegmentation, by HervÃ© Bredin and Antoine Laurent. Online demo is available as a Hugging Face Space.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù†Ø¯ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø±Ø¤ÙŠØ© ÙƒÙ…Ø¨ÙŠÙˆØªØ±ÙŠØ© Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø¯ÙˆÙ† Ù…Ø´Ø§Ù‡Ø¯Ø© \\\"Zero-Shot Video Classification\\\"ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†ÙˆØ¹ÙŠØ© Ù„ØªØ¹Ù„ÙŠÙ…Ù‡ ÙÙ‡Ù… Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØªÙŠ Ù„Ù… ÙŠØ´Ø§Ù‡Ø¯Ù‡Ø§ Ù…Ù† Ù‚Ø¨Ù„.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package, like TimesformerForVideoClassification for the video classification model.\\\\n2. Use the TimesformerForVideoClassification.from_pretrained() function to load the pre-trained model 'facebook/timesformer-hr-finetuned-k600'. This model has been trained for video classification tasks on the Kinetics-600 dataset.\\\\n3. Acquire the video frames in real-time or load the video data from a file.\\\\n4. The loaded model can be used to analyze the video frames and classify the actions taking place in it, which can be useful when identifying dangerous actions in a factory setting.\\\", '<<<code>>>': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\n# Replace \\\"video\\\" with your actual video frames as a list of numpy arrays\\\\nvideo = list(np.random.randn(16, 3, 448, 448))\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained(\\\\'facebook/timesformer-hr-finetuned-k600\\\\')\\\\nmodel = TimesformerForVideoClassification.from_pretrained(\\\\'facebook/timesformer-hr-finetuned-k600\\\\')\\\\n\\\\ninputs = processor(images=video, return_tensors=\\\\'pt\\\\')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/timesformer-hr-finetuned-k600', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')\\\", 'api_arguments': {'images': 'video', 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'numpy', 'torch']], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 448, 448))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-hr-finetuned-k600)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-hr-finetuned-k600)\\\\ninputs = processor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-600', 'accuracy': 'Not provided'}, 'description': 'TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 600 possible Kinetics-600 labels.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªÙ†ÙÙŠØ° Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠ ÙÙŠ Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ: 1. Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¥Ù„Ù‰ Ø¨ÙŠØ¦Ø© Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ. 2. ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ù…Ø·Ø§Ø¨Ù‚ØªÙ‡Ø§ Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. 3. Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠ. 4. ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¶Ø¨Ø· Ø§Ù„Ù‡Ø§ÙŠØ¨Ø±Ø¨Ø§Ø±Ø§Ù…ÙŠØªØ±Ø² Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±. 5. Ù‚ÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠ. Ø§ØªØ¨Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¨Ø¯Ù‚Ø© ÙˆØ¶Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ÙƒÙØ§Ø¡Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø². Ø­Ø¸Ù‹Ø§ Ø³Ø¹ÙŠØ¯Ù‹Ø§!? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Install and import joblib and Pandas libraries.\\\\n2. Use the joblib.load() function to load the pre-trained model, which can predict CO2 emissions of vehicles.\\\\n3. Load any required input data using Pandas, such as a CSV file containing various vehicle features.\\\\n4. Pass the input data through the trained model to obtain the predicted CO2 emissions, which can aid in emissions management.', '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\ndata = pd.read_csv('data.csv')\\\\nfeatures = ['feature1', 'feature2', 'feature3']\\\\ndata = data[features]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': '45473113800', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'samvelkoch/autotrain-data-prknsn-2', 'accuracy': {'Loss': 5.079, 'R2': 0.109, 'MSE': 25.795, 'MAE': 3.78, 'RMSLE': 0.849}}, 'description': 'A tabular regression model trained with AutoTrain for predicting carbon emissions.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù…Ø±Ø¦ÙŠØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨ØµÙˆØ±Ø© Ù…Ø­Ø¯Ø¯Ø© ÙˆØ§Ø·Ù„Ø¨ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙƒØªØ§Ø¨Ø© ÙˆØµÙ Ù†ØµÙŠ Ù„Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø©.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package and the Image class from the PIL package.\\\\n2. Use the from_pretrained method of the VisionEncoderDecoderModel and TrOCRProcessor classes to load the pre-trained model 'microsoft/trocr-large-handwritten' and processing tool required for the image-to-text tasks, respectively.\\\\n3. Load the handwritten image using the Image.open method. Convert the image to RGB format.\\\\n4. Preprocess the image using the TrOCRProcessor instance and retrieve pixel_values from the preprocessing result.\\\\n5. Use the pre-trained model to generate image-to-text information, by passing the pixel_values as input to the model's generate method.\\\\n6. Retrieve the generated text by decoding the returned ids and store it in a variable.\\\", '<<<code>>>': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimage = Image.open('handwritten_image_path.jpg').convert('RGB')\\\\n# replace 'handwritten_image_path.jpg' with path to your handwritten image\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': [{'packages': ['transformers', 'PIL', 'requests']}], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªØµÙ†ÙŠÙ Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø© Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': 'classifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)', '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class from the transformers package. This includes pipeline for zero-shot classification.\\\\n2. We then use the pipeline function to create a zero-shot classifier with the model 'Sahajtomar/German_Zeroshot'. This pre-trained model is fine-tuned specifically for classifying German language text.\\\\n3. With this classifier, we can process German news articles by inputting the article text as 'sequence', and the classifier will predict the most relevant category from the list of provided candidate labels. The hypothesis_template needs to be adjusted to German for better results.\\\\n4. The output will be a dictionary containing the most relevant category along with the confidence score. You can use this output to categorize German news articles.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nclassifier = pipeline('zero-shot-classification', model='Sahajtomar/German_Zeroshot')\\\\nsequence = 'Letzte Woche gab es einen Selbstmord in einer nahe gelegenen kolonie'\\\\ncandidate_labels = ['Verbrechen', 'TragÃ¶die', 'Stehlen']\\\\nhypothesis_template = 'In deisem geht es um {}'\\\\nresult = classifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)\\\", 'api_call': 'classifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)', 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': \\\"AutoModelForSequenceClassification.from_pretrained('Sahajtomar/German_Zeroshot')\\\", 'api_call': 'classifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)', 'api_arguments': {'sequence': 'string', 'candidate_labels': 'list of strings', 'hypothesis_template': 'string'}, 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=Sahajtomar/German_Zeroshot)\\\\nsequence = Letzte Woche gab es einen Selbstmord in einer nahe gelegenen kolonie\\\\ncandidate_labels = [Verbrechen,TragÃ¶die,Stehlen]\\\\nhypothesis_template = In deisem geht es um {}. ## Since monolingual model,its sensitive to hypothesis template. This can be experimented\\\\nclassifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)', 'performance': {'dataset': {'XNLI DEV (german)': {'accuracy': 85.5}, 'XNLI TEST (german)': {'accuracy': 83.6}}}, 'description': 'This model has GBERT Large as base model and fine-tuned it on xnli de dataset. The default hypothesis template is in English: This text is {}. While using this model, change it to In deisem geht es um {}. or something different. While inferencing through huggingface api may give poor results as it uses by default english template. Since model is monolingual and not multilingual, hypothesis template needs to be changed accordingly.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙˆÙ„ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØµÙ†ÙŠÙ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ©ØŸ? [/INST]{'domain': 'Tabular Tabular Classification', 'api_call': \\\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv')))\\\", 'api_provider': 'Scikit-learn', 'explanation': '', 'code': \\\"from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\n\\\\nmodel = joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'sklearn_model.joblib')))\\\\ndata_file = cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv'))\\\\nwine_df = pd.read_csv(data_file, sep=';')\\\\nX = wine_df.drop(['quality'], axis=1)\\\\nY = wine_df['quality']\\\\nlabels = model.predict(X[:3])\\\", 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Scikit-learn', 'functionality': 'Wine Quality classification', 'api_name': 'julien-c/wine-quality', 'api_call': \\\"from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\n\\\\nmodel = joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'sklearn_model.joblib')))\\\\ndata_file = cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv'))\\\\nwine_df = pd.read_csv(data_file, sep=';')\\\\nX = wine_df.drop(['quality'], axis=1)\\\\nY = wine_df['quality']\\\\nlabels = model.predict(X[:3])\\\", 'api_arguments': ['X'], 'python_environment_requirements': ['huggingface_hub', 'joblib', 'pandas'], 'example_code': 'from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nREPO_ID = julien-c/wine-quality\\\\nFILENAME = sklearn_model.joblib\\\\nmodel = joblib.load(cached_download(\\\\n hf_hub_url(REPO_ID, FILENAME)\\\\n))\\\\ndata_file = cached_download(\\\\n hf_hub_url(REPO_ID, winequality-red.csv)\\\\n)\\\\nwinedf = pd.read_csv(data_file, sep=;)\\\\nX = winedf.drop([quality], axis=1)\\\\nY = winedf[quality]\\\\nprint(X[:3])\\\\nlabels = model.predict(X[:3])\\\\nmodel.score(X, Y)', 'performance': {'dataset': 'julien-c/wine-quality', 'accuracy': 0.6616635397123202}, 'description': 'A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙŠØ¯ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ©. Ø§Ø³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ©. Ø§Ø¨Ø¯Ø£ Ø¨Ø¬Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„ÙŠÙ‡Ø§.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': '1. Import required libraries: joblib, json, and pandas.\\\\n2. Load the pre-trained carbon emissions classification model using joblib.load().\\\\n3. Open the configuration file for the model (config.json) and extract required features.\\\\n4. Preprocess the input dataset by applying the necessary transformations and selecting only required features.\\\\n5. Use the model.predict() method to get the category predictions for the given dataset entries.\\\\n6. As a result, you will get the category predictions for the carbon emissions of the given dataset.', '<<<code>>>': \\\"import joblib\\\\nimport json\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tejas23/autotrain-amx2-1702259729', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'json', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'tejas23/autotrain-data-amx2', 'accuracy': 0.824}, 'description': 'A multi-class classification model for predicting carbon emissions.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ ØµÙˆØ± Ø£Ø®Ø±Ù‰ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± ÙˆØªÙ‚Ø¯ÙŠÙ…Ù‡Ø§ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ù†Ø¸Ù…Ø© Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the necessary libraries and modules, including the ControlNetModel and LineartAnimeDetector.\\\\n2. Load the model (lllyasviel/control_v11p_sd15s2_lineart_anime) using `ControlNetModel.from_pretrained()`\\\\n3. Create an instance of LineartAnimeDetector using `LineartAnimeDetector.from_pretrained(\\\\'lllyasviel/Annotators\\\\')`\\\\n4. Provide a text prompt that will inspire the anime illustration, such as \\\"a magical girl flying in the sky\\\"\\\\n5. Prepare the image, and process it into a control image using the LineartAnimeDetector\\\\n6. Generate the output image using the StableDiffusionControlNetPipeline and the text prompt\\\\n7. Save the output image to a file for further review and refinement', '<<<code>>>': 'import torch\\\\nfrom controlnet_aux import LineartAnimeDetector\\\\nfrom diffusers import ControlNetModel, StableDiffusionControlNetPipeline, UniPCMultistepScheduler\\\\nfrom transformers import CLIPTextModel\\\\n\\\\nprompt = \\\"a magical girl flying in the sky\\\"\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/control_v11p_sd15s2_lineart_anime\\\\', torch_dtype=torch.float16)\\\\nprocessor = LineartAnimeDetector.from_pretrained(\\\\'lllyasviel/Annotators\\\\')\\\\ntext_encoder = CLIPTextModel.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', subfolder=\\\\'text_encoder\\\\', num_hidden_layers=11, torch_dtype=torch.float16)\\\\n\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\n\\\\ngenerator = torch.manual_seed(0)\\\\ncontrol_image = processor(None)\\\\noutput_image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\noutput_image.save(\\\\'output_image.png\\\\')', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\\\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': [['pip install diffusers transformers accelerate', 'pip install controlnet_aux==0.3.0']], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import LineartAnimeDetector', 'from transformers import CLIPTextModel', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15s2_lineart_anime', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15s2_lineart_anime/resolve/main/images/input.png', ')', 'image = image.resize((512, 512))', 'prompt = A warrior girl in the jungle', 'processor = LineartAnimeDetector.from_pretrained(lllyasviel/Annotators)', 'control_image = processor(image)', 'control_image.save(./images/control.png)', 'text_encoder = CLIPTextModel.from_pretrained(runwayml/stable-diffusion-v1-5, subfolder=text_encoder, num_hidden_layers=11, torch_dtype=torch.float16)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \\\"image.save('images/image_out.png')\\\"], 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© ØªØ¹Ù‚Ø¨ Ø¹ÙŠÙˆÙ† Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø±Ø¦ÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·. Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ†ÙˆØ§ Ø¹Ù„Ù‰ Ø§Ø³ØªØ¹Ø¯Ø§Ø¯ØŒ Ø§Ø¨Ø¯Ø£ÙˆØ§ Ø¨ØªÙ†ÙÙŠØ° Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¹Ù†ÙŠ Ù‚Ø§Ø¦Ù„ÙŠÙ†: \\\"Ø£ÙŠÙ† ÙŠÙ†Ø¸Ø± Ù‡Ø°Ø§ Ø§Ù„Ø´Ø®ØµØŸ\\\". Ø§Ù‚ØªØ±Ø­ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary class from the transformers library provided by Hugging Face.\\\\n2. Use the from_pretrained method of the AutoModel class to load the pre-trained model 'sheldonxxxx/OFA_model_weights'.\\\\n3. This model can be used to answer questions about products by combining information from both visual and textual sources.\\\\n4. By providing the appropriate input data - an image of the product and the text question from customers, the AI model can generate the most likely answer. This answer can then be utilized by the chat support agents to provide accurate and timely information to customers.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nofa_model = AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\\n# Further model preparation and usage with the associated library from https://github.com/OFA-Sys/OFA\\\", 'api_call': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sheldonxxxx/OFA_model_weights', 'api_call': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªØ¹Ù„Ù… Ø§Ù„Ø¥Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±ØŸ? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-church-256')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. First, we import the necessary classes from the diffusers package. This includes DDPMPipeline for the unconditional image generation model.', 'code': \\\"from diffusers import DDPMPipeline\\\\nddpm = DDPMPipeline.from_pretrained('google/ddpm-church-256')\\\\nimage = ddpm().images[0]\\\\nimage.save('ddpm_generated_image.png')\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-church-256', 'api_call': \\\"from diffusers import DDPMPipeline\\\\nddpm = DDPMPipeline.from_pretrained('google/ddpm-church-256')\\\\nimage = ddpm().images[0]\\\\nimage.save('ddpm_generated_image.png')\\\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-church-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.'}} </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ø±ÙˆØ¨ÙˆØªÙŠØ§Øª Ø§Ù„ØªØ¹Ø²ÙŠØ²ÙŠ.? [/INST]'<<<domain>>>': 'Reinforcement Learning Robotics', '<<<api_call>>>': \\\"AutoModel.from_pretrained('camusean/grasp_diffusion')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We first import the AutoModel class from the transformers library provided by Hugging Face.\\\\n2. We then use the from_pretrained method of the AutoModel class to load the pre-trained model 'camusean/grasp_diffusion'. This model is designed for 6D grasping tasks, which can help the robotic hand in picking and sorting objects optimally.\\\\n3. The loaded model can be used to generate the optimal grasping pose and force to pick an object in real-life scenarios. The generated grasping and motion strategies can be used to control the robotic hand more effectively.\\\\n4. The robot can then execute these strategies to pick and sort objects in the warehouse.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\ngrasp_model = AutoModel.from_pretrained('camusean/grasp_diffusion')\\\\n# Perform the necessary steps to generate the required grasping pose for the robotic hand\\\", 'api_call': \\\"AutoModel.from_pretrained('camusean/grasp_diffusion')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Reinforcement Learning Robotics', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'camusean/grasp_diffusion', 'api_call': \\\"AutoModel.from_pretrained('camusean/grasp_diffusion')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['transformers'], 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Trained Models for Grasp SE(3) DiffusionFields. Check SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion for additional details.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù„ÙÙ‡Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù„Ù†ØµÙˆØµ ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªÙ†Ø¨ÙˆØ¡ Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø³ Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙˆØ«ÙˆÙ‚Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary classes from the transformers package. This includes TapasTokenizer for tokenizing table data and TapasForQuestionAnswering for the table question answering model.\\\\n2. We then use the from_pretrained function to load the pre-trained model 'lysandre/tapas-temporary-repo'. This model specializes in answering questions related to table data which is ideal for productivity apps that deal with task tables.\\\\n3. We process the table and question input with the tokenizer to convert them into arrays of tokens that the model can accept.\\\\n4. Finally, we feed these tokenized inputs to the model and get the answer in the form of answer coordinates and aggregation indices. These indices are then translated back into the answer using the tokenizer.\\\", '<<<code>>>': \\\"from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\ntokenizer = TapasTokenizer.from_pretrained('lysandre/tapas-temporary-repo')\\\\nmodel = TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\\ninputs = tokenizer(table=table_data, queries=question, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\npredicted_answer_coordinates, _ = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach())\\\", 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lysandre/tapas-temporary-repo', 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\", 'api_arguments': ['model_name', 'question', 'table'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"tokenizer = TapasTokenizer.from_pretrained('lysandre/tapas-temporary-repo')\\\\nmodel = TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\\ninputs = tokenizer(table=table, queries=question, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())\\\", 'performance': {'dataset': 'SQA', 'accuracy': 'Not provided'}, 'description': 'TAPAS base model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion and can be used for answering questions related to a table in a conversational set-up.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆØ§Ø¬Ø¯Ø© ÙÙŠ Ø§Ù„ØµÙˆØ±ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8n-csgo-player-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first install ultralyticsplus and ultralytics to access the YOLO model. Then, we import YOLO and render_result from ultralyticsplus library.\\\\n2. Next, we load the pre-trained YOLOv8 model 'keremberke/yolov8n-csgo-player-detection' specifically trained for detecting CS:GO players. This model can recognize the following labels: ['ct', 'cthead', 't', 'thead'].\\\\n3. Configure the required confidences, inter-cell overlap, and maximum detected objects allowed by the model.\\\\n4. Provide the image URL or local path containing CS:GO players and make the predictions using the predict method.\\\\n5. To render and visualize results, use the render_result function with the model, image, and detected results.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path_to_your_csgo_image.jpg'\\\\n# replace 'path_to_your_csgo_image.jpg' with your image URL or local path\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8n-csgo-player-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8n-csgo-player-detection', 'api_call': \\\"YOLO('keremberke/yolov8n-csgo-player-detection')\\\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\\\n\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\n\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\n\\\\nresults = model.predict(image)\\\\n\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.844}, 'description': \\\"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\\\" </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… Ù†Ø§Ø¯ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø±Ø¦ÙŠ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ Ø¨ØªØ¬Ù…ÙŠØ¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ± Ù„ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„Ù‡Ø§ ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„ÙŠÙ‡Ø§. Ù‚Ø¯Ù…ÙˆØ§ Ù„Ù‡Ù… ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬ÙŠØ© Ù„Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø«Ù„Ù‰.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. First, import the AutoModel class from the transformers library provided by Hugging Face.\\\\n2. We load the pre-trained model- 'sheldonxxxx/OFA_model_weights' using the from_pretrained method of the AutoModel class. This particular model is capable of performing visual question-answering tasks.\\\\n3. In order to use this model in your project, you will need to process the given image and question as input to the model.\\\\n4. Please note that this API does not directly support processing image data. To use this model with images, you will need to integrate it with the OFA-Sys/OFA library which can be found at https://github.com/OFA-Sys/OFA.\\\\n5. With the integration, you can use the model to answer questions about the content of the images provided as input.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", 'api_call': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sheldonxxxx/OFA_model_weights', 'api_call': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ø­ÙŠØ« Ù†Ø±ÙŠØ¯ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù„Ù… Ø¢Ù„ÙŠ Ù…ØªØ¹Ø¯Ø¯Ø©. ÙŠØªØ·Ù„Ø¨ Ø°Ù„Ùƒ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© ÙƒØ¨ÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØµÙ†ÙØ© Ù…Ø³Ø¨Ù‚Ù‹Ø§.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"AutoModel.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. Import the necessary libraries, which in this case are 'pipeline' and 'AutoModel' from transformers.\\\", 'code': \\\"from transformers import pipeline, AutoModel\\\\nmodel = AutoModel.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')\\\\nqa_pipeline = pipeline('question-answering', model=model)\\\\nresult = qa_pipeline({'context': 'The product XYZ has a 1-year warranty on parts and 90 days on labor.', 'question': 'How long is the warranty on parts?'})\\\\nprint(result)\\\", 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Question Answering', 'api_name': 'bert-large-cased-whole-word-masking-finetuned-squad', 'api_call': \\\"from transformers import pipeline, AutoModel\\\\nmodel = AutoModel.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')\\\\nqa_pipeline = pipeline('question-answering', model=model)\\\\nresult = qa_pipeline({'context': 'The product XYZ has a 1-year warranty on parts and 90 days on labor.', 'question': 'How long is the warranty on parts?'})\\\\nprint(result)\\\", 'api_arguments': {'model_name_or_path': 'bert-large-cased-whole-word-masking', 'dataset_name': 'squad', 'do_train': True, 'do_eval': True, 'learning_rate': 3e-05, 'num_train_epochs': 2, 'max_seq_length': 384, 'doc_stride': 128, 'output_dir': './examples/models/wwm_cased_finetuned_squad/', 'per_device_eval_batch_size': 3, 'per_device_train_batch_size': 3}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='bert-large-cased-whole-word-masking-finetuned-squad')\\\\nresult = qa_pipeline({'context': 'This is a context example.', 'question': 'What is this example for?'})\\\\nprint(result)\\\", 'performance': {'dataset': [{'name': 'BookCorpus', 'accuracy': 'N/A'}, {'name': 'English Wikipedia', 'accuracy': 'N/A'}]}, 'description': 'BERT large model (cased) whole word masking finetuned on SQuAD. This model is cased and trained with a new technique: Whole Word Masking. After pre-training, this model was fine-tuned on the SQuAD dataset.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙƒØªÙˆØ¨Ø© Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨ØµÙˆØª ÙˆØ§Ø¶Ø­ ÙˆØ¯Ù‚ÙŠÙ‚ Ù„Ø¯Ø¹Ù… Ù…Ø³ØªØ®Ø¯Ù…ÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù…ÙŠÙ„Ù†Ø§ Ø§Ù„Ø°ÙŠÙ† ÙŠØ¹Ø§Ù†ÙˆÙ† Ù…Ù† Ø¥Ø¹Ø§Ù‚Ø© Ø¨ØµØ±ÙŠØ©ØŒ Ø®Ø§ØµØ© Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© ÙÙŠ Ø¬Ø¯Ø§ÙˆÙ„. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„ØµÙˆØª ÙˆØ§Ø¶Ø­Ù‹Ø§ ÙˆØ³Ù‡Ù„ Ø§Ù„ÙÙ‡Ù… Ù„ÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© ÙÙŠ ÙÙ‡Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. The pipeline function is used to create a table-question-answering model, which is capable of answering questions based on the information provided in tabular data.\\\\n3. We specify the model 'google/tapas-large-finetuned-wtq' to be loaded. This model has been trained on a large corpus of text and can extract information from tables to answer questions.\\\\n4. The created table-question-answering pipeline can be used to query a table for specific information, given a natural language question, and will return the answer based on the table data.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\\nresult = qa_pipeline(table=table, query=query)\\\", 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/tapas-large-finetuned-wtq', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", 'api_arguments': {'model': 'google/tapas-large-finetuned-wtq', 'task': 'table-question-answering'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\nqa_pipeline = pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\\n\\\\nresult = qa_pipeline(table=table, query=query)\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.5097}, 'description': 'TAPAS large model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… Ù†Ø¸Ø§Ù…Ù†Ø§ Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø¥Ù„Ù‰ ØµÙŠØºØ© Ø£Ø®Ø±Ù‰ Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ©ØŒ ÙˆÙ†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨ÙŠÙ† Ù‡Ø°Ù‡ Ø§Ù„ØµÙŠØº Ù„Ø¶Ù…Ø§Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØª Ø§Ù„Ù…Ø­Ø³Ù†Ø©.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"BaseModel.from_pretrained('JorisCos/DCUNet_Libri1Mix_enhsingle_16k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import transformers and the necessary libraries.\\\\n2. Use the BaseModel.from_pretrained() function provided by the transformers library to create an audio enhancement model.\\\\n3. Specify the model 'JorisCos/DCUNet_Libri1Mix_enhsingle_16k' to be loaded. This model has been trained to enhance speech signals in noisy environments.\\\\n4. The created enhancer will process the input audio file and generate an output file with enhanced speech.\\\", '<<<code>>>': \\\"from transformers import BaseModel \\\\nfrom asteroid import AudioFileProcessor\\\\nmodel = BaseModel.from_pretrained('JorisCos/DCUNet_Libri1Mix_enhsingle_16k')\\\\nenhancer = AudioFileProcessor(model)\\\\ninput_audio_file = 'path/to/noisy_audio.wav'\\\\noutput_audio_file = 'path/to/enhanced_audio.wav'\\\\nenhancer_process = enhancer.process_file(input_audio_file, output_audio_file)\\\", 'api_call': \\\"BaseModel.from_pretrained('JorisCos/DCUNet_Libri1Mix_enhsingle_16k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DCUNet_Libri1Mix_enhsingle_16k', 'api_call': \\\"BaseModel.from_pretrained('JorisCos/DCUNet_Libri1Mix_enhsingle_16k')\\\", 'api_arguments': 'audio', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Libri1Mix', 'accuracy': {'si_sdr': 13.154035391645971, 'si_sdr_imp': 9.704254085786271, 'sdr': 13.568058873121435, 'sdr_imp': 10.065396073908367, 'sar': 13.568058873121435, 'sar_imp': 10.065396073908367, 'stoi': 0.9199373340235417, 'stoi_imp': 0.12401751048300132}}, 'description': 'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚Ù†Ø§ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©ØŒ ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø®ØªÙŠØ§Ø± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙˆÙØ± Ø¬ÙˆØ¯Ø© ØµÙˆØªÙŠØ© Ø¹Ø§Ù„ÙŠØ© ÙˆØ¯Ù‚Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ù„ØªØ¹Ø²ÙŠØ² ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\\\", '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. First, we import the necessary pipeline class from the transformers package.\\\\n2. Next, create a text-to-speech pipeline using the pretrained model 'espnet/kan-bayashi_ljspeech_vits'. This model has been trained on the ljspeech dataset and can be utilized for converting text input into synthesized speech.\\\\n3. Now, just pass any text input to this pipeline function to synthesize speech audio. You can then play the audio or incorporate it into your application for visually impaired users.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\\\\naudio = tts('Hello World')\\\", 'api_call': \\\"pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\\\", 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kan-bayashi_ljspeech_vits', 'api_call': \\\"pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')\\\", 'performance': {'dataset': 'ljspeech', 'accuracy': 'Not mentioned'}, 'description': 'A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ ÙˆØµÙØ§Ù‹ Ù†ØµÙŠØ§Ù‹ Ù‚ØµÙŠØ±Ø§Ù‹ ÙŠØµÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø°ÙŠ ÙŠÙØ¹Ø±Ø¶ Ø¨Ø´ÙƒÙ„ Ø´Ø§Ù…Ù„ ÙˆÙ…ÙØµÙ„.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': 'TabTransformer.from_config()', '<<<api_provider>>>': 'Keras', '<<<explanation>>>': \\\"1. First, use Keras' TabTransformer API to create a custom-configured TabTransformer model. It uses a self-attention-based Transformers structure followed by multiple feed-forward layers for structured data learning tasks.\\\\n2. This model accepts both numerical and categorical inputs, which can include mileage, make, model, year, and installed options.\\\\n3. Categorical features are encoded into embedding vectors and then fed through Transformer blocks. The output contextual embeddings are concatenated with numerical features and passed through an MLP block.\\\\n4. A SoftMax function is applied at the end of the model for final predictions.\\\\n5. Train this TabTransformer model on your historical automobile data and use the trained model to predict car prices based on the given features.\\\", '<<<code>>>': 'from keras_io.tab_transformer import TabTransformer, TabTransformerConfig\\\\nconfig = TabTransformerConfig()\\\\nmodel = TabTransformer.from_config(config)\\\\n# Train the model on historical automobile data and use the trained model to predict car prices based on given features', 'api_call': 'TabTransformer.from_config()', 'provider': 'Keras', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keras-io/tab_transformer', 'api_call': 'TabTransformer.from_config()', 'api_arguments': 'N/A', 'python_environment_requirements': ['Hugging Face'], 'example_code': 'N/A', 'performance': {'dataset': 'United States Census Income Dataset', 'accuracy': 'N/A'}, 'description': \\\"This model uses self-attention based Transformers structure followed by multiple feed forward layers to serve supervised and semi-supervised learning tasks on tabular data. The model's inputs can contain both numerical and categorical features. Categorical features are encoded into embedding vectors before being fed into a stack of Transformer blocks. The contextual embeddings of the categorical features after the final Transformer layer are concatenated with the input numerical features and fed into a final MLP block. A SoftMax function is applied at the end of the model.\\\" </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±Ø© Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. First, we\\\\'ll import the necessary python packages and the ControlNetModel to generate images with specific patterns or designs using a text description as input.\\\\n2. Then, we will use the pre-trained lllyasviel/control_v11p_sd15_normalbae model combined with the Stable Diffusion ControlNetPipeline for generating the images.\\\\n3. Provide the text prompt describing the specific pattern or design of the garment that we want to generate. For example, \\\"A blue dress with floral pattern.\\\"\\\\n4. Run the pipeline with the given prompt to generate the image, and save it for later use.', '<<<code>>>': 'from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\\\\nfrom controlnet_aux import NormalBaeDetector\\\\nimport torch\\\\nfrom PIL import Image\\\\n\\\\nprompt = \\\"A blue dress with floral pattern\\\"  # replace with desired pattern description\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/control_v11p_sd15_normalbae\\\\', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', controlnet=controlnet, torch_dtype=torch.float16)\\\\n\\\\ngenerator = torch.manual_seed(33)\\\\ncontrol_image = controlnet(control_image)\\\\ngenerated_image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\ngenerated_image.save(\\\\'generated_garment_pattern.png\\\\')', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/control_v11p_sd15_normalbae', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae')\\\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate', 'controlnet_aux']], 'example_code': \\\"import torch\\\\nimport os\\\\nfrom huggingface_hub import HfApi\\\\nfrom pathlib import Path\\\\nfrom diffusers.utils import load_image\\\\nfrom PIL import Image\\\\nimport numpy as np\\\\nfrom controlnet_aux import NormalBaeDetector\\\\nfrom diffusers import (\\\\n ControlNetModel,\\\\n StableDiffusionControlNetPipeline,\\\\n UniPCMultistepScheduler,\\\\n)\\\\ncheckpoint = lllyasviel/control_v11p_sd15_normalbae\\\\nimage = load_image(\\\\n https://huggingface.co/lllyasviel/control_v11p_sd15_normalbae/resolve/main/images/input.png\\\\n)\\\\nprompt = A head full of roses\\\\nprocessor = NormalBaeDetector.from_pretrained(lllyasviel/Annotators)\\\\ncontrol_image = processor(image)\\\\ncontrol_image.save(./images/control.png)\\\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(33)\\\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\nimage.save('images/image_out.png')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù†ØµØŒ Ø­ÙŠØ« Ù†Ø±ÙŠØ¯ Ø¥Ù†Ø´Ø§Ø¡ ØªØ­ÙˆÙ„ Ù†ØµÙŠ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ù…Ø·Ù„ÙˆØ¨.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import required libraries, such as \\\\'T5Tokenizer\\\\' and \\\\'T5ForConditionalGeneration\\\\' from transformers.\\\\n2. Load the tokenizer and the model \\\\'google/flan-t5-base\\\\', which is a state-of-the-art language model fine-tuned on multiple tasks including text generation.\\\\n3. Prepare a prompt like \\\"Generate creative titles for an article about The History of Artificial Intelligence\\\" and tokenize it to create input_ids using the loaded tokenizer.\\\\n4. Pass the input_ids to the model\\\\'s \\\\'generate()\\\\' function, which will generate a token sequence that can be decoded to produce creative article title suggestions.', '<<<code>>>': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'google/flan-t5-base\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'google/flan-t5-base\\\\')\\\\nprompt = \\\"Generate creative titles for an article about The History of Artificial Intelligence\\\"\\\\ninput_ids = tokenizer(prompt, return_tensors=\\\"pt\\\").input_ids\\\\noutputs = model.generate(input_ids)\\\\narticle_titles = tokenizer.decode(outputs[0])', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/flan-t5-base', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\\\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-base)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-base)\\\\ninput_text = translate English to German: How old are you?\\\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))', 'performance': {'dataset': [{'name': 'MMLU', 'accuracy': '75.2%'}]}, 'description': 'FLAN-T5 is a language model fine-tuned on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… API Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ ØµÙˆØªØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªÙˆÙÙŠØ± Ù…ØµØ¯Ø± Ø§Ù„ØµÙˆØª Ø§Ù„Ø£ØµÙ„ÙŠ ÙˆØ§Ù„Ù‡Ø¯ÙØŒ ÙˆØ¶Ø¨Ø· Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙˆØª Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ³Ø¬ÙŠÙ„ ÙˆÙ…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ØµÙˆØªÙŠÙ† Ø§Ù„Ù†Ø§ØªØ¬ÙŠÙ†. Ù†Ù†ØµØ­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†ØµØ§Ø¦Ø­ Ø§Ù„ØµÙˆØªÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': 'S2THubInterface()', '<<<api_provider>>>': 'Fairseq', '<<<explanation>>>': \\\"1. Import the required libraries and modules, such as fairseq, torchaudio, and huggingface_hub.\\\\n2. Download and load the 'facebook/xm_transformer_s2ut_hk-en' model using the load_model_ensemble_and_task_from_hf_hub function provided by fairseq.\\\\n3. Use the model to build a generator.\\\\n4. Load the input audio file using torchaudio.load().\\\\n5. Use S2THubInterface to convert the input audio into a format the model can understand.\\\\n6. Get the translated text using S2THubInterface.get_prediction().\\\\n7. Download and load the HiFiGAN vocoder model to convert text back to speech.\\\\n8. Use VocoderHubInterface to generate an English speech waveform.\\\\n9. Save the English speech waveform and play it using IPython.display.Audio.\\\", '<<<code>>>': '# For detailed code, refer to the example_code provided in the API. Replace the file path with the path of the audio you want to translate.', 'api_call': 'S2THubInterface()', 'provider': 'Fairseq', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'xm_transformer_s2ut_hk-en', 'api_call': 'S2THubInterface()', 'api_arguments': {'task': 'speech_to_text', 'model': 'facebook/xm_transformer_s2ut_hk-en', 'generator': 'task.build_generator([model], cfg)', 'sample': 'S2THubInterface.get_model_input(task, audio)'}, 'python_environment_requirements': [{'fairseq': 'latest', 'torchaudio': 'latest', 'huggingface_hub': 'latest'}], 'example_code': \\\"import json\\\\nimport os\\\\nfrom pathlib import Path\\\\nimport IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n facebook/xm_transformer_s2ut_hk-en,\\\\n arg_overrides={config_yaml: config.yaml, task: speech_to_text},\\\\n cache_dir=cache_dir,\\\\n)\\\\nmodel = models[0].cpu()\\\\ncfg[task].cpu = True\\\\ngenerator = task.build_generator([model], cfg)\\\\naudio, _ = torchaudio.load(/path/to/an/audio/file)\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\\\\nlibrary_name = fairseq\\\\ncache_dir = (\\\\n cache_dir or (Path.home() / .cache / library_name).as_posix()\\\\n)\\\\ncache_dir = snapshot_download(\\\\n ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name\\\\n)\\\\nx = hub_utils.from_pretrained(\\\\n cache_dir,\\\\n model.pt,\\\\n .,\\\\n archive_map=CodeHiFiGANVocoder.hub_models(),\\\\n config_yaml=config.json,\\\\n fp16=False,\\\\n is_vocoder=True,\\\\n)\\\\nwith open(f{x['args']['data']}/config.json) as f:\\\\n vocoder_cfg = json.load(f)\\\\nassert (\\\\n len(x[args][model_path]) == 1\\\\n), Too many vocoder models in the input\\\\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\nipd.Audio(wav, rate=sr)\\\", 'performance': {'dataset': 'TED, drama, TAT domain', 'accuracy': 'Not provided'}, 'description': 'Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq for Hokkien-English. Trained with supervised data in TED, drama, TAT domain, and weakly supervised data in drama domain.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ ØªØ·Ø¨ÙŠÙ‚Ù†Ø§ Ø§Ù„Ø°ÙŠ ÙŠÙ‡ØªÙ… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a translation model.\\\\n3. Specify the model 'Helsinki-NLP/opus-mt-es-en' to be loaded. This model is trained specifically for translating Spanish text to English text.\\\\n4. The created translation model can be used to translate the text from the report in Spanish to English, providing understandable text for English-speaking employees.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_es_to_en\\\\', model=\\\\'Helsinki-NLP/opus-mt-es-en\\\\')\\\\nspanish_report = \\\"Texto del informe en espaÃ±ol aquÃ­...\\\"\\\\nenglish_translation = translator(spanish_report)', 'api_call': \\\"pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-es-en', 'api_call': \\\"pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')('Hola, Â¿cÃ³mo estÃ¡s?')\\\", 'performance': {'dataset': [{'name': 'newssyscomb2009-spaeng.spa.eng', 'accuracy': {'BLEU': 30.6, 'chr-F': 0.57}}, {'name': 'news-test2008-spaeng.spa.eng', 'accuracy': {'BLEU': 27.9, 'chr-F': 0.553}}, {'name': 'newstest2009-spaeng.spa.eng', 'accuracy': {'BLEU': 30.4, 'chr-F': 0.572}}, {'name': 'newstest2010-spaeng.spa.eng', 'accuracy': {'BLEU': 36.1, 'chr-F': 0.614}}, {'name': 'newstest2011-spaeng.spa.eng', 'accuracy': {'BLEU': 34.2, 'chr-F': 0.599}}, {'name': 'newstest2012-spaeng.spa.eng', 'accuracy': {'BLEU': 37.9, 'chr-F': 0.624}}, {'name': 'newstest2013-spaeng.spa.eng', 'accuracy': {'BLEU': 35.3, 'chr-F': 0.609}}, {'name': 'Tatoeba-test.spa.eng', 'accuracy': {'BLEU': 59.6, 'chr-F': 0.739}}]}, 'description': 'Helsinki-NLP/opus-mt-es-en is a machine translation model trained to translate from Spanish to English using the Hugging Face Transformers library. The model is based on the Marian framework and was trained on the OPUS dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®ØµØ§Øª Ù†ØµÙˆØµ Ù…Ù† Ù†ÙˆØ¹ Ø¥Ù„Ù‰ Ù†ÙˆØ¹ (Text2Text Generation) ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙŠØ³ØªØ·ÙŠØ¹ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø´ÙƒÙ„ Ø¥Ù„Ù‰ Ø¢Ø®Ø± Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙØ¹Ø§Ù„.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"pipeline('summarization', model='google/pegasus-xsum')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the pipeline function from the transformers package provided by Hugging Face.\\\\n2. We create a summarization pipeline using the 'google/pegasus-xsum' model. Pegasus is a transformer-based model designed for abstractive summarization.\\\\n3. We input the news article text and let the model generate a summary.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'google/pegasus-xsum\\\\')\\\\nsummary = summarizer(\\\"President Biden signed three executive orders on Tuesday aimed at reversing stubborn wage gaps and promoting better pay equity in the federal workforce. The administration estimates that the third of the orders will increase the wages of nearly 23,000 employees by 2023. The latest moves build on previous efforts by the Biden administration to address systemic inequality and ensure workers earn a living wage. The first order would establish an equity and inclusion council for the federal workforce, while the second would set a target of raising the minimum wage for all federal contractors to $15 per hour.\\\")[\\\\'summary_text\\\\']', 'api_call': \\\"pipeline('summarization', model='google/pegasus-xsum')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/pegasus-xsum', 'api_call': \\\"pipeline('summarization', model='google/pegasus-xsum')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': [{'name': 'xsum', 'accuracy': {'ROUGE-1': 46.862, 'ROUGE-2': 24.453, 'ROUGE-L': 39.055, 'ROUGE-LSUM': 39.099}}, {'name': 'cnn_dailymail', 'accuracy': {'ROUGE-1': 22.206, 'ROUGE-2': 7.67, 'ROUGE-L': 15.405, 'ROUGE-LSUM': 19.218}}, {'name': 'samsum', 'accuracy': {'ROUGE-1': 21.81, 'ROUGE-2': 4.253, 'ROUGE-L': 17.447, 'ROUGE-LSUM': 18.891}}]}, 'description': 'PEGASUS is a pre-trained model for abstractive summarization, developed by Google. It is based on the Transformer architecture and trained on both C4 and HugeNews datasets. The model is designed to extract gap sentences and generate summaries by stochastically sampling important sentences.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙØ±ÙŠØŒ Ø­ÙŠØ« Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙØ¦Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø®ØµØ§Ø¦ØµÙ‡Ø§ Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-9')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to create a zero-shot classification model using the 'valhalla/distilbart-mnli-12-9' model.\\\\n3. We then input the article text and candidate topics (labels) into the model.\\\\n4. The model will analyze the text and provide a score for each topic, indicating the probability that the text belongs to that category.\\\\n5. Based on the topic probabilities, the most appropriate topic can be chosen for the article.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nzero_shot_classifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'valhalla/distilbart-mnli-12-9\\\\')\\\\narticle_text = \\\"Article text goes here...\\\"\\\\ncandidate_labels = [\\\\'politics\\\\', \\\\'technology\\\\', \\\\'sports\\\\', \\\\'entertainment\\\\', \\\\'business\\\\']\\\\nresult = zero_shot_classifier(article_text, candidate_labels)', 'api_call': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-9')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'valhalla/distilbart-mnli-12-9', 'api_call': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-9')\\\", 'api_arguments': 'text, candidate_labels', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'MNLI', 'accuracy': {'matched_acc': 89.56, 'mismatched_acc': 89.52}}, 'description': 'distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is used for zero-shot text classification tasks.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \\\"Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='bigscience/bloom-560m')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'pipeline' from the transformers library - this would be used to create a text-generation model.\\\\n2. Use the 'pipeline' function from transformers to load the model 'bigscience/bloom-560m', which is a state-of-the-art text generation model.\\\\n3. Provide the initial text, i.e., the beginning of the article, to the text-generation model, which will then generate the continuation of the article.\\\\n4. We can then print the generated text as the continuation for the article.\\\", '<<<code>>>': 'from transformers import pipeline\\\\n\\\\nmodel_name = \\\\'bigscience/bloom-560m\\\\'\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=model_name)\\\\n\\\\ninitial_text = \\\"Autonomous vehicles have begun to revolutionize the transportation industry, particularly in urban areas where traffic is congested and public transit is often unreliable.\\\"\\\\noutput = generator(initial_text)\\\\ncontinuation = output[0][\\\\'generated_text\\\\']\\\\nprint(continuation)', 'api_call': \\\"pipeline('text-generation', model='bigscience/bloom-560m')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bigscience/bloom-560m', 'api_call': \\\"pipeline('text-generation', model='bigscience/bloom-560m')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline\\\\n\\\\nmodel_name = 'bigscience/bloom-560m'\\\\napi = pipeline('text-generation', model=model_name)\\\\n\\\\ntext = 'The history of artificial intelligence began in the '\\\\noutput = api(text)\\\\nprint(output[0]['generated_text'])\\\", 'performance': {'dataset': 'Validation', 'accuracy': {'Training Loss': 2.0, 'Validation Loss': 2.2, 'Perplexity': 8.9}}, 'description': 'BLOOM LM is a large open-science, open-access multilingual language model developed by BigScience. It is a transformer-based language model trained on 45 natural languages and 12 programming languages. The model has 559,214,592 parameters, 24 layers, and 16 attention heads.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù†Ø¸Ù… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø¶Ù…Ù†Ù‡Ø§ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Video', '<<<api_call>>>': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the required libraries like torch and tuneavideo.\\\\n2. Import the TuneAVideoPipeline class, the UNet3DConditionModel class and the save_videos_grid function.\\\\n3. Load the pretrained \\\\'nitrosocke/mo-di-diffusion\\\\' model using TuneAVideoPipeline.from_pretrained() function.\\\\n4. Load the pretrained UNet3DConditionModel with the prompt \\\"a magical princess playing guitar, modern Disney style\\\".\\\\n5. Set the video_length, height, width, num_inference_steps, and guidance_scale as required.\\\\n6. Generate the video using the loaded pipeline based on the prompt provided.\\\\n7. Save the generated video as a .gif file with the given prompt as the filename.', '<<<code>>>': \\\"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\\\nfrom tuneavideo.util import save_videos_grid\\\\nimport torch\\\\nunet_model_path = 'Tune-A-Video-library/mo-di-bear-guitar'\\\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\\\npipe = TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=unet, torch_dtype=torch.float16).to('cuda')\\\\npipe.enable_xformers_memory_efficient_attention()\\\\nprompt = 'a magical princess is playing guitar, modern disney style'\\\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\\\nsave_videos_grid(video, f'./{prompt}.gif')\\\", 'api_call': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mo-di-bear-guitar', 'api_call': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': [['torch', 'tuneavideo']], 'example_code': \\\"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\\\nfrom tuneavideo.util import save_videos_grid\\\\nimport torch\\\\npretrained_model_path = nitrosocke/mo-di-diffusion\\\\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\\\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\nprompt = a magical princess is playing guitar, modern disney style\\\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\\\nsave_videos_grid(video, f./{prompt}.gif)\\\", 'performance': {'dataset': 'Not mentioned', 'accuracy': 'Not mentioned'}, 'description': 'Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø¥ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯ÙˆÙ† Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­ÙˆØ§Ø³ÙŠØ¨.? [/INST]{'domain': 'Computer Vision Depth Estimation', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We import AutoModel from transformers and torchvision.transforms for handling image transformations.', 'code': \\\"import torch\\\\nfrom transformers import AutoModel\\\\nfrom torchvision.transforms import ToTensor, Resize, Compose\\\\nfrom PIL import Image\\\\n\\\\nmodel_checkpoint = 'sayakpaul/glpn-nyu-finetuned-diode-221215-093747'\\\\nmodel = AutoModel.from_pretrained(model_checkpoint)\\\\nif torch.cuda.is_available():\\\\n    model.cuda()\\\\n\\\\ninput_image = Image.open('<image_path_here>').convert('RGB')\\\\ntransforms = Compose([Resize((224, 224)), ToTensor()])\\\\ntransformed_image = transforms(input_image).unsqueeze(0).to(model.device)\\\\n\\\\nwith torch.no_grad():\\\\n    output = model(transformed_image)\\\\ndepth_map = output.cpu().numpy()\\\", 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-093747', 'api_call': \\\"import torch\\\\nfrom transformers import AutoModel\\\\nfrom torchvision.transforms import ToTensor, Resize, Compose\\\\nfrom PIL import Image\\\\n\\\\nmodel_checkpoint = 'sayakpaul/glpn-nyu-finetuned-diode-221215-093747'\\\\nmodel = AutoModel.from_pretrained(model_checkpoint)\\\\nif torch.cuda.is_available():\\\\n    model.cuda()\\\\n\\\\ninput_image = Image.open('<image_path_here>').convert('RGB')\\\\ntransforms = Compose([Resize((224, 224)), ToTensor()])\\\\ntransformed_image = transforms(input_image).unsqueeze(0).to(model.device)\\\\n\\\\nwith torch.no_grad():\\\\n    output = model(transformed_image)\\\\ndepth_map = output.cpu().numpy()\\\", 'api_arguments': [], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.'}} </s>\"\n"}
{"text": "<s>[INST]Ø±Ø¬Ø§Ø¡Ù‹ Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø¦ÙŠØ© Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù‚ Ø¨Ø¯Ù‚Ø© ÙˆÙØ¹Ø§Ù„ÙŠØ© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries AutoModel from transformers and torch.\\\\n2. Load the pre-trained depth estimation model using the from_pretrained method from the AutoModel class. The model name is 'sayakpaul/glpn-nyu-finetuned-diode-221215-093747'.\\\\n3. The depth estimation model can then be used to analyze input images and produce depth maps, which represent the environment's depth and allow for effective navigation by drones.\\\\n4. In order to use the model, the input images need to be preprocessed and converted into tensors by following the specific model requirements.\\\\n5. Once processed, the depth map can be obtained and used for further analysis and navigation.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nimport torch\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\\\\n# Preprocess input image and convert it into a tensor\\\\n# input_tensor = preprocess_image(input_image)\\\\n# Obtain depth map from the model\\\\n# depth_map = model(input_tensor)\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221215-093747', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ù„Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load(hf_hub_download('srg/outhimar_64-Close-regression', 'sklearn_model.joblib'))\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': \\\"1. Import the necessary libraries, including joblib and hf_hub_download method.\\\\n2. Download the pre-trained model 'srg/outhimar_64-Close-regression' using hf_hub_download method.\\\\n3. Load the downloaded model using joblib.load().\\\\n4. The loaded model is a regression-based model trained to predict stock closing prices based on historical data.\\\\n5. Pass the feature dataset X (i.e., X_train) to the model to obtain predictions of stock closing prices for the dataset.\\\", '<<<code>>>': \\\"import joblib\\\\nfrom huggingface_hub import hf_hub_download\\\\n\\\\nmodel_path = hf_hub_download('srg/outhimar_64-Close-regression', 'sklearn_model.joblib')\\\\nmodel = joblib.load(model_path)\\\\n\\\\npredicted_closing_prices = model.predict(X_train)\\\", 'api_call': \\\"joblib.load(hf_hub_download('srg/outhimar_64-Close-regression', 'sklearn_model.joblib'))\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'srg/outhimar_64-Close-regression', 'api_call': \\\"joblib.load(hf_hub_download('srg/outhimar_64-Close-regression', 'sklearn_model.joblib'))\\\", 'api_arguments': ['X_train', 'y_train'], 'python_environment_requirements': [['scikit-learn', 'dabl']], 'example_code': \\\"Pipeline(steps=[('easypreprocessor',EasyPreprocessor(types= continuous dirty_float low_card_int ... date free_string useless\\\\nDate False False False ... True False False\\\\nOpen True False False ... False False False\\\\nHigh True False False ... False False False\\\\nLow True False False ... False False False\\\\nAdj Close True False False ... False False False\\\\nVolume True False False ... False False False[6 rows x 7 columns])),('ridge', Ridge(alpha=10))])\\\", 'performance': {'dataset': 'outhimar_64', 'accuracy': {'r2': 0.999858, 'neg_mean_squared_error': -1.067685}}, 'description': 'Baseline Model trained on outhimar_64 to apply regression on Close. Disclaimer: This model is trained with dabl library as a baseline, for better results, use AutoTrain. Logs of training including the models tried in the process can be found in logs.txt.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ø®Ø¨ÙŠØ± ÙÙŠ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠÙ‘Ø© Ù„Ø¥Ù†ØªØ§Ø¬ Ù†ØµÙˆØµ. Ù‡Ù„ ØªØ³ØªØ·ÙŠØ¹ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙˆØ¬ÙŠÙ‡ÙŠ Ù†Ø­Ùˆ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠÙ‘Ø© Ù„Ø°Ù„ÙƒØŸ? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"pipeline('text-generation', model='Pi3141/DialoGPT-medium-elon-3')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We import the pipeline function from the transformers library provided by Hugging Face.', 'code': 'from transformers import pipeline\\\\ntext_generator = pipeline(\\\\'text-generation\\\\', model=\\\\'Pi3141/DialoGPT-medium-elon-3\\\\')\\\\ninput_text = \\\"What do you think about Mars colonization?\\\"\\\\nresponse_output = text_generator(input_text)\\\\nprint(response_output[0][\\\\'generated_text\\\\'])', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Conversational', 'api_name': 'Pi3141/DialoGPT-medium-elon-3', 'api_call': 'from transformers import pipeline\\\\ntext_generator = pipeline(\\\\'text-generation\\\\', model=\\\\'Pi3141/DialoGPT-medium-elon-3\\\\')\\\\ninput_text = \\\"What do you think about Mars colonization?\\\"\\\\nresponse_output = text_generator(input_text)\\\\nprint(response_output[0][\\\\'generated_text\\\\'])', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.', 'performance': {'dataset': 'Twitter tweets by Elon Musk', 'accuracy': 'N/A'}, 'description': 'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ø¸Ù‡Ø§Ø± Ù…Ù‡Ø§Ø±Ø§ØªÙƒ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ Ø¬Ø¯ÙŠØ¯. ÙŠØ¬Ø¨ ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ø¯Ù‚Ø© ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯ÙˆØ§Øª ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ Ø°Ø§Øª Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ©. Ø¹Ù†Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ØŒ Ù‚Ø¯Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨ÙÙŠØ¯ÙŠÙˆ Ù‚Ø¯ ÙŠØªÙ… ØªÙ‚ÙŠÙŠÙ…Ù‡.? [/INST]{'domain': 'Multimodal Text-to-Video', 'api_call': \\\"pipeline('text-to-video', model='chavinlo/TempoFunk')\\\", 'api_provider': 'Hugging Face', 'explanation': '1. First, we need to import the Hugging Face Transformers library, specifically the pipeline function.', 'code': 'from transformers import pipeline\\\\ntext_to_video = pipeline(\\\\'text-to-video\\\\', model=\\\\'chavinlo/TempoFunk\\\\')\\\\ninput_text = \\\"The daily life of a software engineer starts with checking emails and attending meetings, followed by writing code, debugging, and collaborating with colleagues. The day ends with updating documentation and reporting progress.\\\"\\\\ngenerated_video = text_to_video(input_text)', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'chavinlo/TempoFunk', 'api_call': 'from transformers import pipeline\\\\ntext_to_video = pipeline(\\\\'text-to-video\\\\', model=\\\\'chavinlo/TempoFunk\\\\')\\\\ninput_text = \\\"The daily life of a software engineer starts with checking emails and attending meetings, followed by writing code, debugging, and collaborating with colleagues. The day ends with updating documentation and reporting progress.\\\"\\\\ngenerated_video = text_to_video(input_text)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­Ù„ÙŠÙ„ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„ Ø¨ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We need to use the SentenceTransformer class from the sentence_transformers package.\\\\n2. We load the pre-trained model 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2' to obtain a sentence embedding model capable of mapping sentences to a 768-dimensional dense vector space.\\\\n3. We provide a list of sentences as input to the model.\\\\n4. The model.encode() method takes the sentences as input and produces embeddings or dense vector representations that can be used to measure semantic similarity.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nsentences = [\\\"Shakespeare was a poet and playwright.\\\", \\\"He wrote plays and poems like Romeo and Juliet, and Hamlet.\\\", \\\"The renowned writer, Shakespeare, is well-known for his literary works, such as Hamlet and Romeo and Juliet.\\\"]\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/paraphrase-multilingual-mpnet-base-v2\\\\')\\\\nembeddings = model.encode(sentences)', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]Ù…Ù† Ø®Ù„Ø§Ù„ Ù‡Ø°Ø§ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©ØŒ ÙŠÙ…ÙƒÙ† Ù„Ù„Ù†Ø¸Ø§Ù… ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†ØµÙˆØµ ÙˆØµÙÙŠØ© ØªÙˆØ¶Ø­ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø¨Ø§Ù„ØªÙØµÙŠÙ„ØŒ Ù…Ù…Ø§ ÙŠØ³Ù‡Ù„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ÙˆØ§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„ØµÙˆØ±. Ø³ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ø­Ù„ÙˆÙ„ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø­ÙˆØ§Ø³ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙˆØ«ÙˆÙ‚Ø© Ù„Ø¶Ù…Ø§Ù† ÙÙ‡Ù… Ø£ÙØ¶Ù„ Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø±Ø¦ÙŠ. Ù‡Ø°Ø§ ÙŠØ¹ØªØ¨Ø± Ø¬Ø²Ø¡Ù‹Ø§ Ù…Ù† Ù…Ø¬Ø§Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø±Ø¦ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·.? [/INST]{'domain': 'Multimodal Visual Question Answering', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('uclanlp/visualbert-vqa')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import AutoModelForQuestionAnswering, AutoTokenizer\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained('uclanlp/visualbert-vqa')\\\\ntokenizer = AutoTokenizer.from_pretrained('uclanlp/visualbert-vqa')\\\\n# For image feature extraction, we recommend using a method such as faster_rcnn\\\\n# or detectron2.\\\", 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'uclanlp/visualbert-vqa', 'api_call': \\\"from transformers import AutoModelForQuestionAnswering, AutoTokenizer\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained('uclanlp/visualbert-vqa')\\\\ntokenizer = AutoTokenizer.from_pretrained('uclanlp/visualbert-vqa')\\\\n# For image feature extraction, we recommend using a method such as faster_rcnn\\\\n# or detectron2.\\\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A VisualBERT model for Visual Question Answering.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ù‚Ø±Ø§Ø¡Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary modules from the transformers and PIL libraries.\\\\n2. We load the pre-trained model 'microsoft/trocr-base-printed' using the VisionEncoderDecoderModel class from Hugging Face transformers.\\\\n3. This model has been trained specifically for Optical Character Recognition (OCR) and is fine-tuned on the SROIE dataset.\\\\n4. For each image, we use the processor (TrOCRProcessor) to process the images for the model and obtain pixel values.\\\\n5. We then feed these pixel values to the model, which generates IDs corresponding to the extracted text.\\\\n6. Finally, we decode the IDs back into text using the processor, which we can then store in our database.\\\", '<<<code>>>': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nimage = Image.open('scanned_document.jpg')\\\\n# replace 'scanned_document.jpg' with the path to your image\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØªÙ… ØªÙ†Ø¸ÙŠÙ… Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙŠ Ø¥Ù„Ù‰ ØµÙˆØ± ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. ÙŠØ±Ø¬Ù‰ ØªØ²ÙˆÙŠØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We need to import the Text2ImagePipeline from the transformers library provided by Hugging Face.\\\\n2. Next, instantiate the Text2ImagePipeline with the model 'Linaqruf/anything-v3.0'. This model generates images from textual descriptions.\\\\n3. Input the textual description into the Text2ImagePipeline's __call__() method to generate the image.\\\\n4. The generated image can be used by Melanie as a reference for her illustrations.\\\\n(Note: GPT models cannot create images and Hugging Face doesn't have an official text-to-image API. But to provide an idea of what it would look like, we have created a fictional example here.)\\\", '<<<code>>>': 'from transformers import Text2ImagePipeline\\\\nimage_generator = Text2ImagePipeline(model=\\\\'Linaqruf/anything-v3.0\\\\')\\\\ngenerated_image = image_generator(\\\"A cat sitting on a windowsill\\\")', 'api_call': \\\"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \\\"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>\"\n"}
{"text": "<s>[INST]Ù„ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù…Ù„Ø®ØµØ§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙŠØ±Ø¬Ù‰ ØªØµÙ…ÙŠÙ… Ø¢Ù„ÙŠØ© Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙÙ‚Ø±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ. ÙƒÙ…Ø§ ÙŠÙÙ†ØµØ­ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ØªØ¹Ù„ÙŠÙ… Ø¢Ù„ÙŠ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®ØµØ§Øª Ù…ÙÙŠØ¯Ø© ÙˆÙ…Ø®ØªØµØ±Ø©. ÙŠÙ…ÙƒÙ† ØªÙˆÙÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø°Ø§Øª Ø¬ÙˆØ¯Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø². ÙˆÙÙŠ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ÙŠ Ù„ÙØ±ÙŠÙ‚ÙƒØŒ Ù‚Ø¯Ù… Ù„Ù‡Ù… Ø£Ø¯ÙˆØ§Øª Ù‚ÙˆÙŠØ© ÙˆÙØ¹Ø§Ù„Ø© Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªØ¬Ø§Ø±Ø¨ Ù…Ø¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ù„Ø®ØµØ§Øª Ø§Ù„Ø¢Ù„ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"pipeline('summarization', model='facebook/bart-large-cnn')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the pipeline function from the Transformers library provided by Hugging Face.', 'code': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'facebook/bart-large-cnn\\\\')\\\\narticle = \\\"Insert your long article text here\\\"\\\\nsummary = summarizer(article, max_length=130, min_length=30, do_sample=False)\\\\nprint(summary[0][\\\\'summary_text\\\\'])', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Summarization', 'api_name': 'facebook/bart-large-cnn', 'api_call': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'facebook/bart-large-cnn\\\\')\\\\narticle = \\\"Insert your long article text here\\\"\\\\nsummary = summarizer(article, max_length=130, min_length=30, do_sample=False)\\\\nprint(summary[0][\\\\'summary_text\\\\'])', 'api_arguments': ['ARTICLE', 'max_length', 'min_length', 'do_sample'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=facebook/bart-large-cnn)\\\\nARTICLE = ...\\\\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))', 'performance': {'dataset': 'cnn_dailymail', 'accuracy': {'ROUGE-1': 42.949, 'ROUGE-2': 20.815, 'ROUGE-L': 30.619, 'ROUGE-LSUM': 40.038}}, 'description': 'BART (large-sized model), fine-tuned on CNN Daily Mail. BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ†Ùƒ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø¹Ù† Ø¬Ø¯Ø§ÙˆÙ„ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„. Ù‡Ù„ ØªØ±ØºØ¨ ÙÙŠ ØªØ¹Ù„ÙŠÙ… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŸ? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('microsoft/tapex-large-finetuned-wikisql')\\\", 'api_provider': 'PyTorch Transformers', 'explanation': '', 'code': \\\"from transformers import TapexTokenizer, BartForConditionalGeneration\\\\nimport pandas as pd\\\\n\\\\ntokenizer = TapexTokenizer.from_pretrained('microsoft/tapex-large-finetuned-wikisql')\\\\nmodel = BartForConditionalGeneration.from_pretrained('microsoft/tapex-large-finetuned-wikisql')\\\\ndata = {\\\\n    'year': [1896, 1900, 1904, 2004, 2008, 2012],\\\\n    'city': ['Athens', 'Paris', 'St. Louis', 'Athens', 'Beijing', 'London']\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = 'In which year did Beijing host the Olympic Games?'\\\\nencoding = tokenizer(table=table, query=query, return_tensors='pt')\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\\\", 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'PyTorch Transformers', 'functionality': 'Table Question Answering', 'api_name': 'microsoft/tapex-large-finetuned-wikisql', 'api_call': \\\"from transformers import TapexTokenizer, BartForConditionalGeneration\\\\nimport pandas as pd\\\\n\\\\ntokenizer = TapexTokenizer.from_pretrained('microsoft/tapex-large-finetuned-wikisql')\\\\nmodel = BartForConditionalGeneration.from_pretrained('microsoft/tapex-large-finetuned-wikisql')\\\\ndata = {\\\\n    'year': [1896, 1900, 1904, 2004, 2008, 2012],\\\\n    'city': ['Athens', 'Paris', 'St. Louis', 'Athens', 'Beijing', 'London']\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = 'In which year did Beijing host the Olympic Games?'\\\\nencoding = tokenizer(table=table, query=query, return_tensors='pt')\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\\\", 'api_arguments': {'table': 'pd.DataFrame.from_dict(data)', 'query': 'query'}, 'python_environment_requirements': {'transformers': 'TapexTokenizer, BartForConditionalGeneration', 'pandas': 'pd'}, 'example_code': 'from transformers import TapexTokenizer, BartForConditionalGeneration\\\\nimport pandas as pd\\\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-large-finetuned-wikisql)\\\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-large-finetuned-wikisql)\\\\ndata = {\\\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\\\n city: [athens, paris, st. louis, athens, beijing, london]\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = In which year did beijing host the Olympic Games?\\\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))', 'performance': {'dataset': 'wikisql', 'accuracy': 'N/A'}, 'description': 'TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. This model is the tapex-base model fine-tuned on the WikiSQL dataset.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†Ù…Ø§Ø°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ø¨Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import 'GenerativeImage2TextModel' from the transformers library provided by Hugging Face.\\\", 'code': 'from transformers import GenerativeImage2TextModel\\\\nmodel = GenerativeImage2TextModel.from_pretrained(\\\\'microsoft/git-large-coco\\\\')\\\\n\\\\n# Function to convert image file to PIL Image or Tensor\\\\ndef image_preprocessing(image_path):\\\\n    # Implement image preprocessing steps here.\\\\n    pass\\\\n\\\\nimage_input = image_preprocessing(image_file_path)\\\\ncontext_description = \\\"A short context for the desired caption\\\"\\\\ngenerated_caption = model.generate_image_caption(image_input, context_description)', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': 'from transformers import GenerativeImage2TextModel\\\\nmodel = GenerativeImage2TextModel.from_pretrained(\\\\'microsoft/git-large-coco\\\\')\\\\n\\\\n# Function to convert image file to PIL Image or Tensor\\\\ndef image_preprocessing(image_path):\\\\n    # Implement image preprocessing steps here.\\\\n    pass\\\\n\\\\nimage_input = image_preprocessing(image_file_path)\\\\ncontext_description = \\\"A short context for the desired caption\\\"\\\\ngenerated_caption = model.generate_image_caption(image_input, context_description)', 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†Øµ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¨Ø±ÙŠØ© Ù„Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ØŸ? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"pipeline('text-generation', model='microsoft/git-base-coco')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\ncaption_generator = pipeline('text-generation', model='microsoft/git-base-coco')\\\\n\\\\n# For generating captions on images\\\\nresult = caption_generator(image_path_or_url)\\\\n\\\\n# For videos, extract image frames and process them using the model\\\", 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \\\"from transformers import pipeline\\\\ncaption_generator = pipeline('text-generation', model='microsoft/git-base-coco')\\\\n\\\\n# For generating captions on images\\\\nresult = caption_generator(image_path_or_url)\\\\n\\\\n# For videos, extract image frames and process them using the model\\\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªØµÙ†ÙŠÙ Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨ØªÙ‚Ù†ÙŠØ© Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªÙˆÙ‚Ø¹ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary modules from the required libraries, like transformers, numpy, and torch for video classification tasks.\\\\n2. Use the from_pretrained method of the TimesformerForVideoClassification class to load the pre-trained model 'facebook/timesformer-hr-finetuned-k600'. This model has been trained on the Kinetics-600 dataset for video classification tasks.\\\\n3. Load the 16-frame video snippet. The video frames need to be given as numpy arrays with shape (number_of_frames, channel, height, width).\\\\n4. Use the model to analyze the video snippet and identify the action being performed.\\\", '<<<code>>>': 'from transformers import TimesformerForVideoClassification, AutoImageProcessor\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(16, 3, 448, 448))  # Replace this with your video frames\\\\nprocessor = AutoImageProcessor.from_pretrained(\\\\'facebook/timesformer-hr-finetuned-k600\\\\')\\\\nmodel = TimesformerForVideoClassification.from_pretrained(\\\\'facebook/timesformer-hr-finetuned-k600\\\\')\\\\n\\\\ninputs = processor(images=video, return_tensors=\\\\'pt\\\\')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/timesformer-hr-finetuned-k600', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')\\\", 'api_arguments': {'images': 'video', 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'numpy', 'torch']], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 448, 448))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-hr-finetuned-k600)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-hr-finetuned-k600)\\\\ninputs = processor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-600', 'accuracy': 'Not provided'}, 'description': 'TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 600 possible Kinetics-600 labels.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ ØªÙ‚ÙˆÙ… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙˆÙƒÙŠÙ„ Ù„ØªØ­Ø³ÙŠÙ† Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª ÙÙŠ Ø¨Ø­Ø« Ø§Ù„Ø²ÙŠØ§Ø¯Ø© ÙÙŠ Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©ØŒ Ù…Ù† Ø®Ù„Ø§Ù„ ØªÙØ§Ø¹Ù„Ù‡ Ù…Ø¹ Ø¨ÙŠØ¦ØªÙ‡.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"load_from_hub(repo_id='sb3/dqn-CartPole-v1',filename='{MODEL FILENAME}.zip',)\\\", '<<<api_provider>>>': 'Stable-Baselines3', '<<<explanation>>>': \\\"1. Import the necessary libraries, such as 'stable_baselines3' and 'rl_zoo3'.\\\\n2. Utilize the 'load_from_hub' function to obtain the pre-trained DQN (Deep Q-Network) model, designed for the CartPole-v1 environment.\\\\n3. This model has been trained to balance a pole on a cart as it moves, which can serve as a proxy for assessing a self-driving car's performance in a simulated environment.\\\\n4. Deploy the model in your specific environment and evaluate its performance using various metrics or observations.\\\", '<<<code>>>': 'from rl_zoo3 import load_from_hub\\\\nfrom stable_baselines3 import DQN, make_vec_env\\\\n\\\\nenv = make_vec_env(\\\"CartPole-v1\\\")\\\\nmodel = load_from_hub(repo_id=\\\\'sb3/dqn-CartPole-v1\\\\', filename=\\\\'{MODEL FILENAME}.zip\\\\',)\\\\nmodel.set_env(env)\\\\n\\\\nobs = env.reset()\\\\nfor _ in range(1000):\\\\n    action, _states = model.predict(obs, deterministic=True)\\\\n    obs, rewards, dones, info = env.step(action)\\\\n    env.render()', 'api_call': \\\"load_from_hub(repo_id='sb3/dqn-CartPole-v1',filename='{MODEL FILENAME}.zip',)\\\", 'provider': 'Stable-Baselines3', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dqn-CartPole-v1', 'api_call': \\\"load_from_hub(repo_id='sb3/dqn-CartPole-v1',filename='{MODEL FILENAME}.zip',)\\\", 'api_arguments': ['algo', 'env', 'logs'], 'python_environment_requirements': [['rl_zoo3', 'stable-baselines3', 'stable-baselines3-contrib']], 'example_code': 'python train.py --algo dqn --env CartPole-v1 -f logs/', 'performance': {'dataset': 'CartPole-v1', 'accuracy': '500.00 +/- 0.00'}, 'description': 'This is a trained model of a DQN agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨ÙØ¦Ø© Ø§Ù„Ù†Øµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.? [/INST]{'domain': 'Natural Language Processing Text Classification', 'api_call': \\\"pipeline('sentiment-analysis', model='siebert/sentiment-roberta-large-english')\\\", 'api_provider': 'Transformers', 'explanation': '1. First, import the pipeline function from the transformers library provided by Hugging Face.', 'code': 'from transformers import pipeline\\\\nsentiment_analyzer = pipeline(\\\\'sentiment-analysis\\\\', model=\\\\'siebert/sentiment-roberta-large-english\\\\')\\\\n\\\\n# Analyzing the sentiment of a user post\\\\npost = \\\"I had a great time at the party last night!\\\"\\\\nresult = sentiment_analyzer(post)', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Transformers', 'functionality': 'Sentiment Analysis', 'api_name': 'siebert/sentiment-roberta-large-english', 'api_call': 'from transformers import pipeline\\\\nsentiment_analyzer = pipeline(\\\\'sentiment-analysis\\\\', model=\\\\'siebert/sentiment-roberta-large-english\\\\')\\\\n\\\\n# Analyzing the sentiment of a user post\\\\npost = \\\"I had a great time at the party last night!\\\"\\\\nresult = sentiment_analyzer(post)', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import pipeline\\\\nsentiment_analysis = pipeline(sentiment-analysis, model=siebert/sentiment-roberta-large-english)\\\\nprint(sentiment_analysis(I love this!))', 'performance': {'dataset': [{'name': 'McAuley and Leskovec (2013) (Reviews)', 'accuracy': 98.0}, {'name': 'McAuley and Leskovec (2013) (Review Titles)', 'accuracy': 87.0}, {'name': 'Yelp Academic Dataset', 'accuracy': 96.5}, {'name': 'Maas et al. (2011)', 'accuracy': 96.0}, {'name': 'Kaggle', 'accuracy': 96.0}, {'name': 'Pang and Lee (2005)', 'accuracy': 91.0}, {'name': 'Nakov et al. (2013)', 'accuracy': 88.5}, {'name': 'Shamma (2009)', 'accuracy': 87.0}, {'name': 'Blitzer et al. (2007) (Books)', 'accuracy': 92.5}, {'name': 'Blitzer et al. (2007) (DVDs)', 'accuracy': 92.5}, {'name': 'Blitzer et al. (2007) (Electronics)', 'accuracy': 95.0}, {'name': 'Blitzer et al. (2007) (Kitchen devices)', 'accuracy': 98.5}, {'name': 'Pang et al. (2002)', 'accuracy': 95.5}, {'name': 'Speriosu et al. (2011)', 'accuracy': 85.5}, {'name': 'Hartmann et al. (2019)', 'accuracy': 98.0}], 'average_accuracy': 93.2}, 'description': \\\"This model ('SiEBERT', prefix for 'Sentiment in English') is a fine-tuned checkpoint of RoBERTa-large (Liu et al. 2019). It enables reliable binary sentiment analysis for various types of English-language text. For each instance, it predicts either positive (1) or negative (0) sentiment. The model was fine-tuned and evaluated on 15 data sets from diverse text sources to enhance generalization across different types of texts (reviews, tweets, etc.). Consequently, it outperforms models trained on only one type of text (e.g., movie reviews from the popular SST-2 benchmark) when used on new data as shown below.\\\"}} </s>\"\n"}
{"text": "<s>[INST]Ø­Ù…Ù‘Ù„ Ù…Ù„ÙÙŠ ØµÙˆØª Ù…Ø®ØªÙ„ÙÙŠÙ† Ù„Ø£Ø´Ø®Ø§Øµ ÙŠØªØ­Ø¯Ø«ÙˆÙ† Ø¨Ù„ØºØ© Ù…Ø®ØªÙ„ÙØ©. Ø§Ø³ØªØ®Ø¯Ù… ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„ØµÙˆØª Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø§Ù„Ø£ÙˆÙ„ Ø¥Ù„Ù‰ Ù†Øµ? [/INST]{'domain': 'Audio Voice Activity Detection', 'api_call': \\\"Model.from_pretrained('pyannote/brouhaha', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from pyannote.audio import Model, Inference\\\\nmodel = Model.from_pretrained('pyannote/brouhaha', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\\ninference = Inference(model)\\\\noutput = inference('audio.wav')\\\\n# replace 'audio.wav' with path to your audio file\\\", 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Voice Activity Detection, Speech-to-Noise Ratio, and C50 Room Acoustics Estimation', 'api_name': 'pyannote/brouhaha', 'api_call': \\\"from pyannote.audio import Model, Inference\\\\nmodel = Model.from_pretrained('pyannote/brouhaha', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\\ninference = Inference(model)\\\\noutput = inference('audio.wav')\\\\n# replace 'audio.wav' with path to your audio file\\\", 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote-audio', 'brouhaha-vad'], 'example_code': ['from pyannote.audio import Model', 'model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)', 'from pyannote.audio import Inference', 'inference = Inference(model)', 'output = inference(audio.wav)', 'for frame, (vad, snr, c50) in output:', '  t = frame.middle', '  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})'], 'performance': {'dataset': 'LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene', 'accuracy': 'Not provided'}, 'description': 'Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± ÙÙŠ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To achieve this, we first import the necessary packages, such as the pipeline function from the transformers library provided by Hugging Face and the torch library for handling input image tensors.\\\\n2. We use the pipeline function to create an image classification model by specifying the model 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k'. This model is trained on ImageNet-1K and further fine-tuned on ImageNet-12K for image classification tasks, including dog breed classification.\\\\n3. By using this classifier, we can input an image of a dog, and it will be able to predict the breed of the dog with high accuracy.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimport torch\\\\ndog_classifier = pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\\ndog_breed = dog_classifier(dog_image_tensor)\\\", 'api_call': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'api_call': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", 'api_arguments': {'model': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'framework': 'pt'}, 'python_environment_requirements': [['transformers', 'torch']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªÙˆÙ„ÙŠÙ Ø§Ù„Ù†ØµÙŠ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ø°ÙŠ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù†Øµ Ø¢Ø®Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ø¨Ø§Ø±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package, which includes RobertaTokenizer to tokenize the given code snippet and T5ForConditionalGeneration to load the CodeT5 model.\\\\n2. Use the from_pretrained method to load the 'Salesforce/codet5-base' model, which has been pre-trained for code understanding and generation tasks.\\\\n3. Use the tokenizer to convert the provided Python code snippet into a format that can be fed into the model.\\\\n4. Use the generate method to translate the input code snippet into a human-readable description.\\\\n5. Decode the generated output to obtain a plain-text explanation.\\\", '<<<code>>>': 'from transformers import RobertaTokenizer, T5ForConditionalGeneration\\\\ntokenizer = RobertaTokenizer.from_pretrained(\\\\'Salesforce/codet5-base\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'Salesforce/codet5-base\\\\')\\\\ntext = \\\"def greet(user): print(f\\\\'hello {user}!\\\\')\\\"\\\\ninput_ids = tokenizer(text, return_tensors=\\\\'pt\\\\').input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=20)\\\\nhuman_readable_description = tokenizer.decode(generated_ids[0], skip_special_tokens=True)', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Salesforce/codet5-base', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\\\", 'api_arguments': ['text', 'return_tensors', 'input_ids', 'max_length'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import RobertaTokenizer, T5ForConditionalGeneration\\\\ntokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\\\\nmodel = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\\\\ntext = def greet(user): print(f'hello <extra_id_0>!')\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=8)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\\\", 'performance': {'dataset': 'code_search_net', 'accuracy': 'Refer to the paper for evaluation results on several downstream benchmarks'}, 'description': 'CodeT5 is a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. It supports both code understanding and generation tasks and allows for multi-task learning. The model can be used for tasks such as code summarization, code generation, code translation, code refinement, code defect detection, and code clone detection.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†Ù…ÙˆØ°Ø¬ ÙŠØ³ØªØ·ÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ Ù„Ù‚Ø¯ ÙŠØªÙ… ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ù…Ù‚Ø§Ø¨Ù„ Ù†Øµ.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('t5-3b')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary packages: T5ForConditionalGeneration from transformers, and the tokenizer for the T5-3b model.', 'code': 'from transformers import T5ForConditionalGeneration, T5Tokenizer\\\\n\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'t5-3b\\\\')\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'t5-3b\\\\')\\\\n\\\\ninput_text = \\\"Task-specific prompt with input text\\\"\\\\ninputs = tokenizer.encode(input_text, return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs)\\\\ngenerated_text = tokenizer.decode(outputs[0])', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Translation, Summarization, Question Answering, Sentiment Analysis', 'api_name': 't5-3b', 'api_call': 'from transformers import T5ForConditionalGeneration, T5Tokenizer\\\\n\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'t5-3b\\\\')\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'t5-3b\\\\')\\\\n\\\\ninput_text = \\\"Task-specific prompt with input text\\\"\\\\ninputs = tokenizer.encode(input_text, return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs)\\\\ngenerated_text = tokenizer.decode(outputs[0])', 'api_arguments': 'input_text', 'python_environment_requirements': 'transformers', 'example_code': \\\"input_text = 'translate English to French: The quick brown fox jumps over the lazy dog'; inputs = tokenizer.encode(input_text, return_tensors='pt'); outputs = model.generate(inputs); translated_text = tokenizer.decode(outputs[0])\\\", 'performance': {'dataset': 'c4', 'accuracy': 'See research paper, Table 14'}, 'description': 'T5-3B is a Text-To-Text Transfer Transformer (T5) model with 3 billion parameters. It is designed for various NLP tasks such as translation, summarization, question answering, and sentiment analysis. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on multiple supervised and unsupervised tasks.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ† Ù„Ù…Ø·ÙˆØ±ÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«ÙŠØ© Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ ÙÙŠ Ø§Ù„Ù†Ø§Ø¯ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨ÙŠ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„ÙŠØ­Ø³Ù†ÙˆØ§ Ù…Ù‡Ø§Ø±Ø§ØªÙ‡Ù… ÙÙŠ ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ. ÙŠÙÙ†ØµØ­ Ø¨ØªÙˆÙÙŠØ± Ø£Ø¯Ø§Ø© Ù„Ø¹Ø¨ Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù„Ù„Ù…Ø´Ø§Ø±ÙƒÙŠÙ† Ù…Ù† Ø£Ø¬Ù„ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„ØªØ­Ø¯ÙŠ ÙˆØ§Ù„ØªØ¹Ù„Ù….? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('pygmalion-6b')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import needed transformers library classes such as AutoTokenizer and AutoModelForCausalLM.\\\\n2. Load the Pygmalion 6B model using the from_pretrained method of AutoModelForCausalLM.\\\\n3. Tokenize the input prompt (which includes the persona description and conversation history) using the tokenizer.\\\\n4. Generate the model's response to the input dialogue by passing the encoded tokens to the model.\\\\n5. Decode the generated tokens to create a text-based response for the game character.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'pygmalion-6b\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'pygmalion-6b\\\\')\\\\n\\\\ncharacter_persona = \\\"[A few sentences about the character you want the model to play]\\\"\\\\ndialogue_history = \\\"[DIALOGUE HISTORY]\\\"\\\\ninput_message = \\\"You: [Your input message here]\\\"\\\\ninput_prompt = character_persona + \\\"\\\\\\\\n<START>\\\\\\\\n\\\" + dialogue_history + \\\"\\\\\\\\n\\\" + input_message\\\\n\\\\nencoded_input = tokenizer.encode(input_prompt, return_tensors=\\\"pt\\\")\\\\ngenerated_tokens = model.generate(encoded_input, max_length=100, n=num_responses)\\\\ndecoded_response = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('pygmalion-6b')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pygmalion-6b', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('pygmalion-6b')\\\", 'api_arguments': ['input_prompt'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\\\\n<START>\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\\\", 'performance': {'dataset': '56MB of dialogue data', 'accuracy': 'Not specified'}, 'description': \\\"Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B. The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model was initialized from the uft-6b ConvoGPT model and fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨.? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"YOLO('keremberke/yolov8n-pothole-segmentation')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the YOLO class from the ultralyticsplus library and render_result function.', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pothole-segmentation', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.995, 'mAP@0.5(mask)': 0.995}}, 'description': 'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØµÙ†ÙŠÙ Ù„ØºØ§Øª Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a zero-shot classification model based on the pre-trained model 'svalabs/gbert-large-zeroshot-nli'. This model has been trained for natural language inference and is suitable for text classification tasks.\\\\n3. Given a customer complaint, determine possible categories and urgency levels.\\\\n4. Use the created classifier to categorize the customer complaint based on the provided labels.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nzeroshot_pipeline = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'svalabs/gbert-large-zeroshot-nli\\\\')\\\\nsequence = \\\"Ich habe ein Problem mit meinem Iphone, das so schnell wie mÃ¶glich gelÃ¶st werden muss\\\"\\\\nlabels = [\\\\'Computer\\\\', \\\\'Handy\\\\', \\\\'Tablet\\\\', \\\\'dringend\\\\', \\\\'nicht dringend\\\\']\\\\nhypothesis_template = \\\"In diesem Satz geht es um das Thema {}.\\\"\\\\nclassification_results = zeroshot_pipeline(sequence, labels, hypothesis_template=hypothesis_template)', 'api_call': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'svalabs/gbert-large-zeroshot-nli', 'api_call': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", 'api_arguments': ['sequence', 'labels', 'hypothesis_template'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\nzershot_pipeline = pipeline(zero-shot-classification, model=svalabs/gbert-large-zeroshot-nli)\\\\nsequence = Ich habe ein Problem mit meinem Iphone das so schnell wie mÃ¶glich gelÃ¶st werden muss\\\\nlabels = [Computer, Handy, Tablet, dringend, nicht dringend]\\\\nhypothesis_template = In diesem Satz geht es um das Thema {}. ', 'performance': {'dataset': 'XNLI TEST-Set', 'accuracy': '85.6%'}, 'description': 'A German zeroshot classification model based on the German BERT large model from deepset.ai and finetuned for natural language inference using machine-translated nli sentence pairs from mnli, anli, and snli datasets.' </s>\"\n"}
{"text": "<s>[INST]Ù„ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠØŒ Ù‚Ù… Ø¨ØªØ¬Ù…ÙŠØ¹ ÙØ±ÙŠÙ‚ Ù…Ù† Ø§Ù„Ù…Ø·ÙˆØ±ÙŠÙ† Ø§Ù„Ù…Ù‡ØªÙ…ÙŠÙ† Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºÙˆÙŠ. Ø§Ø¨ØªÙƒØ±ÙˆØ§ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© ØªÙ„Ø®ÙŠØµ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„ ÙˆØ¯Ù‚ÙŠÙ‚. Ù„Ø§ ØªÙ†Ø³ÙˆØ§ Ø¥Ø¶Ø§ÙØ© ÙˆØ§Ø¬Ù‡Ø© Ø³Ù‡Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the necessary classes from the transformers library provided by Hugging Face, including BartForConditionalGeneration and BartTokenizer.', 'code': \\\"from transformers import BartForConditionalGeneration, BartTokenizer\\\\nmodel = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\\ntokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\\ninputs = tokenizer(article_text, return_tensors='pt', max_length=512, truncation=True)\\\\nsummary_ids = model.generate(inputs['input_ids'], num_beams=4, min_length=30, max_length=100)\\\\nsummary = tokenizer.decode(summary_ids[0])\\\", 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Transformers', 'functionality': 'Text Summarization', 'api_name': 'sshleifer/distilbart-cnn-6-6', 'api_call': \\\"from transformers import BartForConditionalGeneration, BartTokenizer\\\\nmodel = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\\ntokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\\ninputs = tokenizer(article_text, return_tensors='pt', max_length=512, truncation=True)\\\\nsummary_ids = model.generate(inputs['input_ids'], num_beams=4, min_length=30, max_length=100)\\\\nsummary = tokenizer.decode(summary_ids[0])\\\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': {'cnn_dailymail': {'Rouge 2': 20.17, 'Rouge-L': 29.7}, 'xsum': {'Rouge 2': 20.92, 'Rouge-L': 35.73}}}, 'description': 'DistilBART model for text summarization, trained on the CNN/Daily Mail and XSum datasets. It is a smaller and faster version of BART, suitable for summarizing English text.'}} </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'AutoTokenizer' and 'AutoModelForTokenClassification' from transformers.\\\\n2. Load the tokenizer and the model using the 'from_pretrained' method with the mentioned model name, which is 'Babelscape/wikineural-multilingual-ner'.\\\\n3. Create a pipeline for Named Entity Recognition (NER) using the loaded tokenizer and model.\\\\n4. Use the NER pipeline to analyze the medical history of the patient by providing the patient's text as input.\\\\n5. The pipeline will return the recognized named entities, specifically highlighting life events relevant to the patient's medical history.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Babelscape/wikineural-multilingual-ner\\\\')\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'Babelscape/wikineural-multilingual-ner\\\\')\\\\nnlp = pipeline(\\\\'ner\\\\', model=model, tokenizer=tokenizer)\\\\npatient_history = \\\"Medical history text here...\\\"\\\\nner_results = nlp(patient_history)\\\\nprint(ner_results)', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Babelscape/wikineural-multilingual-ner', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\\\", 'api_arguments': {'model': 'AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)', 'tokenizer': 'AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)'}, 'python_environment_requirements': [{'transformers': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline'}], 'example_code': 'tokenizer = AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\\\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\\\nexample = My name is Wolfgang and I live in Berlin\\\\nner_results = nlp(example)\\\\nprint(ner_results)', 'performance': {'dataset': 'Babelscape/wikineural-multilingual-ner', 'accuracy': 'span-based F1-score up to 6 points over previous state-of-the-art systems for data creation'}, 'description': 'A multilingual Named Entity Recognition (NER) model fine-tuned on the WikiNEuRal dataset, supporting 9 languages (de, en, es, fr, it, nl, pl, pt, ru). It is based on the mBERT architecture and trained on all 9 languages jointly. The model can be used with the Hugging Face Transformers pipeline for NER tasks.' </s>\"\n"}
{"text": "<s>[INST]Ø¥Ù†Ù†Ø§ ÙƒØ´Ø±ÙƒØ© Ù…ØªØ®ØµØµØ© ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†ØµÙˆØµ Ù…Ù†Ø¸ÙˆØ±Ø© Ù„Ø¯ÙŠÙ‡Ø§ Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ÙÙ‡Ù… ØªØ¯ÙÙ‚ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙˆØ§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…ØªÙ‚Ù† Ù„Ø¶Ù…Ø§Ù† Ø£Ø¯Ù‚ÙŠØ© Ø§Ù„Ù†Ù‡Ø¬ ÙˆØ§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='decapoda-research/llama-13b-hf')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the pipeline function from Hugging Face\\\\'s transformers library.\\\\n2. Instantiate a text generation pipeline using the specified model \\\\'decapoda-research/llama-13b-hf\\\\', which has been trained on a variety of tasks, such as question answering and reading comprehension.\\\\n3. Input the starting phrase \\\"Once upon a time, in a world full of robots,\\\" to the pipeline model, which will generate an interesting story about robots in a children\\\\'s context.', '<<<code>>>': 'from transformers import pipeline\\\\nstory_generator = pipeline(\\\\'text-generation\\\\', model=\\\\'decapoda-research/llama-13b-hf\\\\')\\\\nstory = story_generator(\\\"Once upon a time, in a world full of robots,\\\")', 'api_call': \\\"pipeline('text-generation', model='decapoda-research/llama-13b-hf')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'decapoda-research/llama-13b-hf', 'api_call': \\\"pipeline('text-generation', model='decapoda-research/llama-13b-hf')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"generator('Once upon a time')\\\", 'performance': {'dataset': [{'name': 'BoolQ', 'accuracy': '85.3'}, {'name': 'PIQA', 'accuracy': '82.8'}, {'name': 'SIQA', 'accuracy': '52.3'}, {'name': 'HellaSwag', 'accuracy': '84.2'}, {'name': 'WinoGrande', 'accuracy': '77'}, {'name': 'ARC-e', 'accuracy': '81.5'}, {'name': 'ARC-c', 'accuracy': '56'}, {'name': 'OBQACOPA', 'accuracy': '60.2'}]}, 'description': 'LLaMA-13B is an auto-regressive language model based on the transformer architecture developed by the FAIR team of Meta AI. It is designed for research purposes, such as question answering, natural language understanding, and reading comprehension. The model has been trained on a variety of sources, including web data, GitHub, Wikipedia, and books in 20 languages. It has been evaluated on several benchmarks, including BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC, and OpenBookQA.' </s>\"\n"}
{"text": "<s>[INST]Ù„ØªØ­Ù‚ÙŠÙ‚ ØªÙ‚Ø¯Ù… Ø£ÙƒØ¨Ø± ÙÙŠ Ù…Ø¬Ø§Ù„ ØªÙ‚Ù†ÙŠØ© Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙˆØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ±ØŒ Ø§Ø¨ØªÙƒØ±ÙˆØ§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ Ø¨Ø§Ù†ØªØ¸Ø§Ù… Ù„ØªØ·ÙˆÙŠØ± Ù…Ù‡Ø§Ø±Ø§ØªÙƒÙ…. Ù‚Ù…ÙˆØ§ Ø¨ØªÙ†Ø¸ÙŠÙ… Ø¯ÙˆØ±ÙŠØ§Øª Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠØŒ ÙˆØ¶Ø¹ÙˆØ§ Ø®Ø·Ø·Ù‹Ø§ Ù„ØªØ­Ø³ÙŠÙ† Ù‚Ø¯Ø±Ø§ØªÙƒÙ… Ø§Ù„ØªÙˆØ§ØµÙ„ÙŠØ© ÙˆØ§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©. ÙƒÙ…Ø§ ÙŠÙˆØµÙ‰ Ø¨Ø²ÙŠØ§Ø¯Ø© ØªØ­Ø¯ÙŠ Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø®Ù„Ø§Ù„ ØªÙˆÙÙŠØ± Ø£Ø¯ÙˆØ§Øª Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠØŒ Ù„Ø²ÙŠØ§Ø¯Ø© ØªÙØ§Ø¹Ù„ÙƒÙ… Ù…Ø¹ Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ÙˆÙ‚Ø¯Ø±ØªÙƒÙ… Ø¹Ù„Ù‰ Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹Ù‡Ø§.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import necessary libraries, including Mask2FormerForUniversalSegmentation from transformers, Image from PIL, and requests for handling URL requests.\\\\n2. Load the pretrained 'facebook/mask2former-swin-tiny-coco-instance' model using the from_pretrained method.\\\\n3. Load the image you want to segment, either from a file or from an image URL.\\\\n4. Process the image using the processor, which resizes and normalizes the input image.\\\\n5. Use the model to make predictions on the processed image, which will generate class queries logits and masks queries logits.\\\\n6. Post-process the outputs to extract the instance segmentation map for the input image.\\\\n7. The result will be an instance segmentation map that highlights and separates vehicles from the rest of the image.\\\", '<<<code>>>': \\\"from transformers import Mask2FormerForUniversalSegmentation, AutoImageProcessor\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\n\\\\nurl = 'https://your_image_url_here.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\ninputs = processor(images=image, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n\\\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_instance_map = result['segmentation']\\\", 'api_call': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/mask2former-swin-tiny-coco-instance', 'api_call': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-tiny-coco-instance'}, 'python_environment_requirements': [['torch', 'transformers', 'PIL', 'requests']], 'example_code': \\\"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_instance_map = result['segmentation']\\\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù…ØµÙˆØ±Ø© ØªØªØ­ÙˆÙ„ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ ØªØ¨Ø¯Ø£ Ø¨Ø¬Ù…Ù„Ø© \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Multimodal Text-to-Video', '<<<api_call>>>': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'TuneAVideoPipeline', 'UNet3DConditionModel', and 'save_videos_grid'.\\\\n2. Use the 'TuneAVideoPipeline.from_pretrained()' method to create a Text-to-Video pipeline with the nitrosocke/mo-di-diffusion prebuilt model, and the UNet3DConditionModel in 'Tune-A-Video-library/mo-di-bear-guitar'.\\\\n3. To create video snippets from text descriptions, use the 'pipe()' function with arguments such as 'prompt', 'video_length', 'height', 'width', 'num_inference_steps', and 'guidance_scale'.\\\\n4. Save the generated video by using the 'save_videos_grid()' function from the tuneavideo library.\\\", '<<<code>>>': \\\"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\\\nfrom tuneavideo.util import save_videos_grid\\\\nimport torch\\\\n\\\\nunet_model_path = 'Tune-A-Video-library/mo-di-bear-guitar'\\\\npretrained_model_path = 'nitrosocke/mo-di-diffusion'\\\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to('cuda')\\\\npipe.enable_xformers_memory_efficient_attention()\\\\n\\\\nprompt = 'A conference showcasing the latest technology trends'\\\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\\\nsave_videos_grid(video, f'./{prompt}.gif')\\\", 'api_call': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mo-di-bear-guitar', 'api_call': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': [['torch', 'tuneavideo']], 'example_code': \\\"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\\\nfrom tuneavideo.util import save_videos_grid\\\\nimport torch\\\\npretrained_model_path = nitrosocke/mo-di-diffusion\\\\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\\\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\nprompt = a magical princess is playing guitar, modern disney style\\\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\\\nsave_videos_grid(video, f./{prompt}.gif)\\\", 'performance': {'dataset': 'Not mentioned', 'accuracy': 'Not mentioned'}, 'description': 'Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙ„Ø®ØµØ­ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('plguillou/t5-base-fr-sum-cnndm')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries T5Tokenizer and T5ForConditionalGeneration from the transformers package.\\\\n2. Use the 'from_pretrained' method to load the tokenizer and the T5 model with the provided model name 'plguillou/t5-base-fr-sum-cnndm'.\\\\n3. Tokenize the French news article text using the T5Tokenizer.\\\\n4. Pass the tokenized input to the T5ForConditionalGeneration model by setting 'input_ids' and 'attention_mask' values.\\\\n5. Decode the generated tokens to obtain the summarized text.\\\", '<<<code>>>': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'plguillou/t5-base-fr-sum-cnndm\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'plguillou/t5-base-fr-sum-cnndm\\\\')\\\\ninputs = tokenizer(\\\"summarize: \\\" + french_news_article, return_tensors=\\\"pt\\\")\\\\nsummary_ids = model.generate(inputs[\\\\'input_ids\\\\'], num_return_sequences=1, num_beams=4, no_repeat_ngram_size=2, early_stopping=True, length_penalty=1.5)\\\\nsummaries = [tokenizer.decode(summary_id, skip_special_tokens=True, clean_up_tokenization_spaces=True) for summary_id in summary_ids]', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('plguillou/t5-base-fr-sum-cnndm')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'plguillou/t5-base-fr-sum-cnndm', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('plguillou/t5-base-fr-sum-cnndm')\\\", 'api_arguments': {'input_text': 'summarize: ARTICLE'}, 'python_environment_requirements': [{'transformers': 'from transformers import T5Tokenizer, T5ForConditionalGeneration'}], 'example_code': 'tokenizer = T5Tokenizer.from_pretrained(plguillou/t5-base-fr-sum-cnndm)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(plguillou/t5-base-fr-sum-cnndm)', 'performance': {'dataset': 'cnn_dailymail', 'ROUGE-1': 44.5252, 'ROUGE-2': 22.652, 'ROUGE-L': 29.8866}, 'description': 'This model is a T5 Transformers model (JDBN/t5-base-fr-qg-fquad) that was fine-tuned in French for abstractive text summarization.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„ØºÙˆÙŠØ© Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙØ±ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"CrossEncoder('cross-encoder/nli-roberta-base')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': 'from sentence_transformers import CrossEncoder\\\\nencoder = CrossEncoder(\\\\'cross-encoder/nli-roberta-base\\\\')\\\\nmovie_conversation = \\\"Conversation about the movie goes here...\\\"\\\\ncandidate_genres = [\\\\'action\\\\', \\\\'comedy\\\\', \\\\'drama\\\\', \\\\'horror\\\\', \\\\'sci-fi\\\\']\\\\nscores = encoder.predict([(movie_conversation, genre) for genre in candidate_genres])\\\\nresult = {genre: score for genre, score in zip(candidate_genres, scores)}', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Transformers', 'functionality': 'Zero-Shot Classification', 'api_name': 'cross-encoder/nli-roberta-base', 'api_call': 'from sentence_transformers import CrossEncoder\\\\nencoder = CrossEncoder(\\\\'cross-encoder/nli-roberta-base\\\\')\\\\nmovie_conversation = \\\"Conversation about the movie goes here...\\\"\\\\ncandidate_genres = [\\\\'action\\\\', \\\\'comedy\\\\', \\\\'drama\\\\', \\\\'horror\\\\', \\\\'sci-fi\\\\']\\\\nscores = encoder.predict([(movie_conversation, genre) for genre in candidate_genres])\\\\nresult = {genre: score for genre, score in zip(candidate_genres, scores)}', 'api_arguments': ['sentence1', 'sentence2'], 'python_environment_requirements': ['sentence_transformers', 'transformers'], 'example_code': \\\"from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model='cross-encoder/nli-roberta-base')\\\\nsent = Apple just announced the newest iPhone X\\\\ncandidate_labels = [technology, sports, politics]\\\\nres = classifier(sent, candidate_labels)\\\\nprint(res)\\\", 'performance': {'dataset': ['SNLI', 'MultiNLI'], 'accuracy': 'See SBERT.net - Pretrained Cross-Encoder'}, 'description': 'Cross-Encoder for Natural Language Inference trained on the SNLI and MultiNLI datasets. Outputs three scores corresponding to the labels: contradiction, entailment, neutral.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ø¬Ù…Ù„Ø© Ø¨Ø³ÙŠØ·Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ ÙˆØ¬ÙˆØ§Ø¨ Ø¶Ù…Ù† Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Start by importing the necessary classes from the transformers package, including AutoModelForQuestionAnswering, AutoTokenizer, and pipeline.\\\\n2. Load the pre-trained model 'deepset/roberta-base-squad2' using the from_pretrained method of the AutoModelForQuestionAnswering class. This question-answering model has been trained on the SQuAD 2.0 dataset and is suitable for extracting answers from text content.\\\\n3. Load the corresponding tokenizer using the from_pretrained method of the AutoTokenizer class.\\\\n4. Use the pipeline function to create a question-answering pipeline with the model and tokenizer.\\\\n5. Accept user input for a question and use the pipeline to extract an answer from the webpage's text, given both as input to the pipeline.\\\", '<<<code>>>': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel_name = \\\\'deepset/roberta-base-squad2\\\\'\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=model, tokenizer=tokenizer)\\\\n\\\\nquestion = \\\"What is the main topic of the webpage?\\\"  # Replace with user input\\\\nwebpage_text = \\\"\\\"\\\"This is a webpage about...\\\"\\\"\\\"  # Replace with text content of the webpage\\\\n\\\\nQA_input = {\\\\'question\\\\': question, \\\\'context\\\\': webpage_text}\\\\nanswer = nlp(QA_input)', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'optimum/roberta-base-squad2', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')\\\", 'api_arguments': {'model_name': 'deepset/roberta-base-squad2', 'tokenizer': 'deepset/roberta-base-squad2'}, 'python_environment_requirements': [['transformers']], 'example_code': {'a': {'import': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline', 'example': \\\"nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\\\\nQA_input = {\\\\n 'question': 'Why is model conversion important?',\\\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\\\n}\\\\nres = nlp(QA_input)\\\"}, 'b': {'import': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer', 'example': 'model = AutoModelForQuestionAnswering.from_pretrained(model_name)\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)'}}, 'performance': {'dataset': 'squad_v2', 'accuracy': {'exact': 79.87029394424324, 'f1': 82.91251169582613}}, 'description': 'This is an ONNX conversion of the deepset/roberta-base-squad2 model for extractive question answering. It is trained on the SQuAD 2.0 dataset and is compatible with the Transformers library.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆÙ†ÙˆØ¯ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯Ù‚Ø© ÙˆÙÙ‚Ø§Ù‹ Ù„ØªØµÙ†ÙŠÙØ§Øª Ù…Ø­Ø¯Ø¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create an image classification model.\\\\n3. Specify the model 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k' to be loaded. This model has been fine-tuned for image classification tasks based on the ViT (Vision Transformer) architecture.\\\\n4. Once the image_classifier is created, pass an image that needs to be classified into categories as input to the model to get the predictions.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\\nresult = image_classifier(image_path)\\\", 'api_call': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'api_call': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", 'api_arguments': {'model': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'framework': 'pt'}, 'python_environment_requirements': [['transformers', 'torch']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ±ÙŠØ¯ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ù†Øµ. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø­Ø« ÙˆØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨ÙƒÙØ§Ø¡Ø© Ø¹Ø§Ù„ÙŠØ© ÙˆÙŠÙ†Ø§Ø³Ø¨ Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the pipeline function from the transformers library provided by Hugging Face.', 'code': 'from transformers import pipeline\\\\nimage_captioner = pipeline(\\\\'text2text-generation\\\\', model=\\\\'salesforce/blip2-opt-6.7b\\\\')\\\\nimage_path = \\\\'your_image_path.jpg\\\\'\\\\nwith open(image_path, \\\"rb\\\") as image_file:\\\\n    image_data = image_file.read()  # Read the image file as bytes\\\\ncaption = image_captioner(image_data)', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': 'from transformers import pipeline\\\\nimage_captioner = pipeline(\\\\'text2text-generation\\\\', model=\\\\'salesforce/blip2-opt-6.7b\\\\')\\\\nimage_path = \\\\'your_image_path.jpg\\\\'\\\\nwith open(image_path, \\\"rb\\\") as image_file:\\\\n    image_data = image_file.read()  # Read the image file as bytes\\\\ncaption = image_captioner(image_data)', 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØµÙ†ÙŠÙ Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-3')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a zero-shot classification model with the pre-trained model \\\\'valhalla/distilbart-mnli-12-3\\\\'. This model is well-suited for zero-shot classification tasks required in this context.\\\\n3. Define the list of possible dispute categories such as: [\\\"financial issue\\\", \\\"delivery issue\\\", \\\"customer support issue\\\"]\\\\n4. Use the created classifier to classify the given dispute text.\\\\n5. Extract the most likely dispute category.', '<<<code>>>': 'from transformers import pipeline\\\\ndispute_text = \\\"Lauren recently bought a dress from our website, and she is complaining that the item she received is not as described. The quality of the fabric is very poor, and the color is completely different from what was shown in the pictures. She is requesting a full refund.\\\"\\\\ncategories = [\\\"financial issue\\\", \\\"delivery issue\\\", \\\"customer support issue\\\"]\\\\nzero_shot_classifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'valhalla/distilbart-mnli-12-3\\\\')\\\\nclassification_result = zero_shot_classifier(dispute_text, categories)\\\\ntop_category = classification_result[\\\\'labels\\\\'][0]', 'api_call': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-3')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'valhalla/distilbart-mnli-12-3', 'api_call': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-3')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': [{'name': 'matched acc', 'accuracy': 88.1}, {'name': 'mismatched acc', 'accuracy': 88.19}]}, 'description': 'distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is a simple and effective technique with very little performance drop.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ø³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ø£Ø®Ø±Ù‰ØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ù€ \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the required libraries from the transformers package. In this case, import 'BertTokenizerFast' and 'EncoderDecoderModel'.\\\", 'code': 'from transformers import BertTokenizerFast, EncoderDecoderModel\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\\\'kykim/bertshared-kor-base\\\\')\\\\nmodel = EncoderDecoderModel.from_pretrained(\\\\'kykim/bertshared-kor-base\\\\')\\\\n\\\\nkorean_sentence = \\\"í•œêµ­ì–´ ë‰´ìŠ¤ ê¸°ì‚¬ ë¬¸ì¥\\\"\\\\ninputs = tokenizer.encode(korean_sentence, return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs)\\\\nenglish_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kykim/bertshared-kor-base', 'api_call': 'from transformers import BertTokenizerFast, EncoderDecoderModel\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\\\'kykim/bertshared-kor-base\\\\')\\\\nmodel = EncoderDecoderModel.from_pretrained(\\\\'kykim/bertshared-kor-base\\\\')\\\\n\\\\nkorean_sentence = \\\"í•œêµ­ì–´ ë‰´ìŠ¤ ê¸°ì‚¬ ë¬¸ì¥\\\"\\\\ninputs = tokenizer.encode(korean_sentence, return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs)\\\\nenglish_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)', 'api_arguments': {'tokenizer': 'BertTokenizerFast.from_pretrained(kykim/bertshared-kor-base)'}, 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import BertTokenizerFast, EncoderDecoderModel\\\\ntokenizer = BertTokenizerFast.from_pretrained(kykim/bertshared-kor-base)\\\\nmodel = EncoderDecoderModel.from_pretrained(kykim/bertshared-kor-base)', 'performance': {'dataset': '70GB Korean text dataset', 'accuracy': '42000 lower-cased subwords'}, 'description': 'Bert base model for Korean, trained on a 70GB Korean text dataset and 42000 lower-cased subwords. Can be used for Text2Text Generation tasks.'}} </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± Ø¨Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8m-building-segmentation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the 'ultralyticsplus' library using pip, which contains the YOLO object detection model.\\\\n2. Import the necessary classes YOLO and render_result from the 'ultralyticsplus' package.\\\\n3. Initialize the YOLO model with 'keremberke/yolov8m-building-segmentation', which is trained for building segmentation in satellite images.\\\\n4. Set model overrides for 'conf' (object confidence threshold) and 'iou' (IoU threshold for non-maximum suppression) to appropriate values.\\\\n5. Use the 'predict' method to perform building detection and segmentation on a given satellite image URL or local file path.\\\\n6. The result contains detected buildings' bounding boxes and masks that can be used for city planning purposes.\\\\n7. Use the 'render_result' function to visualize the results on the input image.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path_or_url_to_satellite_image'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8m-building-segmentation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8m-building-segmentation', 'api_call': \\\"YOLO('keremberke/yolov8m-building-segmentation')\\\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.623, 'mAP@0.5(mask)': 0.613}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø¨Ø¯ÙˆÙ† Ø´Ø±ÙˆØ· ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ø¨ØªÙƒØ§Ø± ØµÙˆØ± Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯ÙˆÙ† Ø´Ø±ÙˆØ· ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨.? [/INST]{'domain': 'Computer Vision Zero-Shot Image Classification', 'api_call': \\\"CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary libraries, including 'Image' from PIL, 'requests', 'CLIPProcessor', and 'CLIPModel' from transformers.\\\", 'code': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')\\\\nprocessor = CLIPProcessor.from_pretrained('flax-community/clip-rsicd-v2')\\\\nurl = 'https://example.com/path/to/image.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nlabels = ['residential area', 'playground', 'stadium', 'forest', 'airport']\\\\ninputs = processor(text=[f'a photo of a {l}' for l in labels], images=image, return_tensors='pt', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\\nfor l, p in zip(labels, probs[0]):\\\\n    print(f'{l: <16} {p:.4f}')\\\", 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'flax-community/clip-rsicd-v2', 'api_call': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')\\\\nprocessor = CLIPProcessor.from_pretrained('flax-community/clip-rsicd-v2')\\\\nurl = 'https://example.com/path/to/image.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nlabels = ['residential area', 'playground', 'stadium', 'forest', 'airport']\\\\ninputs = processor(text=[f'a photo of a {l}' for l in labels], images=image, return_tensors='pt', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\\nfor l, p in zip(labels, probs[0]):\\\\n    print(f'{l: <16} {p:.4f}')\\\", 'api_arguments': {'text': ['a photo of a residential area', 'a photo of a playground', 'a photo of a stadium', 'a photo of a forest', 'a photo of an airport'], 'images': 'image', 'return_tensors': 'pt', 'padding': 'True'}, 'python_environment_requirements': ['PIL', 'requests', 'transformers'], 'example_code': 'from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(flax-community/clip-rsicd-v2)\\\\nprocessor = CLIPProcessor.from_pretrained(flax-community/clip-rsicd-v2)\\\\nurl = https://raw.githubusercontent.com/arampacha/CLIP-rsicd/master/data/stadium_1.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nlabels = [residential area, playground, stadium, forest, airport]\\\\ninputs = processor(text=[fa photo of a {l} for l in labels], images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\\nfor l, p in zip(labels, probs[0]):\\\\n print(f{l:&lt;16} {p:.4f})', 'performance': {'dataset': {'RSICD': {'original CLIP': {'k=1': 0.572, 'k=3': 0.745, 'k=5': 0.837, 'k=10': 0.939}, 'clip-rsicd-v2 (this model)': {'k=1': 0.883, 'k=3': 0.968, 'k=5': 0.982, 'k=10': 0.998}}}}, 'description': 'This model is a fine-tuned CLIP by OpenAI. It is designed with an aim to improve zero-shot image classification, text-to-image and image-to-image retrieval specifically on remote sensing images.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹ÙŠÙ†Ø©. Ù‚ÙÙ… Ø¨ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ø¨ØµÙˆØªÙƒ Ø§Ù„Ø®Ø§Øµ. Ø¶Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ ÙˆØ¶Ø¹ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ…Ø±. Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØªØ¬Ø±Ø¨Ø© Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ØµÙˆØª. ØªØ­Ù‚Ù‚ Ù…Ù† Ø¯Ù‚Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª. Ø£ØµÙ„Ø­ Ø£ÙŠ Ø£Ø®Ø·Ø§Ø¡ ÙˆÙ‚Ù… Ø¨Ø¶Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù„Ø§Ø­Ù‚Ø§.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"Wav2Vec2ForCTC.from_pretrained('cpierse/wav2vec2-large-xlsr-53-esperanto')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import required libraries, including torchaudio, torch, Wav2Vec2ForCTC, and Wav2Vec2Processor from transformers.\\\\n2. Create a processor instance by calling Wav2Vec2Processor.from_pretrained with the model name 'cpierse/wav2vec2-large-xlsr-53-esperanto'.\\\\n3. Create a model instance by calling Wav2Vec2ForCTC.from_pretrained with the model name 'cpierse/wav2vec2-large-xlsr-53-esperanto'.\\\\n4. Load the provided speech_recording_path using torchaudio.load function, which returns a tuple of speech_array and sampling_rate.\\\\n5. Resample the audio to 16kHz using torchaudio.transforms.Resample.\\\\n6. Pass the resampled audio to the processor instance and call its __call__ function to create the input tensors needed for the model.\\\\n7. Feed the input tensors to the model to get logits, and then find the most likely text using torch.argmax.\\\\n8. Decode the predicted IDs using the processor instance and print the predicted text.\\\", '<<<code>>>': 'import torch\\\\nimport torchaudio\\\\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\\\nprocessor = Wav2Vec2Processor.from_pretrained(\\\\'cpierse/wav2vec2-large-xlsr-53-esperanto\\\\')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\\\'cpierse/wav2vec2-large-xlsr-53-esperanto\\\\')\\\\nspeech_array, sampling_rate = torchaudio.load(\\\\'path_to_the_speech_recording.wav\\\\')\\\\nresampler = torchaudio.transforms.Resample(sampling_rate, 16000)\\\\nresampled_audio = resampler(speech_array)\\\\ninput_values = processor(resampled_audio.squeeze().numpy(), sampling_rate=16000, return_tensors=\\\"pt\\\", padding=True)\\\\nlogits = model(input_values.input_values, attention_mask=input_values.attention_mask).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\npredicted_text = processor.batch_decode(predicted_ids)\\\\nprint(\\\"Predicted text:\\\", predicted_text)', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('cpierse/wav2vec2-large-xlsr-53-esperanto')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cpierse/wav2vec2-large-xlsr-53-esperanto', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('cpierse/wav2vec2-large-xlsr-53-esperanto')\\\", 'api_arguments': ['path', 'sentence', 'speech', 'sampling_rate'], 'python_environment_requirements': [['torch', 'torchaudio', 'datasets', 'transformers', 're', 'jiwer']], 'example_code': 'import torch\\\\nimport torchaudio\\\\nfrom datasets import load_dataset\\\\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\\\ntest_dataset = load_dataset(common_voice, eo, split=test[:2%]) \\\\nprocessor = Wav2Vec2Processor.from_pretrained(cpierse/wav2vec2-large-xlsr-53-esperanto) \\\\nmodel = Wav2Vec2ForCTC.from_pretrained(cpierse/wav2vec2-large-xlsr-53-esperanto) \\\\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\\\\ndef speech_file_to_array_fn(batch):\\\\n speech_array, sampling_rate = torchaudio.load(batch[path])\\\\n batch[speech] = resampler(speech_array).squeeze().numpy()\\\\n return batch\\\\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\\\\ninputs = processor(test_dataset[speech][:2], sampling_rate=16_000, return_tensors=pt, padding=True)\\\\nwith torch.no_grad():\\\\n logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\nprint(Prediction:, processor.batch_decode(predicted_ids))\\\\nprint(Reference:, test_dataset[sentence][:2])', 'performance': {'dataset': 'common_voice', 'accuracy': '12.31%'}, 'description': 'Fine-tuned facebook/wav2vec2-large-xlsr-53 on esperanto using the Common Voice dataset. When using this model, make sure that your speech input is sampled at 16kHz.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ¹Ù„Ù… ÙƒÙŠÙÙŠØ© ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª ÙˆÙÙ‡Ù…Ù‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØµÙˆØª. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆØ¬ÙŠÙ‡ÙŠ Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© Ø§Ù„Ø¨Ø¯Ø¡ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ØŸ? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"WhisperModel('large-v2')\\\", '<<<api_provider>>>': 'CTranslate2', '<<<explanation>>>': \\\"1. Import the WhisperModel class from the faster_whisper library.\\\\n2. Use the WhisperModel function to create the Automatic Speech Recognition (ASR) model by specifying the version as 'large-v2'.\\\\n3. Now, you can use the created ASR model 'large-v2' to transcribe the audio file by calling the transcribe method with the audio file path as the argument.\\\\n4. The transcribe method will return the spoken words as text, along with their start and end times in the audio file.\\\\n5. You can iterate through the segments and print their start and end times along with the corresponding transcribed text.\\\", '<<<code>>>': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(\\\\'large-v2\\\\')\\\\nsegments, info = model.transcribe(\\\\'audio.mp3\\\\')\\\\nfor segment in segments:\\\\n    print(\\\"[%.2fs -> %.2fs] %s\\\" % (segment.start, segment.end, segment.text))', 'api_call': \\\"WhisperModel('large-v2')\\\", 'provider': 'CTranslate2', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': \\\"WhisperModel('large-v2')\\\", 'api_arguments': ['audio.mp3'], 'python_environment_requirements': [['faster_whisper']], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.' </s>\"\n"}
{"text": "<s>[INST]Ù‚ÙÙ… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø© ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬Ù‡Ø§ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙÙŠ Ø§Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© ÙˆØ§Ù„ØªØµÙ†ÙŠÙ.? [/INST]'<<<domain>>>': 'Multimodal Feature Extraction', '<<<api_call>>>': \\\"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the required ASTModel class from the appropriate transformers library/module.\\\\n2. To initialize the Audio Spectrogram Transformer (AST) model, call the from_pretrained method by providing the specified model as input: 'MIT/ast-finetuned-audioset-10-10-0.4593'.\\\\n3. After the AST model is initialized, you can use it to extract features from audio signals by feeding the audio data into the model. The output will be an audio spectrogram representation of the input data, which can be further processed for tasks like recommendation systems.\\\", '<<<code>>>': \\\"from transformers import ASTModel\\\\nast_model = ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\\\", 'api_call': \\\"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'audio-spectrogram-transformer', 'api_call': \\\"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙŠØµÙ Ù…Ø´Ù‡Ø¯Ù‹Ø§ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers library, which include AutoModelForCausalLM, AutoTokenizer, and set_seed. Additionally, import torch for handling tensor computations.\\\\n2. We then load the pre-trained model 'facebook/opt-66b' using the AutoModelForCausalLM class.\\\\n3. To create a tokenizer, we use the from_pretrained method from the AutoTokenizer class.\\\\n4. We set a prompt 'the adventurous journey of a fearless rabbit' to guide the text generation process.\\\\n5. We create input_ids by processing the prompt using the tokenizer and converting it to input tensors.\\\\n6. We use the set_seed function to set a random seed for reproducible results.\\\\n7. The generated text is obtained by calling the generate method from the model and specifying parameters like do_sample, num_return_sequences, and max_length.\\\\n8. After generating the text, we decode it using the tokenizer's batch_decode method to obtain the children's book introduction.\\\", '<<<code>>>': 'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'facebook/opt-66b\\\\', torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/opt-66b\\\\', use_fast=False)\\\\nprompt = \\\"the adventurous journey of a fearless rabbit\\\"\\\\ninput_ids = tokenizer(prompt, return_tensors=\\\\'pt\\\\').input_ids.cuda()\\\\nset_seed(32)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=50)\\\\nintroduction = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/opt-66b', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", 'api_arguments': ['input_ids', 'do_sample', 'num_return_sequences', 'max_length'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-66b, torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-66b, use_fast=False)\\\\nprompt = Hello, I am conscious and\\\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\\\nset_seed(32)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\\\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)', 'performance': {'dataset': 'GPT-3', 'accuracy': 'roughly matched'}, 'description': 'OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. OPT models are trained to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¨Ø±Ù…Ø¬ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ÙŠÙ‚ÙˆÙ… Ø¨ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ù‡Ø¯Ù ØªØ­Ø³ÙŠÙ† ÙÙ‡Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙÙŠØ¯Ø©. ÙˆØ¹Ù„ÙŠÙ‡ Ø£Ù† ÙŠØ¶Ù…Ù† Ø£Ù† Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØµÙ†ÙŠÙ ØªØªÙ†Ø§Ø³Ø¨ Ù…Ø¹ Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª ÙˆØªÙØ¶ÙŠÙ„Ø§Øª Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…ØŒ ÙˆØªÙˆÙØ± Ù„Ù‡Ù… ØªØ¬Ø±Ø¨Ø© ÙØ¹Ø§Ù„Ø© ÙˆØ³Ù‡Ù„Ø© Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©.? [/INST]{'domain': 'Tabular Tabular Classification', 'api_call': \\\"AutoModel.from_pretrained('harithapliyal/autotrain-tatanic-survival-51030121311')\\\", 'api_provider': 'Hugging Face', 'explanation': '1. Import necessary libraries such as joblib and pandas.', 'code': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face', 'functionality': 'Binary Classification', 'api_name': 'harithapliyal/autotrain-tatanic-survival-51030121311', 'api_call': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_arguments': ['data'], 'python_environment_requirements': ['joblib', 'pandas'], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'harithapliyal/autotrain-data-tatanic-survival', 'accuracy': 0.872}, 'description': 'A tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain. The model predicts whether a passenger survived or not based on features such as age, gender, and passenger class.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØªÙŠ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨Ø© Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© ÙÙŠ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers package such as AutoModelForTableQuestionAnswering, AutoTokenizer, and pipeline.\\\\n2. Load the pretrained TAPAS model 'navteca/tapas-large-finetuned-wtq' using the AutoModelForTableQuestionAnswering.from_pretrained() method.\\\\n3. Load the corresponding tokenizer for the TAPAS model using the AutoTokenizer.from_pretrained() method.\\\\n4. Create a pipeline instance using the loaded model and tokenizer for table-question-answering.\\\\n5. Use the pipeline instance with the given table data and the query 'How many products are imported from Germany?'.\\\\n6. Finally, print the result to get the answer.\\\", '<<<code>>>': \\\"from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\\\\ntapas_model = AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\\ntapas_tokenizer = AutoTokenizer.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\\nnlp = pipeline('table-question-answering', model=tapas_model, tokenizer=tapas_tokenizer)\\\\nresult = nlp({'table': {'Country': ['USA', 'Germany', 'Italy'], 'Products': ['50', '30', '25']}, 'query': 'How many products are imported from Germany?'})\\\\nprint(result)\\\", 'api_call': \\\"AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'navteca/tapas-large-finetuned-wtq', 'api_call': \\\"AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\", 'api_arguments': {'table': 'table_data', 'query': 'query'}, 'python_environment_requirements': [{'transformers': 'AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline'}], 'example_code': \\\"from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\\\\n\\\\n# Load model & tokenizer\\\\ntapas_model = AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\\ntapas_tokenizer = AutoTokenizer.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\\n\\\\n# Get predictions\\\\nnlp = pipeline('table-question-answering', model=tapas_model, tokenizer=tapas_tokenizer)\\\\nresult = nlp({'table': {'Repository': ['Transformers', 'Datasets', 'Tokenizers'], 'Stars': ['36542', '4512', '3934'], 'Contributors': ['651', '77', '34'], 'Programming language': ['Python', 'Python', 'Rust, Python and NodeJS']}, 'query': 'How many stars does the transformers repository have?'})\\\\nprint(result)\\\", 'performance': {'dataset': 'wikisql', 'accuracy': 'Not provided'}, 'description': 'TAPAS large model fine-tuned on WikiTable Questions (WTQ). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø·Ø¨ÙŠØ¹ÙŠ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù…Ù† Ø£Ø¬Ù„ ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ø§Ù„Ù‰ Ù†ØµÙˆØµØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ ØªÙˆÙ‚Ø¹ Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ø³ØªÙ†Ø§Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªÙ†ÙˆØ¹Ø© Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-summarize-news')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the AutoModelWithLMHead and AutoTokenizer classes from the transformers library.\\\\n2. Load the pre-trained model and tokenizer 'mrm8488/t5-base-finetuned-summarize-news'.\\\\n3. Define a function summarize that takes a text and generates its summary of the specified max_length (default is 150 words).\\\\n4. The function tokenizes the input text using the tokenizer and encodes it into input_ids tensor.\\\\n5. The input_ids tensor is then fed to the model to generate summaries with specified parameters like num_beams, max_length, repetition_penalty, etc.\\\\n6. Decode the generated summaries into human-readable text format and return the summary.\\\", '<<<code>>>': 'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'mrm8488/t5-base-finetuned-summarize-news\\\\')\\\\nmodel = AutoModelWithLMHead.from_pretrained(\\\\'mrm8488/t5-base-finetuned-summarize-news\\\\')\\\\n\\\\ndef summarize(text, max_length=150):\\\\n    input_ids = tokenizer.encode(text, return_tensors=\\\\'pt\\\\', add_special_tokens=True)\\\\n    generated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\n    preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\n    return preds[0]\\\\n\\\\nsummary = summarize(\\\"<News Article>\\\", max_length=150)', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-summarize-news')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mrm8488/t5-base-finetuned-summarize-news', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-summarize-news')\\\", 'api_arguments': ['text', 'max_length'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\ndef summarize(text, max_length=150):\\\\n input_ids = tokenizer.encode(text, return_tensors=pt, add_special_tokens=True)\\\\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\nreturn preds[0]', 'performance': {'dataset': 'News Summary', 'accuracy': 'Not provided'}, 'description': \\\"Google's T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ù…Ø¯Ù‰ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø§ØªØŒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ù…Ø«Ù„ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙˆÙ†ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø©.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"pipeline('question-answering', model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'), tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'))\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline, AutoModel, AutoTokenizer\\\\nqa_pipeline = pipeline('question-answering', model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'), tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'))\\\\nQA_input = {'question': 'Example question', 'context': 'Example context'}\\\\nanswer = qa_pipeline(QA_input)\\\", 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Question Answering', 'api_name': 'bert-large-uncased-whole-word-masking-squad2', 'api_call': \\\"from transformers import pipeline, AutoModel, AutoTokenizer\\\\nqa_pipeline = pipeline('question-answering', model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'), tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'))\\\\nQA_input = {'question': 'Example question', 'context': 'Example context'}\\\\nanswer = qa_pipeline(QA_input)\\\", 'api_arguments': {'model_name': 'deepset/bert-large-uncased-whole-word-masking-squad2', 'tokenizer': 'deepset/bert-large-uncased-whole-word-masking-squad2'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"QA_input = {\\\\n 'question': 'Why is model conversion important?',\\\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\\\n}\\\\nres = nlp(QA_input)\\\", 'performance': {'dataset': 'squad_v2', 'accuracy': {'Exact Match': 80.885, 'F1': 83.876}}, 'description': 'This is a bert-large model, fine-tuned using the SQuAD2.0 dataset for the task of question answering. It is designed for extractive question answering and supports English language.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ§Øª ØªØµÙˆÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆÙ†Ø·Ù„Ø¨ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ ÙŠÙ„ØªÙ‚Ø·Ù‡Ø§ Ø§Ù„Ù†Ø¸Ø§Ù….? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the necessary classes MBartForConditionalGeneration and MBart50TokenizerFast from the transformers library.', 'code': 'from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\\\\nmodel = MBartForConditionalGeneration.from_pretrained(\\\\'facebook/mbart-large-50\\\\')\\\\ntokenizer = MBart50TokenizerFast.from_pretrained(\\\\'facebook/mbart-large-50\\\\', src_lang=\\\\'en_XX\\\\', tgt_lang=\\\\'ro_RO\\\\')\\\\ninput_text = \\\"Your father\\\\'s biography in English\\\"\\\\ninput_tokens = tokenizer(input_text, return_tensors=\\\\'pt\\\\')\\\\ngenerated_output = model.generate(**input_tokens)\\\\ntranslated_text = tokenizer.batch_decode(generated_output, skip_special_tokens=True)[0]', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Multilingual Sequence-to-Sequence', 'api_name': 'facebook/mbart-large-50', 'api_call': 'from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\\\\nmodel = MBartForConditionalGeneration.from_pretrained(\\\\'facebook/mbart-large-50\\\\')\\\\ntokenizer = MBart50TokenizerFast.from_pretrained(\\\\'facebook/mbart-large-50\\\\', src_lang=\\\\'en_XX\\\\', tgt_lang=\\\\'ro_RO\\\\')\\\\ninput_text = \\\"Your father\\\\'s biography in English\\\"\\\\ninput_tokens = tokenizer(input_text, return_tensors=\\\\'pt\\\\')\\\\ngenerated_output = model.generate(**input_tokens)\\\\ntranslated_text = tokenizer.batch_decode(generated_output, skip_special_tokens=True)[0]', 'api_arguments': {'src_lang': 'en_XX', 'tgt_lang': 'ro_RO'}, 'python_environment_requirements': {'transformers': 'MBartForConditionalGeneration, MBart50TokenizerFast'}, 'example_code': \\\"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\\\\nmodel = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')\\\\ntokenizer = MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50', src_lang='en_XX', tgt_lang='ro_RO')\\\\nsrc_text = ' UN Chief Says There Is No Military Solution in Syria'\\\\ntgt_text = 'Åeful ONU declarÄƒ cÄƒ nu existÄƒ o soluÅ£ie militarÄƒ Ã®n Siria'\\\\nmodel_inputs = tokenizer(src_text, return_tensors='pt')\\\\nwith tokenizer.as_target_tokenizer():\\\\n labels = tokenizer(tgt_text, return_tensors='pt').input_ids\\\\nmodel(**model_inputs, labels=labels)\\\", 'performance': {'dataset': 'Multilingual Denoising Pretraining', 'accuracy': 'Not specified'}, 'description': \\\"mBART-50 is a multilingual Sequence-to-Sequence model pre-trained using the 'Multilingual Denoising Pretraining' objective. It was introduced in Multilingual Translation with Extensible Multilingual Pretraining and Finetuning paper.\\\"}} </s>\"\n"}
{"text": "<s>[INST]Ø£ÙƒØªØ¨ Ø¹Ù†ÙˆØ§Ù†Ù‹Ø§ Ø¬Ø°Ø§Ø¨Ù‹Ø§ Ù„Ù…Ù‚Ø§Ù„Ø© Ø­ÙˆÙ„ ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ¯ÙˆØ±Ù‡Ø§ ÙÙŠ ÙÙ‡Ù… ÙˆØ§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù†ØµÙˆØµ.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the pipeline function from the transformers library provided by Hugging Face to ensure quick and efficient integration of their models.\\\\n2. We create a question-answering pipeline using the pre-trained model 'mrm8488/bert-multi-cased-finetuned-xquadv1'. This model has been fine-tuned on the XQuAD dataset and supports answering in multiple languages.\\\\n3. Given a context and question in any of the supported languages, you can use the question-answering pipeline to generate relevant answers.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\\nresult = qa_pipeline({'context': 'Manuel Romero heeft de laatste tijd hard gewerkt aan de repository hugginface/transformers.', 'question': 'Wie heeft er de laatste tijd hard gewerkt voor hugginface/transformers?'})\\\", 'api_call': \\\"pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mrm8488/bert-multi-cased-finetuned-xquadv1', 'api_call': \\\"pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\", 'api_arguments': {'context': 'string', 'question': 'string'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"qa_pipeline({\\\\n 'context': Manuel Romero has been working hardly in the repository hugginface/transformers lately,\\\\n 'question': Who has been working hard for hugginface/transformers lately?\\\\n})\\\", 'performance': {'dataset': 'XQuAD', 'accuracy': 'Not provided'}, 'description': 'This model is a BERT (base-multilingual-cased) fine-tuned for multilingual Question Answering on 11 different languages using the XQuAD dataset and additional data augmentation techniques.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, in this case, SentenceTransformer and util from sentence_transformers.\\\\n2. Initialize the SentenceTransformer model using the 'sentence-transformers/multi-qa-mpnet-base-cos-v1' model.\\\\n3. Given a specific question (query) and a list of paragraphs or segments (docs), the model can be used to encode the query and docs into high-dimensional representations.\\\\n4. Use the util.dot_score() function to compute the cosine similarity between the query and docs embeddings, and pair the obtained scores with their respective segments.\\\\n5. Sort the pairs in descending order based on the similarity scores, and retrieve the segment with the highest score as the most relevant segment.\\\", '<<<code>>>': \\\"from sentence_transformers import SentenceTransformer, util\\\\nquery = 'Information on company growth...'\\\\ndocs = [\\\\n  'A section about economic policies...',\\\\n  'A section about company growth...',\\\\n  'A section about technology innovations...'\\\\n]\\\\nmodel = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\\\\nquery_emb = model.encode(query)\\\\ndoc_emb = model.encode(docs)\\\\nscores = util.dot_score(query_emb, doc_emb)\\\\ndoc_score_pairs = list(zip(docs, scores))\\\\ndoc_score_pairs.sort(key=lambda x: x[1], reverse=True)\\\\nmost_relevant_segment = doc_score_pairs[0][0]\\\", 'api_call': \\\"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/multi-qa-mpnet-base-cos-v1', 'api_call': \\\"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\\\", 'api_arguments': {'query': 'string', 'docs': 'list of strings'}, 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer, util\\\\nquery = How many people live in London?\\\\ndocs = [Around 9 Million people live in London, London is known for its financial district]\\\\nmodel = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\\\\nquery_emb = model.encode(query)\\\\ndoc_emb = model.encode(docs)\\\\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\\\\ndoc_score_pairs = list(zip(docs, scores))\\\\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\\\\nfor doc, score in doc_score_pairs:\\\\n print(score, doc)\\\", 'performance': {'dataset': '215M (question, answer) pairs from diverse sources', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model that maps sentences and paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ±Ø¬Ù‰ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.? [/INST]{'domain': 'Computer Vision Zero-Shot Image Classification', 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg')\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\nimage_classification = pipeline('image-classification', model='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg')\\\\nimage_path = 'path/to/image.jpg'\\\\nclass_names = 'dog, cat, bird'\\\\nresult = image_classification(image_path, class_names)\\\\nprint(result)\\\", 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg', 'api_call': \\\"from transformers import pipeline\\\\nimage_classification = pipeline('image-classification', model='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg')\\\\nimage_path = 'path/to/image.jpg'\\\\nclass_names = 'dog, cat, bird'\\\\nresult = image_classification(image_path, class_names)\\\\nprint(result)\\\", 'api_arguments': {'image_path': 'Path to the image', 'class_names': 'Comma-separated list of possible class names'}, 'python_environment_requirements': 'transformers', 'example_code': \\\"from transformers import pipeline\\\\nimage_classification = pipeline('image-classification', model='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg')\\\\nimage_path = 'path/to/image.jpg'\\\\nclass_names = 'dog, cat'\\\\nresult = image_classification(image_path, class_names)\\\\nprint(result)\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '70.8-71.7%'}, 'description': 'A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Base model (convnext_base) as the image tower, and the same text tower as the RN50x4 (depth 12, embed dim 640) model from OpenAI CLIP.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙØ§Ø¹Ù„ Ø¨ÙŠÙ† Ø§Ù„Ø¥Ù†Ø³Ø§Ù† ÙˆØ§Ù„Ø­Ø§Ø³ÙˆØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹ÙŠÙ†Ø© ÙˆØ§Ø®ØªØ¨Ø§Ø±Ù‡ Ù„Ø¶Ù…Ø§Ù† ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. First, we import AutoModelForCausalLM and AutoTokenizer from the transformers library for generating responses and tokenizing text respectively.\\\\n2. We then initialize an instance of the tokenizer and model using the specified pre-trained model, 'microsoft/DialoGPT-small', which is designed for dialogue response generation.\\\\n3. For each conversation turn, we encode the user's input text into token ids, append the conversation history, and generate a response by passing the input ids to the model.\\\\n4. The generated response is decoded back into human-readable text and presented as the chatbot's reply.\\\\n5. This process is iterated for multiple conversation turns, allowing the chatbot to engage in a multi-turn conversation about hospital issues.\\\", '<<<code>>>': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'microsoft/DialoGPT-small\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'microsoft/DialoGPT-small\\\\')\\\\nchat_history_ids = None\\\\n\\\\nfor step in range(5):\\\\n    user_input = input(\\\"User: \\\")\\\\n    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\\\\'pt\\\\')\\\\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\\\\n    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n    reply = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\\\\n    print(f\\\"DialoGPT: {reply}\\\")', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/DialoGPT-small', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')\\\", 'api_arguments': ['bot_input_ids', 'max_length', 'pad_token_id'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-small)\\\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-small)\\\\nfor step in range(5):\\\\n new_user_input_ids = tokenizer.encode(input(>> User:) + tokenizer.eos_token, return_tensors='pt')\\\\n bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\\\\n chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n print(DialoGPT: {}.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\\\", 'performance': {'dataset': 'Reddit discussion thread', 'accuracy': 'Comparable to human response quality under a single-turn conversation Turing test'}, 'description': 'DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multiturn conversations. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙˆÙƒØ´Ù Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙ†Ø§ ØªÙˆÙÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„ Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8n-blood-cell-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. In order to detect blood cells in microscope images, we can use the 'keremberke/yolov8n-blood-cell-detection' model from the Hugging Face Model Hub. This model is trained on blood cell object detection and based on the YOLOv8 architecture.\\\\n2. Import the necessary classes from the ultralyticsplus library.\\\\n3. Instantiate the YOLO object for blood cell detection using the provided model name.\\\\n4. Configure the model's parameters, such as confidence threshold, Intersection over Union (IoU) threshold, and maximum detections.\\\\n5. Use the model's predict method to detect blood cells in an input image.\\\\n6. Utilize the render_result function to visualize the detected blood cells on the image.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-blood-cell-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nblood_cell_image = 'https://your_microscope_image_url_here.jpg'\\\\nresults = model.predict(blood_cell_image)\\\\nrender = render_result(model=model, image=blood_cell_image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8n-blood-cell-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8n-blood-cell-detection', 'api_call': \\\"YOLO('keremberke/yolov8n-blood-cell-detection')\\\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-blood-cell-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.893}, 'description': 'This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚Ø¯Ù… Ù„ÙƒÙ… ØªØ·Ø¨ÙŠÙ‚Ù‹Ø§ Ù…Ø¨ØªÙƒØ±Ù‹Ø§ Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ©ØŒ ÙŠØªÙŠØ­ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ ÙˆØ§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª Ø¨Ø³Ù‡ÙˆÙ„Ø©ØŒ Ù…Ø¹ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª ÙˆÙÙ‡Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙˆØªÙŠ Ø¨Ø´ÙƒÙ„ Ø£ÙˆØ³Ø¹ ÙˆØ£Ø¹Ù…Ù‚.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"separator.from_hparams(source='speechbrain/sepformer-whamr-enhancement', savedir='pretrained_models/sepformer-whamr-enhancement')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We'll begin by importing the SepformerSeparation class from the speechbrain pre-trained package, which we'll utilize to separate the desired signals from unwanted noises.\\\\n2. Next, we'll load the pre-trained SepFormer model by invoking the from_hparams method on the SepformerSeparation class, with the model identifier 'speechbrain/sepformer-whamr-enhancement'.\\\\n3. Then, we'll use the separate_file() method of the loaded model to process the input audio file and extract the estimated sources. The result will be an enhanced audio file without the unwanted background noise and reverberation.\\\\n4. Finally, save the enhanced audio file using torchaudio, which can be used for the upgraded conference call system.\\\", '<<<code>>>': \\\"from speechbrain.pretrained import SepformerSeparation as separator\\\\nimport torchaudio\\\\nmodel = separator.from_hparams(source='speechbrain/sepformer-whamr-enhancement', savedir='pretrained_models/sepformer-whamr-enhancement')\\\\nest_sources = model.separate_file(path='input_audio_file.wav')\\\\n# replace 'input_audio_file.wav' with path to your audio file\\\\ntorchaudio.save('enhanced_audio_file.wav', est_sources[:, :, 0].detach().cpu(), 8000)\\\", 'api_call': \\\"separator.from_hparams(source='speechbrain/sepformer-whamr-enhancement', savedir='pretrained_models/sepformer-whamr-enhancement')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'speechbrain/sepformer-whamr-enhancement', 'api_call': \\\"separator.from_hparams(source='speechbrain/sepformer-whamr-enhancement', savedir='pretrained_models/sepformer-whamr-enhancement')\\\", 'api_arguments': {'path': 'Path to the input audio file.'}, 'python_environment_requirements': ['pip install speechbrain'], 'example_code': \\\"from speechbrain.pretrained import SepformerSeparation as separator\\\\nimport torchaudio\\\\nmodel = separator.from_hparams(source=speechbrain/sepformer-whamr-enhancement, savedir='pretrained_models/sepformer-whamr-enhancement')\\\\nest_sources = model.separate_file(path='speechbrain/sepformer-whamr-enhancement/example_whamr.wav')\\\\ntorchaudio.save(enhanced_whamr.wav, est_sources[:, :, 0].detach().cpu(), 8000)\\\", 'performance': {'dataset': 'WHAMR!', 'accuracy': '10.59 dB SI-SNR'}, 'description': 'This repository provides all the necessary tools to perform speech enhancement (denoising + dereverberation) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAMR! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ù‹Ø§ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. ÙŠØ¬Ø¨ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ·ÙŠØ¹ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-russian')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the SpeechRecognitionModel from the huggingsound package, which is a wrapper around Hugging Face Transformers.\\\\n2. Instantiate the model using the pretrained model 'jonatasgrosman/wav2vec2-large-xlsr-53-russian'. This model is specifically tailored for transcribing Russian speech.\\\\n3. You can then use the model to transcribe audio files containing voice messages in Russian. Pass a list of audio file paths to the transcribe() method to get the transcriptions for each file.\\\", '<<<code>>>': \\\"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-russian')\\\\naudio_paths = ['/path/to/russian_voice_message.mp3', '/path/to/another_russian_voice_message.wav']\\\\n# Replace '/path/to/...' with the actual paths to your audio files\\\\ntranscriptions = model.transcribe(audio_paths)\\\", 'api_call': \\\"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-russian')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'jonatasgrosman/wav2vec2-large-xlsr-53-russian', 'api_call': \\\"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-russian')\\\", 'api_arguments': ['audio_paths'], 'python_environment_requirements': [['huggingsound', 'torch', 'librosa', 'datasets', 'transformers']], 'example_code': \\\"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-russian')\\\\naudio_paths = ['/path/to/file.mp3', '/path/to/another_file.wav']\\\\ntranscriptions = model.transcribe(audio_paths)\\\", 'performance': {'dataset': 'mozilla-foundation/common_voice_6_0', 'accuracy': {'Test WER': 13.3, 'Test CER': 2.88, 'Test WER (+LM)': 9.57, 'Test CER (+LM)': 2.24}}, 'description': 'Fine-tuned XLSR-53 large model for speech recognition in Russian. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Russian using the train and validation splits of Common Voice 6.1 and CSS10.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… ÙˆØµÙˆØ± Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1', torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the necessary classes from the diffusers library, including StableDiffusionPipeline and DPMSolverMultistepScheduler.\\\\n2. Load the pre-trained model by calling the from_pretrained method of the StableDiffusionPipeline class and providing the model name \\\\'stabilityai/stable-diffusion-2-1\\\\'. Set the torch_dtype to torch.float16 for faster generation with a slight trade-off in image quality.\\\\n3. Set the scheduler for the pipeline using the DPMSolverMultistepScheduler class that comes with the diffusers package.\\\\n4. Create your text prompt describing the required image, i.e., \\\"a futuristic red dress design\\\".\\\\n5. Use the pipeline with the text prompt to generate the image.\\\\n6. Save the generated image to a file for review and inspiration.', '<<<code>>>': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\\\npipe = StableDiffusionPipeline.from_pretrained(\\\\'stabilityai/stable-diffusion-2-1\\\\', torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to(\\\\'cuda\\\\')\\\\nprompt = \\\"a futuristic red dress design\\\"\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(\\\\'futuristic_red_dress.png\\\\')', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1', torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1', torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors']], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2-1\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…Ù„Ø§Ø¤Ù†Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ¹Ø²ÙŠØ² Ø§Ù„ØªØ¹Ù„Ù… ÙŠØ±ØºØ¨ÙˆÙ† ÙÙŠ ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù„Ù… Ø¢Ù„ÙŠ Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØµÙ…ÙŠÙ… Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª Ø¹Ù„Ù‰ Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø«Ù„Ù‰ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"load_from_hub(repo_id='sb3/ppo-BreakoutNoFrameskip-v4',filename='{MODEL FILENAME}.zip',)\\\", '<<<api_provider>>>': 'Stable-Baselines3', '<<<explanation>>>': '1. First, you need to install the required packages, which are rl_zoo3, stable-baselines3, and stable-baselines3-contrib.\\\\n2. Using the load_from_hub function, we can download the pre-trained PPO agent that has been trained to play the specific game of Breakout without any frames being skipped.\\\\n3. Once the pre-trained agent is loaded, you can run evaluation or enjoy scripts provided by the rl_zoo to evaluate the performance of this trained agent, or visualize the agent playing the game interactively.\\\\n4. This PPO agent has been trained on the BreakoutNoFrameskip-v4 environment and achieves an average score of 398.00 +/- 16.30.', '<<<code>>>': \\\"from rl_zoo3 import load_from_hub\\\\n agent = load_from_hub(repo_id='sb3/ppo-BreakoutNoFrameskip-v4', filename='{MODEL FILENAME}.zip')\\\\n # Replace '{MODEL FILENAME}.zip' with your desired filepath and filename for saving the agent model\\\", 'api_call': \\\"load_from_hub(repo_id='sb3/ppo-BreakoutNoFrameskip-v4',filename='{MODEL FILENAME}.zip',)\\\", 'provider': 'Stable-Baselines3', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ppo-BreakoutNoFrameskip-v4', 'api_call': \\\"load_from_hub(repo_id='sb3/ppo-BreakoutNoFrameskip-v4',filename='{MODEL FILENAME}.zip',)\\\", 'api_arguments': ['algo', 'env', 'orga', 'f'], 'python_environment_requirements': [['rl_zoo3', 'stable-baselines3', 'stable-baselines3-contrib']], 'example_code': ['python -m rl_zoo3.load_from_hub --algo ppo --env BreakoutNoFrameskip-v4 -orga sb3 -f logs/', 'python enjoy.py --algo ppo --env BreakoutNoFrameskip-v4 -f logs/', 'python train.py --algo ppo --env BreakoutNoFrameskip-v4 -f logs/', 'python -m rl_zoo3.push_to_hub --algo ppo --env BreakoutNoFrameskip-v4 -f logs/ -orga sb3'], 'performance': {'dataset': 'BreakoutNoFrameskip-v4', 'accuracy': '398.00 +/- 16.30'}, 'description': 'This is a trained model of a PPO agent playing BreakoutNoFrameskip-v4 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø¥Ù„Ù‰ Ù†Øµ Ø¨ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„Ø¢Ù„ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ ØªØ­ÙˆÙŠÙ„Ù‡ ÙˆØ§Ù†ØªØ¸Ø± Ø¨Ø¶Ø¹ Ø«ÙˆØ§Ù†Ù Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ù†ØªÙŠØ¬Ø©.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\", 'api_provider': 'PyTorch Transformers', 'explanation': '', 'code': \\\"from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/data2vec-audio-base-960h')\\\\nmodel = Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\\ninput_values = processor(political_interview_audio, return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'PyTorch Transformers', 'functionality': 'Automatic Speech Recognition', 'api_name': 'data2vec-audio-base-960h', 'api_call': \\\"from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/data2vec-audio-base-960h')\\\\nmodel = Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\\ninput_values = processor(political_interview_audio, return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers', 'datasets', 'torch', 'jiwer'], 'example_code': 'from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\n# load model and processor\\\\nprocessor = Wav2Vec2Processor.from_pretrained(facebook/data2vec-audio-base-960h)\\\\nmodel = Data2VecForCTC.from_pretrained(facebook/data2vec-audio-base-960h)\\\\n\\\\n# load dummy dataset and read soundfiles\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\n\\\\n# tokenize\\\\ninput_values = processor(ds[0][audio][array],, return_tensors=pt, padding=longest).input_values # Batch size 1\\\\n\\\\n# retrieve logits\\\\nlogits = model(input_values).logits\\\\n\\\\n# take argmax and decode\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'performance': {'dataset': 'librispeech_asr', 'accuracy': {'clean': 2.77, 'other': 7.08}}, 'description': \\\"Facebook's Data2Vec-Audio-Base-960h model is an Automatic Speech Recognition model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It can be used for transcribing audio files and achieves competitive performance on major benchmarks of speech recognition. The model is based on the Data2Vec framework which uses the same learning method for either speech, NLP, or computer vision.\\\"}} </s>\"\n"}
{"text": "<s>[INST]Ø®Ù„Ø§Ù„ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø±Ø¦ÙŠ ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø¨ØµØ±ÙŠ ÙˆØ§Ù„Ù†ØµÙŠ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ù‚Ù… Ø¨ØªØµÙ…ÙŠÙ… Ù†Ù…Ø§Ø°Ø¬ Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚. Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ØªÙ†ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø®ØªÙ„Ù Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from transformers package, which includes ViltForQuestionAnswering for the visual question answering model.\\\\n2. Load the pre-trained model 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering' using the from_pretrained method of the ViltForQuestionAnswering class.\\\\n3. To use the model, ensure you have an image file and a question related to that image.\\\\n4. With the image processed and the question, you can now use the model to answer the question based on the given image.\\\\n5. Note that the performance of the model may strongly depend on the dataset it was trained on.\\\", '<<<code>>>': \\\"from transformers import ViltForQuestionAnswering\\\\nmodel = ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\\\\nimage_path = 'path/to/image/file'\\\\nquestion = 'your_question'\\\\n# Complete the remaining code to process the image and use the model for question answering.\\\", 'api_call': \\\"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering', 'api_call': \\\"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\\\", 'api_arguments': {'image': 'path/to/image/file', 'question': 'your_question'}, 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random model for Visual Question Answering using the VILT framework.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù„Ø§Ù…Ø­ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Feature Extraction', '<<<api_call>>>': \\\"AutoModel.from_pretrained('malteos/scincl')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers library: AutoTokenizer and AutoModel.\\\\n2. To load the model 'malteos/scincl', use the from_pretrained method of AutoModel and AutoTokenizer.\\\\n3. With this trained BERT language model, create a function to generate document-level embeddings for research papers, based on their title and abstract.\\\\n4. These embeddings can then be used as features for the machine learning algorithm to recommend research papers based on their similarity.\\\", '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModel\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\\\n\\\\ndef compute_paper_embeddings(papers):\\\\n    title_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\\\n    inputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors='pt', max_length=512)\\\\n    result = model(**inputs)\\\\n    embeddings = result.last_hidden_state[:, 0, :]\\\\n    return embeddings\\\\n\\\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'}, {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\\\nembeddings = compute_paper_embeddings(papers)\\\", 'api_call': \\\"AutoModel.from_pretrained('malteos/scincl')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'malteos/scincl', 'api_call': \\\"AutoModel.from_pretrained('malteos/scincl')\\\", 'api_arguments': {'tokenizer': \\\"AutoTokenizer.from_pretrained('malteos/scincl')\\\", 'model': \\\"AutoModel.from_pretrained('malteos/scincl')\\\"}, 'python_environment_requirements': [{'transformers': '4.13.0'}], 'example_code': \\\"from transformers import AutoTokenizer, AutoModel\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\\\n\\\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\\\n\\\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\\\n\\\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\\\n\\\\nresult = model(**inputs)\\\\n\\\\nembeddings = result.last_hidden_state[:, 0, :]\\\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>\"\n"}
{"text": "<s>[INST]Ø³ÙŠØªÙ… Ø§Ù„ØªØ¬Ù…Ø¹ ÙÙŠ Ù…Ø®ØªØ¨Ø± Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ø­ÙŠØ« Ø³ÙŠØªØ¹ÙŠÙ† Ø¹Ù„ÙŠÙƒÙ… ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ. Ø¶Ø¹ÙˆØ§ Ø®Ø·Ø© Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø© ÙˆØªÙ‚Ù†ÙŠØ§Øª ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import necessary libraries: ConvNextFeatureExtractor and ConvNextForImageClassification from transformers, torch, and datasets.\\\\n2. Load a dataset containing mobile phone images, such as the 'huggingface/cats-image' dataset used in the example code (replace with a mobile phone images dataset).\\\\n3. Initialize the feature extractor and the ConvNext model for image classification with 'facebook/convnext-base-224' model.\\\\n4. For each image in the dataset, use the feature extractor to convert the image into a tensor, and then forward the tensor through the model to predict the image's category.\\\\n5. Based on the predicted category of the user's favorite images, the recommendation engine will provide personalized discounts for mobile phone products in those categories.\\\", '<<<code>>>': \\\"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset('mobile_phone_images_dataset_name_here')\\\\nimage = dataset['test']['image'][0]\\\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\\\ninputs = feature_extractor(image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/convnext-base-224', 'api_call': \\\"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': [['transformers', 'torch', 'datasets']], 'example_code': \\\"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset('huggingface/cats-image')\\\\nimage = dataset['test']['image'][0]\\\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\\\ninputs = feature_extractor(image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': \\\"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\\\" </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯ÙˆÙ† ØªØµÙ†ÙŠÙ Ù…Ø³Ø¨Ù‚ØŒ ÙŠØ¬Ø¨ Ø¥ØµØ¯Ø§Ø± ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø·Ø§Ù‚. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠØŸ? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary library 'pipeline' from the transformers package provided by Hugging Face.\\\\n2. Use the pipeline function to create an image classification model.\\\\n3. Specify the model 'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft' to be loaded. This model is trained for zero-shot image classification tasks, which makes it perfect for classifying everyday objects in photos.\\\\n4. The classifier takes in the input image and a list of class names. It then ranks the class names based on their likelihood of representing the object present in the image.\\\\n5. Based on the classification results, the app can display the name or information about the object present in the image.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nobject_classifier = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\\\\nimage_path = 'path/to/image.jpg'\\\\nclass_names = ['apple', 'banana', 'car', 'dog', 'building'] # list of possible class names\\\\nclassification_results = object_classifier(image_path, class_names)\\\", 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft', 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\\\", 'api_arguments': {'image_path': 'Path to the image file', 'class_names': 'List of comma-separated class names'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft'); classifier('path/to/image.jpg', ['class1', 'class2'])\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '75.9-76.9%'}, 'description': 'A series of CLIP ConvNeXt-Large models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models achieve between 75.9 and 76.9 top-1 zero-shot accuracy on ImageNet-1k.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ù„ØªÙˆÙ„ÙŠØ¯ ØªØ¹Ù„ÙŠÙ…Ø© ØªØ³Ø£Ù„ Ø¹Ù† ÙƒÙŠÙÙŠØ© Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙÙ‚Ù‹Ø§ Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ù…Ø¹ÙŠÙ†Ø© ÙÙŠ ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ù†ØµØ±ÙŠØ© Ø¨ÙˆØ§Ø³Ø·Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ù…Ø­Ø¯Ø¯Ø©ØŸ? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the necessary classes from the transformers package. This includes T5Tokenizer and T5ForConditionalGeneration.', 'code': \\\"from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\\\\nmodel = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\\\\ninput_text = 'translate English to German: How old are you?'\\\\ninput_ids = tokenizer(input_text, return_tensors='pt').input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))\\\", 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Language model', 'api_name': 'google/flan-t5-base', 'api_call': \\\"from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\\\\nmodel = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\\\\ninput_text = 'translate English to German: How old are you?'\\\\ninput_ids = tokenizer(input_text, return_tensors='pt').input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))\\\", 'api_arguments': 'input_text', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-base)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-base)\\\\ninput_text = translate English to German: How old are you?\\\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))', 'performance': {'dataset': [{'name': 'MMLU', 'accuracy': '75.2%'}]}, 'description': 'FLAN-T5 is a language model fine-tuned on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ù„ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± ØºÙŠØ± Ù…Ø´Ø±ÙˆØ·Ø© Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø§Ø¯Ø±Ù‹Ø§ Ø¹Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙˆØ¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\\nimage = pipeline().images[0]\\\\n# Use and edit image as needed\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'sd-class-pandas-32', 'api_call': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\\nimage = pipeline().images[0]\\\\n# Use and edit image as needed\\\", 'api_arguments': {'pretrained_model': 'schdoel/sd-class-AFHQ-32'}, 'python_environment_requirements': {'package': 'diffusers', 'import': 'from diffusers import DDPMPipeline'}, 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': 'AFHQ', 'accuracy': 'Not provided'}, 'description': 'This model is a diffusion model for unconditional image generation of cute ğŸ¦‹.'}} </s>\"\n"}
{"text": "<s>[INST]Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆØ°Ø¬ ÙØµÙ„ Ø§Ù„ØµÙˆØ±Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"YOLO('keremberke/yolov8n-pothole-segmentation')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the YOLO and render_result classes from the ultralyticsplus package.', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pothole-segmentation', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.995, 'mAP@0.5(mask)': 0.995}}, 'description': 'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªØµÙ…ÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªØ¹ÙˆÙŠØ¶ Ø§Ù„ÙØ¬ÙˆØ© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¬Ù…Ù„ ØºÙŠØ± Ø§Ù„ÙƒØ§Ù…Ù„Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"AutoModelForMaskedLM.from_pretrained('bert-base-chinese')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' function from the transformers library.\\\\n2. Use the 'pipeline' function to load the model 'bert-base-chinese', which is pre-trained for Chinese text and can be used for masked language modeling tasks.\\\\n3. Create a fill-mask pipeline using the loaded model.\\\\n4. Provide the incomplete English sentence with a missing word masked using the '[MASK]' token. The model will predict the best word to complete the sentence.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nfill_mask = pipeline(\\\\'fill-mask\\\\', model=\\\\'bert-base-chinese\\\\')\\\\nenglish_sentence = \\\"The cat is [MASK] on the sofa.\\\"\\\\nbest_word = fill_mask(english_sentence)[0][\\\\'token_str\\\\']', 'api_call': \\\"AutoModelForMaskedLM.from_pretrained('bert-base-chinese')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-base-chinese', 'api_call': \\\"AutoModelForMaskedLM.from_pretrained('bert-base-chinese')\\\", 'api_arguments': {'pretrained_model_name': 'bert-base-chinese'}, 'python_environment_requirements': [{'transformers': 'from transformers import AutoTokenizer, AutoModelForMaskedLM'}], 'example_code': 'tokenizer = AutoTokenizer.from_pretrained(bert-base-chinese)\\\\nmodel = AutoModelForMaskedLM.from_pretrained(bert-base-chinese)', 'performance': {'dataset': '[More Information Needed]', 'accuracy': '[More Information Needed]'}, 'description': 'This model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper). It can be used for masked language modeling.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… Ø§Ù„Ø¨Ø§Ø­Ø«ÙˆÙ† ÙÙŠ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø¨ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø£ÙˆØ³Ø§Ø· Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ø¨Ø¯ÙˆÙ† Ù‚ÙŠÙˆØ¯ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ø¯Ø±Ø© Ø§Ù„ÙØ¹Ø§Ù„Ø© Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØºÙŠØ±Ø§Øª ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©.? [/INST]'<<<domain>>>': 'Multimodal Graph Machine Learning', '<<<api_call>>>': \\\"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary classes from the transformers package. Specifically, we need the AutoModel class to load the Graphormer model.\\\\n2. We use the from_pretrained method of the AutoModel class to load the pre-trained model 'clefourrier/graphormer-base-pcqm4mv2'. This Graphormer model has been pretrained on the PCQM4M-LSCv2 dataset and is designed specifically for graph classification tasks.\\\\n3. Process the molecule data to create a graph representation that can be input into the Graphormer model.\\\\n4. The model can be used to classify molecules based on their properties using machine learning.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\\\\n# Process molecule data into a graph representation\\\\n# Then, use the model for classification based on molecular properties\\\", 'api_call': \\\"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'graphormer-base-pcqm4mv2', 'api_call': \\\"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\\\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': ['transformers'], 'example_code': 'See the Graph Classification with Transformers tutorial.', 'performance': {'dataset': 'PCQM4M-LSCv2', 'accuracy': 'Not provided'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù…Ù†ØªØ¬ Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨ØªØ±Ø¬Ù…Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries from the transformers package, which is 'pipeline' in this case.\\\\n2. Use the 'pipeline' function from the transformers library to create a translation model, and load the pre-trained model 'Helsinki-NLP/opus-mt-en-ar' for English to Arabic translation.\\\\n3. The created translator can then be used to translate any given English text into Arabic.\\\\n4. To achieve this, simply call the translator function with the English text input, and it will return the translated Arabic text.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntranslation = pipeline(\\\\'translation_en_to_ar\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-ar\\\\')\\\\nenglish_text = \\\"We are excited to work with you.\\\"\\\\ntranslated_text = translation(english_text)', 'api_call': \\\"pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-en-ar', 'api_call': \\\"pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\\ntranslated_text = translation('Hello World')\\\", 'performance': {'dataset': 'Tatoeba-test.eng.ara', 'accuracy': {'BLEU': 14.0, 'chr-F': 0.437}}, 'description': \\\"A Hugging Face Transformers model for English to Arabic translation, trained on the Tatoeba dataset. It uses a transformer architecture and requires a sentence initial language token in the form of '>>id<<' (id = valid target language ID).\\\" </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ ØªØµÙ…ÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙˆÙÙŠØ± ØªÙ‚Ø¯ÙŠØ± Ø¯Ù‚ÙŠÙ‚ Ù„Ø¹Ù…Ù‚ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø­ØªÙˆØ§Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡Ø§.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\\\\nmodel = AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\\\\n\\\\ndef get_sentence_embeddings(sentences):\\\\n    encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\\\n    with torch.no_grad():\\\\n        model_output = model(**encoded_input)\\\\n    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\\\\n    return sentence_embeddings\\\\n\\\\nsentences = ['list', 'of', 'sentences', 'in', 'Russian']\\\\nsentence_embeddings = get_sentence_embeddings(sentences)\\\", 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \\\"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\\\\nmodel = AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\\\\n\\\\ndef get_sentence_embeddings(sentences):\\\\n    encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\\\n    with torch.no_grad():\\\\n        model_output = model(**encoded_input)\\\\n    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\\\\n    return sentence_embeddings\\\\n\\\\nsentences = ['list', 'of', 'sentences', 'in', 'Russian']\\\\nsentence_embeddings = get_sentence_embeddings(sentences)\\\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \\\"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\n\\\\n# Mean Pooling - Take attention mask into account for correct averaging\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\n\\\\n# Sentences we want sentence embeddings for sentences = ['?']\\\\n\\\\n# Load AutoModel from huggingface model repository\\\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\n\\\\n# Tokenize sentences\\\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\\\n\\\\n# Compute token embeddings\\\\nwith torch.no_grad():\\\\n    model_output = model(**encoded_input)\\\\n\\\\n# Perform pooling. In this case, mean pooling\\\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\\\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.'}} </s>\"\n"}
{"text": "<s>[INST]Ø³ÙŠÙ‚ÙˆÙ… ÙØ±ÙŠÙ‚ Ø§Ù„Ø¨Ø­Ø« Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…Ø²ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­Ø¯ÙŠØ¯ ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a Part-of-Speech (POS) tagging model, which assigns a POS tag to each token in a sentence.\\\\n3. We specify the model 'ckiplab/bert-base-chinese-pos', which has been pretrained on Chinese text, and the tokenizer 'bert-base-chinese' to tokenize the input text into individual words.\\\\n4. The created pipeline will be used to apply POS tagging to the given Chinese text.\\\\n5. The output will be a list of tuples, where each tuple contains a word and its corresponding POS tag.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\npos_tagger = pipeline('pos-tagging', model='ckiplab/bert-base-chinese-pos', tokenizer='bert-base-chinese')\\\\ntext = 'ä½ å¥½ï¼Œæˆ‘æ˜¯ä¸€åç¨‹åºå‘˜ã€‚'\\\\npos_tags = pos_tagger(text)\\\", 'api_call': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ckiplab/bert-base-chinese-pos', 'api_call': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'api_arguments': {'tokenizer': \\\"BertTokenizerFast.from_pretrained('bert-base-chinese')\\\"}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import (\\\\n  BertTokenizerFast,\\\\n  AutoModel,\\\\n)\\\\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\\\\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).' </s>\"\n"}
{"text": "<s>[INST]Ø§ØµÙ†Ø¹ Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ø¬Ø¯ÙŠØ¯Ù‹Ø§ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯ÙˆÙ† ØªØ³Ù…ÙŠØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ØªØ³Ù…ÙŠØ§Øª Ø°Ø§Øª Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©. Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ Ù„Ù… ØªÙØ±ÙØ³ÙÙ… Ø³Ø§Ø¨Ù‚Ù‹Ø§ Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø¬Ø§Ù„.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We first import the pipeline function from the transformers package.\\\\n2. We then create a pipeline for image classification using the 'laion/CLIP-convnext_base_w-laion2B-s13B-b82K' model. This model has been trained for zero-shot image classification tasks, which is exactly what we need for categorizing the user's profile photo as a cat or a dog.\\\\n3. We then provide the path to the user's uploaded image and a list of possible class names (['cat', 'dog']) as input to the pipeline. The pipeline will perform zero-shot image classification and return the most likely class for the input image.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nclip = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\\\\nresult = clip('/path/to/image.jpg', ['cat', 'dog'])\\\\nprediction = result[0]['label']\\\", 'api_call': \\\"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': \\\"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\\\", 'api_call': \\\"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\\\", 'api_arguments': {'image_path': 'path to the image file', 'labels': 'list of possible class names'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; clip = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion2B-s13B-b82K'); clip('path/to/image.jpg', ['cat', 'dog'])\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '70.8 - 71.7%'}, 'description': 'A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø¥Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ø­Ù„Ù„Ø©.? [/INST]{'domain': 'Natural Language Processing Token Classification', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Babelscape/wikineural-multilingual-ner\\\\')\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'Babelscape/wikineural-multilingual-ner\\\\')\\\\nner_pipeline = pipeline(\\\\'ner\\\\', model=model, tokenizer=tokenizer)\\\\nreview = \\\"My name is Wolfgang and I live in Berlin.\\\"\\\\nner_results = ner_pipeline(review)\\\\nprint(ner_results)', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Named Entity Recognition', 'api_name': 'Babelscape/wikineural-multilingual-ner', 'api_call': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Babelscape/wikineural-multilingual-ner\\\\')\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'Babelscape/wikineural-multilingual-ner\\\\')\\\\nner_pipeline = pipeline(\\\\'ner\\\\', model=model, tokenizer=tokenizer)\\\\nreview = \\\"My name is Wolfgang and I live in Berlin.\\\"\\\\nner_results = ner_pipeline(review)\\\\nprint(ner_results)', 'api_arguments': {'model': 'AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)', 'tokenizer': 'AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)'}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline'}, 'example_code': 'tokenizer = AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\\\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\\\nexample = My name is Wolfgang and I live in Berlin\\\\nner_results = nlp(example)\\\\nprint(ner_results)', 'performance': {'dataset': 'Babelscape/wikineural-multilingual-ner', 'accuracy': 'span-based F1-score up to 6 points over previous state-of-the-art systems for data creation'}, 'description': 'A multilingual Named Entity Recognition (NER) model fine-tuned on the WikiNEuRal dataset, supporting 9 languages (de, en, es, fr, it, nl, pl, pt, ru). It is based on the mBERT architecture and trained on all 9 languages jointly. The model can be used with the Hugging Face Transformers pipeline for NER tasks.'}} </s>\"\n"}
{"text": "<s>[INST]Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…Ø· Ø¬Ø¯ÙŠØ¯ Ù„Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ø¨Ø¯ÙˆÙ† Ø´Ø±ÙˆØ·. Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ¨ÙŠØ±Ø© Ù„ØªØµÙ…ÙŠÙ… ØµÙˆØ± ØºÙŠØ± Ù…Ø³Ø¨ÙˆÙ‚Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the diffusers package using pip.\\\\n2. Import the DDPMPipeline class from the diffusers library.\\\\n3. Use the from_pretrained() method with the model identifier 'google/ddpm-bedroom-256' to load a trained image synthesis model, specifically designed for generating images of bedrooms.\\\\n4. Generate an image by calling the loaded model. The image will be a high-quality, unconditional generation based on the model's knowledge of bedroom environments. This can be used as an asset in the virtual reality game environment.\\\\n5. Save the generated image to a file using the save method of the generated image object.\\\", '<<<code>>>': \\\"!pip install diffusers\\\\nfrom diffusers import DDPMPipeline\\\\nmodel_id = 'google/ddpm-bedroom-256'\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\ngenerated_image = ddpm().images[0]\\\\ngenerated_image.save('ddpm_generated_image.png')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/ddpm-bedroom-256', 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\", 'api_arguments': 'None', 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†Ù…ÙˆØ°Ø¬ ÙŠØªÙ…ÙƒÙ† Ù…Ù† ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø§Ø³ØªÙ†Ø§Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]{'domain': 'Multimodal Text-to-Image', 'api_call': \\\"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\\", 'api_provider': 'Hugging Face', 'explanation': '1. Import the necessary classes from the diffusers package. This includes StableDiffusionInpaintPipeline for the text-to-image model.', 'code': 'from diffusers import StableDiffusionInpaintPipeline\\\\nimport torch\\\\npipe = StableDiffusionInpaintPipeline.from_pretrained(\\\\'stabilityai/stable-diffusion-2-inpainting\\\\', torch_dtype=torch.float16)\\\\nprompt = \\\"happy birthday with a cake and balloons\\\"\\\\nimage = pipe(prompt=prompt).images[0]\\\\nimage.save(\\\\'greeting_card.png\\\\')', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': 'from diffusers import StableDiffusionInpaintPipeline\\\\nimport torch\\\\npipe = StableDiffusionInpaintPipeline.from_pretrained(\\\\'stabilityai/stable-diffusion-2-inpainting\\\\', torch_dtype=torch.float16)\\\\nprompt = \\\"happy birthday with a cake and balloons\\\"\\\\nimage = pipe(prompt=prompt).images[0]\\\\nimage.save(\\\\'greeting_card.png\\\\')', 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \\\"from diffusers import StableDiffusionInpaintPipeline\\\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\\\n\\\\npipe.to(cuda)\\\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\\\nimage.save(./yellow_cat_on_park_bench.png)\\\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ ÙÙŠØ¯ÙŠÙˆ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ ÙˆØªÙ‡Ø¯Ù Ø¥Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù…ØªØ®Ù„ÙØ© Ø¨ÙˆØ¶ÙˆØ­ ÙˆØ¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©.? [/INST]{'domain': 'Multimodal Text-to-Video', 'api_call': \\\"BaseModel.from_pretrained('duncan93/video')\\\", 'api_provider': 'Hugging Face', 'explanation': '1. First, you need to install the required package, Asteroid, and import the necessary libraries.', 'code': \\\"!pip install Asteroid\\\\nfrom asteroid import BaseModel\\\\nmodel = BaseModel.from_pretrained('duncan93/video')\\\\nframes = model.generate_video_frames(text_description)\\\", 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'duncan93/video', 'api_call': \\\"!pip install Asteroid\\\\nfrom asteroid import BaseModel\\\\nmodel = BaseModel.from_pretrained('duncan93/video')\\\\nframes = model.generate_video_frames(text_description)\\\", 'api_arguments': '', 'python_environment_requirements': 'Asteroid', 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"pipeline('summarization', model='it5/it5-base-news-summarization')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"pipeline('summarization', model='it5/it5-base-news-summarization')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'it5-base-news-summarization', 'api_call': \\\"pipeline('summarization', model='it5/it5-base-news-summarization')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': 'newsum(Dal 31 maggio Ã¨ infine partita la piattaforma ITsART, a piÃ¹ di un anno da quando â€“ durante il primo lockdown â€“ il ministro della Cultura Dario Franceschini ne aveva parlato come di Â«una sorta di Netflix della culturaÂ», pensata per Â«offrire a tutto il mondo la cultura italiana a pagamentoÂ». Ãˆ presto per dare giudizi definitivi sulla piattaforma, e di certo sarÃ  difficile farlo anche piÃ¹ avanti senza numeri precisi. Al momento, lâ€™unica cosa che si puÃ² fare Ã¨ guardare comâ€™Ã¨ fatto il sito, contare quanti contenuti ci sono (circa 700 â€œtitoliâ€, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro varietÃ . Intanto, una cosa notata da piÃ¹ parti Ã¨ che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.)', 'performance': {'dataset': 'NewsSum-IT', 'accuracy': {'Rouge1': 0.339, 'Rouge2': 0.16, 'RougeL': 0.263}}, 'description': 'IT5 Base model fine-tuned on news summarization on the Fanpage and Il Post corpora for Italian Language Understanding and Generation.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ ØªØ·Ø¨ÙŠÙ‚ Audio Text-to-Speech Ø§Ù„Ø°ÙŠÙ† ÙŠØ¹Ø§Ù†ÙˆÙ† Ù…Ù† Ø¥Ø¹Ø§Ù‚Ø© Ø¨ØµØ±ÙŠØ©. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø±Ù‹Ø§ Ø¹Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ ØªØµÙ†ÙŠÙØ§Øª Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TT')\\\", 'api_provider': 'Fairseq', 'explanation': '1. Import the necessary libraries and functions needed, such as hub_utils, load_model_ensemble_and_task_from_hf_hub, CodeHiFiGANVocoder, VocoderHubInterface, etc., from fairseq and torchaudio libraries.', 'code': 'import json\\\\nimport os\\\\nfrom pathlib import Path\\\\nimport IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\n\\\\ncache_dir = os.getenv(\\\\'HUGGINGFACE_HUB_CACHE\\\\')\\\\nlibrary_name = \\\\'fairseq\\\\'\\\\ncache_dir = cache_dir or (Path.home() / \\\\'.cache\\\\' / library_name).as_posix()\\\\ncache_dir = snapshot_download(\\\\'facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS\\\\', cache_dir=cache_dir, library_name=library_name)\\\\n\\\\nx = hub_utils.from_pretrained(cache_dir, \\\\'model.pt\\\\', \\\\'.\\\\', archive_map=CodeHiFiGANVocoder.hub_models(), config_yaml=\\\\'config.json\\\\', fp16=False, is_vocoder=True)\\\\n\\\\nwith open(f\\\"{x[\\\\'args\\\\'][\\\\'data\\\\']}/config.json\\\") as f:\\\\n    vocoder_cfg = json.load(f)\\\\n\\\\nassert len(x[\\\\'args\\\\'][\\\\'model_path\\\\']) == 1, \\\"Too many vocoder models in the input\\\"\\\\nvocoder = CodeHiFiGANVocoder(x[\\\\'args\\\\'][\\\\'model_path\\\\'][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(\\\\'TEXT_INPUT_HERE\\\\')  # Replace \\\\'TEXT_INPUT_HERE\\\\' with your input text\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\nipd.Audio(wav, rate=sr)', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Fairseq', 'functionality': 'Text-to-Speech', 'api_name': 'unit_hifigan_HK_layer12.km2500_frame_TAT-TTS', 'api_call': 'import json\\\\nimport os\\\\nfrom pathlib import Path\\\\nimport IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\n\\\\ncache_dir = os.getenv(\\\\'HUGGINGFACE_HUB_CACHE\\\\')\\\\nlibrary_name = \\\\'fairseq\\\\'\\\\ncache_dir = cache_dir or (Path.home() / \\\\'.cache\\\\' / library_name).as_posix()\\\\ncache_dir = snapshot_download(\\\\'facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS\\\\', cache_dir=cache_dir, library_name=library_name)\\\\n\\\\nx = hub_utils.from_pretrained(cache_dir, \\\\'model.pt\\\\', \\\\'.\\\\', archive_map=CodeHiFiGANVocoder.hub_models(), config_yaml=\\\\'config.json\\\\', fp16=False, is_vocoder=True)\\\\n\\\\nwith open(f\\\"{x[\\\\'args\\\\'][\\\\'data\\\\']}/config.json\\\") as f:\\\\n    vocoder_cfg = json.load(f)\\\\n\\\\nassert len(x[\\\\'args\\\\'][\\\\'model_path\\\\']) == 1, \\\"Too many vocoder models in the input\\\"\\\\nvocoder = CodeHiFiGANVocoder(x[\\\\'args\\\\'][\\\\'model_path\\\\'][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(\\\\'TEXT_INPUT_HERE\\\\')  # Replace \\\\'TEXT_INPUT_HERE\\\\' with your input text\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\nipd.Audio(wav, rate=sr)', 'api_arguments': {'unit': 'Text input for the TTS model'}, 'python_environment_requirements': ['fairseq', 'huggingface_hub', 'torchaudio'], 'example_code': \\\"import json\\\\nimport os\\\\nfrom pathlib import Path\\\\nimport IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\\\\nlibrary_name = fairseq\\\\ncache_dir = (\\\\n cache_dir or (Path.home() / .cache / library_name).as_posix()\\\\n)\\\\ncache_dir = snapshot_download(\\\\n ffacebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS, cache_dir=cache_dir, library_name=library_name\\\\n)\\\\nx = hub_utils.from_pretrained(\\\\n cache_dir,\\\\n model.pt,\\\\n .,\\\\n archive_map=CodeHiFiGANVocoder.hub_models(),\\\\n config_yaml=config.json,\\\\n fp16=False,\\\\n is_vocoder=True,\\\\n)\\\\nwith open(f{x['args']['data']}/config.json) as f:\\\\n vocoder_cfg = json.load(f)\\\\nassert (\\\\n len(x[args][model_path]) == 1\\\\n), Too many vocoder models in the input\\\\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\nipd.Audio(wav, rate=sr)\\\", 'performance': {'dataset': 'TAT-TTS', 'accuracy': 'Not provided'}, 'description': 'Hokkien unit HiFiGAN based vocoder from fairseq. Trained with TAT-TTS data with 4 speakers in Taiwanese Hokkien accent.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­ÙˆØ§Ø³ÙŠØ¨ Ù„ØªØµÙ†ÙŠÙÙ‡Ø§. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆØŸ? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-ssv2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary classes and packages from the transformers library. This includes VideoMAEForVideoClassification for the video classification model and VideoMAEImageProcessor to pre-process video frames.\\\\n2. We load the pre-trained video classification model 'MCG-NJU/videomae-base-finetuned-ssv2', which is a model that has been trained on a large collection of categorized video data.\\\\n3. We capture a video of a person dancing and pre-process the video frames using the VideoMAEImageProcessor.\\\\n4. With the processed video frames, we can use the VideoMAEForVideoClassification model to predict the type of dance style being performed.\\\", '<<<code>>>': 'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport cv2\\\\nimport numpy as np\\\\nvideo_file = \\\\'path_to_video_file.mp4\\\\'\\\\nvideo = cv2.VideoCapture(video_file)\\\\nframes = extract_frames_from_video(video)  # create a function to extract frames\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(\\\\'MCG-NJU/videomae-base-finetuned-ssv2\\\\')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(\\\\'MCG-NJU/videomae-base-finetuned-ssv2\\\\')\\\\ninputs = processor(frames, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-ssv2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'MCG-NJU/videomae-base-finetuned-ssv2', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-ssv2')\\\", 'api_arguments': 'video', 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Something-Something-v2', 'accuracy': {'top-1': 70.6, 'top-5': 92.6}}, 'description': 'VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù‚Ø¯Ø±ØªÙ‡Ø§ Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„ØµÙˆØ± Ø¨Ø¯Ù‚Ø© ÙˆØ¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To achieve this task, we can use the pre-trained 'julien-c/hotdog-not-hotdog' model from Hugging Face Transformers, which is capable of classifying images as hotdog or not hotdog.\\\\n2. We first import the necessary library, specifically the 'pipeline' function from the transformers package.\\\\n3. Then, we create a pipeline for image classification using the pre-trained 'julien-c/hotdog-not-hotdog' model, which can classify images into hotdog or not hotdog.\\\\n4. Users can snap a picture of the food items at the food festival, and the model will categorize the images by whether they contain hotdogs or other types of foods.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nclassifier = pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\\\", 'api_call': \\\"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'julien-c/hotdog-not-hotdog', 'api_call': \\\"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\\\", 'api_arguments': 'image', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': 0.825}, 'description': 'A model that classifies images as hotdog or not hotdog.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ù‹Ø§ Ù„Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù‚Ù†ÙˆØ§Øª. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø­Ø±Ù†Ø§Ù…Ø¬ ÙŠØªØ­ÙˆÙ‘Ù„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Video', '<<<api_call>>>': \\\"pipeline('text-to-video-synthesis', model_dir.as_posix())\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. We first import the necessary classes and modules from the respective packages.\\\\n2. We download the pre-trained model \\\\'damo-vilab/modelscope-damo-text-to-video-synthesis\\\\' from Hugging Face Hub.\\\\n3. We then create a pipeline instance for \\\\'text-to-video-synthesis\\\\' using the local model snapshot, which is suitable for generating a video from a textual description.\\\\n4. We then provide the input description, which is your desired text: \\\"A person enjoying a coffee while reading a book at a cozy coffee shop.\\\"\\\\n5. Finally, the model generates the video file that matches the provided description.', '<<<code>>>': \\\"from huggingface_hub import snapshot_download\\\\nfrom modelscope.pipelines import pipeline\\\\nfrom modelscope.outputs import OutputKeys\\\\nimport pathlib\\\\n\\\\nmodel_dir = pathlib.Path('weights')\\\\nsnapshot_download('damo-vilab/modelscope-damo-text-to-video-synthesis', repo_type='model', local_dir=model_dir)\\\\npipe = pipeline('text-to-video-synthesis', model_dir.as_posix())\\\\n\\\\ntest_text = {\\\\n 'text': 'A person enjoying a coffee while reading a book at a cozy coffee shop.',\\\\n}\\\\noutput_video_path = pipe(test_text)[OutputKeys.OUTPUT_VIDEO]\\\", 'api_call': \\\"pipeline('text-to-video-synthesis', model_dir.as_posix())\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'modelscope-damo-text-to-video-synthesis', 'api_call': \\\"pipeline('text-to-video-synthesis', model_dir.as_posix())\\\", 'api_arguments': {'text': 'A short text description in English'}, 'python_environment_requirements': [['modelscope==1.4.2', 'open_clip_torch', 'pytorch-lightning']], 'example_code': \\\"from huggingface_hub import snapshot_download\\\\nfrom modelscope.pipelines import pipeline\\\\nfrom modelscope.outputs import OutputKeys\\\\nimport pathlib\\\\n\\\\nmodel_dir = pathlib.Path('weights')\\\\nsnapshot_download('damo-vilab/modelscope-damo-text-to-video-synthesis',\\\\n repo_type='model', local_dir=model_dir)\\\\n\\\\npipe = pipeline('text-to-video-synthesis', model_dir.as_posix())\\\\n\\\\ntest_text = {\\\\n 'text': 'A panda eating bamboo on a rock.',\\\\n}\\\\n\\\\noutput_video_path = pipe(test_text,)[OutputKeys.OUTPUT_VIDEO]\\\\nprint('output_video_path:', output_video_path)\\\", 'performance': {'dataset': 'Webvid, ImageNet, LAION5B', 'accuracy': 'Not provided'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù…ÙˆØ¯ÙŠÙ„ Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØªÙŠ ÙÙŠ Ø§Ù„ØµÙˆØªÙŠØ§ØªØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ ØªÙ…ÙŠÙŠØ² Ø¨ÙŠÙ† Ø§Ù„ØµÙˆØª Ø§Ù„Ù†Ø´Ø· ÙˆØ§Ù„ØµÙˆØª ØºÙŠØ± Ø§Ù„Ù†Ø´Ø· ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù„Ø¯ÙŠÙ†Ø§.? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Begin by importing the Model class from the pyannote.audio library.\\\\n2. Load the pretrained model 'popcornell/pyannote-segmentation-chime6-mixer6' using the Model.from_pretrained() function. This model is fine-tuned on data from the CHiME-7 DASR Challenge and is designed for diarization (detecting and distinguishing different speakers) in noisy environments. \\\\n3. Once loaded, this model can be used to detect and distinguish voices in audio samples captured by the AI-based doorbell in noisy situations.\\\", '<<<code>>>': \\\"from pyannote.audio import Model\\\\nvoice_activity_detection_model = Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", 'api_call': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'popcornell/pyannote-segmentation-chime6-mixer6', 'api_call': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['pyannote.audio 2.1.1'], 'example_code': 'from pyannote.audio import Model\\\\nmodel = Model.from_pretrained(popcornell/pyannote-segmentation-chime6-mixer6)', 'performance': {'dataset': 'ami', 'accuracy': 'N/A'}, 'description': 'Pyannote Segmentation model fine-tuned on data from CHiME-7 DASR Challenge. Used to perform diarization in the CHiME-7 DASR diarization baseline.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØªØ¹Ù„ÙŠÙ…Ø© ØªÙØµÙŠÙ„ÙŠØ© Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙÙŠ Ø§Ù„ØµÙ†Ù Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Zero-Shot Classification. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯ Ø¯Ù‚ÙŠÙ‚Ù‹Ø§ ÙˆÙ…Ù„Ø§Ø¦Ù…Ù‹Ø§ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯Ù‚Ø© Ù…ØªÙ†Ø§Ù‡ÙŠØ© Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"CrossEncoder('cross-encoder/nli-distilroberta-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include 'CrossEncoder' from 'sentence_transformers'.\\\\n2. Instantiate the 'CrossEncoder' object, specifying the model 'cross-encoder/nli-distilroberta-base'. This model is trained for natural language inference tasks and will generate scores for contradictions, entailments, and neutral relations between sentence pairs.\\\\n3. Provide pairs of sentences, such as news headlines from the tech industry, to the model, which will predict the logical relationship between them.\\\", '<<<code>>>': \\\"from sentence_transformers import CrossEncoder\\\\ncross_encoder = CrossEncoder('cross-encoder/nli-distilroberta-base')\\\\nsentence1 = 'Apple announces new iPhone 13 with advanced features'\\\\nsentence2 = 'Apple unveils its latest smartphone'\\\\nscores = cross_encoder.predict([(sentence1, sentence2)])\\\\nrelationship = ['contradiction', 'entailment', 'neutral'][scores.argmax()]\\\\nprint(relationship)\\\", 'api_call': \\\"CrossEncoder('cross-encoder/nli-distilroberta-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cross-encoder/nli-distilroberta-base', 'api_call': \\\"CrossEncoder('cross-encoder/nli-distilroberta-base')\\\", 'api_arguments': \\\"('A man is eating pizza', 'A man eats something')\\\", 'python_environment_requirements': [['sentence_transformers', 'transformers']], 'example_code': \\\"from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model='cross-encoder/nli-distilroberta-base')\\\\nsent = Apple just announced the newest iPhone X\\\\ncandidate_labels = [technology, sports, politics]\\\\nres = classifier(sent, candidate_labels)\\\\nprint(res)\\\", 'performance': {'dataset': 'SNLI and MultiNLI', 'accuracy': 'See SBERT.net - Pretrained Cross-Encoder for evaluation results'}, 'description': 'This model was trained using SentenceTransformers Cross-Encoder class on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"pipeline('text-generation', model='facebook/opt-6.7b')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. Import the necessary libraries, which include 'pipeline' from the transformers package.\\\", 'code': 'from transformers import pipeline\\\\ntext_gen = pipeline(\\\\'text-generation\\\\', model=\\\\'facebook/opt-6.7b\\\\')\\\\nprompt = \\\"The future of technology in education is\\\"\\\\ngenerated_text = text_gen(prompt, max_length=50, num_return_sequences=1)[0][\\\\'generated_text\\\\']', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Transformers', 'functionality': 'Text Generation', 'api_name': 'facebook/opt-6.7b', 'api_call': 'from transformers import pipeline\\\\ntext_gen = pipeline(\\\\'text-generation\\\\', model=\\\\'facebook/opt-6.7b\\\\')\\\\nprompt = \\\"The future of technology in education is\\\"\\\\ngenerated_text = text_gen(prompt, max_length=50, num_return_sequences=1)[0][\\\\'generated_text\\\\']', 'api_arguments': ['torch_dtype'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained('facebook/opt-6.7b', torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained('facebook/opt-6.7b', use_fast=False)\\\\nprompt = Hello, I'm am conscious and\\\\ninput_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\\\\ngenerated_ids = model.generate(input_ids)\\\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)\\\", 'performance': {'dataset': {'BookCorpus': 'unknown', 'CC-Stories': 'unknown', 'The Pile': 'unknown', 'Pushshift.io Reddit': 'unknown', 'CCNewsV2': 'unknown'}, 'accuracy': 'unknown'}, 'description': 'OPT (Open Pre-trained Transformer Language Models) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. It was trained on a large corpus of text, predominantly in English, using a causal language modeling (CLM) objective. The model can be used for prompting for evaluation of downstream tasks, text generation, and fine-tuning on a downstream task using the CLM example.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¨Ø±Ù…Ø¬ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø¥ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ø¬Ø³Ø§Ù… ÙÙŠ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ØŒ Ø¹Ù„ÙŠÙ‡ Ø£Ù† ÙŠØ¶Ù…Ù† Ø£Ù† Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ÙØ­ÙˆØµØ© ØªÙ„Ø¨ÙŠ Ø§Ù„Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª ÙˆØ§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ØŒ ÙˆØªÙˆÙØ± Ù„Ù‡Ù… ØªØ¬Ø±Ø¨Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø³Ù„Ø³Ø© ÙˆÙ…Ø±ÙŠØ­Ø©.? [/INST]{'domain': 'Computer Vision Object Detection', 'api_call': \\\"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Start by importing the required libraries: DetrForObjectDetection and DetrImageProcessor from transformers, along with Image from PIL, and requests for fetching the image.', 'code': '', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-50', 'api_call': '', 'api_arguments': {'pretrained_model_name': 'facebook/detr-resnet-50'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\\\nimport torch\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-50)\\\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-50)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '42.0 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù…Ø¬ÙŠØ© ØªØ³ØªØ®Ø¯Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©. Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØªØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ­ÙˆÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Start by importing the necessary classes from the transformers package, which include AutoModelForTokenClassification and AutoTokenizer.\\\\n2. Load the pre-trained model 'ismail-lucifer011/autotrain-name_all-904029577' by using the from_pretrained method, and specify the use_auth_token parameter as True for authentication.\\\\n3. Also load the tokenizer associated with this model using the from_pretrained method.\\\\n4. Tokenize the input text document with the tokenizer, setting the return_tensors parameter to 'pt' for PyTorch tensors.\\\\n5. Pass the tokenized inputs to the pre-trained model's forward pass for entity extraction.\\\\n6. Process the model's output to extract and present the entities found in the input text.\\\", '<<<code>>>': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'ismail-lucifer011/autotrain-name_all-904029577\\\\', use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'ismail-lucifer011/autotrain-name_all-904029577\\\\', use_auth_token=True)\\\\ninputs = tokenizer(\\\"I love AutoTrain\\\", return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nentities = process_output(outputs)', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': '904029577', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\\\", 'api_arguments': {'inputs': 'I love AutoTrain'}, 'python_environment_requirements': [{'transformers': 'AutoModelForTokenClassification', 'tokenizer': 'AutoTokenizer'}], 'example_code': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-name_all-904029577, use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-name_all-904029577, use_auth_token=True)\\\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\\\noutputs = model(**inputs)', 'performance': {'dataset': 'ismail-lucifer011/autotrain-data-name_all', 'accuracy': 0.9989316041363876}, 'description': 'This model is trained using AutoTrain for entity extraction. It is based on the DistilBert architecture and has a CO2 Emissions of 0.8375653425894861 grams.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ Ø¨Ø¯Ù‚Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We start by importing the necessary package, which is AutoModel from the transformers library provided by Hugging Face.\\\\n2. We use the from_pretrained method of the AutoModel class to load the pre-trained model 'sayakpaul/glpn-nyu-finetuned-diode-230103-091356'. This model has been trained on the diode_subset dataset and is suitable for estimating depth in computer vision tasks.\\\\n3. Once the model is loaded, it can be used to perform depth estimation on images of objects in the room. The model will return a depth map, which can help the robot safely maneuver through the room by identifying objects and distances.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\\\\n\\\\n# Perform depth estimation on an input image\\\\n# depth_map = model(input_image)\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-230103-091356', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers==4.24.0, pytorch==1.12.1+cu116, datasets==2.8.0, tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.436, 'Mae': 0.4251, 'Rmse': 0.6169, 'Abs Rel': 0.45, 'Log Mae': 0.1721, 'Log Rmse': 0.2269, 'Delta1': 0.3828, 'Delta2': 0.6326, 'Delta3': 0.8051}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ø£Ø³Ø§Ø³ÙŠ Ù„ØªØµÙ†ÙŠÙ Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø¯ÙˆÙ† Ù…Ø¹Ø§ÙŠÙ†Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆØ§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø´Ø§Ù…Ù„Ø© ÙˆÙ…Ø¶Ø¨ÙˆØ·Ø©. Ù‚Ù… Ø¨ØªØ­Ø¯ÙŠØ¯ ÙØ¦Ø§Øª Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù„ØªØµÙ†ÙŠÙ Ø«Ù… Ø§Ø³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø§Ù„Ù„Ø§Ø­Ù‚ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k400')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We need to import the necessary libraries such as transformers, numpy, and torch.\\\\n2. The pre-trained 'facebook/timesformer-hr-finetuned-k400' model should be loaded using the 'TimesformerForVideoClassification.from_pretrained' function. This model is designed for video classification tasks and has been trained on the Kinetics-400 dataset, which contains a wide range of human actions.\\\\n3. Load the gym video and preprocess the video frames.\\\\n4. Pass the video frames to the model, which will classify the type of activity in the video.\\\\n5. Extract the predicted action from the model output.\\\", '<<<code>>>': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo_processor = AutoImageProcessor.from_pretrained(\\\\'facebook/timesformer-hr-finetuned-k400\\\\')\\\\nmodel = TimesformerForVideoClassification.from_pretrained(\\\\'facebook/timesformer-hr-finetuned-k400\\\\')\\\\ninputs = video_processor(images=video_frames, return_tensors=\\\\'pt\\\\')  # Replace \\\\'video_frames\\\\' with your video frames\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k400')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/timesformer-hr-finetuned-k400', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k400')\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': [['transformers', 'numpy', 'torch']], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 448, 448))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-hr-finetuned-k400)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-hr-finetuned-k400)\\\\ninputs = processor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-400', 'accuracy': 'Not specified'}, 'description': 'TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ ÙˆØµÙÙ‹Ø§ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø© ÙˆØ§ÙƒØªØ¨ Ù†ØµÙ‹Ø§ ÙŠØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries: TrOCRProcessor, VisionEncoderDecoderModel from transformers, and Image from PIL.\\\\n2. Load the image of the handwritten document. You can open the image file using the Image.open() method from the PIL library.\\\\n3. Create a TrOCRProcessor instance by loading the pretrained model 'microsoft/trocr-large-handwritten'.\\\\n4. Create a VisionEncoderDecoderModel instance by loading the pretrained model 'microsoft/trocr-large-handwritten'.\\\\n5. Use the processor to get the pixel values from the image and pass them through the model.\\\\n6. Use the processor.batch_decode() method to convert the output of the model into text.\\\", '<<<code>>>': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\ndef image_to_text(image_path):\\\\n    image = Image.open(image_path).convert('RGB')\\\\n    processor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\\\n    model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\\n    pixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\n    generated_ids = model.generate(pixel_values)\\\\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\\n    return generated_text\\\", 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': [{'packages': ['transformers', 'PIL', 'requests']}], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„ ÙˆØ¯Ù‚ÙŠÙ‚ØŒ ÙŠØ¬Ø¨ Ø£Ù† ÙŠØªØ¶Ù…Ù† Ø§Ù„Ù†Ø¸Ø§Ù… Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ ÙÙ‡Ù… ÙˆØ§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø­ÙŠÙˆÙŠØ© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙˆØ¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a summarization pipeline with the pre-trained model 'google/pegasus-newsroom'. This model is designed for summarization tasks and has been trained on both C4 and HugeNews datasets.\\\\n3. With the created summarization pipeline, we can then input any long article text and transform it into a short article summary suitable for posting on Twitter.\\\\n4. This approach allows users to summarize articles and quickly share valuable information on social media platforms.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsummarization_pipeline = pipeline(\\\\'summarization\\\\', model=\\\\'google/pegasus-newsroom\\\\')\\\\nlong_article = \\\"Your long article text goes here...\\\"\\\\nsummary = summarization_pipeline(long_article)[0][\\\\'summary_text\\\\']', 'api_call': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/pegasus-newsroom', 'api_call': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': 'newsroom', 'accuracy': '45.98/34.20/42.18'}, 'description': 'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks.' </s>\"\n"}
{"text": "<s>[INST]ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©ØŒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ØªØ³ØªØ·ÙŠØ¹ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is `pipeline` from transformers.\\\\n2. Use the `pipeline` function from transformers to load the model, which in this case is 'bert-base-cased'.\\\\n3. The loaded model will be used for completing the masked sentence, filling in the word at the position of the mask token `[MASK]`.\\\\n4. Provide the sentence with the `[MASK]` token as input to the model, and the model will predict and return the most likely word to fill in the mask.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'bert-base-cased\\\\')\\\\ninput_text = \\\"I have always wanted to [MASK] a marathon.\\\"\\\\nresult = unmasker(input_text)', 'api_call': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-base-cased', 'api_call': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", 'api_arguments': ['model'], 'python_environment_requirements': [['transformers']], 'example_code': ['from transformers import pipeline', \\\"unmasker = pipeline('fill-mask', model='bert-base-cased')\\\", \\\"unmasker(Hello I'm a [MASK] model.)\\\"], 'performance': {'dataset': 'GLUE', 'accuracy': 79.6}, 'description': \\\"BERT base model (cased) is a pre-trained transformer model on English language using a masked language modeling (MLM) objective. It was introduced in a paper and first released in a repository. This model is case-sensitive, which means it can differentiate between 'english' and 'English'. The model can be used for masked language modeling or next sentence prediction, but it's mainly intended to be fine-tuned on a downstream task.\\\" </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¬Ù…Ù„ØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªÙˆÙ„ÙŠØ¯ ØªÙ†Ø¨Ø¤Ø§Øª Ù„Ø¯Ø±Ø¬Ø© ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ¹Ù„Ù‚Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install 'sentence-transformers' package using pip.\\\\n2. Import 'SentenceTransformer' from the 'sentence_transformers' library.\\\\n3. Create an instance of the 'SentenceTransformer' by specifying the model name 'sentence-transformers/all-roberta-large-v1'.\\\\n4. Encode a list of sentences containing customer reviews using the 'encode()' method on the model instance.\\\\n5. The output will be a list of embeddings with each embedding representing the semantic similarity of the corresponding customer review.\\\\n6. You can perform clustering on the embeddings to group similar reviews together.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\ncustomer_reviews = [\\\"This product is excellent!\\\", \\\"I love using this every day.\\\", \\\"Not satisfied with the purchase.\\\"]\\\\n# List of customer reviews\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-roberta-large-v1\\\\')\\\\nembeddings = model.encode(customer_reviews)\\\\n# Perform clustering on the embeddings to group similar reviews.', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/all-roberta-large-v1', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 1024 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†ØµØ§ ØªÙˆÙ„ÙŠØ¯ÙŠØ§ ÙŠÙ†Ø´Ø£ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆÙŠØ¨Ø¯Ø£ Ø¨Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('conversational', model='PygmalionAI/pygmalion-350m')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. First, we import transformers pipeline to use as conversation pipeline. \\\\n2. Next, we create an NLP pipeline using the pre-trained \\\"PygmalionAI/pygmalion-350m\\\" Conversational AI model. This model has been fine-tuned to be optimized for dialogue generation, specifically to answer questions in a conversational manner.\\\\n3. Then we can use the pipeline to process input questions from customers and generate appropriate responses for the chatbot to provide.\\\\n4. You can provide some common restaurant-related questions as history/context within the chatbot, which will improve the quality of the generated responses.\\\\n5. Keep in mind the model was fine-tuned with NSFW data, so using it for customer interaction requires thorough monitoring.', '<<<code>>>': 'from transformers import pipeline\\\\nconv_pipeline = pipeline(\\\\'conversational\\\\', model=\\\\'PygmalionAI/pygmalion-350m\\\\')\\\\n\\\\ndef reply_question(question):\\\\n    response = conv_pipeline(question)\\\\n    return response[\\\"generated_text\\\"]\\\\n\\\\n# Example usage:\\\\nquestion = \\\"What are the restaurant\\\\'s opening hours?\\\"\\\\nanswer = reply_question(question)\\\\nprint(answer)', 'api_call': \\\"pipeline('conversational', model='PygmalionAI/pygmalion-350m')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pygmalion-350m', 'api_call': \\\"pipeline('conversational', model='PygmalionAI/pygmalion-350m')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['transformers'], 'example_code': 'N/A', 'performance': {'dataset': 'The Pile', 'accuracy': 'N/A'}, 'description': \\\"This is a proof-of-concept fine-tune of Facebook's OPT-350M model optimized for dialogue, to be used as a stepping stone to higher parameter models. Disclaimer: NSFW data was included in the fine-tuning of this model. Although SFW inputs will usually result in SFW outputs, you are advised to chat at your own risk. This model is not suitable for use by minors.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØª Ø¨Ø¥Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ù„ØªØµÙ†ÙŠÙ Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ ÙØ¦Ø§Øª Ù…Ø¹ÙŠÙ†Ø© Ø¨Ø¯Ù‚Ø© ÙˆØ³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ©. ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØµÙ…ÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØµÙˆØª Ø¨Ø¯Ù‚Ø© ÙˆØ³Ø±Ø¹Ø©.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\", '<<<api_provider>>>': 'SpeechBrain', '<<<explanation>>>': \\\"1. First, install the SpeechBrain library via pip.\\\\n2. Import the foreign_class function from speechbrain.pretrained.interfaces.\\\\n3. Instantiate the classifier by calling the foreign_class function with the provided source repository, custom class name ('CustomEncoderWav2vec2Classifier'), and a 'custom_interface.py' file.\\\\n4. This classifier is pretrained on IEMOCAP training data and utilizes a wav2vec2 model for emotion recognition in speech.\\\\n5. Use the 'classify_file' method of the classifier object to detect emotions in an audio file.\\\\n6. The classifier returns output probabilities, scores, indices, and emotion labels ('text_lab'). You can use these emotion labels to make AI responses.\\\", '<<<code>>>': \\\"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\\nout_prob, score, index, text_lab = classifier.classify_file('path/to/audio/file.wav')\\\\nprint(text_lab)\\\", 'api_call': \\\"foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\", 'provider': 'SpeechBrain', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP', 'api_call': \\\"foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\", 'api_arguments': ['file_path'], 'python_environment_requirements': ['pip install speechbrain'], 'example_code': \\\"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\\nout_prob, score, index, text_lab = classifier.classify_file('speechbrain/emotion-recognition-wav2vec2-IEMOCAP/anger.wav')\\\\nprint(text_lab)\\\", 'performance': {'dataset': 'IEMOCAP', 'accuracy': '78.7%'}, 'description': 'This repository provides all the necessary tools to perform emotion recognition with a fine-tuned wav2vec2 (base) model using SpeechBrain. It is trained on IEMOCAP training data.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ ÙØ±ÙŠÙ‚ Ø·Ù„Ø§Ø¨ÙŠ ÙŠÙ‚ÙˆÙ… Ø¨ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ†ØŒ ÙˆÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¬Ù„Ø³Ø§ØªØŒ ÙŠØªÙ†Ø§ÙØ³ Ø§Ù„Ø·Ù„Ø§Ø¨ Ø¨ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the necessary libraries such as \\\"WhisperProcessor\\\", \\\"WhisperForConditionalGeneration\\\" from the transformers package, and \\\"load_dataset\\\" from the datasets package.\\\\n2. Load the pre-trained Whisper model, \\\\'openai/whisper-tiny.en\\\\', which is capable of transcribing speech.\\\\n3. Create a processor instance from the WhisperProcessor class using the same model name.\\\\n4. Load the audio file from the provided dataset or directly from a file.\\\\n5. Process the audio file using the WhisperProcessor to extract input features.\\\\n6. Use the pretrained Whisper model to generate predicted IDs from the input features.\\\\n7. Decode the predicted IDs back into text using the processor\\\\'s batch_decode function.\\\\n8. Now, you can use the generated transcription in your podcast transcription service.', '<<<code>>>': \\\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-tiny.en')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\\\\nsample = {'audio': audio_data, 'sampling_rate': sampling_rate}\\\\n# replace audio_data with actual audio and sampling_rate with the rate\\\\ninput_features = processor(sample['audio'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\\", 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai/whisper-tiny.en', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\\\", 'api_arguments': {'model_name': 'openai/whisper-tiny.en'}, 'python_environment_requirements': [['transformers', 'datasets', 'torch']], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', 'processor = WhisperProcessor.from_pretrained(openai/whisper-tiny.en)', 'model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-tiny.en)', 'ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)', 'sample = ds[0][audio]', 'input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features', 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)'], 'performance': {'dataset': 'LibriSpeech (clean)', 'accuracy': 8.437}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù‘Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ²ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ø­ÙˆØ§Ø±ÙŠØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù„ÙŠÙ… Ø¢Ù„ÙŠ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ø¯Ø§Ø¡ Ù…Ø­Ø³Ù† ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù…Ù‡Ø§Ù… Ø¨Ù…Ø¬Ø§Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ù…Ù† Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ù‡Ø°Ø§ØŸ? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': 'from transformers import AutoTokenizer, AutoModelForTableQuestionAnswering, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'google/tapas-mini-finetuned-wtq\\\\')\\\\nmodel = AutoModelForTableQuestionAnswering.from_pretrained(\\\\'google/tapas-mini-finetuned-wtq\\\\')\\\\nnlp = pipeline(\\\\'table-question-answering\\\\', model=model, tokenizer=tokenizer)\\\\ntable_data = # Provide table data as a list of dictionaries\\\\nquestion = \\\"Insert your query related to the table here\\\"\\\\nresult = nlp(question=question, table=table_data)', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-mini-finetuned-wtq', 'api_call': 'from transformers import AutoTokenizer, AutoModelForTableQuestionAnswering, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'google/tapas-mini-finetuned-wtq\\\\')\\\\nmodel = AutoModelForTableQuestionAnswering.from_pretrained(\\\\'google/tapas-mini-finetuned-wtq\\\\')\\\\nnlp = pipeline(\\\\'table-question-answering\\\\', model=model, tokenizer=tokenizer)\\\\ntable_data = # Provide table data as a list of dictionaries\\\\nquestion = \\\"Insert your query related to the table here\\\"\\\\nresult = nlp(question=question, table=table_data)', 'api_arguments': \\\"tokenizer = AutoTokenizer.from_pretrained('google/tapas-mini-finetuned-wtq'); model = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')\\\", 'python_environment_requirements': 'transformers', 'example_code': \\\"from transformers import pipeline\\\\n\\\\nnlp = pipeline('table-question-answering', model='google/tapas-mini-finetuned-wtq', tokenizer='google/tapas-mini-finetuned-wtq')\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.2854}, 'description': 'TAPAS mini model fine-tuned on WikiTable Questions (WTQ). It is pretrained on a large corpus of English data from Wikipedia and can be used for answering questions related to a table.'}} </s>\"\n"}
{"text": "<s>[INST]Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙˆØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ±ØŒ Ø£Ù†Ø´Ø¦ Ø¬Ù…Ù„Ø© Ù†ØµÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \\\"Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ø£Ù†Ø´Ø¦ Ø³ÙŠØ§Ù‚Ù‹Ø§ Ù†ØµÙŠÙ‹Ø§ Ø¬Ø¯ÙŠØ¯Ù‹Ø§ Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ\\\".? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"YOLO('keremberke/yolov8m-building-segmentation')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage_url = 'https://path.to/your/image.jpg'\\\\nresults = model.predict(image_url)\\\\nrendered_image = render_result(model=model, image=image_url, result=results[0])\\\\nrendered_image.show()\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-building-segmentation', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage_url = 'https://path.to/your/image.jpg'\\\\nresults = model.predict(image_url)\\\\nrendered_image = render_result(model=model, image=image_url, result=results[0])\\\\nrendered_image.show()\\\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.21', 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.623, 'mAP@0.5(mask)': 0.613}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± Ù„ØªÙ…ÙŠÙŠØ² Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ØŸ? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary libraries, such as AutoImageProcessor and Mask2FormerForUniversalSegmentation from transformers, Image from PIL, and requests.', 'code': \\\"from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_instance_map = result['segmentation']\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-tiny-coco-instance', 'api_call': \\\"from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_instance_map = result['segmentation']\\\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-tiny-coco-instance'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': \\\"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_instance_map = result['segmentation']\\\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¬Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø¨Ø¦Ø© Ø§Ù„ÙØ¬ÙˆØ§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©ØŒ Ø¨Ø¨Ø³Ø§Ø·Ø© Ù‚Ù… Ø¨Ù…Ù„Ø¡ Ø§Ù„Ø®Ù„Ø§ÙŠØ§ Ø§Ù„ÙØ§Ø±ØºØ© ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© ÙˆØ³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªÙ†Ø¨Ø¤ Ø§Ù„Ù‚ÙŠÙ… Ù„Ù„Ø®Ù„Ø§ÙŠØ§ Ø§Ù„Ø£Ø®Ø±Ù‰.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': 'RandomForestRegressor(max_depth=10, n_estimators=50, random_state=59)', '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': '1. Import the RandomForestRegressor from the scikit-learn library.\\\\n2. Create the RandomForestRegressor model with the provided parameters (max_depth=10, n_estimators=50, random_state=59).\\\\n3. Fit the model on the training dataset which contains historical electricity consumption data.\\\\n4. Use the fitted model to make predictions about future electricity consumption for the next month.', '<<<code>>>': 'from sklearn.ensemble import RandomForestRegressor\\\\nmodel = RandomForestRegressor(max_depth=10, n_estimators=50, random_state=59)\\\\nmodel.fit(X_train, y_train)\\\\npredictions = model.predict(X_test)', 'api_call': 'RandomForestRegressor(max_depth=10, n_estimators=50, random_state=59)', 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rajistics/MAPIE-TS-Electricity', 'api_call': 'RandomForestRegressor(max_depth=10, n_estimators=50, random_state=59)', 'api_arguments': {'bootstrap': 'True', 'ccp_alpha': '0.0', 'criterion': 'squared_error', 'max_depth': '10', 'max_features': '1.0', 'max_leaf_nodes': '', 'max_samples': '', 'min_impurity_decrease': '0.0', 'min_samples_leaf': '1', 'min_samples_split': '2', 'min_weight_fraction_leaf': '0.0', 'n_estimators': '50', 'n_jobs': '', 'oob_score': 'False', 'random_state': '59', 'verbose': '0', 'warm_start': 'False'}, 'python_environment_requirements': [''], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A RandomForestRegressor model for electricity consumption prediction.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ØªØ¹Ù„Ù… ØªØ¹Ø²ÙŠØ²ÙŠ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Ø±ÙˆØ¨ÙˆØªØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¦Ù‡Ø§ ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ø°Ø§ØªÙŠØ©. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªØ¹Ø²ÙŠØ² ØªØ¹Ù„Ù… Ø§Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª ÙˆØªØ­Ø³ÙŠÙ† Ø³Ù„ÙˆÙƒÙ‡Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ù…Ø­ÙŠØ·Ø©.? [/INST]'<<<domain>>>': 'Reinforcement Learning Robotics', '<<<api_call>>>': \\\"pipeline('robotics', model='Antheia/Hanna')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a robotics model.\\\\n3. Specify the model 'Antheia/Hanna' to be loaded. This model is trained for reinforcement learning tasks specifically in the domain of robotics.\\\\n4. The created model can be used for various warehouse tasks like picking up objects, stacking, and sorting items. The model can be integrated with robotic hardware through a communication interface to perform the actions based on the refined policy.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nrobotics_model = pipeline('robotics', model='Antheia/Hanna')\\\\n\\\\n# Perform actions like picking up an object, stacking, and sorting items\\\\n# using robotics_model\\\", 'api_call': \\\"pipeline('robotics', model='Antheia/Hanna')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Reinforcement Learning Robotics', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Antheia/Hanna', 'api_call': \\\"pipeline('robotics', model='Antheia/Hanna')\\\", 'api_arguments': 'model', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'openai/webgpt_comparisons', 'accuracy': ''}, 'description': 'Antheia/Hanna is a reinforcement learning model for robotics tasks, trained on the openai/webgpt_comparisons dataset.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Øµ Ù…Ø³Ù…ÙˆØ¹ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù…Ù„Ø© \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\" Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙŠ Ø¥Ù„Ù‰ ØµÙˆØª ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-13b')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We import the AutoModelForCausalLM and AutoTokenizer classes from the transformers library provided by Hugging Face.', 'code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\n\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'facebook/opt-13b\\\\', torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/opt-13b\\\\', use_fast=False)\\\\n\\\\nprompt = \\\"In the distant future,\\\"\\\\ninput_ids = tokenizer(prompt, return_tensors=\\\\'pt\\\\').input_ids.cuda()\\\\ngenerated_ids = model.generate(input_ids)\\\\nopening_sentence = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Generation', 'api_name': 'facebook/opt-13b', 'api_call': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\n\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'facebook/opt-13b\\\\', torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/opt-13b\\\\', use_fast=False)\\\\n\\\\nprompt = \\\"In the distant future,\\\"\\\\ninput_ids = tokenizer(prompt, return_tensors=\\\\'pt\\\\').input_ids.cuda()\\\\ngenerated_ids = model.generate(input_ids)\\\\nopening_sentence = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]', 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-13b, torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-13b, use_fast=False)\\\\nprompt = Hello, I'm am conscious and\\\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\\\ngenerated_ids = model.generate(input_ids)\\\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)\\\", 'performance': {'dataset': 'GPT-3', 'accuracy': 'roughly match the performance and sizes of the GPT-3 class of models'}, 'description': 'OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. The models are trained to match the performance and sizes of the GPT-3 class of models. The primary goal is to enable reproducible and responsible research at scale and to bring more voices to the table in studying the impact of large language models. OPT-13B is a 13-billion-parameter model trained predominantly with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¬Ù…Ù„ Ù…Ù† Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¥Ù„Ù‰ Ø¢Ø®Ø±Ù‰ ÙˆÙ„Ù‚ÙŠØ§Ø³ Ø§Ù„Ø´Ø¨Ù‡ Ø¨ÙŠÙ†Ù‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, install the sentence-transformers library using 'pip install -U sentence-transformers'.\\\\n2. Import the SentenceTransformer class from the sentence_transformers library.\\\\n3. Load the pre-trained model 'sentence-transformers/all-MiniLM-L12-v2' using the SentenceTransformer class.\\\\n4. Extract lines of dialogue from movie characters and store them as a list of sentences.\\\\n5. Encode the sentences using the model's 'encode' method, which will generate embeddings for each sentence.\\\\n6. Compare the embeddings using similarity measures like cosine similarity to analyze the relationship between characters based on their lines.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\n\\\\n# Example character lines\\\\ncharacter1_lines = [\\\"We have to do something!\\\", \\\"I think we should split up.\\\"]\\\\ncharacter2_lines = [\\\"We need a plan.\\\", \\\"It might be safer if we split up.\\\"]\\\\n\\\\n# Load the pre-trained model\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-MiniLM-L12-v2\\\\')\\\\n\\\\n# Encode the sentences\\\\ncharacter1_embeddings = model.encode(character1_lines)\\\\ncharacter2_embeddings = model.encode(character2_lines)\\\\n\\\\n# Calculate the similarity between character lines\\\\nsimilarity_matrix = cosine_similarity(character1_embeddings, character2_embeddings)\\\\nprint(similarity_matrix)', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/all-MiniLM-L12-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': '1,170,060,424 training pairs', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆØŸ? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the required libraries from the transformers package, including VideoMAEImageProcessor and VideoMAEForPreTraining. We also import numpy and torch packages.\\\\n2. We have a 100-frame video, and we'll sample 16 frames from it to be used as input to the model.\\\\n3. The VideoMAEImageProcessor is used to preprocess the frames and convert them into pixel values.\\\\n4. We then create an instance of the VideoMAEForPreTraining model by loading the pretrained model 'MCG-NJU/videomae-large'.\\\\n5. Next, we calculate the number of patches per frame and the sequence length.\\\\n6. We create a boolean mask for the masked patches and pass it to the model along with the pixel values.\\\\n7. The model will then process the video and provide an output that can be used to detect vegetables and fruit in the video.\\\", '<<<code>>>': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-large')\\\\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-large')\\\\n\\\\npixel_values = processor(video, return_tensors='pt').pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss\\\", 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'videomae-large', 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-large')\\\", 'api_arguments': 'pixel_values, bool_masked_pos', 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-large)\\\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-large)\\\\npixel_values = processor(video, return_tensors=pt).pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss', 'performance': {'dataset': 'Kinetics-400', 'accuracy': 'Not provided'}, 'description': 'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù…Ù†ØªØ¬ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create a translation pipeline for English to Chinese using the pre-trained model 'Helsinki-NLP/opus-mt-en-zh'. This model is trained on the OPUS dataset and is designed to translate English text into Chinese.\\\\n3. Use the translation pipeline to translate the quarterly review text into Chinese.\\\\n4. The translated text can then be shared with the board members as requested.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntranslator = pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\\\\nquarterly_review_chinese = translator(quarterly_review_text)\\\", 'api_call': \\\"pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-en-zh', 'api_call': \\\"pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\\\\ntranslated_text = translation('Hello, world!')\\\", 'performance': {'dataset': 'Tatoeba-test.eng.zho', 'accuracy': {'BLEU': 31.4, 'chr-F': 0.268}}, 'description': \\\"A translation model for English to Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model and trained on the OPUS dataset. The model requires a sentence initial language token in the form of '>>id<<' (id = valid target language ID).\\\" </s>\"\n"}
{"text": "<s>[INST]Ø§Ù†Ø´Ø§Ø¡ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ù‚ØµÙŠØ± ÙŠØ¨Ø¯Ø£ Ø¨Ø¬Ù…Ù„Ø© \\\"ÙƒØ§Ù† ÙŠÙØ­ÙÙƒÙ‰ØŒ Ø£ÙŠØ§Ù… Ø·ÙˆÙŠÙ„Ø© Ù…Ø¶Øª ÙˆÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\"ØŒ ÙˆÙŠÙƒÙ…Ù„Ù‡ Ù…Ù‚Ø¯Ù…Ø© Ù‚ØµÙŠØ±Ø© Ø¨Ø­Ø¯ÙˆØ¯ 3 Ø¬Ù…Ù„ ØªØªØ­Ø¯Ø« Ø¹Ù† Ø¯ÙˆØ± Ø§Ù„Ù€ Fill-Mask ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"DebertaModel.from_pretrained('microsoft/deberta-v2-xlarge')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries. In this case, we will need Python's transformers library.\\\\n2. Load the large version of the DeBERTa model from the transformers library, which is trained on a range of natural language understanding tasks, using the 'microsoft/deberta-v2-xlarge' identifier.\\\\n3. Create a fill-mask pipeline, which uses the loaded DeBERTa v2 model to generate predictions given a context sentence.\\\\n4. Whenever there is a missing word in a sentence, replace it with the [MASK] token. The model will then generate the most appropriate word to fill the blank space based on the context.\\\", '<<<code>>>': 'from transformers import DebertaModel, fill_mask_pipeline\\\\nmodel = DebertaModel.from_pretrained(\\\\'microsoft/deberta-v2-xlarge\\\\')\\\\npipeline = fill_mask_pipeline(model=model)\\\\nmasked_sentence = \\\"The sun shines during the [MASK] and the moon appears at night.\\\"\\\\nfilled_sentence = pipeline(masked_sentence)', 'api_call': \\\"DebertaModel.from_pretrained('microsoft/deberta-v2-xlarge')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/deberta-v2-xlarge', 'api_call': \\\"DebertaModel.from_pretrained('microsoft/deberta-v2-xlarge')\\\", 'api_arguments': 'Mask token: [MASK]', 'python_environment_requirements': ['PyTorch, TensorFlow'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': [{'name': 'SQuAD 1.1', 'accuracy': '95.8/90.8'}, {'name': 'SQuAD 2.0', 'accuracy': '91.4/88.9'}, {'name': 'MNLI-m/mm', 'accuracy': '91.7/91.6'}, {'name': 'SST-2', 'accuracy': '97.5'}, {'name': 'QNLI', 'accuracy': '95.8'}, {'name': 'CoLA', 'accuracy': '71.1'}, {'name': 'RTE', 'accuracy': '93.9'}, {'name': 'MRPC', 'accuracy': '92.0/94.2'}, {'name': 'QQP', 'accuracy': '92.3/89.8'}, {'name': 'STS-B', 'accuracy': '92.9/92.9'}]}, 'description': 'DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data. This is the DeBERTa V2 xlarge model with 24 layers, 1536 hidden size. The total parameters are 900M and it is trained with 160GB raw data.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±Ø© Ù…Ù† Ø®Ù„Ø§Ù„ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ø¨Ø¯ÙˆÙ† Ø´Ø±Ø·. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the 'diffusers' package, which is required for the DiffusionPipeline.\\\\n2. Import the required library 'DiffusionPipeline' from 'diffusers'.\\\\n3. Use 'DiffusionPipeline.from_pretrained()' function to load the 'CompVis/ldm-celebahq-256' model, which is trained on the CelebA-HQ dataset for generating high-quality human face images.\\\\n4. Generate the image using the 'pipeline' object by specifying the number of inference steps. The higher the number of steps, the more accurate the image will be, but the generation process will take longer.\\\\n5. Save the generated image using the 'save()' method of the image object, which will save the image to the local storage.\\\\nNOTE: This process will provide a unique, realistic image of a human face not matching any specific person or set of features.\\\", '<<<code>>>': \\\"!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = 'CompVis/ldm-celebahq-256'\\\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\\\ngenerated_image = pipeline(num_inference_steps=200)\\\\ngenerated_image[0].save('ldm_generated_image.png')\\\", 'api_call': \\\"DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'CompVis/ldm-celebahq-256', 'api_call': \\\"DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')\\\", 'api_arguments': ['model_id'], 'python_environment_requirements': [['diffusers']], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = CompVis/ldm-celebahq-256\\\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = pipeline(num_inference_steps=200)[sample]\\\\nimage[0].save(ldm_generated_image.png)', 'performance': {'dataset': 'CelebA-HQ', 'accuracy': 'N/A'}, 'description': 'Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨ÙØ¦Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. First, we import the necessary libraries, which are 'json', 'joblib', and 'pandas'.\\\\n2. We then use the joblib.load function to load the pre-trained model 'model.joblib'. This model has been trained for multi-class classification tasks, which is exactly what we need for predicting the popularity of movies.\\\\n3. We read the multiple features from a CSV file ('data.csv'), which contains information about various movie-related factors. These features are used by the model to make a prediction.\\\\n4. The predictions are made using the 'predict' function, and the popularity category is inferred from the results.\\\", '<<<code>>>': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Alexei1/imdb', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['json', 'joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'IMDB', 'accuracy': 0.487}, 'description': 'A tabular classification model trained using AutoTrain for sentiment analysis on the IMDB dataset. The model has a CO2 emission of 0.0186 grams and an accuracy of 0.487.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ø¯Ø¹Ù… Ù…Ø³ØªØ®Ø¯Ù…ÙŠ ØªØ·Ø¨ÙŠÙ‚ ÙŠØ³ØªÙ‡Ø¯Ù Ø£ÙØ±Ø§Ø¯ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the transformers library provided by Hugging Face.\\\\n2. Create a model by loading a pretrained BERT model, \\\"neuralmind/bert-base-portuguese-cased\\\", specific for Brazilian Portuguese language.\\\\n3. Create a \\\\'fill-mask\\\\' pipeline, which will be fine-tuned on the downstream task to identify the missing word in a given context.\\\\n4. Run prediction on the given sentence \\\"Tinha uma [MASK] no meio do caminho.\\\" to predict the missing word.', '<<<code>>>': \\\"from transformers import pipeline, AutoModelForPreTraining, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\\\\nmodel = AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\\\\npipe = pipeline('fill-mask', model=model, tokenizer=tokenizer)\\\\nresult = pipe('Tinha uma [MASK] no meio do caminho.')\\\", 'api_call': \\\"AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'neuralmind/bert-base-portuguese-cased', 'api_call': \\\"AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'neuralmind/bert-base-portuguese-cased'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\npipe = pipeline('fill-mask', model=model, tokenizer=tokenizer)\\\\npipe('Tinha uma [MASK] no meio do caminho.')\\\", 'performance': {'dataset': 'brWaC', 'accuracy': 'state-of-the-art'}, 'description': 'BERTimbau Base is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø±ÙŠØ¯ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©ØŒ ÙˆØ°Ù„Ùƒ Ù„ØªÙ…ÙƒÙŠÙ†Ù†Ø§ Ù…Ù† ØªØ­Ø¯ÙŠØ¯ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„ ÙˆØ¯Ù‚ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries EncoderClassifier from speechbrain.pretrained and torchaudio.\\\\n2. Initialize the EncoderClassifier model using the trained model provided at source 'TalTechNLP/voxlingua107-epaca-tdnn'. This model is trained to recognize speech in 107 different languages.\\\\n3. Load the audio file using the model's load_audio function.\\\\n4. Use the model's classify_batch function to classify the audio signal into a language.\\\\n5. The results will contain the predicted language of the spoken audio in the file.\\\", '<<<code>>>': \\\"import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\nlanguage_id = EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\\nsignal = language_id.load_audio('<your_audio_file_url>')\\\\nprediction = language_id.classify_batch(signal)\\\\nprint(prediction)\\\", 'api_call': \\\"EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'TalTechNLP/voxlingua107-epaca-tdnn', 'api_call': \\\"EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\", 'api_arguments': ['signal'], 'python_environment_requirements': [['speechbrain', 'torchaudio']], 'example_code': 'import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\nlanguage_id = EncoderClassifier.from_hparams(source=TalTechNLP/voxlingua107-epaca-tdnn, savedir=tmp)\\\\nsignal = language_id.load_audio(https://omniglot.com/soundfiles/udhr/udhr_th.mp3)\\\\nprediction = language_id.classify_batch(signal)\\\\nprint(prediction)', 'performance': {'dataset': 'VoxLingua107', 'accuracy': '93%'}, 'description': 'This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain. The model uses the ECAPA-TDNN architecture that has previously been used for speaker recognition. The model can classify a speech utterance according to the language spoken. It covers 107 different languages.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ù…Ø«Ù„ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙˆÙ†ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø©.? [/INST]{'domain': 'Natural Language Processing Token Classification', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('xlm-roberta-large-finetuned-conll03-english')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the necessary classes from the transformers library, such as AutoModelForTokenClassification and AutoTokenizer.', 'code': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'xlm-roberta-large-finetuned-conll03-english\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'xlm-roberta-large-finetuned-conll03-english\\\\')\\\\nuser_review = \\\"User review here...\\\"\\\\nencoded_input = tokenizer(user_review, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**encoded_input)\\\\n# Process output to identify keywords related to money, location, service, and product quality', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Transformers', 'functionality': 'Token Classification', 'api_name': 'xlm-roberta-large-finetuned-conll03-english', 'api_call': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'xlm-roberta-large-finetuned-conll03-english\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'xlm-roberta-large-finetuned-conll03-english\\\\')\\\\nuser_review = \\\"User review here...\\\"\\\\nencoded_input = tokenizer(user_review, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**encoded_input)\\\\n# Process output to identify keywords related to money, location, service, and product quality', 'api_arguments': {'model': 'AutoModelForTokenClassification.from_pretrained(xlm-roberta-large-finetuned-conll03-english)', 'tokenizer': 'AutoTokenizer.from_pretrained(xlm-roberta-large-finetuned-conll03-english)'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"classifier(Hello I'm Omar and I live in ZÃ¼rich.)\\\", 'performance': {'dataset': 'conll2003', 'accuracy': 'More information needed'}, 'description': 'The XLM-RoBERTa model is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data. This model is XLM-RoBERTa-large fine-tuned with the conll2003 dataset in English. It can be used for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚Ø¯Ù… Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ù„ØªÙˆÙ„ÙŠØ¯ Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ù…Ø¬Ø§Ù„ \\\"Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨\\\". Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ ØªÙ†ÙÙŠØ° Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©ØŸ? [/INST]{'domain': 'Computer Vision Depth Estimation', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. First, ensure that you have the required Python packages installed: transformers>=4.24.0, pytorch>=1.12.1, and tokenizers>=0.13.2.', 'code': \\\"from transformers import AutoModel\\\\nimport torch\\\\n# Load the pre-trained model\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\\\\n# Prepare input image tensor and pass it through the model\\\\ninput_image = torch.tensor(input_image_data).unsqueeze(0) # Assuming input_image_data is a properly formatted input image tensor\\\\ndepth_map = model(input_image)[0]\\\", 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-082237', 'api_call': \\\"from transformers import AutoModel\\\\nimport torch\\\\n# Load the pre-trained model\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\\\\n# Prepare input image tensor and pass it through the model\\\\ninput_image = torch.tensor(input_image_data).unsqueeze(0) # Assuming input_image_data is a properly formatted input image tensor\\\\ndepth_map = model(input_image)[0]\\\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3421, 'Mae': 0.27, 'Rmse': 0.4042, 'Abs Rel': 0.3279, 'Log Mae': 0.1132, 'Log Rmse': 0.1688, 'Delta1': 0.5839, 'Delta2': 0.8408, 'Delta3': 0.9309}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.'}} </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù„ÙŠÙƒ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† Ø­Ø§Ø¬Ø© Ù„ØªØµÙ†ÙŠÙØ§Øª Ø³Ø§Ø¨Ù‚Ø©. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø­Ù„ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙØ§Øª Ù…Ù† ØºÙŠØ± ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('Recognai/bert-base-spanish-wwm-cased-xnli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library.\\\\n2. Use the pipeline function to create a zero-shot classification model, specifying the model 'Recognai/bert-base-spanish-wwm-cased-xnli'. This model is fine-tuned on the Spanish portion of the XNLI dataset and can be used for zero-shot classification tasks.\\\\n3. Prepare the news excerpt, candidate labels (such as 'culture', 'society', 'economy', 'health', 'sports'), and hypothesis template ('Este ejemplo es {}').\\\\n4. Use the classifier to determine the most relevant topics in the given news excerpt.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nspanish_classifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'Recognai/bert-base-spanish-wwm-cased-xnli\\\\')\\\\nnews_excerpt = \\\"El autor se perfila, a los 50 aÃ±os de su muerte, como uno de los grandes de su siglo...\\\"\\\\ncandidate_labels = [\\\\'cultura\\\\', \\\\'sociedad\\\\', \\\\'economia\\\\', \\\\'salud\\\\', \\\\'deportes\\\\']\\\\nhypothesis_template = \\\"Este ejemplo es {}.\\\"\\\\nresult = spanish_classifier(news_excerpt, candidate_labels, hypothesis_template)', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('Recognai/bert-base-spanish-wwm-cased-xnli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Recognai/bert-base-spanish-wwm-cased-xnli', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('Recognai/bert-base-spanish-wwm-cased-xnli')\\\", 'api_arguments': ['sequence', 'candidate_labels', 'hypothesis_template'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=Recognai/bert-base-spanish-wwm-cased-xnli)\\\\nclassifier(\\\\nEl autor se perfila, a los 50 aÃ±os de su muerte, como uno de los grandes de su siglo,\\\\ncandidate_labels=[cultura, sociedad, economia, salud, deportes],\\\\nhypothesis_template=Este ejemplo es {}. \\\\n)', 'performance': {'dataset': 'XNLI-es', 'accuracy': '79.9%'}, 'description': 'This model is a fine-tuned version of the spanish BERT model with the Spanish portion of the XNLI dataset. You can have a look at the training script for details of the training.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ¹Ù„Ù‚ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø¨ØªÙ‚Ù†ÙŠØ© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© NLP ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ù…Ø§Ø«Ù„Ø© Ø§Ù„Ø¬Ù…Ù„. ÙˆÙ‡Ù†Ø§Ùƒ Ø­Ø§Ø¬Ø© Ù„Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„ØªÙ†Ø¨Ø¤ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©.? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from sentence_transformers import SentenceTransformer\\\\nabstracts = [\\\"Abstract 1...\\\", \\\"Abstract 2...\\\", \\\"Abstract 3...\\\"]\\\\n# Replace with the actual abstract texts\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/bert-base-nli-mean-tokens\\\\')\\\\nembeddings = model.encode(abstracts)\\\\n# Use clustering techniques to group the embeddings based on similarity', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Sentence Embeddings', 'api_name': 'sentence-transformers/bert-base-nli-mean-tokens', 'api_call': 'from sentence_transformers import SentenceTransformer\\\\nabstracts = [\\\"Abstract 1...\\\", \\\"Abstract 2...\\\", \\\"Abstract 3...\\\"]\\\\n# Replace with the actual abstract texts\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/bert-base-nli-mean-tokens\\\\')\\\\nembeddings = model.encode(abstracts)\\\\n# Use clustering techniques to group the embeddings based on similarity', 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ¹ÙŠÙ† Ø¹Ù„ÙŠÙƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ ØµÙˆØª Ø¢Ø®Ø±. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØ£Ø«ÙŠØ±Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© ÙˆØ¶Ø¨Ø· Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµÙˆØª Ø­Ø³Ø¨ Ø±ØºØ¨ØªÙƒ.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and modules such as transformers, numpy, and soundfile.\\\\n2. Load the model 'microsoft/speecht5_vc' using the from_pretrained method of the SpeechT5ForSpeechToSpeech class.\\\\n3. Load the speaker embeddings to a numpy array and convert it to a torch tensor.\\\\n4. Process the input audio data with the loaded model and the provided speaker embeddings.\\\\n5. Use the generated speech output from the model and save it as a new audio file adjusted to the desired voice.\\\", '<<<code>>>': \\\"from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\\\\nimport numpy as np\\\\nimport torch\\\\nimport soundfile as sf\\\\n\\\\nexample_audio_path = 'path_to_audio_file.wav'\\\\n# replace with the path to the input audio file\\\\nsampling_rate = 16000\\\\nexample_speech, _ = sf.read(example_audio_path)\\\\n\\\\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_vc')\\\\nmodel = SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\\\\n\\\\ninputs = processor(audio=example_speech, sampling_rate=sampling_rate, return_tensors='pt')\\\\n\\\\nspeaker_embeddings = np.load('xvector_speaker_embedding.npy')\\\\nspeaker_embeddings = torch.tensor(speaker_embeddings).unsqueeze(0)\\\\n\\\\nspeech = model.generate_speech(inputs['input_values'], speaker_embeddings, vocoder=vocoder)\\\\n\\\\nsf.write('output_speech.wav', speech.numpy(), samplerate=16000)\\\", 'api_call': \\\"SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/speecht5_vc', 'api_call': \\\"SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\", 'api_arguments': {'audio': 'example_speech', 'sampling_rate': 'sampling_rate', 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'datasets', 'numpy', 'torch', 'soundfile']], 'example_code': \\\"from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset('hf-internal-testing/librispeech_asr_demo', 'clean', split='validation')\\\\ndataset = dataset.sort('id')\\\\nsampling_rate = dataset.features['audio'].sampling_rate\\\\nexample_speech = dataset[0]['audio']['array']\\\\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_vc')\\\\nmodel = SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\\\\ninputs = processor(audio=example_speech, sampling_rate=sampling_rate, return_tensors='pt')\\\\nimport numpy as np\\\\nimport torch\\\\nspeaker_embeddings = np.load('xvector_speaker_embedding.npy')\\\\nspeaker_embeddings = torch.tensor(speaker_embeddings).unsqueeze(0)\\\\nspeech = model.generate_speech(inputs['input_values'], speaker_embeddings, vocoder=vocoder)\\\\nimport soundfile as sf\\\\nsf.write('speech.wav', speech.numpy(), samplerate=16000)\\\", 'performance': {'dataset': 'CMU ARCTIC', 'accuracy': 'Not specified'}, 'description': 'SpeechT5 model fine-tuned for voice conversion (speech-to-speech) on CMU ARCTIC. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. It is designed to improve the modeling capability for both speech and text. This model can be used for speech conversion tasks.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… ÙØ±ÙŠÙ‚Ù†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ù†ØµÙˆØµ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†ØµØŒ ÙˆÙ†Ø­Ù† Ù†Ù‡Ø¯Ù Ø¥Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø®Ù„Ø§Ù„ ØªÙˆØ¸ÙŠÙ ØªÙ‚Ù†ÙŠØ§Øª Ø­Ø¯ÙŠØ«Ø© ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ù†ØµÙˆØµ.? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"pipeline('image-to-text', model='microsoft/git-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the pipeline function from the transformers library provided by Hugging Face.', 'code': \\\"from transformers import pipeline\\\\nimport PIL.Image\\\\n\\\\nimage_captioning = pipeline('image-to-text', model='microsoft/git-base')\\\\nimage_path = 'fruit_and_veg_stands.jpg'\\\\nimage = PIL.Image.open(image_path)\\\\ncaption = image_captioning(image)[0]['text']\\\\n\\\\nprint(caption)\\\", 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \\\"from transformers import pipeline\\\\nimport PIL.Image\\\\n\\\\nimage_captioning = pipeline('image-to-text', model='microsoft/git-base')\\\\nimage_path = 'fruit_and_veg_stands.jpg'\\\\nimage = PIL.Image.open(image_path)\\\\ncaption = image_captioning(image)[0]['text']\\\\n\\\\nprint(caption)\\\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \\\"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\\\"}} </s>\"\n"}
{"text": "<s>[INST]Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨ØªØµÙ†ÙŠÙ Ø¹Ù…Ù‚ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© \\\"Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ±ÙŠ Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\\\"ØŒ Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ø¯Ø¯Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªÙ‚Ø¯ÙŠÙ… ØµÙˆØ±Ø© ÙˆØ§Ø¶Ø­Ø© ÙˆØ¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø© Ù„Ù„Ù†Ø¸Ø§Ù… Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ù†ØªØ§Ø¦Ø¬.? [/INST]{'domain': 'Computer Vision Depth Estimation', 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import required libraries, in this case, 'pipeline' function from transformers package.\\\", 'code': \\\"from transformers import pipeline\\\\ndepth_estimator = pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\\\\n# assuming image is already loaded\\\\n# depth_map = depth_estimator(image)\\\", 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-113853', 'api_call': \\\"from transformers import pipeline\\\\ndepth_estimator = pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\\\\n# assuming image is already loaded\\\\n# depth_map = depth_estimator(image)\\\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3384, 'Mae': 0.2739, 'Rmse': 0.3959, 'Abs Rel': 0.323, 'Log Mae': 0.1148, 'Log Rmse': 0.1651, 'Delta1': 0.5576, 'Delta2': 0.8345, 'Delta3': 0.9398}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.'}} </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙŠØ¯ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ ØµÙˆØª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… API Ø§Ù„ØµÙˆØªÙŠØ©ØŒ Ù‚Ù… Ø¨Ø¨Ø±Ù…Ø¬Ø© Ù†Ø¸Ø§Ù… Ù…Ø¨ØªÙƒØ± Ù„Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„ØµÙˆØªÙŠ. Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‚Ø§Ø¯Ø±Ø§Ù‹ Ø¹Ù„Ù‰ ÙØ­Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„ ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§ Ø¨Ø¯Ù‚Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„ØµÙˆØªÙŠ Ø¥Ù„Ù‰ Ù†Ù…Ø· ØµÙˆØªÙŠ Ø¬Ø¯ÙŠØ¯ ÙˆÙØ¹Ø§Ù„. ÙƒÙ† Ù…Ø¨Ø¯Ø¹Ø§Ù‹ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„Ø¨ÙŠÙˆÙ„ÙˆØ¬ÙŠØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø¯Ø§Ø© Ù…Ø¨ØªÙƒØ±Ø© ÙˆØ°ÙƒÙŠØ© ØªÙ‚ÙˆÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„ØµÙˆØªÙŠ Ø¨Ø¯Ù‚Ø© ÙˆØ³Ù„Ø§Ø³Ø© Ù„ØªÙ„Ø¨ÙŠØ© Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ ØµÙˆØª.? [/INST]{'domain': 'Audio Audio-to-Audio', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk')\\\", 'api_provider': 'Fairseq', 'explanation': '1. Import necessary packages including fairseq, torchaudio, and huggingface_hub.', 'code': '', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Fairseq', 'functionality': 'speech-to-speech-translation', 'api_name': 'xm_transformer_s2ut_en-hk', 'api_call': '', 'api_arguments': {'arg_overrides': {'config_yaml': 'config.yaml', 'task': 'speech_to_text'}, 'cache_dir': 'cache_dir'}, 'python_environment_requirements': ['fairseq', 'huggingface_hub', 'torchaudio'], 'example_code': {'import_modules': ['import json', 'import os', 'from pathlib import Path', 'import IPython.display as ipd', 'from fairseq import hub_utils', 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub', 'from fairseq.models.speech_to_text.hub_interface import S2THubInterface', 'from fairseq.models.text_to_speech import CodeHiFiGANVocoder', 'from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface', 'from huggingface_hub import snapshot_download', 'import torchaudio'], 'load_model': [\\\"cache_dir = os.getenv('HUGGINGFACE_HUB_CACHE')\\\", \\\"models, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk', arg_overrides={'config_yaml': 'config.yaml', 'task': 'speech_to_text'}, cache_dir=cache_dir)\\\", 'model = models[0].cpu()', \\\"cfg['task'].cpu = True\\\"], 'generate_prediction': ['generator = task.build_generator([model], cfg)', \\\"audio, _ = torchaudio.load('/path/to/an/audio/file')\\\", 'sample = S2THubInterface.get_model_input(task, audio)', 'unit = S2THubInterface.get_prediction(task, model, generator, sample)'], 'speech_synthesis': [\\\"library_name = 'fairseq'\\\", \\\"cache_dir = (cache_dir or (Path.home() / '.cache' / library_name).as_posix())\\\", \\\"cache_dir = snapshot_download('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS', cache_dir=cache_dir, library_name=library_name)\\\", \\\"x = hub_utils.from_pretrained(cache_dir, 'model.pt', '.', archive_map=CodeHiFiGANVocoder.hub_models(), config_yaml='config.json', fp16=False, is_vocoder=True)\\\", \\\"with open(f'{x['args']['data']}/config.json') as f:\\\", '  vocoder_cfg = json.load(f)', \\\"assert (len(x['args']['model_path']) == 1), 'Too many vocoder models in the input'\\\", \\\"vocoder = CodeHiFiGANVocoder(x['args']['model_path'][0], vocoder_cfg)\\\", 'tts_model = VocoderHubInterface(vocoder_cfg, vocoder)', 'tts_sample = tts_model.get_model_input(unit)', 'wav, sr = tts_model.get_prediction(tts_sample)', 'ipd.Audio(wav, rate=sr)']}, 'performance': {'dataset': 'MuST-C', 'accuracy': 'Not specified'}, 'description': 'Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain.'}} </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ù€ Natural Language ProcessingØŒ Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\" ÙˆØ§Ù…Ù„Ø£ Ø§Ù„ÙØ¬ÙˆØ© Ø¨ÙƒÙ„Ù…Ø© Ù…Ù†Ø§Ø³Ø¨Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. Import the necessary library, which is \\\\'pipeline\\\\' from transformers.\\\\n2. Use the \\\\'pipeline\\\\' function to load the model \\\\'bert-base-cased\\\\'.\\\\n3. Call the unmasker object with the input sentence containing the MASK token, \\\"Can you structure this article on [MASK] design trends?\\\".\\\\n4. The model uses masked language modeling to fill in the missing word in the given sentence with the most likely prediction.', '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'bert-base-cased\\\\')\\\\nfilled_sentence = unmasker(\\\"Can you structure this article on [MASK] design trends?\\\")', 'api_call': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-base-cased', 'api_call': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", 'api_arguments': ['model'], 'python_environment_requirements': [['transformers']], 'example_code': ['from transformers import pipeline', \\\"unmasker = pipeline('fill-mask', model='bert-base-cased')\\\", \\\"unmasker(Hello I'm a [MASK] model.)\\\"], 'performance': {'dataset': 'GLUE', 'accuracy': 79.6}, 'description': \\\"BERT base model (cased) is a pre-trained transformer model on English language using a masked language modeling (MLM) objective. It was introduced in a paper and first released in a repository. This model is case-sensitive, which means it can differentiate between 'english' and 'English'. The model can be used for masked language modeling or next sentence prediction, but it's mainly intended to be fine-tuned on a downstream task.\\\" </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØª. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø¨Ø¯Ù‚Ø© ÙˆÙØ¹Ø§Ù„ÙŠØ© Ø¹Ø§Ù„ÙŠØ©.? [/INST]{'domain': 'Audio Audio Classification', 'api_call': \\\"foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\", 'api_provider': 'SpeechBrain', 'explanation': \\\"1. Import the necessary library 'foreign_class' from speechbrain.pretrained.interfaces.\\\", 'code': \\\"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\\naudio_file = 'path/to/audio/file'\\\\nout_prob, score, index, emotion = classifier.classify_file(audio_file)\\\\nprint(emotion)\\\", 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'SpeechBrain', 'functionality': 'Emotion Recognition', 'api_name': 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP', 'api_call': \\\"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\\naudio_file = 'path/to/audio/file'\\\\nout_prob, score, index, emotion = classifier.classify_file(audio_file)\\\\nprint(emotion)\\\", 'api_arguments': ['file_path'], 'python_environment_requirements': 'pip install speechbrain', 'example_code': \\\"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\\nout_prob, score, index, text_lab = classifier.classify_file('speechbrain/emotion-recognition-wav2vec2-IEMOCAP/anger.wav')\\\\nprint(text_lab)\\\", 'performance': {'dataset': 'IEMOCAP', 'accuracy': '78.7%'}, 'description': 'This repository provides all the necessary tools to perform emotion recognition with a fine-tuned wav2vec2 (base) model using SpeechBrain. It is trained on IEMOCAP training data.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù…Ù†ØªØ¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù‚ÙŠØ§Ø³ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„ØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªÙ†Ø¨Ø¤ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù…Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªÙˆÙØ±Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we need to install sentence-transformers and import SentenceTransformer class. \\\\n2. Then, create an instance of the SentenceTransformer with the model 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'. This model is designed to map sentences to 384-dimensional dense vectors, which are suitable for identifying similar sentences or phrases.\\\\n3. Using the model's encode function, we can easily convert a list of user phrases into their corresponding embeddings.\\\\n4. These embeddings can then be used to compare and identify similar phrases in real time, which can be useful for the chatbot.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nuser_phrases = [\\\"I want to buy a shirt\\\", \\\"I\\\\'d like to purchase a blouse\\\"]\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\\\\')\\\\nembeddings = model.encode(user_phrases)', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ø´Ø±ÙƒØ© Ù…ØªØ®ØµØµØ© ÙÙŠ ØªØ·ÙˆÙŠØ± Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØµÙˆØ± ÙˆÙ†Ø±ØºØ¨ ÙÙŠ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§ØŒ Ø®Ø§ØµØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø©. Ù†Ø·Ù…Ø­ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… ÙŠÙ…ÙƒÙ†Ù‡ ÙÙ‡Ù… Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØµÙˆØ± Ø¨Ø´ÙƒÙ„ Ù…ØªÙƒØ§Ù…Ù„ Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ø³ØªØ¬Ø§Ø¨ØªÙ‡ Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©.? [/INST]{'domain': 'Multimodal Document Question Answer', 'api_call': \\\"pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\", 'code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\\\nquestion = 'What does the shipping manifest say?'\\\\ndocument = 'path/to/your/image/file.png'\\\\nresult = qa_pipeline(question=question, document=document)\\\\nshipping_manifest_text = result['answer']\\\", 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\\\nquestion = 'What does the shipping manifest say?'\\\\ndocument = 'path/to/your/image/file.png'\\\\nresult = qa_pipeline(question=question, document=document)\\\\nshipping_manifest_text = result['answer']\\\", 'api_arguments': {'model': 'hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa'}, 'python_environment_requirements': {'transformers': '4.27.4', 'pytorch': '2.0.0+cu117', 'datasets': '2.11.0', 'tokenizers': '0.13.2'}, 'example_code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\\\nquestion = 'What is the total amount?'\\\\ndocument = 'path/to/your/image/file.png'\\\\nresult = qa_pipeline(question=question, document=document)\\\\nprint(result)\\\", 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.843}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.'}} </s>\"\n"}
{"text": "<s>[INST]Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªØ­ÙˆÙŠÙ„ Ù…Ù‚Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙ ÙÙŠØ¯ÙŠÙˆ Ø¢Ø®Ø± Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ØªØµÙˆÙŠØ± Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØŒ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø¯Ù‚Ø© Ø§Ù„ØªØµÙ†ÙŠÙ ÙˆØ¬ÙˆØ¯Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨.? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"pipeline('zero-shot-classification', model='MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. First, import the required libraries, specifically 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\\\\')\\\\nsequence_to_classify = \\\"Researchers have recently discovered a new strain of the flu virus that poses a potential threat to global health.\\\"\\\\ncandidate_labels = [\\\\'health\\\\', \\\\'business\\\\', \\\\'sports\\\\', \\\\'entertainment\\\\']\\\\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\\\\nprint(output)', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Classification', 'api_name': 'MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli', 'api_call': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\\\\')\\\\nsequence_to_classify = \\\"Researchers have recently discovered a new strain of the flu virus that poses a potential threat to global health.\\\"\\\\ncandidate_labels = [\\\\'health\\\\', \\\\'business\\\\', \\\\'sports\\\\', \\\\'entertainment\\\\']\\\\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\\\\nprint(output)', 'api_arguments': ['sequence_to_classify', 'candidate_labels', 'multi_label'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli)\\\\nsequence_to_classify = Angela Merkel is a politician in Germany and leader of the CDU\\\\ncandidate_labels = [politics, economy, entertainment, environment]\\\\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\\\\nprint(output)', 'performance': {'dataset': [{'name': 'mnli_test_m', 'accuracy': 0.912}, {'name': 'mnli_test_mm', 'accuracy': 0.908}, {'name': 'anli_test', 'accuracy': 0.702}, {'name': 'anli_test_r3', 'accuracy': 0.64}, {'name': 'ling_test', 'accuracy': 0.87}, {'name': 'wanli_test', 'accuracy': 0.77}]}, 'description': 'This model was fine-tuned on the MultiNLI, Fever-NLI, Adversarial-NLI (ANLI), LingNLI and WANLI datasets, which comprise 885 242 NLI hypothesis-premise pairs. This model is the best performing NLI model on the Hugging Face Hub as of 06.06.22 and can be used for zero-shot classification. It significantly outperforms all other large models on the ANLI benchmark.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ Ø¨Ø´ÙƒÙ„ ØµÙØ±ÙŠ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]{'domain': 'Tabular Tabular Classification', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_provider': 'Joblib', 'explanation': '1. First, import the necessary libraries, including json, joblib, and pandas.', 'code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\n\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\n\\\\npredictions = model.predict(data)\\\", 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Joblib', 'functionality': 'Carbon Emissions', 'api_name': 'omarques/autotrain-in-class-test-demo-1659958767', 'api_call': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\n\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\n\\\\npredictions = model.predict(data)\\\", 'api_arguments': ['data'], 'python_environment_requirements': ['json', 'joblib', 'pandas'], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'omarques/autotrain-data-in-class-test-demo', 'accuracy': 0.983}, 'description': 'A model trained for binary classification of carbon emissions using AutoTrain.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù…Ø¬ÙŠØ© ØªØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ Ø§Ù„ØµÙˆØªÙŠ. Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ©. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø´ÙƒÙ„ Ø®Ø§Øµ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± ÙˆØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ Ø§Ù„ØµÙˆØªÙŠ Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ ÙˆÙØ¹Ø§Ù„.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"pipeline('audio-classification', model='mazkooleg/0-9up-wavlm-base-plus-ft')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'mazkooleg/0-9up-wavlm-base-plus-ft'.\\\\n3. The loaded model will be used for audio classification, which is a task where the model makes a prediction for a given audio sample. In this case, we want to predict the action based on the user's voice command.\\\\n4. We will provide the voice command as input and the model will classify it, allowing the home automation system to execute the appropriate action.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\naudio_classifier = pipeline('audio-classification', model='mazkooleg/0-9up-wavlm-base-plus-ft')\\\\n# Replace 'audio_file_path.wav' with the path to your audio file\\\\naudio_categories = ['lights_on', 'lights_off', 'increase_temp', 'decrease_temp']\\\\npredictions = audio_classifier('audio_file_path.wav', categories = audio_categories)\\\", 'api_call': \\\"pipeline('audio-classification', model='mazkooleg/0-9up-wavlm-base-plus-ft')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mazkooleg/0-9up-wavlm-base-plus-ft', 'api_call': \\\"pipeline('audio-classification', model='mazkooleg/0-9up-wavlm-base-plus-ft')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers==4.27.3, torch==1.11.0, datasets==2.10.1, tokenizers==0.12.1'], 'example_code': '', 'performance': {'dataset': 'mazkooleg/0-9up_google_speech_commands_augmented_raw', 'accuracy': 0.9973}, 'description': 'This model is a fine-tuned version of microsoft/wavlm-base-plus on the None dataset. It achieves the following results on the evaluation set: Loss: 0.0093, Accuracy: 0.9973.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙØ±Ø¬Ù‰ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„ØªÙŠ ØªØ±ØºØ¨ ÙÙŠ ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ø¥Ù„Ù‰ Ø¹Ù†Ø§ØµØ± Ù…Ø®ØªÙ„ÙØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ Ø«Ù… Ù‚Ù… Ø¨ØªÙ†ÙÙŠØ° Ø¹Ù…Ù„ÙŠØ© ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ±Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Firstly, you need to import the necessary classes from the transformers and PIL packages (OneFormerForUniversalSegmentation for image segmentation, OneFormerProcessor for pre-processing and post-processing, and Image for opening image files).\\\\n2. Then, you need to create an instance of the OneFormerForUniversalSegmentation model using the from_pretrained function and passing the 'shi-labs/oneformer_ade20k_swin_large' as the pre-trained model name.\\\\n3. Load the image data you need to process (e.g. from a file or stream).\\\\n4. The model then can be used to segment the image into its semantic constituents (trees, cars, buildings, etc.).\\\", '<<<code>>>': \\\"from transformers import OneFormerForUniversalSegmentation\\\\nfrom transformers import OneFormerProcessor\\\\nfrom PIL import Image\\\\nimage = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with path to your image\\\\nprocessor = OneFormerProcessor.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\\\\nsemantic_inputs = processor(images=image, task_inputs=['semantic'], return_tensors='pt')\\\\nsemantic_outputs = model(**semantic_inputs)\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(semantic_outputs, target_sizes=[image.size[::-1]])[0]\\\", 'api_call': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'shi-labs/oneformer_ade20k_swin_large', 'api_call': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\\\", 'api_arguments': ['images', 'task_inputs', 'return_tensors'], 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\\\nsemantic_outputs = model(**semantic_inputs)\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'scene_parse_150', 'accuracy': None}, 'description': 'OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø§Ù„Ø´Ø±ÙƒØ© Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ø¥Ù†ØªØ§Ø¬ Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠØ³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ.? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"CrossEncoder('cross-encoder/nli-deberta-v3-small')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-small')\\\\nsentence_pairs = [('The sky is blue', 'The sky color is blue'), ('A man is walking', 'A male is strolling')]\\\\nmatching_scores = model.predict(sentence_pairs)\\\", 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Classification', 'api_name': 'cross-encoder/nli-deberta-v3-small', 'api_call': \\\"from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-small')\\\\nsentence_pairs = [('The sky is blue', 'The sky color is blue'), ('A man is walking', 'A male is strolling')]\\\\nmatching_scores = model.predict(sentence_pairs)\\\", 'api_arguments': ['sentence1', 'sentence2'], 'python_environment_requirements': ['sentence_transformers', 'transformers'], 'example_code': \\\"from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-small')\\\\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])\\\", 'performance': {'dataset': {'SNLI-test': '91.65', 'MNLI-mismatched': '87.55'}, 'accuracy': {'SNLI-test': '91.65', 'MNLI-mismatched': '87.55'}}, 'description': 'Cross-Encoder for Natural Language Inference based on microsoft/deberta-v3-small, trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import required libraries 'DetrForObjectDetection' from transformers, 'Image' from PIL, and 'requests'.\\\\n2. Define the URL of the image that you want to detect objects in.\\\\n3. Load the image from the URL using the 'requests' library and the 'PIL.Image.open()' method.\\\\n4. Load the pretrained 'DETR ResNet-101 DC5' model from Facebook Research and enable the model for object detection using 'DetrForObjectDetection.from_pretrained()'.\\\\n5. Create an instance of the 'DetrFeatureExtractor' class, which is responsible for preprocessing the image.\\\\n6. Preprocess the image by calling 'feature_extractor()' on the image.\\\\n7. Pass the preprocessed image to the 'DETR' model to get the logits and bounding box predictions.\\\\n8. Process the output to extract the detected objects and their respective bounding boxes.\\\", '<<<code>>>': \\\"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://example.com/image.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\n\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\", 'api_call': \\\"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/detr-resnet-101-dc5', 'api_call': \\\"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': \\\"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': 'AP 44.9'}, 'description': 'DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ØµØ±ÙŠ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Begin by importing the necessary libraries for this task, which include 'pipeline' from the transformers package and 'Image' from the PIL package.\\\\n2. Use the 'pipeline' function from transformers to load the pre-trained model, 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k' for image classification tasks.\\\\n3. Open the target image, an animal's image, using the PIL 'Image' class.\\\\n4. Use the loaded model to predict the animal species present in the image.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nfrom PIL import Image\\\\nclassifier = pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\\nimage = Image.open('<path_to_animal_image>')\\\\nprediction = classifier(image)\\\\nprint(prediction)\\\", 'api_call': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'api_call': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", 'api_arguments': {'model': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'framework': 'pt'}, 'python_environment_requirements': [['transformers', 'torch']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØªÙˆØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªÙØ¹ÙŠÙ„ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"ORTModelForSeq2SeqLM.from_pretrained('optimum/t5-small')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'pipeline', 'AutoTokenizer', and 'ORTModelForSeq2SeqLM' from transformers and optimum.onnxruntime.\\\\n2. Use 'AutoTokenizer' to load the tokenizer for the 'optimum/t5-small' model.\\\\n3. Load the model, 'optimum/t5-small', using 'ORTModelForSeq2SeqLM.from_pretrained()'.\\\\n4. Create a translation pipeline by passing the target translation task, model, and tokenizer as arguments. In this case, the task is 'translation_en_to_fr'.\\\\n5. Use the created translator pipeline to process the English input text and obtain the translated French output.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'optimum/t5-small\\\\')\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(\\\\'optimum/t5-small\\\\')\\\\ntranslator = pipeline(\\\\'translation_en_to_fr\\\\', model=model, tokenizer=tokenizer)\\\\nenglish_text = \\\"My name is Eustache and I have a pet raccoon.\\\"\\\\nresults = translator(english_text)\\\\nprint(results[0][\\\\'translation_text\\\\'])', 'api_call': \\\"ORTModelForSeq2SeqLM.from_pretrained('optimum/t5-small')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'optimum/t5-small', 'api_call': \\\"ORTModelForSeq2SeqLM.from_pretrained('optimum/t5-small')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers', 'optimum.onnxruntime']], 'example_code': 'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\\\nresults = translator(My name is Eustache and I have a pet raccoon)\\\\nprint(results)', 'performance': {'dataset': 'c4', 'accuracy': 'N/A'}, 'description': 'T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…ÙŠÙ„ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØµÙˆØª Ø¨ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠØªØ·Ù„Ø¨ Ø°Ù„Ùƒ Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø³Ù…ÙˆØ¹ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ.? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur')\\\", 'api_provider': 'Fairseq', 'explanation': '', 'code': \\\"import IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nimport torchaudio\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur')\\\\nmodel = models[0].cpu()\\\\ncfg['task'].cpu = True\\\\n\\\\naudio, _ = torchaudio.load('input_audio_path.flac')\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\\\\nvocoder = load_vocoder_model('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur')\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\n\\\\nipd.Audio(wav, rate=sr)\\\", 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Fairseq', 'functionality': 'Speech-to-speech translation', 'api_name': 'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur', 'api_call': \\\"import IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nimport torchaudio\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur')\\\\nmodel = models[0].cpu()\\\\ncfg['task'].cpu = True\\\\n\\\\naudio, _ = torchaudio.load('input_audio_path.flac')\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\\\\nvocoder = load_vocoder_model('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur')\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\n\\\\nipd.Audio(wav, rate=sr)\\\", 'api_arguments': {'audio': '16000Hz mono channel audio'}, 'python_environment_requirements': ['fairseq', 'hub_utils', 'huggingface_hub', 'IPython.display', 'torchaudio'], 'example_code': ['import json', 'import os', 'from pathlib import Path', 'import IPython.display as ipd', 'from fairseq import hub_utils', 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub', 'from fairseq.models.speech_to_text.hub_interface import S2THubInterface', 'from fairseq.models.text_to_speech import CodeHiFiGANVocoder', 'from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface', 'from huggingface_hub import snapshot_download', 'import torchaudio', 'cache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)', 'models, cfg, task = load_model_ensemble_and_task_from_hf_hub(', 'facebook/xm_transformer_s2ut_800m-es-en-st-asr-bt_h1_2022,', 'arg_overrides={config_yaml: config.yaml, task: speech_to_text},', 'cache_dir=cache_dir,', ')', 'model = models[0].cpu()', 'cfg[task].cpu = True', 'generator = task.build_generator([model], cfg)', '# requires 16000Hz mono channel audio', 'audio, _ = torchaudio.load(/Users/lpw/git/api-inference-community/docker_images/fairseq/tests/samples/sample2.flac)', 'sample = S2THubInterface.get_model_input(task, audio)', 'unit = S2THubInterface.get_prediction(task, model, generator, sample)', 'library_name = fairseq', 'cache_dir = (', ' cache_dir or (Path.home() / .cache / library_name).as_posix()', ')', 'cache_dir = snapshot_download(', ' ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name', ')', 'x = hub_utils.from_pretrained(', ' cache_dir,', ' model.pt,', ' .,', ' archive_map=CodeHiFiGANVocoder.hub_models(),', ' config_yaml=config.json,', ' fp16=False,', ' is_vocoder=True,', ')', \\\"with open(f{x['args']['data']}/config.json) as f:\\\", ' vocoder_cfg = json.load(f)', 'assert (', ' len(x[args][model_path]) == 1', '), Too many vocoder models in the input', 'vocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)', 'tts_model = VocoderHubInterface(vocoder_cfg, vocoder)', 'tts_sample = tts_model.get_model_input(unit)', 'wav, sr = tts_model.get_prediction(tts_sample)', 'ipd.Audio(wav, rate=sr)'], 'performance': {'dataset': 'covost2', 'accuracy': None}, 'description': 'Speech-to-speech translation model from fairseq S2UT (paper/code) for Spanish-English. Trained on mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli.'}} </s>\"\n"}
{"text": "<s>[INST]Ù„ØªØ¹Ù„Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ØŒ Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚. Ø­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙÙŠ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙˆÙ‚Ø¯Ù… Ø­Ù„Ø§Ù‹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© ÙˆØ§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ÙŠØ©. Ø§Ø³ØªÙØ¯ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ø¯Ø§Ø¡ Ù…ØªÙ…ÙŠØ².? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, which include 'pipeline' from the transformers package.\\\\n2. Use the pipeline function to create a depth estimation pipeline. Make sure to specify the model's name, 'sayakpaul/glpn-nyu-finetuned-diode'.\\\\n3. This model is specifically trained to perform depth estimation tasks, and it will help the robot estimate the depth of various objects in its surroundings.\\\\n4. Once you have set up your pipeline and loaded the model, you can input an image and use the pipeline to estimate depth in the scene.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ndepth_estimator = pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\\\\nimage_path = 'path_to_image.jpg'\\\\n# replace 'path_to_image.jpg' with the path to your image\\\\ndepth_map = depth_estimator(image_path)\\\", 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode', 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4359, 'Rmse': 0.4276}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø´ÙƒÙ„ Ù„Ø§ ÙŠØ´ØªØ±Ø· Ø¨Ù‡Ø§ Ø§Ù„ÙˆØµÙØŒ ÙˆÙŠØªÙˆØ¬Ø¨ Ø¹Ù„ÙŠÙ†Ø§ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ù„Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ ØªØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ù‚ØµÙ‰ Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† ØªÙ„Ùƒ Ø§Ù„ØªÙ‚Ù†ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-ffhq-1024')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. In order to achieve the task, we will need to install the 'diffusers' library and import the required class 'DiffusionPipeline' from it.\\\\n2. The 'DiffusionPipeline' class is then used to load the pre-trained model 'google/ncsnpp-ffhq-1024'. This model is specifically created to generate high-quality human face images of resolution 1024x1024 pixels.\\\\n3. Utilizing the loaded model, we can generate an image following the proper sampling procedure. This will output a realistic human face image.\\\\n4. Finally, we can save the generated image to a file.\\\", '<<<code>>>': \\\"!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = 'google/ncsnpp-ffhq-1024'\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[0]\\\\nimage.save('sde_ve_generated_image.png')\\\", 'api_call': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-ffhq-1024')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/ncsnpp-ffhq-1024', 'api_call': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-ffhq-1024')\\\", 'api_arguments': ['model_id'], 'python_environment_requirements': [['diffusers']], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = google/ncsnpp-ffhq-1024\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[sample]\\\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. Achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¨Ø¯Ø£ Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Fill-Mask', 'api_call': \\\"AlbertForMaskedLM.from_pretrained('uer/albert-base-chinese-cluecorpussmall')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': 'from transformers import BertTokenizer, AlbertForMaskedLM, FillMaskPipeline\\\\ntokenizer = BertTokenizer.from_pretrained(\\\\'uer/albert-base-chinese-cluecorpussmall\\\\')\\\\nmodel = AlbertForMaskedLM.from_pretrained(\\\\'uer/albert-base-chinese-cluecorpussmall\\\\')\\\\nunmasker = FillMaskPipeline(model, tokenizer)\\\\nresult = unmasker(\\\"ä¸­å›½çš„é¦–éƒ½æ˜¯[MASK]äº¬ã€‚\\\")\\\\ncompleted_sentence = result[0][\\\"sequence\\\"]', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Transformers', 'functionality': 'Fill-Mask', 'api_name': 'uer/albert-base-chinese-cluecorpussmall', 'api_call': 'from transformers import BertTokenizer, AlbertForMaskedLM, FillMaskPipeline\\\\ntokenizer = BertTokenizer.from_pretrained(\\\\'uer/albert-base-chinese-cluecorpussmall\\\\')\\\\nmodel = AlbertForMaskedLM.from_pretrained(\\\\'uer/albert-base-chinese-cluecorpussmall\\\\')\\\\nunmasker = FillMaskPipeline(model, tokenizer)\\\\nresult = unmasker(\\\"ä¸­å›½çš„é¦–éƒ½æ˜¯[MASK]äº¬ã€‚\\\")\\\\ncompleted_sentence = result[0][\\\"sequence\\\"]', 'api_arguments': ['model', 'tokenizer'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import BertTokenizer, AlbertForMaskedLM, FillMaskPipeline\\\\ntokenizer = BertTokenizer.from_pretrained(uer/albert-base-chinese-cluecorpussmall)\\\\nmodel = AlbertForMaskedLM.from_pretrained(uer/albert-base-chinese-cluecorpussmall)\\\\nunmasker = FillMaskPipeline(model, tokenizer)\\\\nunmasker(ä¸­å›½çš„é¦–éƒ½æ˜¯[MASK]äº¬ã€‚)', 'performance': {'dataset': 'CLUECorpusSmall', 'accuracy': 'Not provided'}, 'description': 'This is the set of Chinese ALBERT models pre-trained by UER-py on the CLUECorpusSmall dataset. The model can be used for tasks like text generation and feature extraction.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚Ø¯Ù… Ù„Ø¹Ù…ÙŠÙ„Ù†Ø§ Ø§Ù„Ø°ÙŠ ÙŠØ¹Ù…Ù„ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ‡ Ù„Ø¯Ø¹Ù… Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. First, import the required pipeline function from the transformers library.\\\\n2. Initialize the table-question-answering pipeline by specifying the model as 'google/tapas-medium-finetuned-wtq'. This model is pretrained on WikiTable Questions (WTQ) and can handle table question answering tasks.\\\\n3. Define the table and the question that needs to be answered. The table should be formatted in a way that the model can understand.\\\\n4. Call the table-question-answering pipeline with the table and question input to get the answer.\\\\n5. The returned result will be the model's best guess for the answer based on the given table and question.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntable_qa = pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\\ntable = {...}\\\\nquery = '...'\\\\nanswer = table_qa(table=table, query=query)\\\", 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/tapas-medium-finetuned-wtq', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", 'api_arguments': 'table, query', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntable_qa = pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\\n\\\\n# Define the table and the query\\\\ntable = {...}\\\\nquery = '...'\\\\n\\\\n# Get the answer\\\\nanswer = table_qa(table=table, query=query)\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.4324}, 'description': 'TAPAS medium model fine-tuned on WikiTable Questions (WTQ). This model is pretrained on a large corpus of English data from Wikipedia and is used for answering questions related to a table.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ ØµÙˆØ±Ø© Ø¬Ø¯ÙŠØ¯Ø© ØªØ­ÙˆÙŠ Ø¹Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ù…Ù† ØµÙˆØ±Ø© Ø¥Ù„Ù‰ ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\\\", '<<<api_provider>>>': 'Diffusers', '<<<explanation>>>': '1. Import the required packages and classes, such as ControlNetModel, StableDiffusionControlNetPipeline, HEDdetector, and UniPCMultistepScheduler, among others.\\\\n2. Load the pre-trained ControlNet model using the from_pretrained method with the checkpoint \\\\'lllyasviel/control_v11p_sd15_scribble\\\\'\\\\n3. Create a processing pipeline using the StableDiffusionControlNetPipeline with the loaded ControlNet model.\\\\n4. Initialize the HEDdetector processor with the pretrained \\\\'lllyasviel/Annotators\\\\'. The processor helps in creating the scribble control images based on the text description.\\\\n5. Provide the input text description, such as \\\"a cozy cabin in the woods\\\".\\\\n6. Generate the scribble control image using the processor.\\\\n7. Finally, use the pre-built pipeline to convert the control image with the provided text description into the final visual image.', '<<<code>>>': 'from controlnet_aux import HEDdetector\\\\nfrom diffusers import (\\\\n    ControlNetModel,\\\\n    StableDiffusionControlNetPipeline,\\\\n    UniPCMultistepScheduler,\\\\n)\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/control_v11p_sd15_scribble\\\\')\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n    \\\\'runwayml/stable-diffusion-v1-5\\\\', controlnet=controlnet)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\nprocessor = HEDdetector.from_pretrained(\\\\'lllyasviel/Annotators\\\\')\\\\nprompt = \\\"a cozy cabin in the woods\\\"\\\\ncontrol_image = processor(prompt, scribble=True)  # Replace with actual image input\\\\nfinal_image = pipe(prompt, num_inference_steps=30, image=control_image).images[0]\\\\nfinal_image.save(\\\\'images/image_out.png\\\\')', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\\\", 'provider': 'Diffusers', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/control_v11p_sd15_scribble', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\\\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_scribble', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0']], 'example_code': \\\"import torch\\\\nimport os\\\\nfrom huggingface_hub import HfApi\\\\nfrom pathlib import Path\\\\nfrom diffusers.utils import load_image\\\\nfrom PIL import Image\\\\nimport numpy as np\\\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\\\nfrom diffusers import (\\\\n ControlNetModel,\\\\n StableDiffusionControlNetPipeline,\\\\n UniPCMultistepScheduler,\\\\n)\\\\ncheckpoint = lllyasviel/control_v11p_sd15_scribble\\\\nimage = load_image(\\\\n https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/images/input.png\\\\n)\\\\nprompt = royal chamber with fancy bed\\\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\\\ncontrol_image = processor(image, scribble=True)\\\\ncontrol_image.save(./images/control.png)\\\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(0)\\\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\nimage.save('images/image_out.png')\\\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ ØªÙˆÙ‚Ø¹ Ù†ÙˆØ¹ Ø§Ù„ØµÙˆØªÙŠØ§Øª ÙˆÙ…Ø­ØªÙˆØ§Ù‡Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries such as 'datasets' to load the speech dataset, and 'pipeline' from transformers.\\\\n2. Use the 'load_dataset()' function to load a dataset containing speech audio files. In this case, ensure that your speech input is sampled at 16kHz.\\\\n3. Create an audio classification pipeline by specifying 'audio-classification' as the task and 'superb/hubert-large-superb-er' as the model.\\\\n4. The classifier can be used to predict emotions in the audio data by providing the audio file to the classifier.\\\\n5. The classifier generates a list of emotions along with their corresponding probabilities. The emotions with the highest probabilities are returned.\\\", '<<<code>>>': \\\"from datasets import load_dataset\\\\nfrom transformers import pipeline\\\\ndataset = load_dataset('anton-l/superb_demo', 'er', split='session1')\\\\nclassifier = pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\\nlabels = classifier(dataset[0]['file'], top_k=5)\\\", 'api_call': \\\"pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'superb/hubert-large-superb-er', 'api_call': \\\"pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\", 'api_arguments': ['model'], 'python_environment_requirements': [['transformers', 'datasets', 'librosa']], 'example_code': \\\"from datasets import load_dataset\\\\nfrom transformers import pipeline\\\\ndataset = load_dataset('anton-l/superb_demo', 'er', split='session1')\\\\nclassifier = pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\\nlabels = classifier(dataset[0]['file'], top_k=5)\\\", 'performance': {'dataset': 'IEMOCAP', 'accuracy': 0.6762}, 'description': \\\"This is a ported version of S3PRL's Hubert for the SUPERB Emotion Recognition task. The base model is hubert-large-ll60k, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.\\\" </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create the table-question-answering pipeline with the \\\\'google/tapas-large-finetuned-wtq\\\\' model, which is specifically designed for answering questions related to tables and spreadsheets.\\\\n3. Apply the qa_pipeline to your table data and provide the query \\\"Total number of SUVs in the inventory\\\".\\\\n4. The model will process the table data and return you the count of SUVs in the inventory.', '<<<code>>>': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\\nquery = 'Total number of SUVs in the inventory'\\\\nresult = qa_pipeline(table=table, query=query)\\\", 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/tapas-large-finetuned-wtq', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", 'api_arguments': {'model': 'google/tapas-large-finetuned-wtq', 'task': 'table-question-answering'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\nqa_pipeline = pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\\n\\\\nresult = qa_pipeline(table=table, query=query)\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.5097}, 'description': 'TAPAS large model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ ØªØ·Ø¨ÙŠÙ‚ ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ø³ØªØ¯Ø¹Ø§Ø¡ ÙˆØ¸ÙŠÙØ© ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø¢Ù„ÙŠ Ù…Ù† Ø£Ø¬Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØªØ­Ø¯ÙŠØ¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ. Ø­Ø§ÙˆÙ„ ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ¶Ø¨Ø· Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ø²ÙŠØ§Ø¯Ø© Ø¯Ù‚Ø© ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ².? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-company_all-903429548', use_auth_token=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries: AutoModelForTokenClassification and AutoTokenizer from the transformers module.\\\\n2. Load the pretrained model by passing the model name 'ismail-lucifer011/autotrain-company_all-903429548', and use_auth_token=True in the from_pretrained function.\\\\n3. Load the tokenizer using the same model name.\\\\n4. Tokenize the input text (news article) using the tokenizer and obtain the input tensors.\\\\n5. Pass the input tensors to the model, and it will return the predictions for each token.\\\\n6. Extract the tokens with the highest confidence scores as company names.\\\", '<<<code>>>': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'ismail-lucifer011/autotrain-company_all-903429548\\\\', use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'ismail-lucifer011/autotrain-company_all-903429548\\\\', use_auth_token=True)\\\\nnews_article = \\\"News article text here...\\\"\\\\ninputs = tokenizer(news_article, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-company_all-903429548', use_auth_token=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': '903429548', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-company_all-903429548', use_auth_token=True)\\\", 'api_arguments': {'inputs': 'I love AutoTrain'}, 'python_environment_requirements': [{'transformers': 'AutoModelForTokenClassification, AutoTokenizer'}], 'example_code': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\\\noutputs = model(**inputs)', 'performance': {'dataset': 'ismail-lucifer011/autotrain-data-company_all', 'accuracy': 0.9979930566588805}, 'description': 'A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ù†Ø´Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØµÙˆØ± Ø¨Ø¯ÙˆÙ† Ù†ØµÙˆØµØŒ ÙˆÙ†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Image-to-Image.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the required classes, such as ControlNetModel, StableDiffusionControlNetPipeline, and UniPCMultistepScheduler, from the diffusers, transformers, and accelerate libraries.\\\\n2. Load the pre-trained ControlNet model \\\\'lllyasviel/control_v11p_sd15_seg\\\\'.\\\\n3. Create a new instance of the StableDiffusionControlNetPipeline class, combining the pre-trained ControlNet model and a stable diffusion model \\\\'runwayml/stable-diffusion-v1-5\\\\'.\\\\n4. Set the scheduler and enable model CPU offloading.\\\\n5. Provide the prompt text description (\\\"A cozy living room with a blue sofa, a wooden coffee table, and a large window overlooking a garden\\\") as input, and generate a new image using the pipeline.\\\\n6. Save the generated image as \\\\'image_out.png\\\\'.', '<<<code>>>': 'from diffusers import ControlNetModel, StableDiffusionControlNetPipeline, UniPCMultistepScheduler\\\\nimport torch\\\\n\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/control_v11p_sd15_seg\\\\', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', controlnet=controlnet, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\n\\\\nprompt = \\\"A cozy living room with a blue sofa, a wooden coffee table, and a large window overlooking a garden.\\\"\\\\ngenerator = torch.manual_seed(0)\\\\nimage = pipe(prompt, num_inference_steps=30, generator=generator).images[0]\\\\nimage.save(\\\\'images/image_out.png\\\\')', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/control_v11p_sd15_seg', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg')\\\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_seg'}, 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate']], 'example_code': \\\"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(0)\\\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\nimage.save('images/image_out.png')\\\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø£ÙˆØ¶Ø§Ø¹ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ù„Ø¶Ù…Ø§Ù† Ø¯Ù‚Ø© ÙˆØ¬ÙˆØ¯Ø© Ø§Ù„ØªØµØ§Ù…ÙŠÙ….? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler), torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the necessary modules and classes from the diffusers library, such as StableDiffusionPipeline and EulerDiscreteScheduler.\\\\n2. Load the pretrained \\\\'stabilityai/stable-diffusion-2\\\\' text-to-image generation model using the from_pretrained method of the StableDiffusionPipeline class.\\\\n3. Load the associated pretrained scheduler for this model using the from_pretrained method of the EulerDiscreteScheduler class.\\\\n4. Use the loaded pipeline to generate an image based on the provided text prompt describing the desired watch design, such as \\\"a gold watch with a black leather strap and a round dial.\\\"\\\\n5. Save the generated image to the desired location and display it on the e-commerce website as the output.', '<<<code>>>': \\\"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = 'stabilityai/stable-diffusion-2'\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder='scheduler')\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to('cuda')\\\\nprompt = 'a gold watch with a black leather strap and a round dial'\\\\nimage = pipe(prompt).images[0]\\\\nimage.save('generated_watch.png')\\\", 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler), torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler), torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors']], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‡Ø¯Ù Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø¥Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ Ù†ØµÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù…ØŒ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙˆØµÙ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù„Ø§Ø²Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙˆØµÙ Ù…Ø´Ø§Ø¨Ù‡ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŸ? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"Tacotron2.from_hparams(source='padmalcom/tts-tacotron2-german')\\\", '<<<api_provider>>>': 'speechbrain', '<<<explanation>>>': \\\"1. Import the necessary functions from speechbrain.pretrained, namely Tacotron2 and HIFIGAN, as well as torchaudio.\\\\n2. Create a Tacotron2 speech synthesis model by loading the pre-trained German model using the source 'padmalcom/tts-tacotron2-german'.\\\\n3. Instantiate a HIFIGAN vocoder model, which will synthesize waveforms from the computed spectrograms. Use the source 'speechbrain/tts-hifigan-ljspeech'.\\\\n4. Encode the German text to be converted to voice using the encode_text method of the Tacotron2 model.\\\\n5. Decode the generated mel spectrograms using the HIFIGAN vocoder to produce the waveforms for the audio.\\\\n6. Use torchaudio to save the resulting waveform as a .wav file for broadcasting in Germany.\\\", '<<<code>>>': 'import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2, HIFIGAN\\\\ntacotron2 = Tacotron2.from_hparams(source=\\\\'padmalcom/tts-tacotron2-german\\\\')\\\\nhifi_gan = HIFIGAN.from_hparams(source=\\\\'speechbrain/tts-hifigan-ljspeech\\\\')\\\\ngerman_text = \\\"Die Sonne schien den ganzen Tag.\\\"\\\\nmel_output, mel_length, alignment = tacotron2.encode_text(german_text)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save(\\\\'example_TTS.wav\\\\', waveforms.squeeze(1),22050)', 'api_call': \\\"Tacotron2.from_hparams(source='padmalcom/tts-tacotron2-german')\\\", 'provider': 'speechbrain', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'padmalcom/tts-tacotron2-german', 'api_call': \\\"Tacotron2.from_hparams(source='padmalcom/tts-tacotron2-german')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['pip install speechbrain']], 'example_code': ['import torchaudio', 'from speechbrain.pretrained import Tacotron2', 'from speechbrain.pretrained import HIFIGAN', 'tacotron2 = Tacotron2.from_hparams(source=padmalcom/tts-tacotron2-german, savedir=tmpdir_tts)', 'hifi_gan = HIFIGAN.from_hparams(source=speechbrain/tts-hifigan-ljspeech, savedir=tmpdir_vocoder)', 'mel_output, mel_length, alignment = tacotron2.encode_text(Die Sonne schien den ganzen Tag.)', 'waveforms = hifi_gan.decode_batch(mel_output)', \\\"torchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\\\"], 'performance': {'dataset': 'custom german dataset', 'accuracy': 'Not provided'}, 'description': 'Text-to-Speech (TTS) with Tacotron2 trained on a custom german dataset with 12 days voice using speechbrain. Trained for 39 epochs (english speechbrain models are trained for 750 epochs) so there is room for improvement and the model is most likely to be updated soon. The hifigan vocoder can fortunately be used language-independently.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… ÙŠØ³ØªØ·ÙŠØ¹ ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„ÙƒØŸ? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-3B')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. First, import the BlenderbotForConditionalGeneration class from the transformers library provided by Hugging Face.', 'code': 'from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\\\\nmodel = BlenderbotForConditionalGeneration.from_pretrained(\\\\'facebook/blenderbot-3B\\\\')\\\\ntokenizer = BlenderbotTokenizer.from_pretrained(\\\\'facebook/blenderbot-3B\\\\')\\\\ninput_text = \\\"Tell me about the movie Inception.\\\"\\\\ninputs = tokenizer(input_text, return_tensors=\\\\'pt\\\\')\\\\noutputs = model.generate(**inputs)\\\\ngenerated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Conversational', 'api_name': 'facebook/blenderbot-3B', 'api_call': 'from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\\\\nmodel = BlenderbotForConditionalGeneration.from_pretrained(\\\\'facebook/blenderbot-3B\\\\')\\\\ntokenizer = BlenderbotTokenizer.from_pretrained(\\\\'facebook/blenderbot-3B\\\\')\\\\ninput_text = \\\"Tell me about the movie Inception.\\\"\\\\ninputs = tokenizer(input_text, return_tensors=\\\\'pt\\\\')\\\\noutputs = model.generate(**inputs)\\\\ngenerated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]', 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'Input a message to start chatting with facebook/blenderbot-3B.', 'performance': {'dataset': 'blended_skill_talk', 'accuracy': 'Not provided'}, 'description': 'BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It is trained on the blended_skill_talk dataset and can engage in multi-turn conversations, providing engaging talking points, asking and answering questions, and displaying knowledge, empathy, and personality. The model is available through the Hugging Face Transformers library.'}} </s>\"\n"}
{"text": "<s>[INST]Ø³Ù†Ù‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø³Ù…ÙˆØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©. ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ ØªØ­ÙˆÙŠÙ„Ù‡ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„Ù‡Ø¬Ø© ÙˆØ§Ù„Ù†Ø¨Ø±Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù„ØµÙˆØª.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes and libraries, which includes SpeechT5Processor, SpeechT5ForTextToSpeech and SpeechT5HifiGan for the purpose of text-to-speech conversion.\\\\n2. Load the pre-trained models for processing text and generating speech, using the 'microsoft/speecht5_tts' model.\\\\n3. Specify the text that you want to convert into speech.\\\\n4. Process the text using the SpeechT5Processor and obtain the input_ids.\\\\n5. Generate the speech with the SpeechT5ForTextToSpeech model using the input_ids, speaker_embeddings (optional), and the vocoder.\\\\n6. Save the generated speech in a .wav file.\\\", '<<<code>>>': 'from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\\\\nimport torch\\\\nimport soundfile as sf\\\\nprocessor = SpeechT5Processor.from_pretrained(\\\\'microsoft/speecht5_tts\\\\')\\\\nmodel = SpeechT5ForTextToSpeech.from_pretrained(\\\\'microsoft/speecht5_tts\\\\')\\\\nvocoder = SpeechT5HifiGan.from_pretrained(\\\\'microsoft/speecht5_hifigan\\\\')\\\\nsummary_text = \\\"During our last week\\\\'s meeting, we discussed project updates, milestones, and upcoming deadlines. We reviewed the budget and assigned new tasks to team members.\\\"\\\\ninputs = processor(text=summary_text, return_tensors=\\\\'pt\\\\')\\\\nspeech = model.generate_speech(inputs[\\\\'input_ids\\\\'], vocoder=vocoder)\\\\nsf.write(\\\\'voice_memo_summary.wav\\\\', speech.numpy(), samplerate=16000)', 'api_call': \\\"SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/speecht5_tts', 'api_call': \\\"SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\\\", 'api_arguments': ['text', 'return_tensors', 'input_ids', 'speaker_embeddings', 'vocoder'], 'python_environment_requirements': ['!pip install git+https://github.com/huggingface/transformers sentencepiece datasets'], 'example_code': \\\"from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\nimport soundfile as sf\\\\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_tts')\\\\nmodel = SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\\\\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\\\\ninputs = processor(text='Hello, my dog is cute', return_tensors='pt')\\\\nembeddings_dataset = load_dataset('Matthijs/cmu-arctic-xvectors', split='validation')\\\\nspeaker_embeddings = torch.tensor(embeddings_dataset[7306]['xvector']).unsqueeze(0)\\\\nspeech = model.generate_speech(inputs['input_ids'], speaker_embeddings, vocoder=vocoder)\\\\nsf.write('speech.wav', speech.numpy(), samplerate=16000)\\\", 'performance': {'dataset': 'LibriTTS', 'accuracy': 'Not specified'}, 'description': 'SpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS. It is a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. It can be used for a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†Ù…ÙˆØ°Ø¬ Ù„Ø±Ø¨Ø· Ø§Ù„ØµÙˆØ± Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø¨Ø§Ù„Ù†ØµÙˆØµ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes and functions from Hugging Face's transformers library, as well as the PIL (Python Imaging Library) and requests packages to handle image data.\\\\n2. Use the method 'from_pretrained' of the TrOCR VisionEncoderDecoderModel class to create a model based on the 'microsoft/trocr-large-handwritten' pre-trained model.\\\\n3. Load the image you want to transcribe using the Image class from PIL. This image data will be used as input for the TrOCR model.\\\\n4. Transform the image data to tensors using the processor, and pass the tensors to the TrOCR model to generate the output text transcription.\\\", '<<<code>>>': 'from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\n\\\\nimage = Image.open(\\\\'handwritten_text_image.jpg\\\\').convert(\\\"RGB\\\")\\\\n# replace \\\\'handwritten_text_image.jpg\\\\' with path to your image\\\\nprocessor = TrOCRProcessor.from_pretrained(\\\\'microsoft/trocr-large-handwritten\\\\')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained(\\\\'microsoft/trocr-large-handwritten\\\\')\\\\n\\\\npixel_values = processor(images=image, return_tensors=\\\\'pt\\\\').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': [{'packages': ['transformers', 'PIL', 'requests']}], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ¹Ù„Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ²ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚ÙŠÙŠÙ… ÙƒÙØ§Ø¡Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©.? [/INST]{'domain': 'Reinforcement Learning', 'api_call': \\\"load_from_hub(repo_id='HumanCompatibleAI/ppo-seals-CartPole-v0', filename='{MODEL FILENAME}.zip')\\\", 'api_provider': 'Stable-Baselines3', 'explanation': '', 'code': \\\"from rl_zoo3 import load_from_hub\\\\nimport gym\\\\n\\\\nrepo_id = 'HumanCompatibleAI/ppo-seals-CartPole-v0'\\\\nfilename = 'your_model_filename.zip'\\\\nmodel = load_from_hub(repo_id=repo_id, filename=filename)\\\\n\\\\nenv = gym.make('seals/CartPole-v0')\\\\nobs = env.reset()\\\\n\\\\ndone = False\\\\nwhile not done:\\\\n    action, _states = model.predict(obs)\\\\n    obs, reward, done, info = env.step(action)\\\\n    env.render()\\\\nenv.close()\\\", 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Stable-Baselines3', 'functionality': 'deep-reinforcement-learning', 'api_name': 'ppo-seals-CartPole-v0', 'api_call': \\\"from rl_zoo3 import load_from_hub\\\\nimport gym\\\\n\\\\nrepo_id = 'HumanCompatibleAI/ppo-seals-CartPole-v0'\\\\nfilename = 'your_model_filename.zip'\\\\nmodel = load_from_hub(repo_id=repo_id, filename=filename)\\\\n\\\\nenv = gym.make('seals/CartPole-v0')\\\\nobs = env.reset()\\\\n\\\\ndone = False\\\\nwhile not done:\\\\n    action, _states = model.predict(obs)\\\\n    obs, reward, done, info = env.step(action)\\\\n    env.render()\\\\nenv.close()\\\", 'api_arguments': ['algo', 'env', 'f'], 'python_environment_requirements': ['rl_zoo3', 'stable-baselines3', 'stable-baselines3-contrib'], 'example_code': 'python -m rl_zoo3.load_from_hub --algo ppo --env seals/CartPole-v0 -orga HumanCompatibleAI -f logs/', 'performance': {'dataset': 'seals/CartPole-v0', 'accuracy': '500.00 +/- 0.00'}, 'description': 'This is a trained model of a PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙˆÙ†ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØµÙˆØ± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„ØªØ­Ø³ÙŠÙ† ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø´Ø®Ø§Øµ ÙÙŠ Ø§Ù„ØµÙˆØ±.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes, such as Swin2SRForImageSuperResolution for the image super-resolution model and Image to process the image data.\\\\n2. Use the from_pretrained method with the model\\\\\\\\textsuperscript{â€™}s name 'caidas/swin2sr-classical-sr-x2-64' to load a pretrained Swin2SR model. This model is designed to upscale images and make them look better.\\\\n3. Load the user's online profile image using the Image class from the PIL package.\\\\n4. Feed the image into the model, which will process it and produce an upscaled, higher-quality version of the image.\\\\n5. Save the image and use it to replace the user's original online photo, thereby improving their profile's appearance.\\\", '<<<code>>>': \\\"from transformers import Swin2SRForImageSuperResolution\\\\nfrom PIL import Image\\\\nimage = Image.open('path_to_user_profile_image.jpg')\\\\nmodel = Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'api_call': \\\"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'caidas/swin2SR-classical-sr-x2-64', 'api_call': \\\"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\\\", 'api_arguments': 'image, model, feature_extractor', 'python_environment_requirements': ['transformers'], 'example_code': 'Refer to the documentation.', 'performance': {'dataset': 'arxiv: 2209.11345', 'accuracy': 'Not provided'}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ø¬Ø¯ÙˆÙ„ ØªÙˆØ¶Ø­ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ ØªØ§Ø¨Ø¹ÙŠ.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\", '<<<api_provider>>>': 'Keras', '<<<explanation>>>': \\\"1. To use the model for detecting anomalies, first, import the necessary libraries: TensorFlow, and Keras.\\\\n2. Load the model using the TFAutoModelForSequenceClassification.from_pretrained() function provided by the Keras library with the model name 'keras-io/timeseries-anomaly-detection'.\\\\n3. The loaded model is a reconstruction convolutional autoencoder, which is suitable for detecting anomalies in timeseries data.\\\\n4. By using the model and the dataset of temperature recordings, you can train the model and identify the anomalies within the dataset.\\\\n5. You will need to split the data into training and validation sets to train and validate the model, and calculate the loss during its training process.\\\", '<<<code>>>': \\\"import tensorflow.compat.v2 as tf\\\\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\\\\nfrom tensorflow.keras.models import TFAutoModelForSequenceClassification\\\\ntemp_data = [series of temperature readings]\\\\nsequence_length = length_of_sequence_for_inputs\\\\n\\\\n# Prepare data for LSTM\\\\ngenerator = TimeseriesGenerator(temp_data, temp_data, length=sequence_length, batch_size=1)\\\\n# Load model\\\\nmodel = TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\\n# Train the model\\\\nmodel.fit(generator, epochs=number_of_epochs)\\\", 'api_call': \\\"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\", 'provider': 'Keras', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keras-io/timeseries-anomaly-detection', 'api_call': \\\"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\", 'api_arguments': {'optimizer': {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'training_precision': 'float32'}, 'python_environment_requirements': [['tensorflow', 'keras']], 'example_code': '', 'performance': {'dataset': 'Numenta Anomaly Benchmark(NAB)', 'accuracy': {'Train Loss': 0.006, 'Validation Loss': 0.008}}, 'description': 'This script demonstrates how you can use a reconstruction convolutional autoencoder model to detect anomalies in timeseries data. We will use the Numenta Anomaly Benchmark(NAB) dataset. It provides artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered, timestamped, single-valued metrics.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ù…Ù‚Ø·Ø¹ ÙÙŠØ¯ÙŠÙˆ Ù…Ø¹ÙŠÙ†.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries: AutoModelForVideoClassification from transformers and torch.\\\\n2. Load the pre-trained model 'lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb' using the from_pretrained method of the AutoModelForVideoClassification class.\\\\n3. The model is trained to recognize aggressive behavior in video footage. Use the model to make predictions on the input video frames or sequence of frames.\\\\n4. The model will output classification probabilities for different categories, and the system can use these probabilities to make a decision about whether aggressive behavior was detected or not.\\\", '<<<code>>>': \\\"from transformers import AutoModelForVideoClassification\\\\nimport torch\\\\nmodel = AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb')\\\\ninputs = torch.tensor(video_frames)\\\\noutputs = model(inputs)\\\\npredictions = torch.softmax(outputs.logits, dim=-1)\\\", 'api_call': \\\"AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb', 'api_call': \\\"AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers, torch, tokenizers, datasets'], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': 0.7298}, 'description': 'This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.5482, Accuracy: 0.7298.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙŠØ³ØªØ®Ø¯Ù… Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¬Ù…Ù„ Ù„ØªØ­Ù„ÙŠÙ„ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ Ù…ØªØ§Ø­Ø© Ø¹Ø§Ù…Ø© Ù…Ø«Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª SNLIØŒ Ø£Ùˆ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ Ø®Ø§ØµØ© Ø¨Ùƒ. Ø§Ø³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª ÙƒØ§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„ÙƒÙ„Ø§Ù…ÙŠ ÙˆØ§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('nikcheerla/nooks-amd-detection-realtime')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the SentenceTransformer class from the sentence_transformers library.\\\\n2. Initialize the SentenceTransformer model with the pre-trained model name 'nikcheerla/nooks-amd-detection-realtime'.\\\\n3. Use the encode method of the model to generate embeddings for both the input question and the set of frequently asked questions (FAQs).\\\\n4. Calculate similarity scores between the input question embedding and each FAQ embedding using a similarity metric like cosine similarity.\\\\n5. Based on the similarity scores, identify and return the most similar questions in the FAQ set.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nmodel = SentenceTransformer(\\\\'nikcheerla/nooks-amd-detection-realtime\\\\')\\\\ninput_question = \\\"How do I reset my password?\\\"\\\\nfaq_questions = [...]  # List of frequently asked questions\\\\nembeddings = model.encode([input_question] + faq_questions)\\\\nsimilarity_scores = cosine_similarity(embeddings[0].reshape(1, -1), embeddings[1:])\\\\nmost_similar_question_idx = similarity_scores.argmax()\\\\nmost_similar_question = faq_questions[most_similar_question_idx]', 'api_call': \\\"SentenceTransformer('nikcheerla/nooks-amd-detection-realtime')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nikcheerla/nooks-amd-detection-realtime', 'api_call': \\\"SentenceTransformer('nikcheerla/nooks-amd-detection-realtime')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('{MODEL_NAME}')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ¹ÙŠÙ† Ø¹Ù„ÙŠÙƒ ØªÙ†ÙÙŠØ° Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØµÙ†ÙØ© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ø¨Ø§Ù„Ù†Ø¸Ø± Ø§Ù„Ø¹Ù†ØµØ±ÙŠ: Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø£ÙˆÙ„Ù‰ØŒ Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ ÙˆØªØ«Ø¨ÙŠØª Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø§Ù„ØµÙØ±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø«Ø§Ù†ÙŠØ©ØŒ Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ±ÙŠ Ø§Ù„ØµÙØ±ÙŠØ©. Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø«Ø§Ù„Ø«Ø©ØŒ Ù‚Ù… Ø¨ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙØ© Ø³Ø§Ø¨Ù‚Ø§ ÙˆØªÙ‚ÙŠÙŠÙ… ÙØ¹Ø§Ù„ÙŠØªÙ‡ ÙÙŠ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ±ÙŠ Ù„Ù„Ù†ÙˆØ¹ Ø§Ù„Ø¬Ø¯ÙŠØ¯.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"CLIPModel.from_pretrained('laion/CLIP-ViT-L-14-laion2B-s32B-b82K')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To do zero-shot image classification, we can use the 'laion/CLIP-ViT-L-14-laion2B-s32B-b82K' model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. This model works without specific training for the desired task.\\\\n2. Import the necessary classes from the transformers package. This includes CLIPModel for the image classification model.\\\\n3. Use the from_pretrained method of the CLIPModel class to load the pre-trained model 'laion/CLIP-ViT-L-14-laion2B-s32B-b82K'.\\\\n4. The loaded model can then be used to analyze an image and make zero-shot classification predictions, allowing your app to classify the image and improve its visual search capabilities.\\\", '<<<code>>>': \\\"from transformers import CLIPModel\\\\n\\\\nmodel = CLIPModel.from_pretrained('laion/CLIP-ViT-L-14-laion2B-s32B-b82K')\\\\n\\\\n# Replace 'path/to/image' with the path of your input image\\\\nimage_input = 'path/to/image'\\\\nclassification_output = model(image_input)\\\", 'api_call': \\\"CLIPModel.from_pretrained('laion/CLIP-ViT-L-14-laion2B-s32B-b82K')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-ViT-L-14-laion2B-s32B-b82K', 'api_call': \\\"CLIPModel.from_pretrained('laion/CLIP-ViT-L-14-laion2B-s32B-b82K')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'ImageNet-1k', 'accuracy': 75.3}, 'description': 'A CLIP ViT L/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. Intended for research purposes and exploring zero-shot, arbitrary image classification. Can be used for interdisciplinary studies of the potential impact of such model.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ. Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('mrm8488/bert2bert_shared-spanish-finetuned-summarization')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries, including AutoModelForSeq2SeqLM and AutoTokenizer from transformers.', 'code': 'from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\\\n\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'mrm8488/bert2bert_shared-spanish-finetuned-summarization\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'mrm8488/bert2bert_shared-spanish-finetuned-summarization\\\\')\\\\n\\\\ndef generate_summary(text):\\\\n    inputs = tokenizer([text], padding=True, truncation=True, max_length=512, return_tensors=\\\\'pt\\\\')\\\\n    input_ids = inputs.input_ids\\\\n    attention_mask = inputs.attention_mask\\\\n    output = model.generate(input_ids, attention_mask=attention_mask)\\\\n    return tokenizer.decode(output[0], skip_special_tokens=True)\\\\n\\\\ntext = \\\"Your Spanish text here...\\\"\\\\nsummary = generate_summary(text)', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Text2Text Generation', 'api_name': 'mrm8488/bert2bert_shared-spanish-finetuned-summarization', 'api_call': 'from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\\\n\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'mrm8488/bert2bert_shared-spanish-finetuned-summarization\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'mrm8488/bert2bert_shared-spanish-finetuned-summarization\\\\')\\\\n\\\\ndef generate_summary(text):\\\\n    inputs = tokenizer([text], padding=True, truncation=True, max_length=512, return_tensors=\\\\'pt\\\\')\\\\n    input_ids = inputs.input_ids\\\\n    attention_mask = inputs.attention_mask\\\\n    output = model.generate(input_ids, attention_mask=attention_mask)\\\\n    return tokenizer.decode(output[0], skip_special_tokens=True)\\\\n\\\\ntext = \\\"Your Spanish text here...\\\"\\\\nsummary = generate_summary(text)', 'api_arguments': ['text'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': 'def generate_summary(text):\\\\n  inputs = tokenizer([text], padding=max_length, truncation=True, max_length=512, return_tensors=pt)\\\\n  input_ids = inputs.input_ids.to(device)\\\\n  attention_mask = inputs.attention_mask.to(device)\\\\n  output = model.generate(input_ids, attention_mask=attention_mask)\\\\n  return tokenizer.decode(output[0], skip_special_tokens=True)\\\\ntext = Your text here...\\\\ngenerate_summary(text)', 'performance': {'dataset': 'mlsum', 'accuracy': {'Rouge1': 26.24, 'Rouge2': 8.9, 'RougeL': 21.01, 'RougeLsum': 21.02}}, 'description': 'Spanish BERT2BERT (BETO) fine-tuned on MLSUM ES for summarization'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ¹ÙŠÙ† Ø¹Ù„ÙŠÙ†Ø§ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¶Ù…Ù† Ø¬Ø¯Ø§ÙˆÙ„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ø­ÙŠØ« ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ù„ÙŠÙ„ ÙˆØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙˆØ¯Ù‚ÙŠÙ‚.? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-sqa')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': 'from transformers import pipeline\\\\ntable_qa_pipeline = pipeline(\\\\'table-question-answering\\\\', model=\\\\'google/tapas-medium-finetuned-sqa\\\\')\\\\ntable = [[\\\"Date\\\", \\\"Product\\\", \\\"Revenue\\\"], [\\\"2021-09-01\\\", \\\"Product A\\\", 1500], [\\\"2021-09-02\\\", \\\"Product B\\\", 2500], [\\\"2021-09-03\\\", \\\"Product C\\\", 1000]]\\\\nquery = \\\"What is the total revenue?\\\"\\\\nresult = table_qa_pipeline(table=table, query=query)', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-medium-finetuned-sqa', 'api_call': 'from transformers import pipeline\\\\ntable_qa_pipeline = pipeline(\\\\'table-question-answering\\\\', model=\\\\'google/tapas-medium-finetuned-sqa\\\\')\\\\ntable = [[\\\"Date\\\", \\\"Product\\\", \\\"Revenue\\\"], [\\\"2021-09-01\\\", \\\"Product A\\\", 1500], [\\\"2021-09-02\\\", \\\"Product B\\\", 2500], [\\\"2021-09-03\\\", \\\"Product C\\\", 1000]]\\\\nquery = \\\"What is the total revenue?\\\"\\\\nresult = table_qa_pipeline(table=table, query=query)', 'api_arguments': 'table, query', 'python_environment_requirements': 'transformers', 'example_code': \\\"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntable_qa_pipeline = pipeline('table-question-answering', model='google/tapas-medium-finetuned-sqa')\\\\n\\\\n# Provide the table and query\\\\nresult = table_qa_pipeline(table=table, query='What is the total revenue?')\\\", 'performance': {'dataset': 'msr_sqa', 'accuracy': 0.6561}, 'description': 'TAPAS medium model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia and uses relative position embeddings. It can be used for answering questions related to a table in a conversational set-up.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ÙˆØ§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆØµÙˆÙ Ø¥Ù„Ù‰ Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ ØªÙ†ÙÙŠØ° Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers, PIL, and requests packages. This includes DeformableDetrForObjectDetection for the object detection model and Image for processing image data.\\\\n2. We then use the from_pretrained method of the DeformableDetrForObjectDetection class to load the pre-trained model 'SenseTime/deformable-detr'. This model has been trained for object detection tasks, which is perfect for detecting objects in a security application.\\\\n3. We load the image data from a file or from a URL, or acquire it in real-time from the security camera.\\\\n4. This model can then be used to analyze an image and identify the various objects in it, which can help with security monitoring.\\\", '<<<code>>>': \\\"from transformers import DeformableDetrForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'http://path/to/image'  # Replace with your image URL\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\\n\\\\ninputs = processor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'api_call': \\\"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'deformable-detr', 'api_call': \\\"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': [['transformers', 'torch', 'PIL', 'requests']], 'example_code': \\\"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\\\nimport torch\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\\ninputs = processor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'performance': {'dataset': 'COCO 2017', 'accuracy': 'Not provided'}, 'description': 'Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ²ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆØ§Ù‡Ø§ ÙˆÙ…ÙˆØ¶ÙˆØ¹Ø§ØªÙ‡Ø§ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©.? [/INST]{'domain': 'Natural Language Processing Text Classification', 'api_call': \\\"pipeline('text-classification', model='roberta-base-openai-detector')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. Import the necessary libraries, which are 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\ndetector = pipeline(\\\\'text-classification\\\\', model=\\\\'roberta-base-openai-detector\\\\')\\\\ntext = \\\"Hello world! Is this content AI-generated?\\\"\\\\nresult = detector(text)', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Transformers', 'functionality': 'Detect GPT-2 generated text', 'api_name': 'roberta-base-openai-detector', 'api_call': 'from transformers import pipeline\\\\ndetector = pipeline(\\\\'text-classification\\\\', model=\\\\'roberta-base-openai-detector\\\\')\\\\ntext = \\\"Hello world! Is this content AI-generated?\\\"\\\\nresult = detector(text)', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import pipeline\\\\npipe = pipeline(text-classification, model=roberta-base-openai-detector)\\\\nprint(pipe(Hello world! Is this content AI-generated?))', 'performance': {'dataset': 'WebText', 'accuracy': '95%'}, 'description': 'RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model.'}} </s>\"\n"}
{"text": "<s>[INST]ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª (API) Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ Ø¥Ù„Ù‰ Ù†Øµ. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø®Ø¯Ù…Ø§Øª Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„ØªÙŠ ØªÙ‚Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø®Ø¯Ù…Ø© ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙˆØ§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ Ù‡Ø°Ø§ Ø§Ù„Ù‡Ø¯Ù. Ø¥Ø°Ø§ ÙƒÙ†Øª Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©ØŒ ÙÙ„Ø§ ØªØªØ±Ø¯Ø¯ ÙÙŠ Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': \\\"unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')\\\", 'api_provider': 'Fairseq', 'explanation': '', 'code': 'from fairseq import unit\\\\ntts_model = unit.TTS.from_pretrained(\\\\'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10\\\\')\\\\n\\\\n# Example text input for Text-to-Speech model\\\\ntext = \\\"Replace this text with the content of your audiobook in one of the supported languages.\\\"\\\\noutput = tts_model.synthesize(text)\\\\n\\\\nwith open(\\\"output_file.wav\\\", \\\"wb\\\") as f:\\\\n    f.write(output)', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Fairseq', 'functionality': 'Text-to-Speech', 'api_name': 'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10', 'api_call': 'from fairseq import unit\\\\ntts_model = unit.TTS.from_pretrained(\\\\'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10\\\\')\\\\n\\\\n# Example text input for Text-to-Speech model\\\\ntext = \\\"Replace this text with the content of your audiobook in one of the supported languages.\\\"\\\\noutput = tts_model.synthesize(text)\\\\n\\\\nwith open(\\\"output_file.wav\\\", \\\"wb\\\") as f:\\\\n    f.write(output)', 'api_arguments': None, 'python_environment_requirements': 'huggingface_hub, fairseq', 'example_code': None, 'performance': {'dataset': 'covost2', 'accuracy': None}, 'description': 'A text-to-speech model trained on multiple datasets including mtedx, covost2, europarl_st, and voxpopuli. Supports English, Spanish, French, and Italian languages.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ Ù„Ø¬Ù…Ù„Ø© Ù‚ØµÙŠØ±Ø© ØªØªØ­Ø¯Ø« Ø¹Ù† ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§ØªØŒ Ø«Ù… Ù‚Ù… Ø¨ØªØ·Ø¨ÙŠÙ‚ Ù…Ø­Ø±Ùƒ Ø§Ù„ØµÙˆØª Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØ­ÙˆÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø¥Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ Ø¢Ø®Ø± ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ÙØ³ Ø§Ù„Ø¬Ù…Ù„Ø© Ø¨ØµÙˆØª Ù…Ø®ØªÙ„Ù ÙˆÙ…Ø´Ø§Ø¨Ù‡ Ù„Ù„ØµÙˆØª Ø§Ù„Ø£ØµÙ„ÙŠ. Ù‚Ø¯Ù… Ø§Ù„ØµÙˆØªÙŠÙ† Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"Wav2Vec2ForSequenceClassification.from_pretrained('hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers and torchaudio packages, such as Wav2Vec2ForSequenceClassification for the sentiment classification and torchaudio for processing the audio data.\\\\n2. We then use the from_pretrained method of the Wav2Vec2ForSequenceClassification class to load the pre-trained model 'hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD'.\\\\n3. Now, we can process the audio data, which can be acquired from recorded audio calls. You can use torchaudio.load() to load an .wav audio file.\\\\n4. Finally, we can use this model to analyze the sentiment of the audio and classify it.\\\\n5. Keep in mind that this model works for Spanish audio/speech.\\\", '<<<code>>>': \\\"from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Tokenizer\\\\nimport torchaudio\\\\n\\\\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained('hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD')\\\\ntokenizer = Wav2Vec2Tokenizer.from_pretrained('hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD')\\\\n\\\\nfile_name = 'path_to_your_audio_file.wav' # replace with the path to the audio file\\\\naudio_data, _ = torchaudio.load(file_name)\\\\ninput_values = tokenizer(audio_data, return_tensors='pt').input_values\\\\nlogits = model(input_values).logits\\\", 'api_call': \\\"Wav2Vec2ForSequenceClassification.from_pretrained('hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD', 'api_call': \\\"Wav2Vec2ForSequenceClassification.from_pretrained('hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD')\\\", 'api_arguments': {'model_name': 'hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD'}, 'python_environment_requirements': [{'transformers': '4.17.0', 'pytorch': '1.10.0+cu111', 'datasets': '2.0.0', 'tokenizers': '0.11.6'}], 'example_code': '', 'performance': {'dataset': 'MESD', 'accuracy': 0.9308}, 'description': 'This model is a fine-tuned version of facebook/wav2vec2-base on the MESD dataset. It is trained to classify underlying sentiment of Spanish audio/speech.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ø¢Ù„ÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ± Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…Ø´Ø±ÙˆØ·. Ù†Ø±ÙŠØ¯ Ø¨Ø±Ù†Ø§Ù…Ø¬Ø§Ù‹ ÙŠØ³ØªØ·ÙŠØ¹ ÙÙ‡Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© Ø¨Ù…Ø®ØªÙ„Ù Ø§Ù„ØµÙŠØº Ù…Ø«Ù„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØµÙˆØ± ÙˆØ§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-cifar10-32')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the necessary class from the diffusers package. This includes DDPMPipeline for the unconditional image generation model.', 'code': '', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'ddpm-cifar10-32', 'api_call': '', 'api_arguments': 'None', 'python_environment_requirements': '!pip install diffusers', 'example_code': 'from diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-cifar10-32\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset. Supports various discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù‚ÙŠØ§Ø³ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„Ø© Ø¨ÙŠÙ† Ù†ØµÙˆØµ Ù…Ø®ØªÙ„ÙØ©ØŒ Ø­ÙŠØ« ØªÙ‚ÙˆÙ… Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù„ØºÙˆÙŠØ© ÙˆØ§Ù„Ø¯Ù„Ø§Ù„ÙŠØ© Ù„Ù„Ù†ØµÙˆØµ Ù„Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ© ÙˆØªØ­Ø¯ÙŠØ¯ Ù…Ø¯Ù‰ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ†Ù‡Ø§.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library 'SentenceTransformer' from sentence-transformers.\\\\n2. With the SentenceTransformer, load the model 'sentence-transformers/paraphrase-MiniLM-L6-v2'.\\\\n3. Apply the model on your text to generate sentence embeddings representing semantic information of the sentences in your blog.\\\\n4. Apply clustering or other similarity comparison methods to identify semantically similar sections.\\\\n5. Create an index for your blog based on the similarities to make it more navigable for readers.\\\", '<<<code>>>': \\\"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\\\\nsentences = blog_post.split('\\\\\\\\n')\\\\nembeddings = model.encode(sentences)\\\\n\\\\n# Apply clustering or similarity comparison methods to organize your blog's content into sections\\\\n# Create an index based on the clustered content structure\\\", 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/paraphrase-MiniLM-L6-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': [['sentence-transformers']], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†Ù…ÙˆØ°Ø¬ ØªØ¹Ù„Ù… Ø¢Ù„ÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': 'RandomForestRegressor()', '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': '1. Import the necessary libraries, including RandomForestRegressor and necessary data handling functions from scikit-learn.\\\\n2. Load the California Housing dataset which contains features like median_income, housing_median_age, and total_rooms along with the target variable, median_house_value.\\\\n3. Preprocess the data, splitting it into training and testing sets and scaling the features.\\\\n4. Create a RandomForestRegressor object with suitable hyperparameters.\\\\n5. Fit the model on the training data.\\\\n6. Use the model to make predictions on the testing data.\\\\n7. Evaluate the performance of the model using suitable metrics, such as mean squared error or R2 score.', '<<<code>>>': 'from sklearn.datasets import fetch_california_housing\\\\nfrom sklearn.model_selection import train_test_split\\\\nfrom sklearn.preprocessing import StandardScaler\\\\nfrom sklearn.ensemble import RandomForestRegressor\\\\nfrom sklearn.metrics import mean_squared_error, r2_score\\\\n\\\\ndata = fetch_california_housing()\\\\nX, y = data.data, data.target\\\\n\\\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\\\n\\\\nscaler = StandardScaler()\\\\nX_train_scaled = scaler.fit_transform(X_train)\\\\nX_test_scaled = scaler.transform(X_test)\\\\n\\\\nregressor = RandomForestRegressor(n_estimators=100, random_state=42)\\\\nregressor.fit(X_train_scaled, y_train)\\\\n\\\\ny_pred = regressor.predict(X_test_scaled)\\\\nmse = mean_squared_error(y_test, y_pred)\\\\nr2 = r2_score(y_test, y_pred)\\\\n\\\\nprint(\\\"Mean squared error:\\\", mse)\\\\nprint(\\\"R2 score:\\\", r2)', 'api_call': 'RandomForestRegressor()', 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rajistics/california_housing', 'api_call': 'RandomForestRegressor()', 'api_arguments': {'bootstrap': 'True', 'ccp_alpha': '0.0', 'criterion': 'squared_error', 'max_depth': '', 'max_features': '1.0', 'max_leaf_nodes': '', 'max_samples': '', 'min_impurity_decrease': '0.0', 'min_samples_leaf': '1', 'min_samples_split': '2', 'min_weight_fraction_leaf': '0.0', 'n_estimators': '100', 'n_jobs': '', 'oob_score': 'False', 'random_state': '', 'verbose': '0', 'warm_start': 'False'}, 'python_environment_requirements': ['Scikit-learn'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A RandomForestRegressor model trained on the California Housing dataset for predicting housing prices.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. First, we need to import the necessary classes from the transformers package. This includes AutoModelForSequenceClassification, AutoTokenizer, and pipeline for creating a zero-shot classification pipeline.\\\\n2. Initialize the zero-shot classification pipeline using the \\\\'facebook/bart-large-mnli\\\\' model.\\\\n3. Pass the sequence (the article title \\\"one day I will see the world\\\") and a list of candidate labels that the classifier needs to predict from (in this case, \\\\'travel\\\\', \\\\'cooking\\\\', and \\\\'dancing\\\\').\\\\n4. The model will return the predicted category along with its confidence probability.', '<<<code>>>': \\\"from transformers import pipeline\\\\nclassifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\\\\nsequence_to_classify = 'one day I will see the world'\\\\ncandidate_labels = ['travel', 'cooking', 'dancing']\\\\nclassifier(sequence_to_classify, candidate_labels)\\\", 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large-mnli', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\\\", 'api_arguments': {'sequence_to_classify': 'one day I will see the world', 'candidate_labels': \\\"['travel', 'cooking', 'dancing']\\\"}, 'python_environment_requirements': [{'transformers': 'from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline'}], 'example_code': {'with_pipeline': \\\"from transformers import pipeline\\\\nclassifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\\\\nsequence_to_classify = 'one day I will see the world'\\\\ncandidate_labels = ['travel', 'cooking', 'dancing']\\\\nclassifier(sequence_to_classify, candidate_labels)\\\", 'with_manual_pytorch': \\\"from transformers import AutoModelForSequenceClassification, AutoTokenizer\\\\nnli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\\\\ntokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\\\\npremise = sequence\\\\nhypothesis = f'This example is {label}.'\\\\nx = tokenizer.encode(premise, hypothesis, return_tensors='pt', truncation_strategy='only_first')\\\\nlogits = nli_model(x.to(device))[0]\\\\nentail_contradiction_logits = logits[:,[0,2]]\\\\nprobs = entail_contradiction_logits.softmax(dim=1)\\\\nprob_label_is_true = probs[:,1]\\\"}, 'performance': {'dataset': 'multi_nli', 'accuracy': 'Not specified'}, 'description': 'This is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset. The model can be used for zero-shot text classification by posing the sequence to be classified as the NLI premise and constructing a hypothesis from each candidate label. The probabilities for entailment and contradiction are then converted to label probabilities.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ ØµÙˆØªÙŠØ© Ø¨Ø¯ÙˆÙ† Ø´Ø±ÙˆØ· ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù…ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ ØµÙˆØªÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙˆØºÙŠØ± Ù…Ø´Ø±ÙˆØ·.? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'api_provider': 'ESPnet', 'explanation': \\\"1. First, clone the ESPnet repository and switch to the specific commit 'd5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f' that contains the desired functionality.\\\", 'code': 'cd espnet\\\\ngit checkout d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\\\\npip install -e .\\\\ncd egs2/amadeus/tts1\\\\n./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'ESPnet', 'functionality': 'Text-to-Speech', 'api_name': 'mio/amadeus', 'api_call': 'cd espnet\\\\ngit checkout d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\\\\npip install -e .\\\\ncd egs2/amadeus/tts1\\\\n./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'api_arguments': {'model_name': 'mio/amadeus'}, 'python_environment_requirements': {'espnet': 'd5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f', 'transformers': 'latest'}, 'example_code': 'cd espnet\\\\ngit checkout d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\\\\npip install -e .\\\\ncd egs2/amadeus/tts1\\\\n./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'performance': {'dataset': 'amadeus', 'accuracy': 'Not provided'}, 'description': 'This model was trained by mio using amadeus recipe in espnet.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ØªØ±Ø­ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø§Ø¯ÙŠ ØµÙˆØªÙŠ Ù„Ù„ØªØµÙ†ÙŠÙ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø´Ù‡Ø± Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ø£Ø¹Ø¶Ø§Ø¡ Ø¨Ø£Ø¯ÙˆØ§Øª Ù„ØªØµÙ†ÙŠÙ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©.? [/INST]'<<<domain>>>': 'Audio Classification', '<<<api_call>>>': \\\"pipeline('audio-classification', model='mazkooleg/0-9up-unispeech-sat-base-ft')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, you need to install the required Python packages, such as transformers, pytorch, datasets, and tokenizers.\\\\n2. Import 'pipeline' from transformers to instantiate the audio-classification pipeline.\\\\n3. Load the pretrained model 'mazkooleg/0-9up-unispeech-sat-base-ft' and create the classification pipeline. This model is designed to recognize digits from 0 to 9.\\\\n4. Pass the audio input to the pipeline, and it will return the predicted class (digit) with the associated confidence score.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nclassifier = pipeline('audio-classification', model='mazkooleg/0-9up-unispeech-sat-base-ft')\\\\n\\\\n# Load and preprocess the audio file\\\\n# ...\\\\n\\\\n# Perform prediction on the preprocessed audio file\\\\nresult = classifier(audio_processed)\\\", 'api_call': \\\"pipeline('audio-classification', model='mazkooleg/0-9up-unispeech-sat-base-ft')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mazkooleg/0-9up-unispeech-sat-base-ft', 'api_call': \\\"pipeline('audio-classification', model='mazkooleg/0-9up-unispeech-sat-base-ft')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers==4.27.1, pytorch==1.11.0, datasets==2.10.1, tokenizers==0.12.1'], 'example_code': '', 'performance': {'dataset': 'mazkooleg/0-9up_google_speech_commands_augmented_raw', 'accuracy': 0.9979}, 'description': 'This model is a fine-tuned version of microsoft/unispeech-sat-base on the None dataset. It achieves the following results on the evaluation set: Loss: 0.0123, Accuracy: 0.9979.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¬Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ØªØ¹Ø¨Ø¦Ø© Ø§Ù„ÙØ¬ÙˆØ§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¥Ø¯Ø®Ø§Ù„ Ø¹Ø¨Ø§Ø±Ø© Ù†Ø§Ù‚ØµØ© Ù…Ø¹ Ø¥Ø´Ø§Ø±Ø© Ø§Ù„ØªØ¹ÙˆÙŠØ¶ ÙÙŠ Ø§Ù„Ù†Øµ ÙˆØ³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù…Ù„Ø§Ø¡Ù…Ø© Ø§Ù„ÙØ¬ÙˆØ© Ø¨ÙƒÙ„Ù…Ø© Ù…Ù†Ø§Ø³Ø¨Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. The pipeline function is used to create a text classification model, which helps to classify text into predefined categories such as positive, negative, and neutral.\\\\n3. We load the model 'Seethal/sentiment_analysis_generic_dataset', a pre-trained model designed for sentiment analysis.\\\\n4. The created classifier can be used to analyze Twitter messages or any text, and classify their sentiment as positive, negative, or neutral.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsentiment_analysis = pipeline(\\\\'text-classification\\\\', model=\\\\'Seethal/sentiment_analysis_generic_dataset\\\\')\\\\nresult = sentiment_analysis(\\\"I\\\\'m having a great time at the event!\\\")', 'api_call': \\\"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentiment_analysis_generic_dataset', 'api_call': \\\"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': \\\"sentiment_analysis('I love this product!')\\\", 'performance': {'dataset': 'generic_dataset', 'accuracy': 'Not specified'}, 'description': 'This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text classification.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ù…Ù‡ØªÙ…ÙˆÙ† Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙØ§Øª Ø¨ÙˆØ§Ø³Ø·Ø© ØªÙ‚Ù†ÙŠØ© Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ Ø­ÙŠØ« ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¨ØµØ±ÙŠ ÙÙŠ Ø§Ù„ØµÙˆØ±. Ù‡Ù„ Ù„Ø¯ÙŠÙƒ Ø§Ù„Ø®Ø¨Ø±Ø© Ø§Ù„Ù„Ø§Ø²Ù…Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ØŸ? [/INST]{'domain': 'Computer Vision Image-to-Image', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': 'from PIL import Image\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom controlnet_aux import OpenposeDetector\\\\nfrom diffusers.utils import load_image\\\\n\\\\nopenpose = OpenposeDetector.from_pretrained(\\\\'lllyasviel/ControlNet\\\\')\\\\nimage = load_image(\\\\'patient_exercise_image.jpg\\\\')\\\\n# replace \\\\'patient_exercise_image.jpg\\\\' with path to your image\\\\nimage = openpose(image)\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/sd-controlnet-openpose\\\\', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\nestimated_pose_image = pipe(\\\"exercise description\\\", image, num_inference_steps=20).images[0]\\\\nestimated_pose_image.save(\\\\'patient_pose_estimated.jpg\\\\')', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Human Pose Estimation', 'api_name': 'lllyasviel/sd-controlnet-openpose', 'api_call': 'from PIL import Image\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom controlnet_aux import OpenposeDetector\\\\nfrom diffusers.utils import load_image\\\\n\\\\nopenpose = OpenposeDetector.from_pretrained(\\\\'lllyasviel/ControlNet\\\\')\\\\nimage = load_image(\\\\'patient_exercise_image.jpg\\\\')\\\\n# replace \\\\'patient_exercise_image.jpg\\\\' with path to your image\\\\nimage = openpose(image)\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/sd-controlnet-openpose\\\\', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\nestimated_pose_image = pipe(\\\"exercise description\\\", image, num_inference_steps=20).images[0]\\\\nestimated_pose_image.save(\\\\'patient_pose_estimated.jpg\\\\')', 'api_arguments': {'text': 'chef in the kitchen', 'image': 'image', 'num_inference_steps': 20}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': \\\"from PIL import Image\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom controlnet_aux import OpenposeDetector\\\\nfrom diffusers.utils import load_image\\\\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\\\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\\\\nimage = openpose(image)\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\\\\n)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\\\\nimage.save('images/chef_pose_out.png')\\\", 'performance': {'dataset': '200k pose-image, caption pairs', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.'}} </s>\"\n"}
{"text": "<s>[INST]Ø¥Ù†Ø´Ø§Ø¡ Ø³Ø¤Ø§Ù„ ÙŠØ·Ù„Ø¨ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥ÙŠØ¬Ø§Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ù† Ø®Ù„Ø§Ù„ ØµÙˆØ±Ø© ÙˆÙ†Øµ Ù‚ØµÙŠØ±.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"pipeline('document-question-answering', model='donut-base-finetuned-docvqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required 'pipeline' function from the transformers library.\\\\n2. Initialize the 'document-question-answering' pipeline using the model 'naver-clova-ix/donut-base-finetuned-docvqa' which is trained for this purpose.\\\\n3. Load an image containing the table and ask a question related to the statistics in the table.\\\\n4. The 'pipeline' function creates a model that can identify text from the image, intelligently analyze the content, and generate a relevant response to the question provided.\\\\n5. Finally, the model will answer the question based on its understanding of the image's content.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ndoc_qa = pipeline('document-question-answering', model='naver-clova-ix/donut-base-finetuned-docvqa')\\\\nimage_path = 'path_to_image'\\\\nquestion = 'your_question'\\\\nanswer = doc_qa({'image': image_path, 'question': question})\\\\nprint(answer)\\\", 'api_call': \\\"pipeline('document-question-answering', model='donut-base-finetuned-docvqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'naver-clova-ix/donut-base-finetuned-docvqa', 'api_call': \\\"pipeline('document-question-answering', model='donut-base-finetuned-docvqa')\\\", 'api_arguments': {'image': 'path_to_image', 'question': 'your_question'}, 'python_environment_requirements': ['Transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ndoc_qa = pipeline('document-question-answering', model='naver-clova-ix/donut-base-finetuned-docvqa')\\\\n\\\\n# Load an image and ask a question\\\\nimage_path = 'path_to_image'\\\\nquestion = 'your_question'\\\\n\\\\n# Get the answer\\\\nanswer = doc_qa({'image': image_path, 'question': question})\\\\nprint(answer)\\\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository. Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù…ÙˆØ§ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ ÙƒÙ„Ù…Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†Øµ Ø§Ù„ØµÙˆØªÙŠ. Ø§Ù†Ø·Ù„Ù‚ÙˆØ§ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù…. Ù‚Ù…ÙˆØ§ Ø¨ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„Ø°ÙŠ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ ÙˆÙ…Ù‚Ø§Ø±Ù†ØªÙ‡ Ù…Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ. Ù‚ÙˆÙ…ÙˆØ§ Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙØ±ÙˆÙ‚ ÙˆØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¨Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø­Ù„ÙˆÙ„ Ø¥Ø¶Ø§ÙÙŠØ©.? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': \\\"Tacotron2.from_hparams(source='speechbrain/tts-tacotron2-ljspeech')\\\", 'api_provider': 'SpeechBrain', 'explanation': '1. We first import the necessary libraries and classes. This includes Tacotron2 from SpeechBrain for the text-to-speech model and torchaudio for audio processing.', 'code': 'import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2, HIFIGAN\\\\n\\\\ntext = \\\"Introducing our new product, the XYZ Widget, designed to make your life easier!\\\"\\\\ntacotron2 = Tacotron2.from_hparams(source=\\\\'speechbrain/tts-tacotron2-ljspeech\\\\')\\\\nhifi_gan = HIFIGAN.from_hparams(source=\\\\'speechbrain/tts-hifigan-ljspeech\\\\')\\\\n\\\\nmel_output, _, _ = tacotron2.encode_text(text)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save(\\\\'product_information_TTS.wav\\\\', waveforms.squeeze(1), 22050)', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'SpeechBrain', 'functionality': 'Text-to-Speech', 'api_name': 'speechbrain/tts-tacotron2-ljspeech', 'api_call': 'import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2, HIFIGAN\\\\n\\\\ntext = \\\"Introducing our new product, the XYZ Widget, designed to make your life easier!\\\"\\\\ntacotron2 = Tacotron2.from_hparams(source=\\\\'speechbrain/tts-tacotron2-ljspeech\\\\')\\\\nhifi_gan = HIFIGAN.from_hparams(source=\\\\'speechbrain/tts-hifigan-ljspeech\\\\')\\\\n\\\\nmel_output, _, _ = tacotron2.encode_text(text)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save(\\\\'product_information_TTS.wav\\\\', waveforms.squeeze(1), 22050)', 'api_arguments': ['text'], 'python_environment_requirements': ['speechbrain'], 'example_code': ['import torchaudio', 'from speechbrain.pretrained import Tacotron2', 'from speechbrain.pretrained import HIFIGAN', 'tacotron2 = Tacotron2.from_hparams(source=speechbrain/tts-tacotron2-ljspeech, savedir=tmpdir_tts)', 'hifi_gan = HIFIGAN.from_hparams(source=speechbrain/tts-hifigan-ljspeech, savedir=tmpdir_vocoder)', 'mel_output, mel_length, alignment = tacotron2.encode_text(Mary had a little lamb)', 'waveforms = hifi_gan.decode_batch(mel_output)', \\\"torchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\\\"], 'performance': {'dataset': 'LJSpeech', 'accuracy': 'Not specified'}, 'description': 'This repository provides all the necessary tools for Text-to-Speech (TTS) with SpeechBrain using a Tacotron2 pretrained on LJSpeech. The pre-trained model takes in input a short text and produces a spectrogram in output. One can get the final waveform by applying a vocoder (e.g., HiFIGAN) on top of the generated spectrogram.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØª Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØª Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ø«Ù… Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ù„ØªØµÙ†ÙŠÙÙ‡Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù‚Ù… Ø¨ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ‚Ø§Ø±Ù†Ù‡ Ø¨Ù†Ù…Ø§Ø°Ø¬ Ù…Ø®ØªÙ„ÙØ© Ù„ØªØ­Ø¯ÙŠØ¯ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØª.? [/INST]{'domain': 'Audio Audio Classification', 'api_call': \\\"Wav2Vec2ForSpeechClassification.from_pretrained('harshit345/xlsr-wav2vec-speech-emotion-recognition')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary libraries, which include transformers and torchaudio.', 'code': 'from transformers import Wav2Vec2ForSpeechClassification, Wav2Vec2Processor\\\\nimport torchaudio\\\\nimport torch\\\\n\\\\ntokenizer = Wav2Vec2Processor.from_pretrained(\\\"harshit345/xlsr-wav2vec-speech-emotion-recognition\\\")\\\\nmodel = Wav2Vec2ForSpeechClassification.from_pretrained(\\\"harshit345/xlsr-wav2vec-speech-emotion-recognition\\\")\\\\n\\\\ndef load_audio_file(file_path, sampling_rate=16000):\\\\n    speech, _ = torchaudio.load(file_path)\\\\n    return speech\\\\n\\\\ndef get_emotion_prediction(file_path, sampling_rate=16000):\\\\n    speech = load_audio_file(file_path, sampling_rate)\\\\n    input_values = tokenizer(speech, return_tensors=\\\"pt\\\", padding=\\\"longest\\\").input_values\\\\n    audio_logits = model(input_values).logits\\\\n    predicted_emotion_id = torch.argmax(audio_logits, dim=-1)\\\\n    return tokenizer.decode(predicted_emotion_id)\\\\n\\\\nemotion_prediction = get_emotion_prediction(customer_support_call_file_path)', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Speech Emotion Recognition', 'api_name': 'harshit345/xlsr-wav2vec-speech-emotion-recognition', 'api_call': 'from transformers import Wav2Vec2ForSpeechClassification, Wav2Vec2Processor\\\\nimport torchaudio\\\\nimport torch\\\\n\\\\ntokenizer = Wav2Vec2Processor.from_pretrained(\\\"harshit345/xlsr-wav2vec-speech-emotion-recognition\\\")\\\\nmodel = Wav2Vec2ForSpeechClassification.from_pretrained(\\\"harshit345/xlsr-wav2vec-speech-emotion-recognition\\\")\\\\n\\\\ndef load_audio_file(file_path, sampling_rate=16000):\\\\n    speech, _ = torchaudio.load(file_path)\\\\n    return speech\\\\n\\\\ndef get_emotion_prediction(file_path, sampling_rate=16000):\\\\n    speech = load_audio_file(file_path, sampling_rate)\\\\n    input_values = tokenizer(speech, return_tensors=\\\"pt\\\", padding=\\\"longest\\\").input_values\\\\n    audio_logits = model(input_values).logits\\\\n    predicted_emotion_id = torch.argmax(audio_logits, dim=-1)\\\\n    return tokenizer.decode(predicted_emotion_id)\\\\n\\\\nemotion_prediction = get_emotion_prediction(customer_support_call_file_path)', 'api_arguments': {'model_name_or_path': 'harshit345/xlsr-wav2vec-speech-emotion-recognition'}, 'python_environment_requirements': ['pip install git+https://github.com/huggingface/datasets.git', 'pip install git+https://github.com/huggingface/transformers.git', 'pip install torchaudio', 'pip install librosa'], 'example_code': \\\"path = '/data/jtes_v1.1/wav/f01/ang/f01_ang_01.wav'\\\\noutputs = predict(path, sampling_rate)\\\", 'performance': {'dataset': 'JTES v1.1', 'accuracy': {'anger': 0.82, 'disgust': 0.85, 'fear': 0.78, 'happiness': 0.84, 'sadness': 0.86, 'overall': 0.806}}, 'description': 'This model is trained on the JTES v1.1 dataset for speech emotion recognition. It uses the Wav2Vec2 architecture for audio classification and can recognize emotions like anger, disgust, fear, happiness, and sadness.'}} </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø§Ø¨Ø¯Ø£ Ø¨ØªÙ‡ÙŠØ¦Ø© ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ø³ØªØ¬ÙˆØ§Ø¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…. Ø«Ù… Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ ÙˆØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙÙ‡Ù… Ù‚Ø§Ø¨Ù„ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù‚Ù… Ø¨Ø¯Ù…Ø¬ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ ÙÙŠ ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù„Ø¯ÙŠÙƒ. Ø§Ø®ØªØ¨Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯ÙˆØ§Øª ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø© ÙˆÙ‚Ù… Ø¨ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ Ù„ØªØ¹Ø²ÙŠØ² Ø§Ù„Ù‚Ø¯Ø±Ø§Øª Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ© Ù„Ù„Ù†Ø¸Ø§Ù….? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelWithLMHead.from_pretrained('output-small')\\\", '<<<api_provider>>>': 'PyTorch Transformers', '<<<explanation>>>': '1. Import the required libraries: AutoTokenizer and AutoModelWithLMHead from transformers.\\\\n2. Prepare the tokenizer by using AutoTokenizer.from_pretrained(\\\\'microsoft/DialoGPT-small\\\\').\\\\n3. Load the pretrained model \\\\'output-small\\\\' using AutoModelWithLMHead.from_pretrained(\\\\'output-small\\\\'). This model is trained on the dialogues of a character, Joshua, from the game \\\"The World Ends With You\\\".\\\\n4. Define a loop in which user input will be taken and the model will generate a response. Tokenize the user input and combine it with previous conversation history (if any).\\\\n5. Use the model.generate() function to generate a response from the model up to a maximum length of 500 tokens. Fine-tune the generated response by adjusting parameters like temperature, top_k, top_p, and no_repeat_ngram_size.\\\\n6. Decode the generated response back to text and display it.', '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModelWithLMHead\\\\nimport torch\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\\\\nmodel = AutoModelWithLMHead.from_pretrained('output-small')\\\\n\\\\nfor step in range(100):\\\\n    new_user_input_ids = tokenizer.encode(input('>> User:') + tokenizer.eos_token, return_tensors='pt')\\\\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\\\\n    chat_history_ids = model.generate(bot_input_ids, max_length=500, pad_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=3, do_sample=True, top_k=100, top_p=0.7, temperature=0.8)\\\\n    print('AI: {}'.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\\\", 'api_call': \\\"AutoModelWithLMHead.from_pretrained('output-small')\\\", 'provider': 'PyTorch Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'satvikag/chatbot', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('output-small')\\\", 'api_arguments': {'tokenizer': \\\"AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\\\", 'model': \\\"AutoModelWithLMHead.from_pretrained('output-small')\\\"}, 'python_environment_requirements': [['torch', 'transformers']], 'example_code': \\\"for step in range(100):\\\\n  new_user_input_ids = tokenizer.encode(input('&gt;&gt; User:') + tokenizer.eos_token, return_tensors='pt')\\\\n  bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step &gt; 0 else new_user_input_ids\\\\n  chat_history_ids = model.generate(bot_input_ids, max_length=500, pad_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=3, do_sample=True, top_k=100, top_p=0.7, temperature = 0.8)\\\\n  print('AI: {}'.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\\\", 'performance': {'dataset': 'Kaggle game script dataset', 'accuracy': 'Not provided'}, 'description': 'DialoGPT Trained on the Speech of a Game Character, Joshua from The World Ends With You.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ØªØ±Ø­ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø§Ø¯ÙŠ ØµÙˆØªÙŠ-ØµÙˆØ±ÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø´Ù‡Ø± Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª ÙˆØ§Ù„ØµÙˆØ± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ø£Ø¹Ø¶Ø§Ø¡ Ø¨Ø£Ø¯ÙˆØ§Øª Ù„ØªØµÙ†ÙŠÙ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØµÙˆØªÙŠØ© ÙˆØ§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-anime-1.0', torch_dtype=torch.float16)(prompt, negative_prompt=negative_prompt)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the StableDiffusionPipeline from the diffusers library and torch.\\\\n2. Load the Dreamlike Anime 1.0 model using the from_pretrained method of StableDiffusionPipeline, specifying the torch_dtype as 'torch.float16'.\\\\n3. Create a positive text prompt that describes the desired high-quality anime artwork.\\\\n4. Optionally, create a negative text prompt to guide the model away from specific unwanted attributes.\\\\n5. Use the loaded pipeline with the provided text prompts to generate an anime image.\\\\n6. Save the generated anime image to a desired location.\\\", '<<<code>>>': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\npipe = StableDiffusionPipeline.from_pretrained(\\\\'dreamlike-art/dreamlike-anime-1.0\\\\', torch_dtype=torch.float16)\\\\nprompt = \\\"anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, blue eyes, skirt\\\"\\\\nnegative_prompt = \\\"simple background, duplicate, retro style, low quality, lowres, watermark, signature, text, jpeg artifacts, blurry\\\"\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(\\\\'./result.jpg\\\\')', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-anime-1.0', torch_dtype=torch.float16)(prompt, negative_prompt=negative_prompt)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dreamlike-art/dreamlike-anime-1.0', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-anime-1.0', torch_dtype=torch.float16)(prompt, negative_prompt=negative_prompt)\\\", 'api_arguments': ['prompt', 'negative_prompt'], 'python_environment_requirements': [['diffusers', 'torch']], 'example_code': \\\"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = 'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': \\\"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\\\" </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ ÙˆØµÙÙ‹Ø§ Ù…ÙØ±Ø¦ÙŠÙ‹Ø§ Ù„ØµÙˆØ±Ø© Ù…ÙØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· ØªÙØ³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the necessary libraries: requests, PIL (Python Imaging Library), BlipProcessor, and Blip2ForConditionalGeneration from transformers.\\\\n2. Instantiate a BlipProcessor and Blip2ForConditionalGeneration model by using the pretrained model \\\\'Salesforce/blip2-flan-t5-xxl\\\\'.\\\\n3. Load the image by providing a URL or accessing the image file on disk and convert the image into an appropriate format.\\\\n4. Ask the model a question about the image, such as \\\"How many dogs are in the picture?\\\".\\\\n5. Process the image and question using the BlipProcessor instance to generate input tensors for the model.\\\\n6. Generate an output by passing the input tensors to the model, and then decode the output to produce a text description of the image.\\\\n7. The text description can be used to answer the quiz question.', '<<<code>>>': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\\\nprocessor = BlipProcessor.from_pretrained('Salesforce/blip2-flan-t5-xxl')\\\\nmodel = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\\\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\\\nquestion = 'How many dogs are in the picture?'\\\\ninputs = processor(raw_image, question, return_tensors='pt')\\\\nout = model.generate(**inputs)\\\\ntext_description = processor.decode(out[0], skip_special_tokens=True)\\\", 'api_call': \\\"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \\\"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\\\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': [['requests', 'PIL', 'transformers']], 'example_code': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\\\nquestion = how many dogs are in the picture?\\\\ninputs = processor(raw_image, question, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. First, we will import the necessary libraries that are needed to work with the model. That includes json, joblib, and pandas.\\\\n2. Next, we will load the pre-trained model using joblib.load(). This model is stored in a file named 'model.joblib'. The model is trained to predict carbon emissions from given input data.\\\\n3. Now, we will load our data using pandas' read_csv() function and filter it based on the required features mentioned in the config file.\\\\n4. We will then pass that filtered data into the model using model.predict(data), and obtain the predictions.\\\\n5. These predictions will provide us with the carbon emissions that can be used to propose solutions for reducing CO2 emissions.\\\", '<<<code>>>': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Robertooo/autotrain-hmaet-2037366891', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['json', 'joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'Robertooo/autotrain-data-hmaet', 'accuracy': {'Loss': 0.067, 'R2': 0.486, 'MSE': 0.005, 'MAE': 0.055, 'RMSLE': 0.036}}, 'description': 'A tabular regression model trained with AutoTrain to predict carbon emissions.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙ Ù…Ø³Ø¨Ù‚. Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‚Ø§Ø¯Ø±Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('sileod/deberta-v3-base-tasksource-nli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries like AutoModelForSequenceClassification from transformers.\\\\n2. Load the DeBERTa model for sequence classification by using the from_pretrained method with the model name 'sileod/deberta-v3-base-tasksource-nli'. This model has been fine-tuned on 520 tasks using multi-task learning and has strong zero-shot validation performance on many tasks.\\\\n3. This model can be used for zero-shot NLI (Natural Language Inference) pipeline tasks, similar to bart-mnli but with better performance.\\\\n4. With the loaded model, you can perform zero-shot classification tasks and natural language inference.\\\", '<<<code>>>': \\\"from transformers import AutoModelForSequenceClassification\\\\nmodel = AutoModelForSequenceClassification.from_pretrained('sileod/deberta-v3-base-tasksource-nli')\\\", 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('sileod/deberta-v3-base-tasksource-nli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sileod/deberta-v3-base-tasksource-nli', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('sileod/deberta-v3-base-tasksource-nli')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': ['glue', 'piqa', 'sciq'], 'accuracy': '70% on WNLI'}, 'description': 'DeBERTa-v3-base fine-tuned with multi-task learning on 520 tasks of the tasksource collection. This checkpoint has strong zero-shot validation performance on many tasks, and can be used for zero-shot NLI pipeline (similar to bart-mnli but better).' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ­ØªØ§Ø¬ Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ¹Ø¯Ø¯ÙŠØ© Ø§Ù„ÙˆØ³ÙŠÙ„Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†Øµ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¥Ù„Ù‰ Ø³Ø¤Ø§Ù„ ÙˆØ¬ÙˆØ§Ø¨.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model for Optical Character Recognition (OCR), which in this case is 'kha-white/manga-ocr-base'.\\\\n3. This loaded model was specifically designed to work with Japanese text and has been trained primarily for Japanese manga image-to-text tasks.\\\\n4. After loading the OCR model, use the pipeline to provide the manga image and the model will return the recognized text from the manga image.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nocr_pipeline = pipeline(\\\\'ocr\\\\', model=\\\\'kha-white/manga-ocr-base\\\\')\\\\nimage_path = \\\"path/to/your/manga/image.jpg\\\"\\\\nwith open(image_path, \\\\'rb\\\\') as image_file:\\\\n    image = image_file.read()\\\\nrecognized_text = ocr_pipeline(image)', 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'api_arguments': 'image', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø© ÙˆØ§Ù„Ø¯Ù‚Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§.? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\\\nprocessor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\\\\nmodel = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\\\\nraw_image = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with the path to your image\\\\ninputs = processor(raw_image, 'a photography of', return_tensors='pt')\\\\noutputs = model.generate(**inputs)\\\\ncaption = processor.decode(outputs[0], skip_special_tokens=True)\\\\nprint(caption)\\\", 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \\\"from PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\\\nprocessor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\\\\nmodel = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\\\\nraw_image = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with the path to your image\\\\ninputs = processor(raw_image, 'a photography of', return_tensors='pt')\\\\noutputs = model.generate(**inputs)\\\\ncaption = processor.decode(outputs[0], skip_special_tokens=True)\\\\nprint(caption)\\\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\\\ntext = a photography of\\\\ninputs = processor(raw_image, text, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ù…Ù‚Ø§Ø·Ø¹ ÙÙŠØ¯ÙŠÙˆ Ù„Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨ÙˆØ§Ø³Ø·Ø© ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØµÙˆÙŠØ± Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠ ÙˆØ§Ù„ØªØµÙ†ÙŠÙ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-ssv2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary classes from the transformers library, which are VideoMAEImageProcessor and VideoMAEForVideoClassification.', 'code': 'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(\\\\'MCG-NJU/videomae-base-finetuned-ssv2\\\\')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(\\\\'MCG-NJU/videomae-base-finetuned-ssv2\\\\')\\\\ninputs = processor(video, return_tensors=\\\\'pt\\\\')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'MCG-NJU/videomae-base-finetuned-ssv2', 'api_call': 'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(\\\\'MCG-NJU/videomae-base-finetuned-ssv2\\\\')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(\\\\'MCG-NJU/videomae-base-finetuned-ssv2\\\\')\\\\ninputs = processor(video, return_tensors=\\\\'pt\\\\')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_arguments': 'video', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Something-Something-v2', 'accuracy': {'top-1': 70.6, 'top-5': 92.6}}, 'description': 'VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]Ø¥Ø°Ø§ ÙƒÙ†Øª Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ ØµÙˆØª Ø¨Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ ØµÙˆØª. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ²ÙˆÙŠØ¯ Ø§Ù„ØµÙˆØª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù„ØªØ­ÙˆÙŠÙ„Ù‡ØŒ ÙˆØ³ØªØ­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"Asteroid('JorisCos/ConvTasNet_Libri2Mix_sepclean_16k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first install the necessary Python packet: Asteroid.\\\\n2. We import the Asteroid class and instantiate it with the 'JorisCos/ConvTasNet_Libri2Mix_sepclean_16k' model provided by Hugging Face Transformers. This model has been trained to separate clean speech from noisy audio, which is exactly what we need for removing background noise from podcasts.\\\\n3. With the instantiated model, we can process any audio file, separate the clean speech, and save the cleaned audio result as a new file.\\\", '<<<code>>>': \\\"from asteroid import Asteroid\\\\nmodel = Asteroid('JorisCos/ConvTasNet_Libri2Mix_sepclean_16k')\\\\ncleaned_audio = model.denoise(audio_file_path)\\\\n# Replace 'audio_file_path' with the path to your audio file\\\\ncleaned_audio.export('denoised_podcast.wav', format='wav')\\\\n# Replace 'denoised_podcast.wav' with the desired output file name\\\", 'api_call': \\\"Asteroid('JorisCos/ConvTasNet_Libri2Mix_sepclean_16k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ConvTasNet_Libri2Mix_sepclean_16k', 'api_call': \\\"Asteroid('JorisCos/ConvTasNet_Libri2Mix_sepclean_16k')\\\", 'api_arguments': [], 'python_environment_requirements': [['asteroid']], 'example_code': '', 'performance': {'dataset': 'Libri2Mix', 'accuracy': {'si_sdr': 15.243671356901526, 'si_sdr_imp': 15.243034178473609, 'sdr': 15.668108919568112, 'sdr_imp': 15.578229918028036, 'sir': 25.295100756629957, 'sir_imp': 25.205219921301754, 'sar': 16.307682590197313, 'sar_imp': -51.64989963759405, 'stoi': 0.9394951175291422, 'stoi_imp': 0.22640192740016568}}, 'description': 'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ØŸ? [/INST]{'domain': 'Natural Language Processing Conversational', 'api_call': \\\"pipeline('text-generation', model='pygmalion-6b')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import pipeline\\\\n\\\\nchatbot = pipeline(\\\\'text-generation\\\\', model=\\\\'pygmalion-6b\\\\')\\\\ninput_prompt = \\\"You: Hey, do you know about any recent breakthroughs in artificial intelligence?\\\"\\\\n\\\\nresponse_text = chatbot(input_prompt)\\\\nresponse = response_text[0][\\\\'generated_text\\\\']', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Generation', 'api_name': 'pygmalion-6b', 'api_call': 'from transformers import pipeline\\\\n\\\\nchatbot = pipeline(\\\\'text-generation\\\\', model=\\\\'pygmalion-6b\\\\')\\\\ninput_prompt = \\\"You: Hey, do you know about any recent breakthroughs in artificial intelligence?\\\"\\\\n\\\\nresponse_text = chatbot(input_prompt)\\\\nresponse = response_text[0][\\\\'generated_text\\\\']', 'api_arguments': ['input_prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \\\"[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\\\\n<START>\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\\\", 'performance': {'dataset': '56MB of dialogue data', 'accuracy': 'Not specified'}, 'description': \\\"Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B. The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model was initialized from the uft-6b ConvoGPT model and fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed.\\\"}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù…Ù„ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ù…Ø¯Ù‰ ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù†ØµÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the SentenceTransformer class from the sentence_transformers package.\\\\n2. Instantiate the SentenceTransformer class with 'sentence-transformers/nli-mpnet-base-v2', which is a pre-trained model that maps texts to dense vector embeddings.\\\\n3. Given a list of research paper abstracts as strings, encode them using the encode method of the SentenceTransformer instance. This will return a list of embeddings for each abstract.\\\\n4. Compute similarity between abstract embeddings using cosine similarity, and identify the most similar research papers based on their similarity scores.\\\", '<<<code>>>': \\\"from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nmodel = SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\\n\\\\nabstracts = [\\\\n    'Abstract of paper 1...',\\\\n    'Abstract of paper 2...',\\\\n    'Abstract of paper 3...',\\\\n    # Add more abstracts\\\\n]\\\\n\\\\nembeddings = model.encode(abstracts)\\\\nsimilarity_matrix = cosine_similarity(embeddings)\\\", 'api_call': \\\"SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/nli-mpnet-base-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­Ø³ÙŠÙ† ØªÙ‚Ù†ÙŠØ© ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨.? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"YOLO('keremberke/yolov8s-building-segmentation')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the necessary YOLO class and render_result function from the ultralyticsplus package.', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-building-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://example.com/satellite-image.jpg'\\\\n# replace the URL with the actual satellite image URL\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-building-segmentation', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-building-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://example.com/satellite-image.jpg'\\\\n# replace the URL with the actual satellite image URL\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_arguments': ['conf', 'iou', 'agnostic_nms', 'max_det', 'image'], 'python_environment_requirements': ['ultralyticsplus==0.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8s-building-segmentation')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'print(results[0].masks)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.661, 'mAP@0.5(mask)': 0.651}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ØµØ±ÙŠ Ù„Ù„ØµÙˆØ± ÙÙŠ Ø´Ø±ÙƒØªÙ†Ø§ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ¸ÙŠÙØ© Ø¨Ø±Ø§ØªØ¨ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙƒØ§Ù„ÙˆØ±ÙŠÙˆØ³ Ù…ØªØ§Ø­Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡.? [/INST]{'domain': 'Computer Vision Image Classification', 'api_call': \\\"timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import necessary libraries: timm module for creating and using the model, PIL.Image module for processing image files, and urllib.request to download images from the internet.', 'code': 'from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimage_url = \\\"https://example.com/path/to/image.jpg\\\"\\\\nimg = Image.open(urlopen(image_url))\\\\nmodel = timm.create_model(\\\\'convnext_base.fb_in1k\\\\', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification, Feature Map Extraction, Image Embeddings', 'api_name': 'convnext_base.fb_in1k', 'api_call': 'from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimage_url = \\\"https://example.com/path/to/image.jpg\\\"\\\\nimg = Image.open(urlopen(image_url))\\\\nmodel = timm.create_model(\\\\'convnext_base.fb_in1k\\\\', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))', 'api_arguments': {'pretrained': 'True', 'features_only': 'True', 'num_classes': '0'}, 'python_environment_requirements': ['timm'], 'example_code': ['from urllib.request import urlopen', 'from PIL import Image', 'import timm', \\\"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\\", \\\"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\", 'model = model.eval()', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'output = model(transforms(img).unsqueeze(0))'], 'performance': {'dataset': 'imagenet-1k', 'accuracy': '83.82%'}, 'description': 'A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù…Ù†ØªØ¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµØŒ Ø­ÙŠØ« Ù†Ù‡Ø¯Ù Ø¥Ù„Ù‰ ØªÙ†Ø¨Ø¤ Ø±ØªØ¨ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers library, which are AutoTokenizer and AutoModelForSequenceClassification.\\\\n2. Load the pre-trained model using the from_pretrained method with model name 'cross-encoder/ms-marco-TinyBERT-L-2-v2'. This model was trained on the MS Marco Passage Ranking task, which makes it suitable for information retrieval tasks such as matching user queries with book descriptions.\\\\n3. Instantiate the tokenizer using the AutoTokenizer.from_pretrained method with the same model name, which will be used to tokenize the input texts (queries and book descriptions).\\\\n4. Tokenize the user queries and corresponding book descriptions using the tokenizer.encode function with appropriate padding, truncation, and return_tensors options.\\\\n5. Pass the tokenized inputs to the model for SequenceClassification and obtain ranking scores for each query-description pair.\\\\n6. Sort the book descriptions based on their ranking scores to find the best matching books for the given user query.\\\", '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nimport torch\\\\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\\\\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\\\\nqueries = ['First user query', 'Second user query']\\\\nbook_descriptions = ['First book description', 'Second book description']\\\\nfeatures = tokenizer(queries, book_descriptions, padding=True, truncation=True, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    scores = model(**features).logits\\\", 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cross-encoder/ms-marco-TinyBERT-L-2-v2', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\\\", 'api_arguments': {'tokenizer': \\\"tokenizer = AutoTokenizer.from_pretrained('model_name')\\\", 'features': \\\"features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors='pt')\\\"}, 'python_environment_requirements': [['transformers', 'torch']], 'example_code': {'import': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification', 'model': \\\"model = AutoModelForSequenceClassification.from_pretrained('model_name')\\\", 'tokenizer': \\\"tokenizer = AutoTokenizer.from_pretrained('model_name')\\\", 'features': \\\"features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors='pt')\\\", 'scores': 'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)'}, 'performance': {'dataset': 'TREC Deep Learning 2019', 'accuracy': '69.84 (NDCG@10)'}, 'description': 'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ ÙˆÙ†Ø´Ø¬Ø¹ Ø¹Ù„Ù‰ ØªÙˆÙÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ³Ø§Ø¦Ø· Ù…ØªØ¹Ø¯Ø¯Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯ÙˆÙ† ØªØµÙ†ÙŠÙ Ù…Ø³Ø¨Ù‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"CLIPModel.from_pretrained('openai/clip-vit-large-patch14').\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. For Zero-Shot Image Classification, you can use the CLIP model. We first import the necessary classes from the transformers package. This includes CLIPModel and CLIPConfig.\\\\n2. We then use the from_pretrained method of the CLIPModel class to load the pre-trained model 'openai/clip-vit-large-patch14'.\\\\n3. This model can be used to analyze an image and identify the best-matching labels for it without requiring labeled data.\\\\n4. You must provide your own image, and the model should be able to classify the image based on the information it has learned during its pre-training.\\\", '<<<code>>>': \\\"from transformers import CLIPModel, CLIPConfig, CLIPProcessor\\\\nconfig = CLIPConfig.from_pretrained('openai/clip-vit-large-patch14')\\\\nmodel = CLIPModel.from_pretrained('openai/clip-vit-large-patch14', config=config)\\\\nimage_path = 'path_to_image.jpg' # replace this with the actual path to the image\\\\ntokenizer = AutoTokenizer.from_pretrained('openai/clip-vit-large-patch14')\\\\nprocessor = CLIPProcessor.from_pretrained('openai/clip-vit-large-patch14')\\\\ninputs = processor(images=image_path, text=labels, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'api_call': \\\"CLIPModel.from_pretrained('openai/clip-vit-large-patch14').\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai/clip-vit-large-patch14-336', 'api_call': \\\"CLIPModel.from_pretrained('openai/clip-vit-large-patch14').\\\", 'api_arguments': 'image_path, tokenizer, model', 'python_environment_requirements': ['Transformers 4.21.3, TensorFlow 2.8.2, Tokenizers 0.12.1'], 'example_code': 'N/A', 'performance': {'dataset': 'unknown', 'accuracy': 'N/A'}, 'description': 'This model was trained from scratch on an unknown dataset.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ø³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ø§Ù„Ù†Øµ: \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"LEDForConditionalGeneration.from_pretrained('MingZhong/DialogLED-base-16384')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the LEDForConditionalGeneration class from the transformers library provided by Hugging Face.\\\\n2. Load the pre-trained DialogLED model with the \\\\'MingZhong/DialogLED-base-16384\\\\' model ID. DialogLED is designed for long dialogue understanding and summarization.\\\\n3. Use the loaded model to generate a summary of the provided long conversation, which discusses the book \\\"Anna Karenina\\\" by Leo Tolstoy.\\\\n4. You can then use the generated summary to get an overview of the main points discussed in the conversation.', '<<<code>>>': 'from transformers import LEDForConditionalGeneration, LEDTokenizer\\\\nmodel = LEDForConditionalGeneration.from_pretrained(\\\\'MingZhong/DialogLED-base-16384\\\\')\\\\ntokenizer = LEDTokenizer.from_pretrained(\\\\'MingZhong/DialogLED-base-16384\\\\')\\\\ninput_text = \\\"Anna Karenina is a novel by the Russian author Leo Tolstoy, first published in book form in 1878...\\\"\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=\\\\'pt\\\\')\\\\nsummary_ids = model.generate(input_ids, num_beams=4, length_penalty=2.0, min_length=30, max_length=150)\\\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)', 'api_call': \\\"LEDForConditionalGeneration.from_pretrained('MingZhong/DialogLED-base-16384')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DialogLED-base-16384', 'api_call': \\\"LEDForConditionalGeneration.from_pretrained('MingZhong/DialogLED-base-16384')\\\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'arxiv', 'accuracy': '2109.02492'}, 'description': 'DialogLED is a pre-trained model for long dialogue understanding and summarization. It builds on the Longformer-Encoder-Decoder (LED) architecture and uses window-based denoising as the pre-training task on a large amount of long dialogue data for further training. Here is a base version of DialogLED, the input length is limited to 16,384 in the pre-training phase.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ø§Ù„Ù†Ø¸Ø§Ù… Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØª ÙÙŠ Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØªØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù†Ø¸Ø§Ù… Ù„ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØª.? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"Pipeline.from_pretrained('philschmid/pyannote-speaker-diarization-endpoint')\\\", '<<<api_provider>>>': 'pyannote.audio', '<<<explanation>>>': \\\"1. First, import the Pipeline class from the pyannote.audio library.\\\\n2. Then, use the from_pretrained method to load the pre-trained model 'philschmid/pyannote-speaker-diarization-endpoint'. This model provides a speaker diarization pipeline that detects and separates speakers in audio recordings.\\\\n3. Apply the diarization pipeline to the meeting recordings. This will yield a speaker diarization result for each given audio file or stream.\\\\n4. Optionally, you can write speaker diarization results to an RTTM (Rich Transcription Time Marked) file for storage and analysis.\\\", '<<<code>>>': \\\"from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained('philschmid/pyannote-speaker-diarization-endpoint')\\\\ndiarization = pipeline('audio_file.wav')\\\\n# replace 'audio_file.wav' with path to your audio file\\\\nwith open('output_file.rttm', 'w') as rttm:\\\\n    diarization.write_rttm(rttm)\\\", 'api_call': \\\"Pipeline.from_pretrained('philschmid/pyannote-speaker-diarization-endpoint')\\\", 'provider': 'pyannote.audio', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'philschmid/pyannote-speaker-diarization-endpoint', 'api_call': \\\"Pipeline.from_pretrained('philschmid/pyannote-speaker-diarization-endpoint')\\\", 'api_arguments': ['num_speakers', 'min_speakers', 'max_speakers', 'segmentation_onset'], 'python_environment_requirements': ['pyannote.audio 2.0'], 'example_code': ['from pyannote.audio import Pipeline', 'pipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)', 'diarization = pipeline(audio.wav)', 'with open(audio.rttm, w) as rttm:', '  diarization.write_rttm(rttm)'], 'performance': {'dataset': [{'name': 'AISHELL-4', 'accuracy': {'DER%': 14.61, 'FA%': 3.31, 'Miss%': 4.35, 'Conf%': 6.95}}, {'name': 'AMI Mix-Headset only_words', 'accuracy': {'DER%': 18.21, 'FA%': 3.28, 'Miss%': 11.07, 'Conf%': 3.87}}, {'name': 'AMI Array1-01 only_words', 'accuracy': {'DER%': 29.0, 'FA%': 2.71, 'Miss%': 21.61, 'Conf%': 4.68}}, {'name': 'CALLHOME Part2', 'accuracy': {'DER%': 30.24, 'FA%': 3.71, 'Miss%': 16.86, 'Conf%': 9.66}}, {'name': 'DIHARD 3 Full', 'accuracy': {'DER%': 20.99, 'FA%': 4.25, 'Miss%': 10.74, 'Conf%': 6.0}}, {'name': 'REPERE Phase 2', 'accuracy': {'DER%': 12.62, 'FA%': 1.55, 'Miss%': 3.3, 'Conf%': 7.76}}, {'name': 'VoxConverse v0.0.2', 'accuracy': {'DER%': 12.76, 'FA%': 3.45, 'Miss%': 3.85, 'Conf%': 5.46}}]}, 'description': 'A speaker diarization pipeline that uses pyannote.audio to perform voice activity detection, speaker change detection, and overlapped speech detection. It can handle fully automatic processing with no manual intervention and can be fine-tuned with various hyperparameters.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ ÙˆØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ²ÙŠØŒ Ø§Ù„Ø°ÙŠ ÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ØªØ­Ù‚ÙŠÙ‚ Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø² ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ¦Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø³Ù„ÙˆÙƒ ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø³Ù„Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.? [/INST]{'domain': 'Multimodal Document Question Answer', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer\\\\n\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained(\\\\'hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering\\\\')\\\\n\\\\nquestion = \\\"What is the contract termination date?\\\"\\\\ncontext = \\\"This is a fictional legal document text containing the termination date: December 31, 2022.\\\"\\\\n\\\\ninputs = tokenizer(question, context, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**inputs)\\\\nanswer_start = output.start_logits.argmax().item()\\\\nanswer_end = output.end_logits.argmax().item()\\\\nanswer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs.input_ids[0][answer_start:answer_end+1]))', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMForQuestionAnswering', 'api_call': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer\\\\n\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained(\\\\'hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering\\\\')\\\\n\\\\nquestion = \\\"What is the contract termination date?\\\"\\\\ncontext = \\\"This is a fictional legal document text containing the termination date: December 31, 2022.\\\"\\\\n\\\\ninputs = tokenizer(question, context, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**inputs)\\\\nanswer_start = output.start_logits.argmax().item()\\\\nanswer_end = output.end_logits.argmax().item()\\\\nanswer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs.input_ids[0][answer_start:answer_end+1]))', 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLM model for question answering. This model is not pretrained and serves as an example for the LayoutLM architecture.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙŠÙ…ÙƒÙ†Ù‡ Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ§Øª ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-char')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required libraries from the transformers package, including AutoModelForMaskedLM and AutoTokenizer.\\\\n2. Load the pre-trained BERT model for the Japanese language with character-level tokenization using the 'cl-tohoku/bert-base-japanese-char' model identifier.\\\\n3. Initialize the tokenizer to work with the Japanese language using the 'cl-tohoku/bert-base-japanese-char' tokenizer.\\\\n4. Replace the missing word(s) in your text with the [MASK] token.\\\\n5. Tokenize and convert the input text with the missing word(s) into a format that the model can understand.\\\\n6. Pass the processed input text to the pre-trained model for prediction.\\\\n7. Finally, predict the most suitable word for the [MASK] token in the input sentence.\\\", '<<<code>>>': 'from transformers import AutoModelForMaskedLM, AutoTokenizer\\\\nmodel = AutoModelForMaskedLM.from_pretrained(\\\\'cl-tohoku/bert-base-japanese-char\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'cl-tohoku/bert-base-japanese-char\\\\')\\\\ntext_with_mask = \\\"ãã®å¾Œã€å½¼ã¯ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ã«[MASK]ãã¾ã—ãŸã€‚\\\"\\\\ninput = tokenizer(text_with_mask, return_tensors=\\\"pt\\\")\\\\noutput = model(**input)\\\\npredictions = output.logits.argmax(-1)\\\\npredicted_words = tokenizer.batch_decode(predictions)', 'api_call': \\\"AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-char')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cl-tohoku/bert-base-japanese-char', 'api_call': \\\"AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-char')\\\", 'api_arguments': 'text', 'python_environment_requirements': [['transformers']], 'example_code': \\\"fill_mask('[MASK]')\\\", 'performance': {'dataset': 'wikipedia', 'accuracy': 'N/A'}, 'description': 'This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by character-level tokenization.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ù…Ù„Ø£ Ø§Ù„ÙØ±Ø§Øº Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©: \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©ØŒ Ø£Ø¸Ù‡Ø± Ø¹Ù„Ù…Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø£Ù† Ø§Ù„Ù€ [MASK] ÙŠÙ…ÙƒÙ†Ù‡ Ø£Ù† ÙŠÙƒÙˆÙ† Ù…ÙÙŠØ¯Ù‹Ø§ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ.\\\"? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the pretrained BERT model 'cl-tohoku/bert-base-japanese-whole-word-masking'.\\\\n3. The loaded model is trained to fill in masked tokens in a Japanese text, and can be used to provide suggestions for completing sentences or phrases.\\\\n4. Enter the text with a [MASK] token where the suggestion is needed, and the model will generate suggestions for the masked token.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nfill_mask = pipeline('fill-mask', model='cl-tohoku/bert-base-japanese-whole-word-masking')\\\\nsentence_with_mask = 'ä»Šæ—¥ã®å¤©æ°—ã¯[MASK]ã§ã™ã€‚'\\\\nsuggestions = fill_mask(sentence_with_mask)\\\", 'api_call': \\\"AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cl-tohoku/bert-base-japanese-whole-word-masking', 'api_call': \\\"AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\\\", 'api_arguments': {'model': 'cl-tohoku/bert-base-japanese-whole-word-masking'}, 'python_environment_requirements': [{'transformers': 'latest'}], 'example_code': \\\"fill_mask('[MASK]')\\\", 'performance': {'dataset': 'Japanese Wikipedia', 'accuracy': 'Not provided'}, 'description': 'This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization. Additionally, the model is trained with the whole word masking enabled for the masked language modeling (MLM) objective.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ø³Ø¤Ø§Ù„Ù‹Ø§ Ù‚ØµÙŠØ±Ù‹Ø§ Ø­ÙˆÙ„ Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„ØªÙŠ Ø³ØªÙØ¹Ø±Ø¶ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.? [/INST]{'domain': 'Computer Vision Object Detection', 'api_call': \\\"YOLO('keremberke/yolov8m-blood-cell-detection')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries YOLO and render_result from ultralyticsplus.', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-blood-cell-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nresults = model.predict(image_path)\\\\nrender = render_result(model=model, image=image_path, result=results[0])\\\\nrender.show()\\\", 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-blood-cell-detection', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-blood-cell-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nresults = model.predict(image_path)\\\\nrender = render_result(model=model, image=image_path, result=results[0])\\\\nrender.show()\\\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8m-blood-cell-detection')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.927}, 'description': 'A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ‚Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙˆÙ„ÙŠØ©. Ø§Ø³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': 'RandomForestRegressor()', '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': '1. We first import the RandomForestRegressor class from the scikit-learn library.\\\\n2. We then create an instance of the RandomForestRegressor class with the specified parameters for housing price prediction.\\\\n3. After preparing a dataset with features of houses in California, we split the data into train and test sets, and fit the RandomForestRegressor model to the training data.\\\\n4. Once the model is trained, we can use it to make predictions on the test set or any new data to estimate house prices in California.', '<<<code>>>': 'from sklearn.ensemble import RandomForestRegressor\\\\nfrom sklearn.model_selection import train_test_split\\\\nfrom sklearn.metrics import mean_squared_error\\\\n\\\\n# Load and preprocess the dataset\\\\nX, y = process_data(dataset)\\\\n\\\\n# Split the dataset into train and test sets\\\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\\\n\\\\n# Create a RandomForestRegressor model\\\\nmodel = RandomForestRegressor()\\\\n\\\\n# Fit the model to the training data\\\\nmodel.fit(X_train, y_train)\\\\n\\\\n# Make predictions on the test set\\\\ny_pred = model.predict(X_test)\\\\n\\\\n# Calculate the mean squared error of the predictions\\\\nmse = mean_squared_error(y_test, y_pred)', 'api_call': 'RandomForestRegressor()', 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rajistics/california_housing', 'api_call': 'RandomForestRegressor()', 'api_arguments': {'bootstrap': 'True', 'ccp_alpha': '0.0', 'criterion': 'squared_error', 'max_depth': '', 'max_features': '1.0', 'max_leaf_nodes': '', 'max_samples': '', 'min_impurity_decrease': '0.0', 'min_samples_leaf': '1', 'min_samples_split': '2', 'min_weight_fraction_leaf': '0.0', 'n_estimators': '100', 'n_jobs': '', 'oob_score': 'False', 'random_state': '', 'verbose': '0', 'warm_start': 'False'}, 'python_environment_requirements': ['Scikit-learn'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A RandomForestRegressor model trained on the California Housing dataset for predicting housing prices.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§ØªØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ Ù…ØªÙˆØ§ÙÙ‚Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠ.? [/INST]'<<<domain>>>': 'Audio Classification', '<<<api_call>>>': \\\"HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries: HubertForSpeechClassification, Wav2Vec2FeatureExtractor, AutoConfig, torch, numpy, and pydub.\\\\n2. Load the pretrained model 'Rajaram1996/Hubert_emotion' using HubertForSpeechClassification.from_pretrained() and feature extractor using Wav2Vec2FeatureExtractor.from_pretrained('facebook/hubert-base-ls960').\\\\n3. Write a function called speech_file_to_array() to convert the audio file to a numpy array.\\\\n4. Use the function predict_emotion_hubert() that takes an audio file path as input, preprocesses the audio, and returns the top two emotions expressed in the audio segment along with the probability scores.\\\\n5. Call the function with the audio file path.\\\", '<<<code>>>': \\\"from audio_models import HubertForSpeechClassification\\\\nfrom transformers import Wav2Vec2FeatureExtractor, AutoConfig\\\\nimport torch.nn.functional as F\\\\nimport torch\\\\nimport numpy as np\\\\nfrom pydub import AudioSegment\\\\n\\\\n# Load the pretrained model and feature extractor\\\\nmodel = HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')\\\\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/hubert-base-ls960')\\\\n\\\\naudio_file = '/path/to/podcast_audio.wav'\\\\n\\\\nresult = predict_emotion_hubert(audio_file)\\\\nprint(result)\\\", 'api_call': \\\"HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Rajaram1996/Hubert_emotion', 'api_call': \\\"HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')\\\", 'api_arguments': {'audio_file': 'string'}, 'python_environment_requirements': [['audio_models', 'transformers', 'torch', 'numpy', 'pydub']], 'example_code': \\\"def predict_emotion_hubert(audio_file):\\\\n from audio_models import HubertForSpeechClassification\\\\n from transformers import Wav2Vec2FeatureExtractor, AutoConfig\\\\n import torch.nn.functional as F\\\\n import torch\\\\n import numpy as np\\\\n from pydub import AudioSegment\\\\nmodel = HubertForSpeechClassification.from_pretrained(Rajaram1996/Hubert_emotion)\\\\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(facebook/hubert-base-ls960)\\\\nsampling_rate=16000\\\\nconfig = AutoConfig.from_pretrained(Rajaram1996/Hubert_emotion)\\\\ndef speech_file_to_array(path, sampling_rate):\\\\n sound = AudioSegment.from_file(path)\\\\n sound = sound.set_frame_rate(sampling_rate)\\\\n sound_array = np.array(sound.get_array_of_samples())\\\\n return sound_array\\\\nsound_array = speech_file_to_array(audio_file, sampling_rate)\\\\ninputs = feature_extractor(sound_array, sampling_rate=sampling_rate, return_tensors=pt, padding=True)\\\\ninputs = {key: inputs[key].to(cpu).float() for key in inputs}\\\\nwith torch.no_grad():\\\\n logits = model(**inputs).logits\\\\nscores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\\\\noutputs = [{\\\\n emo: config.id2label[i],\\\\n score: round(score * 100, 1)}\\\\n for i, score in enumerate(scores)\\\\n]\\\\nreturn [row for row in sorted(outputs, key=lambda x:x[score], reverse=True) if row['score'] != '0.0%'][:2]\\\\nresult = predict_emotion_hubert(male-crying.mp3)\\\\nresult\\\", 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'A pretrained model for predicting emotion in local audio files using Hubert.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='bert-large-cased')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We import the 'pipeline' function of the transformers library.\\\\n2. We create an instance of the pipeline using the 'fill-mask' task and the BERT large cased model.\\\\n3. We pass the input text with a masked token [MASK] to the pipeline. The model will complete the missing word(s) in the given text based on its knowledge and training.\\\\n4. The completed text will be returned as output.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'bert-large-cased\\\\')\\\\ninput_text = \\\"All work and no play makes [MASK] a dull boy.\\\"\\\\noutput_text = unmasker(input_text)', 'api_call': \\\"pipeline('fill-mask', model='bert-large-cased')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-large-cased', 'api_call': \\\"pipeline('fill-mask', model='bert-large-cased')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"unmasker(Hello I'm a [MASK] model.)\\\", 'performance': {'dataset': {'SQUAD 1.1': {'F1': 91.5, 'EM': 84.8}, 'Multi NLI': {'accuracy': 86.09}}}, 'description': 'BERT large model (cased) pretrained on English language using a masked language modeling (MLM) objective. It has 24 layers, 1024 hidden dimensions, 16 attention heads, and 336M parameters.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø³Ø¬Ù„ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. The first step is to create the desired children's story text input.\\\\n2. Upon obtaining the desired input text, we use the ESPnet2 TTS model pre-trained on the 'amadeus' dataset.\\\\n3. To accomplish this, we utilize the 'run.sh' script provided by the 'kazusam/kt' API.\\\\n4. Include relevant flags to skip_data_prep and skip_train and specify the model to be downloaded, mio/amadeus.\\\\n5. The pre-trained model is then utilized to generate audio speech from the input text, resulting in a children's bedtime audiobook.\\\", '<<<code>>>': '# First, write the children\\\\'s story text input as a string\\\\ninput_text = \\\"Once upon a time in a magical forest...\\\"\\\\n\\\\n# Next, convert the input text to audio using the ESPnet2 TTS model\\\\n# Note: You need to have the ESPnet2 library and run.sh script properly set up\\\\n./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'api_call': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kazusam/kt', 'api_call': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'api_arguments': ['input_text'], 'python_environment_requirements': [['Hugging Face', 'ESPnet']], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': 'amadeus', 'accuracy': 'Not provided'}, 'description': 'An ESPnet2 TTS model trained by mio using amadeus recipe in espnet.' </s>\"\n"}
{"text": "<s>[INST]Ù„ØªØ­Ø³ÙŠÙ† Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµØŒ Ø£Ù‚ØªØ±Ø­ ØªØ´ÙƒÙŠÙ„ ÙØ±ÙŠÙ‚ Ø·Ù„Ø§Ø¨ÙŠ ÙŠÙ„ØªÙ‚ÙˆÙ† Ø¨Ø§Ù†ØªØ¸Ø§Ù… Ù„Ù…Ù†Ø§Ù‚Ø´Ø© Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª ÙˆØ¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ø­Ù„ÙˆÙ„. ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªÙˆÙÙŠØ± Ø£ÙØ¶Ù„ Ø§Ù„Ø£Ø¯ÙˆØ§Øª ÙˆØ§Ù„Ù…ÙˆØ§Ø±Ø¯ Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¨Ù†Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ù†ØµÙŠØ© ÙˆØªØ·Ø¨ÙŠÙ‚Ù‡Ø§ Ø¹Ù„Ù‰ ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"pipeline('text2text-generation', model='google/t5-v1_1-base')\\\", 'api_provider': 'Transformers', 'explanation': '1. To translate the text, we use the text-to-text generation pipeline from the transformers library provided by Hugging Face.', 'code': \\\"from transformers import pipeline\\\\nt5 = pipeline('text2text-generation', model='google/t5-v1_1-base')\\\\ntranslated_text = t5('translate English to German: The highly experienced team at our company is dedicated to transforming the world of innovation. Our products and services bridge the gap between cutting-edge technology and practical applications, providing solutions for industries such as healthcare, robotics, and renewable energy. By partnering with us, you can unlock unlimited potential for growth and success.')\\\", 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Text Generation', 'api_name': 'google/t5-v1_1-base', 'api_call': \\\"from transformers import pipeline\\\\nt5 = pipeline('text2text-generation', model='google/t5-v1_1-base')\\\\ntranslated_text = t5('translate English to German: The highly experienced team at our company is dedicated to transforming the world of innovation. Our products and services bridge the gap between cutting-edge technology and practical applications, providing solutions for industries such as healthcare, robotics, and renewable energy. By partnering with us, you can unlock unlimited potential for growth and success.')\\\", 'api_arguments': {'model': 'google/t5-v1_1-base'}, 'python_environment_requirements': {'transformers': '>=4.0.0'}, 'example_code': \\\"from transformers import pipeline\\\\n\\\\nt5 = pipeline('text2text-generation', model='google/t5-v1_1-base')\\\\nt5('translate English to French: Hugging Face is a great company')\\\", 'performance': {'dataset': 'c4', 'accuracy': 'Not provided'}, 'description': \\\"Google's T5 Version 1.1 is a state-of-the-art text-to-text transformer model that achieves high performance on various NLP tasks such as summarization, question answering, and text classification. It is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on downstream tasks.\\\"}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø±ÙŠØ¯ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙŠÙ‚ÙˆÙ… Ø¨Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¬Ù…Ù„ Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† ÙÙ‡Ù… Ø§Ù„Ù†ØµÙˆØµ. ÙŠÙØ³Ù‡Ù… Ø°Ù„Ùƒ ÙÙŠ ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆÙÙ‡Ù… Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we need to install and import the sentence-transformers library.\\\\n2. Next, we can use the provided model 'sentence-transformers/all-MiniLM-L12-v2' to create a SentenceTransformer instance.\\\\n3. Using this model, we can convert the product reviews into dense vector representations (embeddings) by calling the encode method.\\\\n4. With these embeddings, we can perform clustering or semantic search to group similar product reviews together and analyze patterns, common features, or complaints.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-MiniLM-L12-v2\\\\')\\\\nreviews = [\\\"This product is great!\\\", \\\"Amazing quality!\\\", \\\"I am not satisfied with the purchase.\\\"]\\\\nembeddings = model.encode(reviews)\\\\n# Apply clustering or semantic search methods like K-means, DBSCAN, or Cosine Similarity on embeddings', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/all-MiniLM-L12-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': '1,170,060,424 training pairs', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙƒØªÙˆØ¨ Ø¥Ù„Ù‰ Ù…Ù„Ù ØµÙˆØªÙŠ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoTokenizer.from_pretrained('Salesforce/codegen-350M-multi')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes AutoTokenizer and AutoModelForCausalLM from the transformers package provided by Hugging Face.\\\\n2. Use the from_pretrained() method to load the pre-trained tokenizer and model with the identifier 'Salesforce/codegen-350M-multi'. This model is trained for generating executable code snippets in multiple programming languages, including Python.\\\\n3. When making a code generation request, tokenize the input text using the loaded tokenizer to obtain input IDs. Pass these input IDs into the generate method of the loaded model to generate the output code snippet.\\\\n4. Finally, use the tokenizer's decode method to convert the generated output into a human-readable format.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Salesforce/codegen-350M-multi\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'Salesforce/codegen-350M-multi\\\\')\\\\n\\\\n# Example of generating a code snippet:\\\\nprompt = \\\"Create a Python function that outputs \\\\'Hello, World!\\\\'\\\"\\\\ninput_ids = tokenizer(prompt, return_tensors=\\\"pt\\\").input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\ncode_snippet = tokenizer.decode(generated_ids[0], skip_special_tokens=True)', 'api_call': \\\"AutoTokenizer.from_pretrained('Salesforce/codegen-350M-multi')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Salesforce/codegen-350M-multi', 'api_call': \\\"AutoTokenizer.from_pretrained('Salesforce/codegen-350M-multi')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval and MTPB', 'accuracy': 'Refer to the paper for accuracy details'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'pardeepSF/layoutlm-vqa' that uses the LayoutLM architecture for document question answering.\\\\n3. The loaded model will be used for answering questions from the given document.\\\\n4. Provide the question and the context (i.e., the document) as input to the model, and it will return an answer based on the provided information.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nquestion_answering = pipeline(\\\\'question-answering\\\\', model=\\\\'pardeepSF/layoutlm-vqa\\\\')\\\\nquestion = \\\"What year was the company founded?\\\"\\\\ncontext = \\\"The company was founded in 2005 and has been providing high-quality products and services ever since.\\\"\\\\nresult = question_answering(question=question, context=context)\\\\nprint(result[\\\\'answer\\\\'])', 'api_call': \\\"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'layoutlm-vqa', 'api_call': \\\"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\\\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering using the LayoutLM architecture.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ±Ø© Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ± Ø§Ù„ÙˆØ§Ø±Ø¯Ø©ØŒ Ø­ÙŠØ« ÙŠØ¬Ø¨ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø¨ÙŠÙ† Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„ØµÙˆØ± ÙˆØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚Ù‡Ø§ Ø¨Ø¯Ù‚Ø©ØŒ Ù„Ø°Ø§ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙˆØ«ÙˆÙ‚Ø© Ù„Ø¶Ù…Ø§Ù† ÙØ¹Ø§Ù„ÙŠØ© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the required classes from the transformers and PIL packages for performing image segmentation. This includes SegformerForSemanticSegmentation for the model and Image for processing image data.\\\\n2. We then use the from_pretrained method of the SegformerForSemanticSegmentation class to load the pre-trained model 'nvidia/segformer-b5-finetuned-ade-640-640'. This fine-tuned model has been trained to carry out segmentation tasks, which is exactly what we need to identify various components in an environment.\\\\n3. We load the image that we want to segment from a file or a URL. Alternatively, it can also be acquired in real-time from a camera on the site.\\\\n4. We use this model to analyze the given image, and it segments it into different regions based on the identified landscape and building components.\\\", '<<<code>>>': \\\"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\\nimage = Image.open('image_path.jpg')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\", 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nvidia/segformer-b5-finetuned-ade-640-640', 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20K', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ ØªÙˆØ¬ÙŠÙ‡ÙŠ Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ Ù†ØµÙŠ Ù„Ù†Øµ ÙŠØµÙ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ØªØ³ØªØ·ÙŠØ¹ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø±ØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"AutoModelWithLMHead.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. First, import the required libraries from transformers library provided by Hugging Face, including AutoTokenizer and AutoModelWithLMHead.\\\\n2. The AutoModelWithLMHead.from_pretrained() function is used to load the pretrained model 'SEBIS/code_trans_t5_base_code_documentation_generation_python'.\\\\n3. The model is designed to generate code documentation based on tokenized Python code.\\\\n4. Also, load the tokenizer specific to the model using AutoTokenizer.from_pretrained().\\\\n5. Finally, use the SummarizationPipeline with the loaded model and tokenizer to transform the code snippet into a brief description.\\\", '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline\\\\npipeline = SummarizationPipeline(\\\\n    model=AutoModelWithLMHead.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python'),\\\\n    tokenizer=AutoTokenizer.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python', skip_special_tokens=True),\\\\n    device=0\\\\n)\\\\ncode_snippet = 'def e(message, exit_code=None): print_log(message, YELLOW, BOLD) if exit_code is not None: sys.exit(exit_code)'\\\\ndescription = pipeline([code_snippet])\\\", 'api_call': \\\"AutoModelWithLMHead.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'code_trans_t5_base_code_documentation_generation_python', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python')\\\", 'api_arguments': ['tokenized_code'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline\\\\npipeline = SummarizationPipeline(\\\\n model=AutoModelWithLMHead.from_pretrained(SEBIS/code_trans_t5_base_code_documentation_generation_python),\\\\n tokenizer=AutoTokenizer.from_pretrained(SEBIS/code_trans_t5_base_code_documentation_generation_python, skip_special_tokens=True),\\\\n device=0\\\\n)\\\\ntokenized_code = def e ( message , exit_code = None ) : print_log ( message , YELLOW , BOLD ) if exit_code is not None : sys . exit ( exit_code )\\\\npipeline([tokenized_code])', 'performance': {'dataset': 'CodeSearchNet Corpus python dataset', 'accuracy': '20.26 BLEU score'}, 'description': 'This CodeTrans model is based on the t5-base model and is trained on tokenized python code functions. It can be used to generate descriptions for python functions or be fine-tuned on other python code tasks. The model works best with tokenized python functions but can also be used on unparsed and untokenized python code.' </s>\"\n"}
{"text": "<s>[INST]Ø³ÙˆÙ ØªØ¹Ù…Ù„ Ø§Ù„Ø´Ø±ÙƒØ© Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªØ¹Ø±Ù Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„ØµÙˆØªÙŠØ©ØŒ ÙˆØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØµÙ…ÙŠÙ… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªØ³ØªØ·ÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù…Ø¶Ø§Ù…ÙŠÙ† Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"pipeline.from_pretrained('pyannote/overlapped-speech-detection', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", '<<<api_provider>>>': 'pyannote.audio', '<<<explanation>>>': \\\"1. Import the Pipeline class from the pyannote.audio library.\\\\n2. Instantiate the pipeline using the from_pretrained method with the 'pyannote/overlapped-speech-detection' model. Make sure to replace 'ACCESS_TOKEN_GOES_HERE' with your actual access token.\\\\n3. Use the created pipeline instance to process the conference call audio file (e.g., 'audio.wav').\\\\n4. Iterate through the output.get_timeline().support() to find intervals where two or more speakers are active simultaneously.\\\\n5. With these intervals, you can identify points in the conference call where multiple people are speaking at the same time.\\\", '<<<code>>>': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(\\\\'pyannote/overlapped-speech-detection\\\\', use_auth_token=\\\\'ACCESS_TOKEN_GOES_HERE\\\\')\\\\noutput = pipeline(\\\\'audio.wav\\\\')\\\\nfor speech in output.get_timeline().support():\\\\n    start = speech.start\\\\n    end = speech.end\\\\n    print(f\\\"Overlapping speech detected between {start} seconds and {end} seconds.\\\")', 'api_call': \\\"pipeline.from_pretrained('pyannote/overlapped-speech-detection', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", 'provider': 'pyannote.audio', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pyannote/overlapped-speech-detection', 'api_call': \\\"pipeline.from_pretrained('pyannote/overlapped-speech-detection', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", 'api_arguments': ['audio.wav'], 'python_environment_requirements': [['pyannote.audio 2.1']], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...', 'performance': {'dataset': 'ami', 'accuracy': None}, 'description': 'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. ÙŠØªØ¹ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„ÙØ±ÙŠÙ‚ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ·ÙŠØ¹ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯ ÙˆØªØµÙ†ÙŠÙÙ‡Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries from transformers, namely VideoMAEImageProcessor and VideoMAEForVideoClassification.\\\\n2. Load the pre-trained video classification model 'MCG-NJU/videomae-large-finetuned-kinetics' using the VideoMAEForVideoClassification.from_pretrained() function.\\\\n3. Load the processor for this pre-trained model using VideoMAEImageProcessor.from_pretrained() method with the same model name.\\\\n4. Use the processor to convert the input video into the required format (return_tensors='pt').\\\\n5. Pass the processed input to the video classification model to obtain the predicted output.\\\\n6. Extract the predicted class index from the logits and use the id2label dictionary to obtain the corresponding action label.\\\", '<<<code>>>': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')\\\\ninputs = processor(video, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\\\", 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'MCG-NJU/videomae-large-finetuned-kinetics', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')\\\", 'api_arguments': ['video'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-large-finetuned-kinetics)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-large-finetuned-kinetics)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-400', 'accuracy': {'top-1': 84.7, 'top-5': 96.5}}, 'description': 'VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ. ÙŠÙØ±Ø¬Ù‰ ØªÙˆÙÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. Import the 'pipeline' function from transformers.\\\", 'code': 'from transformers import pipeline\\\\ntranslation = pipeline(\\\\'translation_it_to_en\\\\', model=\\\\'Helsinki-NLP/opus-mt-it-en\\\\')\\\\ntext = \\\"Dolce al limone dato forma e porzione, su ciascuno dei quali Ã¨ presente una sua immagine di un principe di giocatori di calcio. Richiedi il tuo preferito e riceve una risposta personalizzata. Inoltre contiene tutti gli ingredienti necessari per l\\\\'ultima festa di compleanno che ti farÃ  sentire sempre speciale.\\\"\\\\ntranslated_text = translation(text)[0][\\\\'translation_text\\\\']', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Transformers', 'functionality': 'Translation', 'api_name': 'Helsinki-NLP/opus-mt-it-en', 'api_call': 'from transformers import pipeline\\\\ntranslation = pipeline(\\\\'translation_it_to_en\\\\', model=\\\\'Helsinki-NLP/opus-mt-it-en\\\\')\\\\ntext = \\\"Dolce al limone dato forma e porzione, su ciascuno dei quali Ã¨ presente una sua immagine di un principe di giocatori di calcio. Richiedi il tuo preferito e riceve una risposta personalizzata. Inoltre contiene tutti gli ingredienti necessari per l\\\\'ultima festa di compleanno che ti farÃ  sentire sempre speciale.\\\"\\\\ntranslated_text = translation(text)[0][\\\\'translation_text\\\\']', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')('Ciao mondo!')\\\", 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': {'newssyscomb2009.it.en': 35.3, 'newstest2009.it.en': 34.0, 'Tatoeba.it.en': 70.9}, 'chr-F': {'newssyscomb2009.it.en': 0.6, 'newstest2009.it.en': 0.594, 'Tatoeba.it.en': 0.808}}}, 'description': 'A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ²ÙŠ Ø¹Ù„Ù‰ Ù…Ø´ÙƒÙ„Ø© ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù„ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ÙŠØªØ¹Ù„Ù… ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ù…ÙƒØ§ÙØ£Ø© Ø¹Ø¨Ø± Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ù…Ø­Ø¯Ø¯Ø©.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary class from the transformers package. This includes AutoModel for the reinforcement learning model.\\\\n2. We then use the from_pretrained method of the AutoModel class to load the pre-trained model 'edbeeching/decision-transformer-gym-walker2d-expert'. This model has been trained for controlling a bipedal robot in the Gym Walker2d environment and can be a suitable choice for controlling a bipedal robot efficiently.\\\\n3. By utilizing the pre-trained model, we can now use it to make decisions and control the robot.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\\\", 'api_call': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'edbeeching/decision-transformer-gym-walker2d-expert', 'api_call': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\\\", 'api_arguments': {'mean': [1.2384834, 0.19578537, -0.10475016, -0.18579608, 0.23003316, 0.022800924, -0.37383768, 0.337791, 3.925096, -0.0047428459, 0.025267061, -0.0039287535, -0.01736751, -0.48212224, 0.00035432147, -0.0037124525, 0.0026285544], 'std': [0.06664903, 0.16980624, 0.17309439, 0.21843709, 0.74599105, 0.02410989, 0.3729872, 0.6226182, 0.9708009, 0.72936815, 1.504065, 2.495893, 3.511518, 5.3656907, 0.79503316, 4.317483, 6.1784487]}, 'python_environment_requirements': [['transformers']], 'example_code': 'See our Blog Post, Colab notebook or Example Script for usage.', 'performance': {'dataset': 'Gym Walker2d environment', 'accuracy': 'Not provided'}, 'description': 'Decision Transformer model trained on expert trajectories sampled from the Gym Walker2d environment.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ù ØµÙˆØªÙŠ Ù…Ù† ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ Ø¢Ø®Ø±. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk')\\\", '<<<api_provider>>>': 'Fairseq', '<<<explanation>>>': \\\"1. Import the necessary modules, including fairseq, torchaudio, and huggingface_hub.\\\\n2. Load the xm_transformer_s2ut_en-hk model from Hugging Face hub using the load_model_ensemble_and_task_from_hf_hub function.\\\\n3. Load an audio file containing the English speech using torchaudio.load().\\\\n4. Prepare the task with the provided args_overrides (config_yaml, task) and input the English audio file's waveform.\\\\n5. Get the prediction from the S2THubInterface, which will contain translated Hokkien speech.\\\\n6. Use the speech synthesis code provided in the example_code to convert the Hokkien text into speech waveform and sampling rate.\\\\n7. Use IPython.display to play the generated audio.\\\", '<<<code>>>': \\\"import os\\\\nfrom pathlib import Path\\\\nimport torchaudio\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\n\\\\ncache_dir = os.getenv('HUGGINGFACE_HUB_CACHE')\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk', arg_overrides={'config_yaml': 'config.yaml', 'task': 'speech_to_text'}, cache_dir=cache_dir)\\\\nmodel = models[0].cpu()\\\\ncfg['task'].cpu = True\\\\n\\\\naudio, _ = torchaudio.load('/path/to/an/audio/file')\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\nunit = S2THubInterface.get_prediction(task, model, task.build_generator([model], cfg), sample)\\\\n\\\\ncache_dir = snapshot_download('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS', cache_dir=cache_dir, library_name='fairseq')\\\\nx = hub_utils.from_pretrained(cache_dir, 'model.pt', '.', archive_map=CodeHiFiGANVocoder.hub_models(), config_yaml='config.json', fp16=False, is_vocoder=True)\\\\nvocoder = CodeHiFiGANVocoder(x['args']['model_path'][0], x['args'])\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\n\\\\nfrom IPython.display import Audio\\\\nAudio(wav, rate=sr)\\\", 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk')\\\", 'provider': 'Fairseq', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'xm_transformer_s2ut_en-hk', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk')\\\", 'api_arguments': {'arg_overrides': {'config_yaml': 'config.yaml', 'task': 'speech_to_text'}, 'cache_dir': 'cache_dir'}, 'python_environment_requirements': [['fairseq', 'huggingface_hub', 'torchaudio']], 'example_code': {'import_modules': ['import json', 'import os', 'from pathlib import Path', 'import IPython.display as ipd', 'from fairseq import hub_utils', 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub', 'from fairseq.models.speech_to_text.hub_interface import S2THubInterface', 'from fairseq.models.text_to_speech import CodeHiFiGANVocoder', 'from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface', 'from huggingface_hub import snapshot_download', 'import torchaudio'], 'load_model': [\\\"cache_dir = os.getenv('HUGGINGFACE_HUB_CACHE')\\\", \\\"models, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk', arg_overrides={'config_yaml': 'config.yaml', 'task': 'speech_to_text'}, cache_dir=cache_dir)\\\", 'model = models[0].cpu()', \\\"cfg['task'].cpu = True\\\"], 'generate_prediction': ['generator = task.build_generator([model], cfg)', \\\"audio, _ = torchaudio.load('/path/to/an/audio/file')\\\", 'sample = S2THubInterface.get_model_input(task, audio)', 'unit = S2THubInterface.get_prediction(task, model, generator, sample)'], 'speech_synthesis': [\\\"library_name = 'fairseq'\\\", \\\"cache_dir = (cache_dir or (Path.home() / '.cache' / library_name).as_posix())\\\", \\\"cache_dir = snapshot_download('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS', cache_dir=cache_dir, library_name=library_name)\\\", \\\"x = hub_utils.from_pretrained(cache_dir, 'model.pt', '.', archive_map=CodeHiFiGANVocoder.hub_models(), config_yaml='config.json', fp16=False, is_vocoder=True)\\\", \\\"with open(f'{x['args']['data']}/config.json') as f:\\\", '  vocoder_cfg = json.load(f)', \\\"assert (len(x['args']['model_path']) == 1), 'Too many vocoder models in the input'\\\", \\\"vocoder = CodeHiFiGANVocoder(x['args']['model_path'][0], vocoder_cfg)\\\", 'tts_model = VocoderHubInterface(vocoder_cfg, vocoder)', 'tts_sample = tts_model.get_model_input(unit)', 'wav, sr = tts_model.get_prediction(tts_sample)', 'ipd.Audio(wav, rate=sr)']}, 'performance': {'dataset': 'MuST-C', 'accuracy': 'Not specified'}, 'description': 'Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØµÙØ±ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø±Ù…ÙˆØ².? [/INST]{'domain': 'Natural Language Processing Token Classification', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('vblagoje/bert-english-uncased-finetuned-pos')\\\", 'api_provider': 'Transformers', 'explanation': '1. We import the necessary modules from the transformers library, specifically the AutoModelForTokenClassification class.', 'code': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'vblagoje/bert-english-uncased-finetuned-pos\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'vblagoje/bert-english-uncased-finetuned-pos\\\\')\\\\ntext = \\\"Once upon a time, in a land far away, there lived a brave little girl who loved exploring the forest with her friends.\\\"\\\\ninputs = tokenizer(text, return_tensors=\\\"pt\\\")\\\\noutputs = model(**inputs)\\\\npredictions = outputs[0]\\\\npredicted_parts_of_speech = predictions.argmax(-1)', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Transformers', 'functionality': 'Token Classification', 'api_name': 'vblagoje/bert-english-uncased-finetuned-pos', 'api_call': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'vblagoje/bert-english-uncased-finetuned-pos\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'vblagoje/bert-english-uncased-finetuned-pos\\\\')\\\\ntext = \\\"Once upon a time, in a land far away, there lived a brave little girl who loved exploring the forest with her friends.\\\"\\\\ninputs = tokenizer(text, return_tensors=\\\"pt\\\")\\\\noutputs = model(**inputs)\\\\npredictions = outputs[0]\\\\npredicted_parts_of_speech = predictions.argmax(-1)', 'api_arguments': 'model', 'python_environment_requirements': ['transformers'], 'example_code': \\\"nlp('Hello world!')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A BERT model fine-tuned for Part-of-Speech (POS) tagging in English text.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ø­Ù„ÙˆÙ„ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ØªØªØ¶Ù…Ù† Ø¯Ù…Ø¬ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ù„Ø®Ù„Ø§ØµØ© Ù…ÙˆØ¬Ø²Ø© ÙˆÙ…ÙÙŠØ¯Ø©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªÙ„Ø®ÙŠØµ ØªØ³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙ†Ø§ Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªÙ†ÙˆØ¹Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers package. This includes BarthezModel for text summarization.\\\\n2. Use the from_pretrained method of the BarthezModel class to load the pre-trained model 'moussaKam/barthez-orangesum-abstract'.\\\\n3. This model has been trained for French text summarization tasks, which is exactly what we need for summarizing a large French article.\\\\n4. Provide the text of the French article as input to the BarthezModel.\\\\n5. The model will return a summarized version of the input text.\\\", '<<<code>>>': \\\"from transformers import BarthezTokenizer, BarthezModel\\\\ntokenizer = BarthezTokenizer.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\\nmodel = BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\\ninputs = tokenizer(article_text, return_tensors='pt', truncation=True, max_length=512)\\\\noutputs = model.generate(**inputs)\\\\nsummary = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\", 'api_call': \\\"BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'moussaKam/barthez-orangesum-abstract', 'api_call': \\\"BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'orangeSum', 'accuracy': ''}, 'description': 'Barthez model finetuned on orangeSum for abstract generation in French language' </s>\"\n"}
{"text": "<s>[INST]Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙˆÙ„ÙŠØ© Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©ØŒ Ù‚Ù… Ø¨ØªÙ‚Ø³ÙŠÙ… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹ØªÙŠÙ†: Ù…Ø¬Ù…ÙˆØ¹Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ Ø³ØªÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªØ¯Ø±ÙŠØ¨Ù‡Ø§ØŒ ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø© Ø£Ø®Ø±Ù‰ ØªØ­ÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. Import necessary libraries, such as joblib, pandas, and json.\\\\n2. Load the pre-trained model 'model.joblib' using joblib.load.\\\\n3. Read the JSON config file 'config.json' to get the input features.\\\\n4. Load the given dataset 'data.csv' as a Pandas dataframe.\\\\n5. Pre-process the dataframe.\\\\n6. Use the loaded model to make predictions for the carbon emissions of each manufacturing process.\\\", '<<<code>>>': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pcoloc/autotrain-mikrotik-7-7-1860563588', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['json', 'joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'pcoloc/autotrain-data-mikrotik-7-7', 'accuracy': {'Loss': 48.213, 'R2': 0.654, 'MSE': 2324.518, 'MAE': 32.634, 'RMSLE': 0.586}}, 'description': 'A tabular regression model trained with AutoTrain to predict carbon emissions based on input features.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù… Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØŒ Ø«Ù… Ù‚Ù… Ø¨ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø®Ù„Ù‚ Ù…Ø­Ø§Ø¯Ø«Ø© ØªÙØ§Ø¹Ù„ÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø§Ø¨ Ù„ØªØ­Ù„ÙŠÙ„ ØªØ­Ù‚Ù‚ Ø§Ù„Ø¥ÙŠØ¶Ø§Ø­ Ùˆ ØªÙ‚ÙŠÙŠÙ… Ø·Ø±Ù‚ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ ØªØ¬Ø§Ù‡ Ù…Ø³Ø£Ù„Ø© Ø§Ù„ØªØµÙ†ÙŠÙ ÙÙŠ ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø¯ÙˆØ§Ø¦Ø± Ù„ØªÙ†Ø´ÙŠØ· Ø§Ù„Ø±Ø¯ÙˆØ¯.? [/INST]{'domain': 'Tabular Tabular Regression', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_provider': 'Joblib', 'explanation': '', 'code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('industry_data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Joblib', 'functionality': 'Carbon Emissions', 'api_name': 'pcoloc/autotrain-dragino-7-7-max_300m-1861063640', 'api_call': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('industry_data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_arguments': ['model.joblib', 'config.json', 'data.csv'], 'python_environment_requirements': ['json', 'joblib', 'pandas'], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'pcoloc/autotrain-data-dragino-7-7-max_300m', 'accuracy': {'Loss': 50.918, 'R2': 0.304, 'MSE': 2592.667, 'MAE': 39.693, 'RMSLE': 0.429}}, 'description': 'A tabular regression model for predicting carbon emissions using the pcoloc/autotrain-dragino-7-7-max_300m-1861063640 dataset. Trained with AutoTrain.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£ÙÙ†Ø´Ø¦ Ø¬Ø¯ÙˆÙ„Ù‹Ø§ ØªØ§Ø¨Ø¹ÙŠÙ‹Ø§ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø¥Ø³ØªØ¯Ù„Ø§Ù„ ØªØ§Ø¨Ø¹ÙŠ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª.? [/INST]{'domain': 'Audio Audio Classification', 'api_call': \\\"foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\", 'api_provider': 'SpeechBrain', 'explanation': \\\"1. Import the 'foreign_class' from the speechbrain.pretrained.interfaces package.\\\", 'code': '', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'SpeechBrain', 'functionality': 'Emotion Recognition', 'api_name': 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP', 'api_call': '', 'api_arguments': ['file_path'], 'python_environment_requirements': 'pip install speechbrain', 'example_code': \\\"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\\nout_prob, score, index, text_lab = classifier.classify_file('speechbrain/emotion-recognition-wav2vec2-IEMOCAP/anger.wav')\\\\nprint(text_lab)\\\", 'performance': {'dataset': 'IEMOCAP', 'accuracy': '78.7%'}, 'description': 'This repository provides all the necessary tools to perform emotion recognition with a fine-tuned wav2vec2 (base) model using SpeechBrain. It is trained on IEMOCAP training data.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯ÙˆÙ† Ù…Ø´Ø§Ù‡Ø¯Ø© Ø¹Ø¨Ø± Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ Ù„Ù… ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ø¹Ù„ÙŠÙ‡Ø§ØŸ? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"CLIPModel.from_pretrained('openai/clip-vit-large-patch14').\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the Transformers package. This includes the CLIPModel class for the zero-shot image classification model and the corresponding tokenizer.\\\\n2. We use the from_pretrained method to load the pre-trained model 'openai/clip-vit-large-patch14'. This model has been designed to perform zero-shot classification tasks and is suitable for assessing artworks.\\\\n3. We can load an image of an artwork and tokenize it using the provided tokenizer.\\\\n4. This model can then be used to analyze the image and classify the artwork into various categories.\\\\n\\\\nNote that you need to update the Transformers version to 4.21.3 and Tokenizers to 0.12.1.\\\", '<<<code>>>': 'from transformers import CLIPModel, CLIPProcessor\\\\nfrom PIL import Image\\\\nimage = Image.open(\\\\'artwork_image.jpg\\\\')\\\\n# replace \\\\'artwork_image.jpg\\\\' with the path to your image\\\\nprocessor = CLIPProcessor.from_pretrained(\\\\'openai/clip-vit-large-patch14\\\\')\\\\nmodel = CLIPModel.from_pretrained(\\\\'openai/clip-vit-large-patch14\\\\')\\\\ninputs = processor(images=image, return_tensors=\\\"pt\\\")\\\\noutputs = model(**inputs)', 'api_call': \\\"CLIPModel.from_pretrained('openai/clip-vit-large-patch14').\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai/clip-vit-large-patch14-336', 'api_call': \\\"CLIPModel.from_pretrained('openai/clip-vit-large-patch14').\\\", 'api_arguments': 'image_path, tokenizer, model', 'python_environment_requirements': ['Transformers 4.21.3, TensorFlow 2.8.2, Tokenizers 0.12.1'], 'example_code': 'N/A', 'performance': {'dataset': 'unknown', 'accuracy': 'N/A'}, 'description': 'This model was trained from scratch on an unknown dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ®Ø¯Ù… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØ±Ø© Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø®Ø§Øµ Ø¨Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. First, import the pipeline method from the transformers library provided by Hugging Face.\\\\n2. Then create a text-to-image model using the provided model name \\\\'22h/vintedois-diffusion-v0-1\\\\'. This model is trained to generate images based on given textual prompts.\\\\n3. Pass the text description \\\"landscape with a lake and mountains in the background\\\" to the model along with other model parameters like CFG Scale, Scheduler, Steps, and Seed.\\\\n4. The generated image created in response to this prompt can be used as the digital painting that the client requested.', '<<<code>>>': \\\"from transformers import pipeline\\\\ntext2img = pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\\\\nimage = text2img('landscape with a lake and mountains in the background', CFG_Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\\\", 'api_call': \\\"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \\\"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\\\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\\\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ØŸ? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"load('path_to_folder/sklearn_model.joblib')\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': \\\"1. Install the required packages, including scikit-learn and joblib.\\\\n2. Use the joblib.load function to load the pre-trained scikit-learn model 'julien-c/skops-digits' by passing the model file path as an argument.\\\\n3. Process your dataset containing user behavior to create a model input.\\\\n4. Call predict on the loaded model with the input data to get predictions for mobile plan preferences.\\\\n5. The output will be an array of predicted mobile plan preferences based on the user behavior input.\\\", '<<<code>>>': \\\"import joblib\\\\nfrom sklearn.preprocessing import StandardScaler\\\\nmodel = joblib.load('/path_to_folder/sklearn_model.joblib')\\\\n\\\\n# Assuming you have a dataset `user_behavior_data`\\\\nscaler = StandardScaler()\\\\nscaled_input = scaler.fit_transform(user_behavior_data)\\\\nplan_predictions = model.predict(scaled_input)\\\", 'api_call': \\\"load('path_to_folder/sklearn_model.joblib')\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'julien-c/skops-digits', 'api_call': \\\"load('path_to_folder/sklearn_model.joblib')\\\", 'api_arguments': {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': 'False', 'epsilon': 1e-08, 'hidden_layer_sizes': '(100,)', 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 200, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': 'True', 'power_t': 0.5, 'random_state': None, 'shuffle': 'True', 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 'False', 'warm_start': 'False'}, 'python_environment_requirements': ['scikit-learn, joblib'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': None, 'accuracy': None}, 'description': 'A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Instantiate a question-answering pipeline using the 'mrm8488/bert-multi-cased-finetuned-xquadv1' model and tokenizer. This model is specifically designed for multilingual question answering tasks and can answer historical questions, like the one about Charlemagne's birth.\\\\n3. You can then use the instantiated pipeline to ask the question and provide any relevant context that might be needed for the question. In this case, the model will need a passage of text containing information about Charlemagne's birth.\\\\n4. The model will then return the answer extracted from the given context.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\'question-answering\\\\', model=\\\\'mrm8488/bert-multi-cased-finetuned-xquadv1\\\\', tokenizer=\\\\'mrm8488/bert-multi-cased-finetuned-xquadv1\\\\')\\\\ncontext = \\\\'Charlemagne, also known as Charles the Great, was king of the Franks and ruler of the Holy Roman Empire. He was born on April 2, 742, and became king in 768.\\\\'\\\\nquestion = \\\"When was Charlemagne born?\\\"\\\\nresult = qa_pipeline({\\\\'context\\\\': context, \\\\'question\\\\': question})\\\\nanswer = result[\\\\'answer\\\\']', 'api_call': \\\"pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mrm8488/bert-multi-cased-finetuned-xquadv1', 'api_call': \\\"pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\", 'api_arguments': {'context': 'string', 'question': 'string'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"qa_pipeline({\\\\n 'context': Manuel Romero has been working hardly in the repository hugginface/transformers lately,\\\\n 'question': Who has been working hard for hugginface/transformers lately?\\\\n})\\\", 'performance': {'dataset': 'XQuAD', 'accuracy': 'Not provided'}, 'description': 'This model is a BERT (base-multilingual-cased) fine-tuned for multilingual Question Answering on 11 different languages using the XQuAD dataset and additional data augmentation techniques.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ Ø¨ÙˆØ§Ø³Ø·Ø© Ø´Ø±ÙƒØªÙ†Ø§ Ù„ØªØµÙ†ÙŠÙ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙˆÙ„ÙŠØ© ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§ØŒ Ø¨Ù‡Ø¯Ù ØªØ­Ù‚ÙŠÙ‚ ØªÙ†Ø¨Ø¤ Ø¯Ù‚ÙŠÙ‚ Ø¨Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'sklearn_model.joblib')))\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': '1. Import the necessary libraries including hf_hub_url, cached_download from huggingface_hub, joblib, and pandas.\\\\n2. Load the Scikit-learn model from Hugging Face Hub using the cached_download and joblib.load functions.\\\\n3. Load the CSV file containing the wine data using pandas, and separate the input features (X) and target labels (Y).\\\\n4. Use the model to predict the wine quality for specific samples from the input features.', '<<<code>>>': \\\"from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nREPO_ID = 'julien-c/wine-quality'\\\\nFILENAME = 'sklearn_model.joblib'\\\\nmodel = joblib.load(cached_download(hf_hub_url(REPO_ID, FILENAME)))\\\\ndata_file = cached_download(hf_hub_url(REPO_ID, 'winequality-red.csv'))\\\\nwinedf = pd.read_csv(data_file, sep=';')\\\\nX = winedf.drop(['quality'], axis=1)\\\\nY = winedf['quality']\\\\nsample_input = X[:3]\\\\nlabels = model.predict(sample_input)\\\", 'api_call': \\\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'sklearn_model.joblib')))\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'osanseviero/wine-quality', 'api_call': \\\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'sklearn_model.joblib')))\\\", 'api_arguments': 'X', 'python_environment_requirements': [['huggingface_hub', 'joblib', 'pandas']], 'example_code': 'from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nREPO_ID = julien-c/wine-quality\\\\nFILENAME = sklearn_model.joblib\\\\nmodel = joblib.load(cached_download(\\\\n hf_hub_url(REPO_ID, FILENAME)\\\\n))\\\\ndata_file = cached_download(\\\\n hf_hub_url(REPO_ID, winequality-red.csv)\\\\n)\\\\nwinedf = pd.read_csv(data_file, sep=;)\\\\nX = winedf.drop([quality], axis=1)\\\\nY = winedf[quality]\\\\nprint(X[:3])\\\\nlabels = model.predict(X[:3])', 'performance': {'dataset': 'winequality-red.csv', 'accuracy': 0.6616635397123202}, 'description': 'A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù†Ø¸Ø§Ù… ÙŠØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ø¯ÙŠØ¯ ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙÙŠ Ø§Ù„Ù†ØµØŒ Ù…Ø«Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Natural Language Processing Fill-Mask.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a fill-mask model.\\\\n3. Specify the model 'camembert-base' to be used for predictions. This model is pretrained on French text data and can be used to complete sentences in French.\\\\n4. Set the tokenizer also to 'camembert-base', which is the tokenizer specifically designed for the CamemBERT model to process French text.\\\\n5. The created model can be used to complete sentences in French by filling in the missing words (<mask>).\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ncamembert_fill_mask = pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')\\\\nresults = camembert_fill_mask('Le camembert est <mask> :)')\\\", 'api_call': \\\"pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'camembert-base', 'api_call': \\\"pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')\\\", 'api_arguments': ['model', 'tokenizer'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline; camembert_fill_mask = pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base'); results = camembert_fill_mask('Le camembert est <mask> :)')\\\", 'performance': {'dataset': 'oscar', 'accuracy': 'N/A'}, 'description': 'CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. It is available on Hugging Face in 6 different versions with varying number of parameters, amount of pretraining data, and pretraining data source domains. It can be used for Fill-Mask tasks.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯Ø§Ø© ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ. Ù‚Ù… Ø¨Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ù„ØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ø¬Ø¯ÙŠØ¯ ÙŠÙ…ÙƒÙ† ØªØ¹Ø¨Ø¦Ø© Ø§Ù„ÙØ±Ø§ØºØ§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ø¨Ù‡Ø§.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create a text-generation pipeline with the \\\\'EleutherAI/gpt-neo-2.7B\\\\' model by calling the pipeline function with the proper arguments. This model is a powerful GPT-3 style language model capable of generating coherent and diverse text based on the given input prompt.\\\\n3. Provide the input prompt \\\"Delivering Happiness\\\" to the text-generation pipeline, and set the options like do_sample=True for a more creative output and min_length to control the minimum length of the generated text.\\\\n4. The generated text will be a catchy slogan based on the original input slogan that can be used by the marketing department for their campaign.', '<<<code>>>': 'from transformers import pipeline\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'EleutherAI/gpt-neo-2.7B\\\\')\\\\ngenerated_text = generator(\\\"Delivering Happiness\\\", do_sample=True, min_length=50)\\\\ncatchy_slogan = generated_text[0][\\\\'generated_text\\\\']', 'api_call': \\\"pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'EleutherAI/gpt-neo-2.7B', 'api_call': \\\"pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\\\", 'api_arguments': ['text', 'do_sample', 'min_length'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\n generator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\\\\n generator(EleutherAI has, do_sample=True, min_length=50)\\\", 'performance': {'dataset': 'the_pile', 'accuracy': {'Lambada_Acc': '62.22%', 'Winogrande': '56.50%', 'Hellaswag': '42.73%'}}, 'description': \\\"GPT-Neo 2.7B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. It was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model. This model is best suited for generating texts from a prompt and can be used directly with a pipeline for text generation.\\\" </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ±Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§ØªØŸ? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'MaskFormerFeatureExtractor', 'MaskFormerForInstanceSegmentation', 'Image', and 'requests'.\\\\n2. Use the 'from_pretrained' method to load the pre-trained model 'facebook/maskformer-swin-base-coco' for instance segmentation (specifically designed to segment objects in images).\\\\n3. Open the input image, which is the image of the kitchen table with cookwares, using the 'Image' library.\\\\n4. Use the 'feature_extractor' method to pre-process the input image and prepare it to be fed into the model.\\\\n5. Finally, feed the pre-processed image into the model and obtain the segmented kitchen objects using 'outputs' and 'post_process_panoptic_segmentation' method.\\\", '<<<code>>>': 'from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(\\\\'facebook/maskformer-swin-base-coco\\\\')\\\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(\\\\'facebook/maskformer-swin-base-coco\\\\')\\\\nimage_path = \\\"kitchen_table_image.jpg\\\"\\\\nimage = Image.open(image_path)\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\n\\\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result[\\\\'segmentation\\\\']', 'api_call': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/maskformer-swin-base-coco', 'api_call': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\", 'api_arguments': ['image'], 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': \\\"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\\n\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\n\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\n\\\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result['segmentation']\\\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¬Ù‡ÙŠØ² Ù…ÙƒØªØ¨Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØªÙŠ Ù„ÙˆÙƒØ§Ù„Ø© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø¹Ø¨Ø± ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª ÙˆØªØ¶Ù…ÙŠÙ† Ø§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø£ØµÙˆØ§Øª ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ù…Ù‡Ù…Ø©.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We import the necessary functionality from the transformers library provided by Hugging Face. This includes WhisperProcessor and WhisperForConditionalGeneration classes for speech recognition.', 'code': \\\"from transformers import WhisperForConditionalGeneration, WhisperProcessor\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\\ninput_features = processor(conference_call_audio, sampling_rate=audio_sampling_rate, return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\\", 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Automatic Speech Recognition and Speech Translation', 'api_name': 'openai/whisper-large-v2', 'api_call': \\\"from transformers import WhisperForConditionalGeneration, WhisperProcessor\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\\ninput_features = processor(conference_call_audio, sampling_rate=audio_sampling_rate, return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\\", 'api_arguments': {'forced_decoder_ids': \\\"WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')\\\"}, 'python_environment_requirements': ['transformers', 'datasets'], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', \\\"processor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\\\", \\\"model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", 'model.config.forced_decoder_ids = None', \\\"ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')\\\", \\\"sample = ds[0]['audio']\\\", \\\"input_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\\\", 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)'], 'performance': {'dataset': 'LibriSpeech test-clean', 'accuracy': 3.0003583080317573}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø£ÙˆØ¬Ù‡ Ø¥Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© ÙˆØ£Ø¬ÙˆØ¨Ø©ØŒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¬ÙˆØ¯Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…ØªÙˆÙ„Ø¯Ø©.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'layoutlm-invoices', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\\\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers'], 'example_code': \\\"nlp(question='What is the total amount?', context='your_invoice_text')\\\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ø¨Ø§Ø­Ø« Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‚Ø§Ù… Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¨ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© ÙˆÙ†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('cointegrated/rubert-base-cased-nli-threeway')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Start by importing necessary libraries like torch and transformers.\\\\n2. Load the tokenizer and model using 'cointegrated/rubert-base-cased-nli-threeway' checkpoint.\\\\n3. Ensure the model is loaded onto the appropriate device (GPU or CPU).\\\\n4. Take two input sentences (text1 and text2) and tokenize them using the tokenizer.\\\\n5. Run the model with the tokenized inputs and obtain logits.\\\\n6. Apply softmax function to the logits to get probabilities for the three categories: entailment, contradiction, and neutral.\\\\n7. Return the classification result and corresponding probabilities.\\\", '<<<code>>>': \\\"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nmodel_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\\\\nif torch.cuda.is_available():\\\\n    model.cuda()\\\\n\\\\ndef classify_relationship(text1, text2):\\\\n    with torch.inference_mode():\\\\n        out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\\\\n        proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\\\\n    return {v: proba[k] for k, v in model.config.id2label.items()}\\\", 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('cointegrated/rubert-base-cased-nli-threeway')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cointegrated/rubert-base-cased-nli-threeway', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('cointegrated/rubert-base-cased-nli-threeway')\\\", 'api_arguments': ['text1', 'text2'], 'python_environment_requirements': [['transformers', 'sentencepiece']], 'example_code': \\\"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nmodel_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\\\\nif torch.cuda.is_available():\\\\n model.cuda()\\\\ntext1 = '.'\\\\ntext2 = '.'\\\\nwith torch.inference_mode():\\\\n out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\\\\n proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\\\\nprint({v: proba[k] for k, v in model.config.id2label.items()})\\\", 'performance': {'dataset': ['JOCI', 'MNLI', 'MPE', 'SICK', 'SNLI', 'ANLI', 'NLI-style FEVER', 'IMPPRES'], 'accuracy': {'ROC AUC': {'entailment': 0.91, 'contradiction': 0.71, 'neutral': 0.79}}}, 'description': 'This is the DeepPavlov/rubert-base-cased fine-tuned to predict the logical relationship between two short texts: entailment, contradiction, or neutral.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙˆÙ†ÙŠØ© Ù„Ø±Ø¨Ø· Ø§Ù„ØµÙˆØ± Ø¨Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù…Ø¬Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†Øµ Ù„Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import necessary classes from the transformers library. This includes VisionEncoderDecoderModel for the image captioning model, ViTFeatureExtractor for vision inputs, AutoTokenizer for processing text, and other required packages like torch and PIL.\\\\n2. We then use the from_pretrained method to load the pre-trained model 'facebook/mmt-en-de'. This model has been trained for image captioning tasks, which is exactly what we need for generating captions for product images in our online store.\\\\n3. We create a function `predict_step` that takes a list of image file paths as input and outputs captions for each image.\\\\n4. Load the images, extract features, tokenize the inputs, and pass them to the model to generate captions.\\\\n5. The generated captions can then be used as a basis for online store product descriptions.\\\", '<<<code>>>': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\\\\nfrom PIL import Image\\\\nimport torch\\\\n\\\\nmodel = VisionEncoderDecoderModel.from_pretrained(\\\\'facebook/mmt-en-de\\\\')\\\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(\\\\'facebook/vit-base-224-pt-16\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/mmt-en-de\\\\')\\\\n\\\\ndef predict_step(images: list):\\\\n    captions = []\\\\n    for image_path in images:\\\\n        image = Image.open(image_path)\\\\n        inputs = feature_extractor(images=image, return_tensors=\\\"pt\\\")\\\\n        input_ids = torch.ones((1, 1), dtype=torch.long)\\\\n        gen_tokens = model.generate(input_ids=input_ids, vision_features=inputs[\\\\'pixel_values\\\\']).image[0]\\\\n        captions.append(tokenizer.decode(gen_tokens))\\\\n     \\\\n    return captions', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\\\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': [{'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}], 'example_code': \\\"predict_step(['Image URL.jpg'])\\\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªØŸ? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. First, import the necessary libraries like \\\\'AutoModelForAudioClassification\\\\' and \\\\'AudioConfig\\\\' from the transformers package. \\\\n2. Load the pre-trained \\\\'MIT/ast-finetuned-speech-commands-v2\\\\' model, which is designed for classifying commands heard in audio files.\\\\n3. Create a function \\\\'audio_classifier\\\\' to process the audio input and use the model to predict the command present in the audio.\\\\n4. Given the audio input, the program should classify the command as one of the options: \\\"up\\\", \\\"down\\\", \\\"left\\\", \\\"right\\\", \\\"stop\\\", \\\"go\\\".', '<<<code>>>': \\\"from transformers import AutoModelForAudioClassification, AudioConfig\\\\nmodel = AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\\naudio_config = AudioConfig.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\\n\\\\ndef audio_classifier(audio_file_path):\\\\n    audio = Audio.open(audio_file_path, audio_config)\\\\n    logits = model(audio.input_values.to(audio.device), attention_mask=audio.attention_mask.to(audio.device)).logits\\\\n    predicted_id = torch.argmax(logits, dim=-1)\\\\n    return model.config.id2label[int(predicted_id)]\\\\n\\\\nresult = audio_classifier('path/to/audio/file.wav')\\\", 'api_call': \\\"AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ast-finetuned-speech-commands-v2', 'api_call': \\\"AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\", 'api_arguments': 'audio file', 'python_environment_requirements': ['transformers library'], 'example_code': \\\"result = audio_classifier('path/to/audio/file.wav')\\\", 'performance': {'dataset': 'Speech Commands v2', 'accuracy': '98.120'}, 'description': 'Audio Spectrogram Transformer (AST) model fine-tuned on Speech Commands v2. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØªØ¹Ø±Ù ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„ÙƒÙ„Ø§Ù… Ø¯ÙˆÙ† Ø´Ø±ÙˆØ· ÙÙŠ Ù…Ø¬Ø§Ù„ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØªØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ ØªØ¹Ø±Ù ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¬Ø¯ÙŠØ¯ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙˆØºÙŠØ± Ù…Ø´Ø±ÙˆØ·.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-dutch')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-dutch')\\\\naudio_paths = ['/path/to/call1.mp3', '/path/to/call2.wav']\\\\n# Replace '/path/to/call1.mp3' and '/path/to/call2.wav' with your audio files\\\\ntranscriptions = model.transcribe(audio_paths)\\\", 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Speech Recognition', 'api_name': 'jonatasgrosman/wav2vec2-large-xlsr-53-dutch', 'api_call': \\\"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-dutch')\\\\naudio_paths = ['/path/to/call1.mp3', '/path/to/call2.wav']\\\\n# Replace '/path/to/call1.mp3' and '/path/to/call2.wav' with your audio files\\\\ntranscriptions = model.transcribe(audio_paths)\\\", 'api_arguments': ['audio_paths'], 'python_environment_requirements': ['huggingsound', 'torch', 'librosa', 'datasets', 'transformers'], 'example_code': \\\"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-dutch')\\\\naudio_paths = ['/path/to/file.mp3', '/path/to/another_file.wav']\\\\ntranscriptions = model.transcribe(audio_paths)\\\", 'performance': {'dataset': 'Common Voice nl', 'accuracy': {'Test WER': 15.72, 'Test CER': 5.35, 'Test WER (+LM)': 12.84, 'Test CER (+LM)': 4.64}}, 'description': 'Fine-tuned XLSR-53 large model for speech recognition in Dutch. Fine-tuned on Dutch using the train and validation splits of Common Voice 6.1 and CSS10.'}} </s>\"\n"}
{"text": "<s>[INST]ØªØ¹Ù…Ù„ Ø´Ø±ÙƒØªÙ†Ø§ Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ù… ØªÙ…ÙŠÙŠØ² Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„ØµÙˆØªÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„ÙŠÙ‡Ø§.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers and datasets packages. This includes WhisperProcessor for processing audio data and WhisperForConditionalGeneration for the transcription model.\\\\n2. We then use the from_pretrained method of the WhisperForConditionalGeneration class to load the pre-trained model 'openai/whisper-small'. This model has been trained for automating speech recognition tasks, which is what we need for transcribing voice meetings.\\\\n3. Load the audio file from the meeting and process it using the WhisperProcessor.\\\\n4. This model can then be used to generate a transcription of the meeting, which can be attached to the meeting minutes.\\\", '<<<code>>>': \\\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-small')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\\\\naudio_data = 'path_to_audio_file.wav'  # replace with the path to your audio file\\\\ninput_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\\\nprint(transcription)\\\", 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai/whisper-small', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\\\", 'api_arguments': {'language': 'english', 'task': 'transcribe'}, 'python_environment_requirements': [{'transformers': 'latest', 'datasets': 'latest'}], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', 'processor = WhisperProcessor.from_pretrained(openai/whisper-small)', 'model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-small)', 'model.config.forced_decoder_ids = None', 'ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)', 'sample = ds[0][audio]', 'input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features', 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)', 'print(transcription)'], 'performance': {'dataset': 'LibriSpeech (clean) test set', 'accuracy': '3.432 WER'}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and supports transcription and translation in various languages.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ø¬Ù…Ù„Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \\\"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙŠ Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.\\\" Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"HappyTextToText('T5', 'vennify/t5-base-grammar-correction')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, install the required happytransformer library using 'pip install happytransformer'.\\\\n2. Import the HappyTextToText class and TTSettings from the happytransformer library.\\\\n3. Initialize the HappyTextToText model using the 'T5' tokenizer and the 'vennify/t5-base-grammar-correction' model.\\\\n4. Create a TTSettings object to set the number of beams and minimum length required for the output.\\\\n5. Call the generate_text() method on the HappyTextToText model to generate grammatically correct text using the input text with incorrect grammar.\\\\n6. Print the corrected text after revising it with the model.\\\", '<<<code>>>': 'from happytransformer import HappyTextToText, TTSettings\\\\nhappy_tt = HappyTextToText(\\\\'T5\\\\', \\\\'vennify/t5-base-grammar-correction\\\\')\\\\nargs = TTSettings(num_beams=5, min_length=1)\\\\ninput_text = \\\"Grammar: This sentences has has bads grammar.\\\"\\\\nresult = happy_tt.generate_text(input_text, args=args)\\\\nprint(result.text)', 'api_call': \\\"HappyTextToText('T5', 'vennify/t5-base-grammar-correction')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'vennify/t5-base-grammar-correction', 'api_call': \\\"HappyTextToText('T5', 'vennify/t5-base-grammar-correction')\\\", 'api_arguments': {'num_beams': 5, 'min_length': 1}, 'python_environment_requirements': [{'package': 'happytransformer', 'installation': 'pip install happytransformer'}], 'example_code': 'from happytransformer import HappyTextToText, TTSettings\\\\nhappy_tt = HappyTextToText(T5, vennify/t5-base-grammar-correction)\\\\nargs = TTSettings(num_beams=5, min_length=1)\\\\nresult = happy_tt.generate_text(grammar: This sentences has has bads grammar., args=args)\\\\nprint(result.text)', 'performance': {'dataset': 'jfleg', 'accuracy': 'Not provided'}, 'description': 'This model generates a revised version of inputted text with the goal of containing fewer grammatical errors. It was trained with Happy Transformer using a dataset called JFLEG.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø¨Ø­Ø« Ø¹Ù† Ù…Ø·ÙˆÙ‘Ø± ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ·Ø¨ÙŠÙ‚ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø±Ù‹Ø§ Ø¹Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª ÙˆØ§Ù„ÙØ¦Ø§Øª ÙÙŠ Ø§Ù„ØµÙˆØ± ÙˆØªÙ‚Ø¯ÙŠÙ… ØªØµÙ†ÙŠÙ Ø¯Ù‚ÙŠÙ‚. ÙŠØ±Ø¬Ù‰ Ø§Ø¹ØªÙ…Ø§Ø¯ ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ ÙˆØ¶Ù…Ø§Ù† Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù„Ù„Ø£Ø¯Ø§Ø¡.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import AutoImageProcessor and AutoModelForImageClassification classes from the transformers library.\\\\n2. Load the image data of the product.\\\\n3. Use the from_pretrained method of the AutoImageProcessor and AutoModelForImageClassification classes to load the pre-trained model 'google/mobilenet_v1_0.75_192'. This model is trained for image classification tasks and can be used to identify products from their images.\\\\n4. Process the image and pass it through the classification model.\\\\n5. Obtain the predicted class index and use the model's config to map it to a specific label.\\\\n6. The identified product label can be used for other tasks, such as product catalog management and recommendation systems.\\\", '<<<code>>>': \\\"from transformers import AutoImageProcessor, AutoModelForImageClassification\\\\nfrom PIL import Image\\\\n\\\\nimage = Image.open('product_image.jpg')\\\\n# replace 'product_image.jpg' with your image file path\\\\npreprocessor = AutoImageProcessor.from_pretrained('google/mobilenet_v1_0.75_192')\\\\nmodel = AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\\\\ninputs = preprocessor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nproduct_class = model.config.id2label[predicted_class_idx]\\\", 'api_call': \\\"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/mobilenet_v1_0.75_192', 'api_call': \\\"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'google/mobilenet_v1_0.75_192'}, 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\\\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\\\\ninputs = preprocessor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ØŒ Ø§Ø¨Ø¯Ø£ Ø¨ØªØµÙ†ÙŠÙ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø© ÙˆØ¨ÙˆØ¶ÙˆØ­ Ù…Ø­Ø¯Ø¯.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a zero-shot image classification model.\\\\n3. Specify the model 'laion/CLIP-ViT-g-14-laion2B-s34B-b88K' to be loaded. This model is based on OpenCLIP and is capable of classifying images into various categories without requiring training data for those categories.\\\\n4. Provide the path to the image file and a list of class names that the classifier should consider. The model will classify the image and assign a probability to each class.\\\\n5. The created classifier can be used to categorize the images in your website into their appropriate sections.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\\\\nclass_names = ['Animals', 'Vehicles', 'Landscape', 'Technology', 'People']\\\\nclassified_image = image_classifier('path/to/image/file', class_names=class_names)\\\", 'api_call': \\\"pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-ViT-g-14-laion2B-s34B-b88K', 'api_call': \\\"pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\\\", 'api_arguments': {'image': 'path/to/image/file', 'class_names': 'list_of_class_names'}, 'python_environment_requirements': [{'huggingface_hub': '0.0.17', 'transformers': '4.11.3', 'torch': '1.9.0', 'torchvision': '0.10.0'}], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¨ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØªÙ‚Ø¯ÙŠÙ…Ù‡Ø§ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø±ØªØ¨Ø© Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"pipeline(sentiment-analysis, model='cardiffnlp/twitter-xlm-roberta-base-sentiment')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function with the 'sentiment-analysis' parameter and specify the model 'cardiffnlp/twitter-xlm-roberta-base-sentiment'. This model is pre-trained for sentiment analysis and supports multiple languages, allowing you to classify sentiments without the need for translation first.\\\\n3. Provide a tweet to the pipeline, and the model will return the sentiment classification (positive, negative, or neutral).\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nsentiment_task = pipeline('sentiment-analysis', model='cardiffnlp/twitter-xlm-roberta-base-sentiment')\\\\ntweet = 'Mi piace il tuo sorriso!'\\\\nresult = sentiment_task(tweet)\\\", 'api_call': \\\"pipeline(sentiment-analysis, model='cardiffnlp/twitter-xlm-roberta-base-sentiment')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cardiffnlp/twitter-xlm-roberta-base-sentiment', 'api_call': \\\"pipeline(sentiment-analysis, model='cardiffnlp/twitter-xlm-roberta-base-sentiment')\\\", 'api_arguments': ['model_path'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\nmodel_path = cardiffnlp/twitter-xlm-roberta-base-sentiment\\\\nsentiment_task = pipeline(sentiment-analysis, model=model_path, tokenizer=model_path)\\\\nsentiment_task(T'estimo!)\\\", 'performance': {'dataset': 'Twitter', 'accuracy': 'Not provided'}, 'description': 'This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ ØµÙˆØ±Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­ÙˆØ§Ø³ÙŠØ¨ØŒ ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø§Ø­Ø«ÙŠÙ† ØªØ·ÙˆÙŠØ± Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙˆÙ†ÙŠØ© ØªØ¯Ù…Ø¬ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù…Ø¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨. ÙŠÙ†ØµØ­ Ø¨Ø¥Ø¶Ø§ÙØ© Ø¢Ù„ÙŠØ© ØªØ¹Ù„Ù… ØªÙ‚ÙˆÙ… Ø¨ØªÙ†Ø¸ÙŠÙ… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø´ÙƒÙ„ Ù…Ù„Ø§Ø¦Ù… Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ø¯Ø§Ø¡ Ø¹Ø§Ù„ÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬. ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹Ø²ÙŠØ² Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…Ù‡Ù…Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary libraries and classes, including ControlNetModel from the diffusers package.\\\\n2. Load the 'lllyasviel/sd-controlnet-seg' model using the from_pretrained method of the ControlNetModel class. This model is trained for image segmentation tasks, which is suitable for segmenting satellite images of cities.\\\\n3. Apply the model to a given satellite image using the pipe function and save the segmented image to a file.\\\\n4. The segmented image can then be analyzed by urban planners to study different aspects of city growth and development.\\\", '<<<code>>>': \\\"from diffusers import ControlNetModel\\\\nfrom utils import pipe\\\\n\\\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\\nsatellite_image = 'path_to_satellite_image'\\\\n\\\\nsegmented_image = pipe(satellite_image, controlnet, num_inference_steps=20).images[0]\\\\nsegmented_image.save('segmented_city.png')\\\", 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/sd-controlnet-seg', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", 'api_arguments': ['torch_dtype'], 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate']], 'example_code': \\\"image = pipe(house, image, num_inference_steps=20).images[0]\\\\nimage.save('./images/house_seg_out.png')\\\", 'performance': {'dataset': 'ADE20K', 'accuracy': 'Trained on 164K segmentation-image, caption pairs'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ø§ØªØ¨Ø¹ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©: Ù¡. Ø§Ø¨Ø¯Ø£ Ø§Ù„Ø¬Ù„Ø³Ø© Ø¨Ø§Ù„ØªØ­ÙŠØ© Ø§Ù„Ù„Ø§Ø¦Ù‚Ø©. Ù¢. Ø·Ø±Ø­ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨ÙˆØ¶ÙˆØ­ ÙˆØ¥ÙŠØ¶Ø§Ø­. Ù£. Ù‚Ù… Ø¨Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ù„Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø§Ù†ØªØ¨Ø§Ù‡ ÙˆØ¯Ù‚Ø©. Ù¤. ØªØ­Ù‚Ù‚ Ù…Ù† ÙÙ‡Ù… Ø§Ù„Ø±Ø¯ ÙˆØ§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¯Ù‚ØªÙ‡. Ù¥. ÙÙŠ Ø­Ø§Ù„Ø© Ø¹Ø¯Ù… Ø§Ù„ÙÙ‡Ù…ØŒ Ø§Ø·Ù„Ø¨ ØªÙˆØ¶ÙŠØ­ Ø£Ùˆ Ø§Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø´ÙƒÙ„ Ù…Ø®ØªÙ„Ù. Ù¦. Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø¯ÙˆØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØ­Ø³ÙŠÙ† ØªÙØ§Ø¹Ù„Ø§ØªÙƒ Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"pipeline('conversational', model='hyunwoongko/blenderbot-9B')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a conversational model.\\\\n3. Specify the model 'hyunwoongko/blenderbot-9B' to be loaded. This model has been trained on a large dataset containing multi-turn conversations and excels at engaging users in natural conversations.\\\\n4. The created chatbot can be used to engage users in natural conversations by providing input text to the model and generating appropriate responses.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nchatbot = pipeline(\\\\'conversational\\\\', model=\\\\'hyunwoongko/blenderbot-9B\\\\')\\\\nconversation = \\\"What\\\\'s your favorite movie?\\\"\\\\nresponse = chatbot(conversation)\\\\nprint(response[\\\\'generated_text\\\\'])', 'api_call': \\\"pipeline('conversational', model='hyunwoongko/blenderbot-9B')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hyunwoongko/blenderbot-9B', 'api_call': \\\"pipeline('conversational', model='hyunwoongko/blenderbot-9B')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': 'Input a message to start chatting with hyunwoongko/blenderbot-9B.', 'performance': {'dataset': 'blended_skill_talk', 'accuracy': 'Not provided'}, 'description': 'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù„ÙŠÙƒ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨. Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù‡Ùˆ ØªØµÙ…ÙŠÙ… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯Ù‚Ø© ÙˆØ§Ø¹ØªÙ…Ø§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. We first import the necessary classes from the transformers, torch, and datasets packages. This includes AutoImageProcessor for image processing, ResNetForImageClassification for the image classification model, and load_dataset for loading your cat image dataset.\\\\n2. We then use the \\\"from_pretrained\\\" method of the ResNetForImageClassification class to load the pre-trained model \\\\'microsoft/resnet-50\\\\'. This model has been trained for image classification tasks and can be fine-tuned on specific classification problems, such as cat breed classification.\\\\n3. We load the cat image dataset using the load_dataset function from the datasets package.\\\\n4. We process the image data and create input tensors for the model using the AutoImageProcessor class.\\\\n5. The model is then used to predict the cat breed for a given image, with the label output by the model corresponding to the predicted breed.', '<<<code>>>': \\\"from transformers import AutoImageProcessor, ResNetForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\n\\\\ndataset = load_dataset('your_cat_dataset')\\\\n# replace 'your_cat_dataset' with the name of your cat dataset\\\\nimage = dataset['test']['image'][0]\\\\nprocessor = AutoImageProcessor.from_pretrained('microsoft/resnet-50')\\\\nmodel = ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\\\\ninputs = processor(image, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\n    predicted_label = logits.argmax(-1).item()\\\\n    print(model.config.id2label[predicted_label])\\\", 'api_call': \\\"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/resnet-50', 'api_call': \\\"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\\\", 'api_arguments': {'from_pretrained': 'microsoft/resnet-50'}, 'python_environment_requirements': [{'transformers': 'AutoImageProcessor, ResNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}], 'example_code': 'from transformers import AutoImageProcessor, ResNetForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset(huggingface/cats-image)\\\\nimage = dataset[test][image][0]\\\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/resnet-50)\\\\nmodel = ResNetForImageClassification.from_pretrained(microsoft/resnet-50)\\\\ninputs = processor(image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': '~0.5% top1'}, 'description': 'ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙØ±Ø¬Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± ÙÙŠ Ù…Ø¬Ø§Ù„ \\\"Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨\\\"ØŒ ÙŠØªÙ…ÙƒÙ† Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ø£Ù‚Ø³Ø§Ù… ÙˆØ§Ø¹ØªØ¨Ø§Ø± ÙƒÙ„ Ù‚Ø³Ù… Ø¹Ù„Ù‰ Ø£Ù†Ù‡ ÙƒØ§Ø¦Ù† Ù…Ø³ØªÙ‚Ù„. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø§Ø¯Ø±Ù‹Ø§ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„ÙˆØ§Ø±Ø¯Ø© ÙˆØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ù„ØªÙ…ÙŠÙŠØ² Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙÙŠ Ø§Ù„ØµÙˆØ±. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ·ÙˆÙŠØ± Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŸ? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries, which are YOLO and render_result from ultralyticsplus.', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\n\\\\npcb_image = 'path_to_your_pcb_image'\\\\nresults = model.predict(pcb_image)\\\\n\\\\nidentified_defects = results[0].boxes\\\\nrendered_image = render_result(model=model, image=pcb_image, result=results[0])\\\\nrendered_image.show()\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pcb-defect-segmentation', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\n\\\\npcb_image = 'path_to_your_pcb_image'\\\\nresults = model.predict(pcb_image)\\\\n\\\\nidentified_defects = results[0].boxes\\\\nrendered_image = render_result(model=model, image=pcb_image, result=results[0])\\\\nrendered_image.show()\\\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\n\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.515, 'mAP@0.5(mask)': 0.491}}, 'description': 'YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø§Ù„Ø´Ø±ÙƒØ© Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ø¬Ø¯ÙŠØ¯ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ©ØŒ ÙˆÙ‡Ùˆ ÙŠØªØ·Ù„Ø¨ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠØ³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ Ù‡Ø°Ø§ Ø§Ù„Ù‡Ø¯Ù.? [/INST]{'domain': 'Tabular Tabular Classification', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary libraries, which are 'joblib' and 'pandas'.\\\", 'code': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\ndata = pd.read_csv('data.csv')\\\\n# replace 'data.csv' with path to your data file\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Tabular Classification', 'api_name': 'datadmg/autotrain-test-news-44534112235', 'api_call': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\ndata = pd.read_csv('data.csv')\\\\n# replace 'data.csv' with path to your data file\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_arguments': ['data'], 'python_environment_requirements': ['joblib', 'pandas'], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'datadmg/autotrain-data-test-news', 'accuracy': 0.333}, 'description': 'This model is trained for Multi-class Classification on CO2 Emissions dataset. It uses the Hugging Face Transformers framework and is based on the extra_trees algorithm. The model is trained with AutoTrain and has a tabular classification functionality.'}} </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"CrossEncoder('cross-encoder/nli-deberta-v3-base')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. To use this API, you need to import the 'CrossEncoder' class from the 'sentence_transformers' package.\\\", 'code': 'from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder(\\\\'cross-encoder/nli-deberta-v3-base\\\\')\\\\nsentence_pair = (\\\"When an ice skater moves their arms closer to the body, the spin becomes faster.\\\", \\\"This phenomenon proves the conservation of angular momentum.\\\")\\\\nscores = model.predict([sentence_pair])', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Transformers', 'functionality': 'Cross-Encoder for Natural Language Inference', 'api_name': 'cross-encoder/nli-deberta-v3-base', 'api_call': 'from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder(\\\\'cross-encoder/nli-deberta-v3-base\\\\')\\\\nsentence_pair = (\\\"When an ice skater moves their arms closer to the body, the spin becomes faster.\\\", \\\"This phenomenon proves the conservation of angular momentum.\\\")\\\\nscores = model.predict([sentence_pair])', 'api_arguments': ['sentence_pairs'], 'python_environment_requirements': ['sentence_transformers', 'transformers'], 'example_code': \\\"from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-base')\\\\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])\\\", 'performance': {'dataset': {'SNLI-test': '92.38', 'MNLI mismatched set': '90.04'}}, 'description': 'This model is based on microsoft/deberta-v3-base and was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØªÙ… ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©. ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ ÙÙ‡Ù… ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ ÙˆÙØ¹Ù‘Ø§Ù„ Ù„ØªØ­Ø³ÙŠÙ† ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØªÙˆØ¬ÙŠÙ‡Ù‡.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('facebook/blenderbot-90M')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import necessary libraries, including AutoModelForCausalLM and AutoTokenizer from the transformers package.\\\\n2. Load the pre-trained model 'facebook/blenderbot-90M' using AutoModelForCausalLM.from_pretrained method, which is optimized for multi-turn dialogue tasks.\\\\n3. Load the corresponding tokenizer using AutoTokenizer.from_pretrained method.\\\\n4. To make the AI respond to an input message, tokenize this input message and append an end-of-string marker to it.\\\\n5. The generated tokens are passed to the model, which will output a sequence of tokens representing its response.\\\\n6. Finally, decode the response tokens into a string and display the AI's response.\\\", '<<<code>>>': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained('facebook/blenderbot-90M')\\\\nmodel = AutoModelForCausalLM.from_pretrained('facebook/blenderbot-90M')\\\\n\\\\ninput_message = 'What is your favorite color?'\\\\ntokenized_input = tokenizer.encode(input_message + tokenizer.eos_token, return_tensors='pt')\\\\n\\\\noutput = model.generate(tokenized_input, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n\\\\nresponse = tokenizer.decode(output[:, tokenized_input.shape[-1]:][0], skip_special_tokens=True)\\\\nprint(response)\\\", 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/blenderbot-90M')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/blenderbot-90M', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/blenderbot-90M')\\\", 'api_arguments': {'input_message': 'str'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained('facebook/blenderbot-90M')\\\\nmodel = AutoModelForCausalLM.from_pretrained('facebook/blenderbot-90M')\\\\n\\\\n# Chat with the model\\\\ninput_message = 'What is your favorite color?'\\\\ntokenized_input = tokenizer.encode(input_message + tokenizer.eos_token, return_tensors='pt')\\\\n\\\\noutput = model.generate(tokenized_input, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n\\\\nresponse = tokenizer.decode(output[:, tokenized_input.shape[-1]:][0], skip_special_tokens=True)\\\\nprint(response)\\\", 'performance': {'dataset': 'blended_skill_talk', 'accuracy': 'Not provided'}, 'description': 'BlenderBot-90M is a conversational AI model developed by Facebook AI. It is trained on the Blended Skill Talk dataset and aims to provide engaging and human-like responses in a multi-turn dialogue setting. The model is deprecated, and it is recommended to use the identical model https://huggingface.co/facebook/blenderbot_small-90M instead.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªØ¹Ù„Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ² Ø¹Ù„Ù‰ Ø±ÙˆØ¨ÙˆØª Ù„ØªØ¹Ù„ÙŠÙ…Ù‡ ÙƒÙŠÙÙŠØ© Ø§Ù„ØªÙ†Ù‚Ù„ Ø¯Ø§Ø®Ù„ Ø¨ÙŠØ¦Ø© Ù…Ø¬Ù‡ÙˆÙ„Ø©.? [/INST]'<<<domain>>>': 'Reinforcement Learning Robotics', '<<<api_call>>>': \\\"pipeline('robotics', model='Antheia/Hanna')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a reinforcement learning model for robotics.\\\\n3. Specify the model 'Antheia/Hanna' to be loaded. This model is trained on the openai/webgpt_comparisons dataset for various robotics tasks.\\\\n4. The created model can be used as the decision-making system for the robotic arm. By providing it with the current context of the environment (e.g., object positions on the conveyor belt), the model can help determine the appropriate action (where to grab objects) based on its reinforcement learning training.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nrobotics_pipeline = pipeline('robotics', model='Antheia/Hanna')\\\\nrobotic_arm_decisions = robotics_pipeline(context)\\\", 'api_call': \\\"pipeline('robotics', model='Antheia/Hanna')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Reinforcement Learning Robotics', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Antheia/Hanna', 'api_call': \\\"pipeline('robotics', model='Antheia/Hanna')\\\", 'api_arguments': 'model', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'openai/webgpt_comparisons', 'accuracy': ''}, 'description': 'Antheia/Hanna is a reinforcement learning model for robotics tasks, trained on the openai/webgpt_comparisons dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØ± ÙŠØ³ØªØ·ÙŠØ¹ Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚. Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø³ÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙÙŠ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚Ù‡ Ù„ÙØ¦Ø© Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø¹Ø¨Ø± ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØ± Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting', revision=fp16, torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the StableDiffusionInpaintPipeline model from the diffusers library.\\\\n2. Instantiate the model by loading it from Hugging Face\\\\'s model hub using the from_pretrained() method.\\\\n3. Provide the given text prompt \\\"A yellow cat sitting on a park bench\\\" to the model.\\\\n4. Use the model to generate a photo-realistic image based on the provided text prompt, without the need for mask_image in this case since we are not inpainting existing images.\\\\n5. Save the generated image to a file, such as \\\"yellow_cat_on_park_bench.png\\\".', '<<<code>>>': 'from diffusers import StableDiffusionInpaintPipeline\\\\npipe = StableDiffusionInpaintPipeline.from_pretrained(\\\\'runwayml/stable-diffusion-inpainting\\\\', revision=\\\\'fp16\\\\', torch_dtype=torch.float16)\\\\nprompt = \\\"A yellow cat sitting on a park bench\\\"\\\\ngenerated_image = pipe(prompt=prompt).images[0]\\\\ngenerated_image.save(\\\"yellow_cat_on_park_bench.png\\\")', 'api_call': \\\"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting', revision=fp16, torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \\\"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting', revision=fp16, torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': [{'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}], 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„ØªØ­Ø¯ÙŠØ¯ ÙˆØªØªØ¨Ø¹ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§ØªØŸ? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8m-forklift-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the ultralyticsplus package. The YOLO and render_result classes are required for object detection using the YOLOv8 framework.\\\\n2. Load the pre-trained model 'keremberke/yolov8m-forklift-detection' using the YOLO class. This model has been specifically trained for detecting forklifts and people, which is suitable for our warehouse safety system.\\\\n3. Adjust the model parameters such as confidence threshold, IOU threshold, and maximum detections to better fit our use case.\\\\n4. We can then input the image (URL or local path) containing the dangerous area and run the model to detect forklifts and people in that image.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-forklift-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://sample_url_here/image_name'  # replace with URL or local path to your image\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8m-forklift-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8m-forklift-detection', 'api_call': \\\"YOLO('keremberke/yolov8m-forklift-detection')\\\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8m-forklift-detection')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'forklift-object-detection', 'accuracy': 0.846}, 'description': 'A YOLOv8 model for detecting forklifts and persons in images.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ·ÙˆÙ‘Ø± Ø§Ù„Ø¹Ù…ÙŠÙ„ ØªØ·Ø¨ÙŠÙ‚Ù‹Ø§ Ø®Ø§ØµÙ‹Ø§ Ø¨Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"AutoModelForTTS.from_pretrained('lakahaga/novel_reading_tts')\\\", '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers package. This includes AutoModelForTTS for the text-to-speech model.\\\\n2. We then load the pre-trained model 'lakahaga/novel_reading_tts' that is designed specifically for Korean text-to-speech tasks.\\\\n3. We prepare the input text from our Korean novel.\\\\n4. The model is then used to generate the audiobook version of the novel content, and it can be saved as an audio file for distribution to consumers.\\\", '<<<code>>>': 'from transformers import AutoModelForTTS, AutoTokenizer\\\\nmodel = AutoModelForTTS.from_pretrained(\\\\'lakahaga/novel_reading_tts\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'lakahaga/novel_reading_tts\\\\')\\\\n\\\\ntext = \\\"ì´ê²ƒì€ ìƒˆë¡œ ì¶œíŒëœ í•œêµ­ì–´ ì†Œì„¤ì˜ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\\\"  # replace with the novel text\\\\ninputs = tokenizer(text, return_tensors=\\\\'pt\\\\')\\\\ngenerated_audio = model.generate(**inputs)', 'api_call': \\\"AutoModelForTTS.from_pretrained('lakahaga/novel_reading_tts')\\\", 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lakahaga/novel_reading_tts', 'api_call': \\\"AutoModelForTTS.from_pretrained('lakahaga/novel_reading_tts')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"inputs = processor(text, return_tensors='pt'); generated_audio = model.generate(**inputs);\\\", 'performance': {'dataset': 'novelspeech', 'accuracy': None}, 'description': 'This model was trained by lakahaga using novelspeech recipe in espnet. It is designed for Korean text-to-speech tasks.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¯ÙˆÙ† Ù…Ø³Ø¨Ù‚ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"pipeline('token-classification', model='kredor/punctuate-all')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' function from the transformers library.\\\\n2. Load the pre-trained 'kredor/punctuate-all' model, which is designed for punctuation prediction in different languages, using the 'pipeline' function.\\\\n3. Use the loaded model to predict the punctuation for the given text about a historical event.\\\", '<<<code>>>': 'from transformers import pipeline\\\\npunctuate_pipeline = pipeline(\\\\'token-classification\\\\', model=\\\\'kredor/punctuate-all\\\\')\\\\ntext = \\\"On April 16 1912 the Titanic sank in the early hours of the morning after striking an iceberg on its maiden voyage from Southampton England to New York City the disaster resulted in the loss of over 1500 lives and remains one of the deadliest peacetime maritime disasters in history\\\"\\\\npunctuated_text = punctuate_pipeline(text)', 'api_call': \\\"pipeline('token-classification', model='kredor/punctuate-all')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kredor/punctuate-all', 'api_call': \\\"pipeline('token-classification', model='kredor/punctuate-all')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'multilingual', 'accuracy': 0.98}, 'description': 'A finetuned xlm-roberta-base model for punctuation prediction on twelve languages: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ ØµÙˆØ±Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù…Ø³ØªÙˆØ­Ø§Ø© Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ: \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the \\\\'pipeline\\\\' function from the \\\\'transformers\\\\' library provided by Hugging Face.\\\\n2. The \\\\'pipeline\\\\' function is used to create a text-to-image model converting a given text prompt into a corresponding visual content.\\\\n3. We use \\\\'22h/vintedois-diffusion-v0-1\\\\' as the model to load for this task.\\\\n4. Call the created pipeline function with the narrative \\\"a beautiful mountain scenery during the sunset\\\" and additional parameters such as \\\\'CFG Scale\\\\', \\\\'Scheduler\\\\', \\\\'Steps\\\\', and \\\\'Seed\\\\' to generate the image.', '<<<code>>>': \\\"from transformers import pipeline\\\\ntext2img = pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\\\\ngenerated_image = text2img('a beautiful mountain scenery during the sunset', CFG_Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\\\", 'api_call': \\\"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \\\"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\\\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\\\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙ†Ø§ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù„ØªØ­ÙˆÙŠÙ„ Ù…Ù† Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø³Ù…ÙˆØ¹ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØªÙŠØ§ØªØŒ ÙˆÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…ÙÙ‡ÙˆÙ…. ÙŠØªØ·Ù„Ø¨ Ø°Ù„Ùƒ Ø¯Ù‚Ø© ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø³Ù…ÙˆØ¹.? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'api_provider': 'ESPnet', 'explanation': '', 'code': '', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'ESPnet', 'functionality': 'Text-to-Speech', 'api_name': 'kazusam/kt', 'api_call': '', 'api_arguments': ['input_text'], 'python_environment_requirements': ['Hugging Face', 'ESPnet'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': 'amadeus', 'accuracy': 'Not provided'}, 'description': 'An ESPnet2 TTS model trained by mio using amadeus recipe in espnet.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµØŒ Ø­ÙŠØ« ÙŠÙ‚ÙˆÙ… Ø¨ÙÙ‡Ù… Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…ÙƒØªÙˆØ¨Ø© ÙˆØ§Ù„Ù…Ù‚Ø±ÙˆØ¡Ø© Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø¥Ù†Ø³Ø§Ù†ØŒ Ø«Ù… ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ÙÙ‡Ù… ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„ÙŠÙ‡Ø§ Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ù†Ø§ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ‡ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"AutoModelWithLMHead.from_pretrained('flexudy/t5-base-multi-sentence-doctor')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. First, you will need to import AutoTokenizer and AutoModelWithLMHead classes from the transformers library.\\\\n2. Initialize both the tokenizer and the model by calling the from_pretrained() method, providing \\\\'flexudy/t5-base-multi-sentence-doctor\\\\' as the pre-trained model name.\\\\n3. Prepare the input text with the text that needs repair using the format \\\"repair_sentence: {input_sentence} context: {additional_context} </s>\\\", where input_sentence is the target text and additional_context can provide extra information to help the model with the correction.\\\\n4. Encode the input text using the tokenizer, to generate the input_ids for the model.\\\\n5. Use the generate() function on the model to create the corrected-output sentence.\\\\n6. Decode the output sentence(s) using tokenizer.decode() method with parameters to skip_special_tokens and clean_up_tokenization_spaces.\\\\n7. The final decoded sentence will contain the corrected version of the input text.', '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'flexudy/t5-base-multi-sentence-doctor\\\\')\\\\nmodel = AutoModelWithLMHead.from_pretrained(\\\\'flexudy/t5-base-multi-sentence-doctor\\\\')\\\\ninput_text = \\\"repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\"\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=\\\\'pt\\\\')\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('flexudy/t5-base-multi-sentence-doctor')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('flexudy/t5-base-multi-sentence-doctor')\\\", 'api_arguments': ['input_text'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ù…Ù†Ø§Ø·Ù‚ ØªÙ…Ø«Ù„ ÙƒÙ„ Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§ØªØŸ? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries: Image from PIL, AutoImageProcessor from transformers, and Mask2FormerForUniversalSegmentation from transformers.', 'code': \\\"from PIL import Image\\\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\nimport requests\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-cityscapes-semantic', 'api_call': \\\"from PIL import Image\\\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\nimport requests\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-large-cityscapes-semantic'}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \\\"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.'}} </s>\"\n"}
{"text": "<s>[INST]Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ ØµÙˆØ±ØŒ Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙˆÙ†ÙŠØ© Ø¹Ù…ÙŠÙ‚Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ù…Ø«Ù„ GANs. Ù‚Ù… Ø¨ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„ØµÙˆØ± Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ù„ØªÙ‚ÙˆÙŠÙ… ÙØ¹Ø§Ù„ÙŠØ© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. ÙˆÙŠØ¬Ø¨ Ø£Ø®ÙŠØ±Ø§Ù‹ Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ø³ØªØ¯Ø¹Ø§Ø¦ÙŠØ© Ù„Ø¶Ù…Ø§Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ± Ø§Ù„Ù†Ø§ØªØ¬Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We first import necessary libraries and classes, including PIL's Image library, and the ControlNetModel class and various other tools from the diffusers package.\\\\n2. We will then be using the MLSDdetector to detect straight lines in the input image of the building.\\\\n3. The ControlNet model will be employed after loading it using the from_pretrained method. This model is designed to control diffusion models by adding extra conditions.\\\\n4. The image will be processed by the model and straight lines will be detected and highlighted.\\\", '<<<code>>>': \\\"from PIL import Image\\\\nfrom diffusers import ControlNetModel, StableDiffusionControlNetPipeline, load_image\\\\nfrom controlnet_aux import MLSDdetector\\\\nimport torch\\\\n\\\\nmlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\\\nimage = load_image('image_path.png')\\\\n# replace 'image_path.png' with path to your image\\\\nimage = mlsd(image)\\\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\n\\\\nimage = pipe(room, image, num_inference_steps=20).images[0]\\\\nimage.save('images/building_mlsd_out.png')\\\", 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/sd-controlnet-mlsd', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\\\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': [{'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}], 'example_code': {'import': ['from PIL import Image', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'import torch', 'from controlnet_aux import MLSDdetector', 'from diffusers.utils import load_image'], 'setup': [\\\"mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)', 'image = mlsd(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)'], 'execution': ['pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', 'image = pipe(room, image, num_inference_steps=20).images[0]', \\\"image.save('images/room_mlsd_out.png')\\\"]}, 'performance': {'dataset': '600k edge-image, caption pairs generated from Places2', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù†Ù…Ø§Ø°Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙˆÙ„ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØ­Ø¯ÙŠØ¯ ÙØ¦Ø§ØªÙ‡Ø§ Ø¨Ø¯Ù‚Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. In this case, we'll use the 'joblib' library to load a pre-trained machine learning model. This model has been trained on a dataset of plants, so it can classify plant species based on their characteristics.\\\\n2. We import 'joblib', 'pandas', and 'json' libraries for this task. We use 'joblib' to load the pre-trained model, 'pandas' to process tabular data, and 'json' to read the configuration file.\\\\n3. We load the pre-trained model using 'joblib.load' and load the configuration file to know which features are needed.\\\\n4. Given plant characteristics like sepal and petal dimensions, we can predict the species using the 'predict' method of the loaded model.\\\", '<<<code>>>': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.DataFrame({'characteristic_1': [value_1], 'characteristic_2': [value_2], 'characteristic_3': [value_3], 'characteristic_4': [value_4]})\\\\n# Input characteristics of the plant\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'abhishek/autotrain-iris-xgboost', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'scikit-learn/iris', 'accuracy': 0.8666666666666667}, 'description': 'A tabular classification model trained on the Iris dataset using XGBoost and AutoTrain. The model is capable of multi-class classification and has an accuracy of 86.67%.' </s>\"\n"}
{"text": "<s>[INST]Ø³ÙˆÙ ØªØ¹Ù…Ù„ Ø§Ù„Ø´Ø±ÙƒØ© Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†ØµÙˆØµØŒ ÙˆØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØµÙ…ÙŠÙ… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªØ³ØªØ·ÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ¹Ù„Ù‚Ø© Ø¨ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù…Ø¶Ù…ÙˆÙ† Ù…Ù† Ù„ØºØ© Ø¥Ù„Ù‰ Ø£Ø®Ø±Ù‰.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. First, import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a machine translation model by specifying the model 'Helsinki-NLP/opus-mt-es-en'. This model is designed to translate Spanish text to English.\\\\n3. Provide the Spanish email body as input to the model and get back an English translation of the email content.\\\\n4. Read the translated email content to understand the message from the client.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntranslation_model = pipeline(\\\\'translation_es_to_en\\\\', model=\\\\'Helsinki-NLP/opus-mt-es-en\\\\')\\\\nspanish_email = \\\"Estimado/a, me gustarÃ­a obtener mÃ¡s informaciÃ³n sobre sus servicios y precios. AdemÃ¡s, podrÃ­a facilitarnos un folleto con los detalles de sus productos? Agradezco su atenciÃ³n. Saludos cordiales.\\\"\\\\nenglish_translation = translation_model(spanish_email)[0][\\\\'translation_text\\\\']', 'api_call': \\\"pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-es-en', 'api_call': \\\"pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')('Hola, Â¿cÃ³mo estÃ¡s?')\\\", 'performance': {'dataset': [{'name': 'newssyscomb2009-spaeng.spa.eng', 'accuracy': {'BLEU': 30.6, 'chr-F': 0.57}}, {'name': 'news-test2008-spaeng.spa.eng', 'accuracy': {'BLEU': 27.9, 'chr-F': 0.553}}, {'name': 'newstest2009-spaeng.spa.eng', 'accuracy': {'BLEU': 30.4, 'chr-F': 0.572}}, {'name': 'newstest2010-spaeng.spa.eng', 'accuracy': {'BLEU': 36.1, 'chr-F': 0.614}}, {'name': 'newstest2011-spaeng.spa.eng', 'accuracy': {'BLEU': 34.2, 'chr-F': 0.599}}, {'name': 'newstest2012-spaeng.spa.eng', 'accuracy': {'BLEU': 37.9, 'chr-F': 0.624}}, {'name': 'newstest2013-spaeng.spa.eng', 'accuracy': {'BLEU': 35.3, 'chr-F': 0.609}}, {'name': 'Tatoeba-test.spa.eng', 'accuracy': {'BLEU': 59.6, 'chr-F': 0.739}}]}, 'description': 'Helsinki-NLP/opus-mt-es-en is a machine translation model trained to translate from Spanish to English using the Hugging Face Transformers library. The model is based on the Marian framework and was trained on the OPUS dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªÙ†Ø´ÙŠØ· Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ø¬Ø±Ø§Ø¡ Ø¹Ù…Ù„ÙŠØ§Øª ØªØ¨Ø§ÙŠÙ† Ù„Ø§Ø³ØªÙƒØ´Ø§Ù Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© ÙˆØ¶Ø¨Ø· Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡ ÙÙŠ ØªÙˆÙ‚Ø¹ Ù†ØµÙˆØµ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„ÙƒÙ„Ø§Ù…. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø·Ø§ÙˆÙ„Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ùƒ ÙÙŠ Ø£Ø¯Ø§Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„Ù…Ø®ØµØµØ©.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': 'pipeline(\\\"automatic-speech-recognition\\\", model=\\\"openai/whisper-tiny\\\")', 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the required libraries, such as 'pipeline' from the transformers package.\\\", 'code': 'from transformers import pipeline\\\\nasr_model = pipeline(\\\"automatic-speech-recognition\\\", model=\\\"openai/whisper-tiny\\\")\\\\naudio_file = \\\"path_to_advertisement_audio_file\\\"\\\\ntranscription = asr_model(audio_file)', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Transcription and Translation', 'api_name': 'openai/whisper-tiny', 'api_call': 'from transformers import pipeline\\\\nasr_model = pipeline(\\\"automatic-speech-recognition\\\", model=\\\"openai/whisper-tiny\\\")\\\\naudio_file = \\\"path_to_advertisement_audio_file\\\"\\\\ntranscription = asr_model(audio_file)', 'api_arguments': {'forced_decoder_ids': \\\"WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')\\\"}, 'python_environment_requirements': ['transformers', 'datasets'], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', \\\"processor = WhisperProcessor.from_pretrained('openai/whisper-tiny')\\\", \\\"model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny')\\\", 'model.config.forced_decoder_ids = None', \\\"ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')\\\", \\\"sample = ds[0]['audio']\\\", \\\"input_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\\\", 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)'], 'performance': {'dataset': 'LibriSpeech (clean)', 'accuracy': 7.54}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model that can be used for transcription and translation tasks.'}} </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ©Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù„ÙŠØ³ØªÙÙŠØ¯ÙˆØ§ Ù…Ù†Ù‡ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/minilm-uncased-squad2')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers package. This includes AutoModelForQuestionAnswering, AutoTokenizer for handling tokenization, and the pipeline function.\\\\n2. We then use the from_pretrained method of the AutoModelForQuestionAnswering class to load the pre-trained model 'deepset/minilm-uncased-squad2'. This model has been trained for question answering tasks, which is exactly what we need for automatic FAQ answering.\\\\n3. We create a question-answering pipeline with the model and tokenizer. A sample input containing the user question and context from the FAQ page is provided to the pipeline.\\\\n4. The output from the pipeline will be the answer to the user's question.\\\", '<<<code>>>': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel_name = \\\\'deepset/minilm-uncased-squad2\\\\'\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=model_name, tokenizer=model_name)\\\\nQA_input = {\\\\n \\\\'question\\\\': \\\"What is the purpose of user\\\\'s freedom?\\\",\\\\n \\\\'context\\\\': \\\"The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\\\"\\\\n}\\\\nres = nlp(QA_input)', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/minilm-uncased-squad2')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'deepset/minilm-uncased-squad2', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/minilm-uncased-squad2')\\\", 'api_arguments': {'model_name': 'deepset/minilm-uncased-squad2'}, 'python_environment_requirements': [['transformers']], 'example_code': {'a': {'code': \\\"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel_name = deepset/minilm-uncased-squad2\\\\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\\\\nQA_input = {\\\\n 'question': 'Why is model conversion important?',\\\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\\\n}\\\\nres = nlp(QA_input)\\\"}, 'b': {'code': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer\\\\nmodel_name = deepset/minilm-uncased-squad2\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)'}}, 'performance': {'dataset': 'squad_v2', 'accuracy': {'exact': 76.13071675229513, 'f1': 79.49786500219953}}, 'description': 'MiniLM-L12-H384-uncased is a language model fine-tuned for extractive question answering on the SQuAD 2.0 dataset. It is based on the microsoft/MiniLM-L12-H384-uncased model and can be used with the Hugging Face Transformers library.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ± ØºÙŠØ± Ø§Ù„Ù…Ø´Ø±ÙˆØ· Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Computer Vision. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø¯ÙˆÙ† Ø´Ø±ÙˆØ·. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ®Ø²ÙŠÙ† Ø§Ù„ØµÙˆØ± Ø§Ù„Ù†Ø§ØªØ¬Ø© ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬. Ø§Ø³ØªÙ…ØªØ¹ Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ù…Ø¨ØªÙƒØ±Ø© ÙˆÙØ±ÙŠØ¯Ø© Ø¨ÙƒÙ„ Ø³Ù‡ÙˆÙ„Ø©.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the DDPMPipeline class from the diffusers package, which is required for unconditional image generation using the pre-trained model.', 'code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\\nimage = pipeline().images[0]\\\\nimage.save('butterfly_image.png')\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'ocariz/butterfly_200', 'api_call': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\\nimage = pipeline().images[0]\\\\nimage.save('butterfly_image.png')\\\", 'api_arguments': '', 'python_environment_requirements': 'diffusers', 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is a diffusion model for unconditional image generation of cute butterflies trained for 200 epochs.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù…Ù„. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø±Ø§Ù‹ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù…Ù„ ÙˆØªÙ‚Ø¯ÙŠÙ…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø³Ù‚ Ù„ØªØ³Ù‡ÙŠÙ„ ÙÙ‡Ù… Ø§Ù„Ù†ØµÙˆØµ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries from the transformers package provided by Hugging Face, such as AutoModel and AutoTokenizer.\\\\n2. Using AutoModel.from_pretrained, load the 'princeton-nlp/unsup-simcse-roberta-base' pre-trained model. This model is designed for unsupervised sentence embedding based on the SimCSE method, which helps measure the similarity between sentences.\\\\n3. For each document, tokenize its sentences using the AutoTokenizer, and then use the loaded model to convert them into embeddings.\\\\n4. When comparing two documents, calculate the similarity between their embeddings by a cosine similarity measurement.\\\\n5. The calculated similarity score can be used to determine the similarity between any two documents.\\\", '<<<code>>>': \\\"from transformers import AutoModel, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\\\nmodel = AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\\\n\\\\n# Convert documents to embeddings\\\\ndoc1_embeddings = model(**tokenizer(doc1, return_tensors='pt')).last_hidden_state\\\\ndoc2_embeddings = model(**tokenizer(doc2, return_tensors='pt')).last_hidden_state\\\\n\\\\n# Calculate cosine similarity between embeddings\\\\ncosine_similarity = doc1_embeddings.cpu().detach().numpy() @ doc2_embeddings.cpu().detach().numpy().T\\\\n\\\\n# Analyze the similarity score between documents\\\\nsimilarity_score = cosine_similarity[0][0]\\\", 'api_call': \\\"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \\\"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\\", 'api_arguments': None, 'python_environment_requirements': [['transformers']], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙˆÙ„ÙŠ Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ø¥Ù„Ù‰ ØªÙ†Ø¨Ø¤Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙŠ ÙÙ‡Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø¨ÙØ¹Ø§Ù„ÙŠØ©.? [/INST]{'domain': 'Tabular Tabular Regression', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_provider': 'Joblib', 'explanation': '', 'code': 'import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\\\'model.joblib\\\\')\\\\nsquare_footage_data = pd.DataFrame([{\\\"sqft\\\": 1500}, {\\\"sqft\\\": 2000}, {\\\"sqft\\\": 2500}], columns=[\\\"sqft\\\"])\\\\npredictions = model.predict(square_footage_data)', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Joblib', 'functionality': 'Single Column Regression', 'api_name': 'jwan2021/autotrain-us-housing-prices-1771761513', 'api_call': 'import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\\\'model.joblib\\\\')\\\\nsquare_footage_data = pd.DataFrame([{\\\"sqft\\\": 1500}, {\\\"sqft\\\": 2000}, {\\\"sqft\\\": 2500}], columns=[\\\"sqft\\\"])\\\\npredictions = model.predict(square_footage_data)', 'api_arguments': ['data'], 'python_environment_requirements': ['joblib', 'pandas'], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'jwan2021/autotrain-data-us-housing-prices', 'accuracy': {'Loss': 100581.032, 'R2': 0.922, 'MSE': 10116543945.03, 'MAE': 81586.656, 'RMSLE': 0.101}}, 'description': 'A single column regression model for predicting US housing prices, trained with AutoTrain and using the Joblib framework.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ¹ÙŠÙ† Ø¹Ù„ÙŠÙ†Ø§ Ø§Ù„Ø¢Ù† Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ø¬Ø¯ÙŠØ¯Ø© ØªÙ…Ø§Ù…Ø§ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¯ÙˆÙ† Ù‚ÙŠÙˆØ¯. ØªÙˆØ¬Ù‡ Ø¥Ù„Ù‰ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙˆÙ‚Ù… Ø¨ØªÙˆØµÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ø¨Ø¯Ø¡ Ø¥Ù†Ø´Ø§Ø¡ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø±Ø§Ø¦Ø¹.? [/INST]{'domain': 'Natural Language Processing Text Classification', 'api_call': \\\"pipeline('text-classification', model='prithivida/parrot_adequacy_model')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. We import the necessary 'pipeline' function from the transformers library provided by Hugging Face.\\\", 'code': 'from transformers import pipeline\\\\nadequacy_classifier = pipeline(\\\\'text-classification\\\\', model=\\\\'prithivida/parrot_adequacy_model\\\\')\\\\nparaphrases = [\\\"paraphrase 1\\\", \\\"paraphrase 2\\\", \\\"paraphrase 3\\\"]\\\\nadequacy_scores = [adequacy_classifier(p)[\\\\'label\\\\'] for p in paraphrases]\\\\nbest_paraphrase = paraphrases[adequacy_scores.index(\\\"LABEL_1\\\")]', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'prithivida/parrot_adequacy_model', 'api_call': 'from transformers import pipeline\\\\nadequacy_classifier = pipeline(\\\\'text-classification\\\\', model=\\\\'prithivida/parrot_adequacy_model\\\\')\\\\nparaphrases = [\\\"paraphrase 1\\\", \\\"paraphrase 2\\\", \\\"paraphrase 3\\\"]\\\\nadequacy_scores = [adequacy_classifier(p)[\\\\'label\\\\'] for p in paraphrases]\\\\nbest_paraphrase = paraphrases[adequacy_scores.index(\\\"LABEL_1\\\")]', 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Parrot is a paraphrase-based utterance augmentation framework purpose-built to accelerate training NLU models. This model is an ancillary model for Parrot paraphraser.'}} </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ù…Ø¬Ø§Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': './run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori', '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. Install the necessary libraries and tools, which include 'espnet2' and 'huggingface_hub'.\\\\n2. Use the provided shell script 'run.sh' to download and load the pretrained text-to-speech model 'mio/tokiwa_midori' from Hugging Face Hub.\\\\n3. Set the configuration options to skip data preparation and training, as you only want to load the pretrained model for inference.\\\\n4. Input the educational text into the Text2Speech module of the loaded model, and it will generate the speech audio file for the narration.\\\\n5. Save the generated audio file for use in the e-learning platform.\\\", '<<<code>>>': \\\"# Install the necessary libraries and tools\\\\n!pip install espnet2 huggingface_hub\\\\n\\\\n# Load the pretrained TTS model\\\\n!./run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori\\\\n\\\\n# Convert educational text to speech\\\\neducational_text = 'Educational text goes here...'\\\\ntts = Text2Speech(config, model_path)\\\\nwav, sr = tts.synthesize(educational_text)\\\\nwavfile.write('output_narration.wav', sr, wav.numpy())\\\", 'api_call': './run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori', 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mio/tokiwa_midori', 'api_call': './run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori', 'api_arguments': ['espnet', 'mio/tokiwa_midori', 'Text2Speech', 'config', 'model_path'], 'python_environment_requirements': [['espnet2', 'huggingface_hub']], 'example_code': './run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori', 'performance': {'dataset': 'amadeus', 'accuracy': 'Not provided'}, 'description': 'This model was trained by mio using amadeus recipe in espnet.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙˆÙØ± Ù„Ù†Ø§ Ø¹Ø¯Ø© Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¬Ù…ÙˆØ¹Ø© ÙˆØ§Ø³Ø¹Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ù„Ø§Ø³ØªÙƒØ´Ø§Ù ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠ. Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ ØªØµÙ†ÙŠÙ Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù„Ù‰ Ø¥Ø¯Ø®Ø§Ù„Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© ÙˆÙ‚Ù… Ø¨ØªÙ‚Ø¯ÙŠÙ…Ù‡Ø§ Ù„Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ Ù„ÙŠÙÙ„Ø¹ÙØ¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': 'TF_Decision_Trees(input_features, target)', '<<<api_provider>>>': 'Keras', '<<<explanation>>>': \\\"1. Start by gathering the required dataset, which includes employee demographics and their respective salaries. Ensure the data is cleaned and preprocessed before it is used.\\\\n2. We will use TensorFlow's Gradient Boosted Trees model for binary classification of the structured data that comprises employee demographics.\\\\n3. Input features will be considered, such as age, education, work experience, and occupation. Our target is to predict whether an employee makes over 50,000 USD per year.\\\\n4. We will employ a custom binary target encoder as a Keras preprocessing layer to encode the categorical features according to their target value co-occurrences. Then, we will use the encoded features to develop a decision forests model for salary prediction.\\\", '<<<code>>>': 'import tensorflow as tf\\\\nfrom TF_Decision_Trees import TF_Decision_Trees\\\\nimport pandas as pd\\\\n\\\\n# Load and preprocess data\\\\ndata = pd.read_csv(\\\"employee_demographics.csv\\\")\\\\ninput_features = data[[\\\\'age\\\\', \\\\'education\\\\', \\\\'work_experience\\\\', \\\\'occupation\\\\']] \\\\ntarget = (data[\\\\'salary\\\\'] > 50000).astype(int)\\\\n\\\\n# Build and train the Decision Trees model\\\\nmodel = TF_Decision_Trees(input_features, target)\\\\nhistory = model.fit(input_features, target, epochs=100, batch_size=32, validation_split=0.2)', 'api_call': 'TF_Decision_Trees(input_features, target)', 'provider': 'Keras', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'TF_Decision_Trees', 'api_call': 'TF_Decision_Trees(input_features, target)', 'api_arguments': ['input_features', 'target'], 'python_environment_requirements': [['tensorflow >= 7.0']], 'example_code': 'https://github.com/tdubon/TF-GB-Forest/blob/c0cf4c7e3e29d819b996cfe4eecc1f2728115e52/TFDecisionTrees_Final.ipynb', 'performance': {'dataset': 'Census-Income Data Set', 'accuracy': 96.57}, 'description': \\\"Use TensorFlow's Gradient Boosted Trees model in binary classification of structured data. Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is trained on the US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables. The task is to determine whether a person makes over 50k a year.\\\" </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ²ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù„Ù… Ø¢Ù„ÙŠ Ù„Ù„Ø£ØªÙ…ØªØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"load_from_hub(repo_id='sb3/ppo-CartPole-v1',filename='{MODEL FILENAME}.zip',)\\\", '<<<api_provider>>>': 'Stable-Baselines3', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"load_from_hub(repo_id='sb3/ppo-CartPole-v1',filename='{MODEL FILENAME}.zip',)\\\", 'provider': 'Stable-Baselines3', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sb3/ppo-CartPole-v1', 'api_call': \\\"load_from_hub(repo_id='sb3/ppo-CartPole-v1',filename='{MODEL FILENAME}.zip',)\\\", 'api_arguments': ['algo', 'env', 'f'], 'python_environment_requirements': [['rl_zoo3', 'stable-baselines3', 'stable-baselines3-contrib']], 'example_code': 'python -m rl_zoo3.load_from_hub --algo ppo --env CartPole-v1 -orga sb3 -f logs/', 'performance': {'dataset': 'CartPole-v1', 'accuracy': '500.00 +/- 0.00'}, 'description': 'This is a trained model of a PPO agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.' </s>\"\n"}
{"text": "<s>[INST]Ø³ÙŠØªÙ… ØªÙ†ÙÙŠØ° Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø¹ÙŠÙ†. ÙŠØªØ·Ù„Ø¨ Ø°Ù„Ùƒ ÙÙ‡Ù… Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„ ÙˆØ¥Ù†Ø´Ø§Ø¡ Ù†Øµ Ø®Ø±Ø¬ÙŠ Ù…Ø¹Ø§Ø¯ ØªÙˆÙ„ÙŠØ¯Ù‡.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5Model.from_pretrained('t5-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, T5Tokenizer and T5Model, from the transformers package.\\\\n2. Load the pre-trained T5 model and tokenizer using the 't5-base' name.\\\\n3. Tokenize the input sentence using the T5 tokenizer, including a prefix such as 'translate <LANGUAGE>:' before the sentence for the desired target language (e.g., 'translate Spanish:').\\\\n4. Use the pre-trained T5 model to generate the translated text based on the tokenized input.\\\\n5. Decode the output tokens from the model to construct the translated sentence in the desired language.\\\", '<<<code>>>': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\n\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'t5-base\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'t5-base\\\\')\\\\ninput_text = \\\"translate Spanish: The advancements in artificial intelligence have revolutionized the way we live.\\\"\\\\ninput_ids = tokenizer(input_text, return_tensors=\\\\'pt\\\\').input_ids\\\\ntranslated_tokens = model.generate(input_ids)\\\\ntranslated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)', 'api_call': \\\"T5Model.from_pretrained('t5-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 't5-base', 'api_call': \\\"T5Model.from_pretrained('t5-base')\\\", 'api_arguments': ['input_ids', 'decoder_input_ids'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import T5Tokenizer, T5Model\\\\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\\\\nmodel = T5Model.from_pretrained('t5-base')\\\\ninput_ids = tokenizer('Studies have been shown that owning a dog is good for you', return_tensors='pt').input_ids\\\\ndecoder_input_ids = tokenizer('Studies show that', return_tensors='pt').input_ids\\\\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\\\\nlast_hidden_states = outputs.last_hidden_state\\\", 'performance': {'dataset': 'c4', 'accuracy': 'See research paper, Table 14'}, 'description': 'T5-Base is a Text-To-Text Transfer Transformer (T5) model with 220 million parameters. It is designed to perform various NLP tasks, including machine translation, document summarization, question answering, and text classification. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be used with the Transformers library.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ Ù†ØµÙŠ Ù„Ù…Ø­ØªÙˆÙ‰ Ù…ÙƒØªÙˆØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ù…Ø­Ø¯Ø¯ Ø£Ø¹Ù„Ø§Ù‡. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary classes from the transformers library and custom tokenization module. This includes PegasusForConditionalGeneration for the text summarization model and PegasusTokenizer for processing Chinese text.\\\\n2. We then use the from_pretrained method to load the pre-trained model 'IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese'. This model has been fine-tuned on multiple Chinese text summarization datasets and is ideal for generating a summary of a Chinese article.\\\\n3. We load the text in the variable 'text' and tokenize it using the PegasusTokenizer.\\\\n4. The model processes the tokenized text and generates a summary, which can be obtained by decoding the tokens.\\\", '<<<code>>>': \\\"from transformers import PegasusForConditionalGeneration\\\\nfrom tokenizers_pegasus import PegasusTokenizer\\\\nmodel = PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntokenizer = PegasusTokenizer.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntext = 'ä½ çš„ä¸­æ–‡æ–‡ç« å†…å®¹'\\\\ninputs = tokenizer(text, max_length=1024, return_tensors='pt')\\\\nsummary_ids = model.generate(inputs['input_ids'])\\\\nsummary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\\\", 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Randeng-Pegasus-238M-Summary-Chinese', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\", 'api_arguments': {'text': 'string', 'max_length': 'integer'}, 'python_environment_requirements': [['transformers', 'tokenizers_pegasus.py', 'data_utils.py']], 'example_code': \\\"from transformers import PegasusForConditionalGeneration\\\\nfrom tokenizers_pegasus import PegasusTokenizer\\\\nmodel = PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntokenizer = PegasusTokenizer.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntext = '1'\\\\ninputs = tokenizer(text, max_length=1024, return_tensors='pt')\\\\nsummary_ids = model.generate(inputs['input_ids'])\\\\ntokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\\\", 'performance': {'dataset': 'LCSTS', 'accuracy': {'rouge-1': 43.46, 'rouge-2': 29.59, 'rouge-L': 39.76}}, 'description': 'Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus. It is fine-tuned on 7 Chinese text summarization datasets including education, new2016zh, nlpcc, shence, sohu, thucnews, and weibo. The model can be used to generate summaries for Chinese text inputs.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¢ØªÙŠ ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„ÙŠÙ‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©: \\\"Ù…Ø§ Ù‡Ùˆ ØªØ£Ø«ÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙÙŠ ØªØ­Ø³ÙŠÙ† ÙÙ‡Ù… Ø§Ù„Ù†ØµÙˆØµ ÙˆØ¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©ØŸ\\\"? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'), tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'))\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required libraries: pipeline from transformers, AutoModel, and AutoTokenizer.\\\\n2. Load the model by calling the pipeline function with 'question-answering' as task and providing the pretrained model 'deepset/bert-large-uncased-whole-word-masking-squad2'.\\\\n3. After loading the model and tokenizer, you can use them to answer questions related to any given context. In our case, the chatbot will answer questions about a specific text.\\\\n4. Prepare the input by providing the question and context where the answer will be extracted.\\\\n5. Call the loaded pipeline with the prepared input and it will return the answer to the question based on the context.\\\", '<<<code>>>': \\\"from transformers import AutoModel, AutoTokenizer, pipeline\\\\nqa_pipeline = pipeline(\\\\n    'question-answering',\\\\n    model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'),\\\\n    tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2')\\\\n)\\\\ninput_qa = {\\\\n    'question': 'What is the purpose of the chatbot?',\\\\n    'context': 'The chatbot is designed to answer questions based on a specific text, helping users find information quickly and easily.'\\\\n}\\\\nresponse = qa_pipeline(input_qa)\\\", 'api_call': \\\"pipeline('question-answering', model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'), tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'))\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-large-uncased-whole-word-masking-squad2', 'api_call': \\\"pipeline('question-answering', model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'), tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'))\\\", 'api_arguments': {'model_name': 'deepset/bert-large-uncased-whole-word-masking-squad2', 'tokenizer': 'deepset/bert-large-uncased-whole-word-masking-squad2'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"QA_input = {\\\\n 'question': 'Why is model conversion important?',\\\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\\\n}\\\\nres = nlp(QA_input)\\\", 'performance': {'dataset': 'squad_v2', 'accuracy': {'Exact Match': 80.885, 'F1': 83.876}}, 'description': 'This is a bert-large model, fine-tuned using the SQuAD2.0 dataset for the task of question answering. It is designed for extractive question answering and supports English language.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ­ÙˆÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØ±Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ ØªØ·Ø¨ÙŠÙ‚Ù†Ø§ Ø§Ù„Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Token Classification', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': 'from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\\\\n\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'d4data/biomedical-ner-all\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'d4data/biomedical-ner-all\\\\')\\\\nbiomedical_ner = pipeline(\\\\'ner\\\\', model=model, tokenizer=tokenizer)\\\\nanalyzed_report = biomedical_ner(\\\"The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.\\\")', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Transformers', 'functionality': 'Named Entity Recognition', 'api_name': 'd4data/biomedical-ner-all', 'api_call': 'from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\\\\n\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'d4data/biomedical-ner-all\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'d4data/biomedical-ner-all\\\\')\\\\nbiomedical_ner = pipeline(\\\\'ner\\\\', model=model, tokenizer=tokenizer)\\\\nanalyzed_report = biomedical_ner(\\\"The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.\\\")', 'api_arguments': {'model': 'AutoModelForTokenClassification.from_pretrained(d4data/biomedical-ner-all)', 'tokenizer': 'AutoTokenizer.from_pretrained(d4data/biomedical-ner-all)', 'aggregation_strategy': 'simple'}, 'python_environment_requirements': {'transformers': 'from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification'}, 'example_code': 'pipe(The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.)', 'performance': {'dataset': 'Maccrobat', 'accuracy': 'Not provided'}, 'description': 'An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased.'}} </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ÙƒØ´Ù Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØªÙŠ.? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"Pipeline.from_pretrained('pyannote/speaker-diarization@2.1',use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", '<<<api_provider>>>': 'pyannote.audio', '<<<explanation>>>': \\\"1. Import the Pipeline function from the pyannote.audio library.\\\\n2. Use the Pipeline's from_pretrained method to load the pre-trained model 'pyannote/speaker-diarization@2.1'. You will need to provide your access token as an argument.\\\\n3. The loaded speaker diarization model can process an audio file of the court proceeding.\\\\n4. The model outputs the speaker information, and the output can be saved in RTTM format. This will provide timestamps indicating who is speaking during different segments of the audio.\\\", '<<<code>>>': \\\"from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained('pyannote/speaker-diarization@2.1', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\\ndiarization = pipeline('court_proceeding_audio.wav')\\\\nwith open('court_proceeding_audio.rttm', 'w') as rttm:\\\\n    diarization.write_rttm(rttm)\\\", 'api_call': \\\"Pipeline.from_pretrained('pyannote/speaker-diarization@2.1',use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", 'provider': 'pyannote.audio', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'johnislarry/cloned-pyannote-speaker-diarization-endpoint', 'api_call': \\\"Pipeline.from_pretrained('pyannote/speaker-diarization@2.1',use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", 'api_arguments': ['num_speakers', 'min_speakers', 'max_speakers', 'segmentation_onset'], 'python_environment_requirements': ['pyannote.audio 2.0'], 'example_code': {'load_pipeline': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)', 'apply_pipeline': 'diarization = pipeline(audio.wav)', 'save_output': 'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)'}, 'performance': {'dataset': [{'name': 'AISHELL-4', 'accuracy': {'DER%': 14.61, 'FA%': 3.31, 'Miss%': 4.35, 'Conf%': 6.95}}, {'name': 'AMI Mix-Headset only_words', 'accuracy': {'DER%': 18.21, 'FA%': 3.28, 'Miss%': 11.07, 'Conf%': 3.87}}, {'name': 'AMI Array1-01 only_words', 'accuracy': {'DER%': 29.0, 'FA%': 2.71, 'Miss%': 21.61, 'Conf%': 4.68}}, {'name': 'CALLHOME Part2', 'accuracy': {'DER%': 30.24, 'FA%': 3.71, 'Miss%': 16.86, 'Conf%': 9.66}}, {'name': 'DIHARD 3 Full', 'accuracy': {'DER%': 20.99, 'FA%': 4.25, 'Miss%': 10.74, 'Conf%': 6.0}}, {'name': 'REPERE Phase 2', 'accuracy': {'DER%': 12.62, 'FA%': 1.55, 'Miss%': 3.3, 'Conf%': 7.76}}, {'name': 'VoxConverse v0.0.2', 'accuracy': {'DER%': 12.76, 'FA%': 3.45, 'Miss%': 3.85, 'Conf%': 5.46}}]}, 'description': 'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ù‡Ø¯Ù Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØªØµÙ†ÙŠÙ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙˆØ§Ù„ØªØ¹Ø§Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù„ØºÙˆÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-job_all-903929564', use_auth_token=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers package. This includes AutoModelForTokenClassification for the entity extraction model and AutoTokenizer for processing text data.\\\\n2. We then use the from_pretrained method of the AutoModelForTokenClassification class to load the pre-trained model 'ismail-lucifer011/autotrain-job_all-903929564'. This model has been trained for entity extraction tasks, which is exactly what we need to identify transaction details from customers' messages.\\\\n3. The loaded model will be used for token classification, specifically for entity extraction in this case. The input text will be tokenized and processed by the model to identify and classify entities within the text.\\\", '<<<code>>>': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'ismail-lucifer011/autotrain-job_all-903929564\\\\', use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'ismail-lucifer011/autotrain-job_all-903929564\\\\', use_auth_token=True)\\\\n# replace the msg variable with client\\\\'s message\\\\nmsg = \\\"I transferred $500 to John\\\\'s account. Transaction ID is A123456.\\\"\\\\ninputs = tokenizer(msg, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nentities = process_entities(outputs)', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-job_all-903929564', use_auth_token=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': '903929564', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-job_all-903929564', use_auth_token=True)\\\", 'api_arguments': {'inputs': 'I love AutoTrain'}, 'python_environment_requirements': [{'transformers': 'AutoModelForTokenClassification, AutoTokenizer'}], 'example_code': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-job_all-903929564, use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-job_all-903929564, use_auth_token=True)\\\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\\\noutputs = model(**inputs)', 'performance': {'dataset': 'ismail-lucifer011/autotrain-data-job_all', 'accuracy': 0.9989412009896035}, 'description': 'A Token Classification model trained using AutoTrain for Entity Extraction. The model is based on distilbert and achieves high accuracy, precision, recall, and F1 score.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ ØªØ³Ø¬ÙŠÙ„ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© Ø¨Ø§Ù„ØµÙˆØª ØªØ¨Ø¯Ø£ Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\" Ù„ØªØ­Ø³ÙŠÙ† Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries and classes from the transformers package, numpy, and torch. This includes VideoMAEImageProcessor and VideoMAEForPreTraining.', 'code': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\npixel_values = processor(video, return_tensors='pt').pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss\\\", 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'videomae-base-short-ssv2', 'api_call': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\npixel_values = processor(video, return_tensors='pt').pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss\\\", 'api_arguments': ['video', 'return_tensors'], 'python_environment_requirements': ['transformers', 'numpy', 'torch'], 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\\\npixel_values = processor(video, return_tensors=pt).pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss', 'performance': {'dataset': 'Something-Something-v2', 'accuracy': 'N/A'}, 'description': 'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Start by installing the necessary library sentence-transformers.\\\\n2. Import the SentenceTransformer package from sentence_transformers.\\\\n3. Initialize the SentenceTransformer model using 'sentence-transformers/paraphrase-distilroberta-base-v2' which is trained to compute semantic similarity between sentences.\\\\n4. Encode each forum title or description using the model to get their embedding vectors.\\\\n5. Perform clustering on the embeddings to group related forums together (e.g., using K-means, DBSCAN or other clustering algorithms).\\\\n6. Assign the forums to their respective clusters based on similarity.\\\", '<<<code>>>': \\\"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\\nforum_sentences = [...] # Forum titles or descriptions\\\\nembeddings = model.encode(forum_sentences)\\\\n\\\\n# Perform clustering on embeddings and assign forums to clusters (e.g., using K-means, DBSCAN, or other clustering algorithms)\\\", 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/paraphrase-distilroberta-base-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ø³ÙŠÙ† Ù„Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ø²Ø¬Ø§Ø¬ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ù‡Ø°Ø§ØŸ? [/INST]{'domain': 'Tabular Tabular Regression', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\ndata = pd.read_csv('data.csv')\\\\n# replace 'data.csv' with path to your input data\\\\npredictions = model.predict(data)\\\", 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face', 'functionality': 'Carbon Emissions', 'api_name': '45473113800', 'api_call': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\ndata = pd.read_csv('data.csv')\\\\n# replace 'data.csv' with path to your input data\\\\npredictions = model.predict(data)\\\", 'api_arguments': ['data'], 'python_environment_requirements': ['joblib', 'pandas'], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'samvelkoch/autotrain-data-prknsn-2', 'accuracy': {'Loss': 5.079, 'R2': 0.109, 'MSE': 25.795, 'MAE': 3.78, 'RMSLE': 0.849}}, 'description': 'A tabular regression model trained with AutoTrain for predicting carbon emissions.'}} </s>\"\n"}
{"text": "<s>[INST]Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ù„Ù„ØªØ¹Ø±Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù…: \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\", '<<<api_provider>>>': 'PyTorch Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes and functions from the transformers and datasets libraries.\\\\n2. Load the Data2VecForCTC model pretrained on the 'facebook/data2vec-audio-base-960h' dataset, which supports the transcription of audio files and delivers competitive performance on major speech recognition benchmarks.\\\\n3. Load the Wav2Vec2Processor to convert the raw audio input into the form required by the model.\\\\n4. Transcribe the audio from the presentation by first processing it using the processor and then running it through the model to retrieve logits. Decode the logits to obtain the corresponding transcriptions.\\\", '<<<code>>>': \\\"from transformers import Data2VecForCTC, Wav2Vec2Processor\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/data2vec-audio-base-960h')\\\\nmodel = Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\\n\\\\ninput_values = processor(audio_data, return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_call': \\\"Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\", 'provider': 'PyTorch Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'data2vec-audio-base-960h', 'api_call': \\\"Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers', 'datasets', 'torch', 'jiwer']], 'example_code': 'from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\n# load model and processor\\\\nprocessor = Wav2Vec2Processor.from_pretrained(facebook/data2vec-audio-base-960h)\\\\nmodel = Data2VecForCTC.from_pretrained(facebook/data2vec-audio-base-960h)\\\\n\\\\n# load dummy dataset and read soundfiles\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\n\\\\n# tokenize\\\\ninput_values = processor(ds[0][audio][array],, return_tensors=pt, padding=longest).input_values # Batch size 1\\\\n\\\\n# retrieve logits\\\\nlogits = model(input_values).logits\\\\n\\\\n# take argmax and decode\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'performance': {'dataset': 'librispeech_asr', 'accuracy': {'clean': 2.77, 'other': 7.08}}, 'description': \\\"Facebook's Data2Vec-Audio-Base-960h model is an Automatic Speech Recognition model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It can be used for transcribing audio files and achieves competitive performance on major benchmarks of speech recognition. The model is based on the Data2Vec framework which uses the same learning method for either speech, NLP, or computer vision.\\\" </s>\"\n"}
{"text": "<s>[INST]ØªØ­ØªØ§Ø¬ Ø´Ø±ÙƒØªÙ†Ø§ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø¨Ø¯Ù‚Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ù…Ø¶Ù…ÙˆÙ† Ø§Ù„ØµÙˆØª ÙˆØ§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£ØµÙˆØ§Øª Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the encoder_classifier and torchaudio libraries.\\\\n2. Load the pretrained model 'TalTechNLP/voxlingua107-epaca-tdnn' using the from_hparams() method from the EncoderClassifier class.\\\\n3. The loaded model can then be used to load audio files and process them accordingly.\\\\n4. The model can classify a speech utterance according to the language spoken and covers 107 different languages.\\\", '<<<code>>>': \\\"from speechbrain.pretrained import EncoderClassifier\\\\nimport torchaudio\\\\n\\\\nlanguage_id = EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\\nsignal, _ = torchaudio.load(audio_file_path)\\\\nprediction = language_id.classify_batch(signal)\\\\n\\\\nprint(prediction)\\\", 'api_call': \\\"EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'TalTechNLP/voxlingua107-epaca-tdnn', 'api_call': \\\"EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\", 'api_arguments': ['signal'], 'python_environment_requirements': [['speechbrain', 'torchaudio']], 'example_code': 'import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\nlanguage_id = EncoderClassifier.from_hparams(source=TalTechNLP/voxlingua107-epaca-tdnn, savedir=tmp)\\\\nsignal = language_id.load_audio(https://omniglot.com/soundfiles/udhr/udhr_th.mp3)\\\\nprediction = language_id.classify_batch(signal)\\\\nprint(prediction)', 'performance': {'dataset': 'VoxLingua107', 'accuracy': '93%'}, 'description': 'This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain. The model uses the ECAPA-TDNN architecture that has previously been used for speaker recognition. The model can classify a speech utterance according to the language spoken. It covers 107 different languages.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ±Ø¬Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ù…Ù„Ø© Ù‚ØµÙŠØ±Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙØ¬ÙˆØ© ÙŠØ¬Ø¨ Ù…Ù„Ø¦Ù‡Ø§ Ø¨ÙƒÙ„Ù…Ø© Ù…Ù†Ø§Ø³Ø¨Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù†\\\" ÙˆØ°Ù„Ùƒ Ù„ØªØ­Ø³ÙŠÙ† Ù†Ù…Ø§Ø°Ø¬ Ù…Ù„Ø¦ Ø§Ù„ÙØ¬ÙˆØ§Øª.? [/INST]{'domain': 'Natural Language Processing Fill-Mask', 'api_call': \\\"AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary libraries and classes. For this task, we need AutoModelForMaskedLM and AutoTokenizer from the transformers library.', 'code': 'from transformers import AutoModelForMaskedLM, AutoTokenizer\\\\nmodel = AutoModelForMaskedLM.from_pretrained(\\\\'cl-tohoku/bert-base-japanese-whole-word-masking\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'cl-tohoku/bert-base-japanese-whole-word-masking\\\\')\\\\nsentence_with_blank = \\\"ã“ã®ç©ºé–“ã«ã¯_ç©ºç™½_ã®éƒ¨å±‹ãŒã‚ã‚Šã¾ã™.\\\" # Fill the blank with the appropriate word\\\\nsentence_with_mask = sentence_with_blank.replace(\\\"_ç©ºç™½_\\\", \\\"[MASK]\\\")\\\\ninputs = tokenizer(sentence_with_mask, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\npredictions = outputs.logits.argmax(dim=-1)\\\\npredicted_word = tokenizer.decode(predictions[0]).replace(\\\" \\\",\\\"\\\")\\\\nfilled_sentence = sentence_with_blank.replace(\\\"_ç©ºç™½_\\\", predicted_word)', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Fill-Mask', 'api_name': 'cl-tohoku/bert-base-japanese-whole-word-masking', 'api_call': 'from transformers import AutoModelForMaskedLM, AutoTokenizer\\\\nmodel = AutoModelForMaskedLM.from_pretrained(\\\\'cl-tohoku/bert-base-japanese-whole-word-masking\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'cl-tohoku/bert-base-japanese-whole-word-masking\\\\')\\\\nsentence_with_blank = \\\"ã“ã®ç©ºé–“ã«ã¯_ç©ºç™½_ã®éƒ¨å±‹ãŒã‚ã‚Šã¾ã™.\\\" # Fill the blank with the appropriate word\\\\nsentence_with_mask = sentence_with_blank.replace(\\\"_ç©ºç™½_\\\", \\\"[MASK]\\\")\\\\ninputs = tokenizer(sentence_with_mask, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\npredictions = outputs.logits.argmax(dim=-1)\\\\npredicted_word = tokenizer.decode(predictions[0]).replace(\\\" \\\",\\\"\\\")\\\\nfilled_sentence = sentence_with_blank.replace(\\\"_ç©ºç™½_\\\", predicted_word)', 'api_arguments': {'model': 'cl-tohoku/bert-base-japanese-whole-word-masking'}, 'python_environment_requirements': {'transformers': 'latest'}, 'example_code': \\\"fill_mask('[MASK]')\\\", 'performance': {'dataset': 'Japanese Wikipedia', 'accuracy': 'Not provided'}, 'description': 'This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization. Additionally, the model is trained with the whole word masking enabled for the masked language modeling (MLM) objective.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ ØªÙ†Ø¨Ø¤ ØªØµÙ†ÙŠÙ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù†ØµÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. To achieve this, we import the 'pipeline' function from the transformers library provided by Hugging Face.\\\\n2. We use the 'pipeline' function to create a text-classification model that can predict sentiments and emotions from the text.\\\\n3. We specify the model 'joeddav/distilbert-base-uncased-go-emotions-student' for loading. This model is distilled from the zero-shot classification pipeline on the GoEmotions dataset, which consists of text data annotated with various emotions.\\\\n4. The created model can be used to classify the text into various emotions like happiness, sadness, anger, etc.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nnlp = pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')\\\\nresult = nlp('I am so happy today!')\\\", 'api_call': \\\"pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'joeddav/distilbert-base-uncased-go-emotions-student', 'api_call': \\\"pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')\\\", 'api_arguments': 'text', 'python_environment_requirements': [['transformers', 'torch', 'tensorflow']], 'example_code': \\\"from transformers import pipeline\\\\nnlp = pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')\\\\nresult = nlp('I am so happy today!')\\\", 'performance': {'dataset': 'go_emotions'}, 'description': 'This model is distilled from the zero-shot classification pipeline on the unlabeled GoEmotions dataset. It is primarily intended as a demo of how an expensive NLI-based zero-shot model can be distilled to a more efficient student, allowing a classifier to be trained with only unlabeled data.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ ÙˆÙ†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø© ÙˆØ§Ù„Ø¯Ù‚Ø© Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ø§ØªØ¬.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary libraries, including joblib, pandas and open the config.json file to get the features that the trained model is expecting.\\\\n2. Load the 'model.joblib' binary classification model. This model has been trained on the IMDb sentiment analysis dataset, so it's capable of predicting sentiment (positive or negative) for movie reviews.\\\\n3. Read movie reviews using pandas, from a data.csv file, and preprocess data according to the config.json file.\\\\n4. Transform column names to match expected format in the model and predict sentiment scores using the loaded model.\\\\n5. The predictions will return a binary sentiment result (positive or negative) for each movie review in the dataset.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nimport json\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\n\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\n\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'desertdev/autotrain-imdb-sentiment-analysis-44994113085', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'desertdev/autotrain-data-imdb-sentiment-analysis', 'accuracy': 0.565}, 'description': 'A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØµÙˆØªÙŠ Ù‚ØµÙŠØ± ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ ØµÙˆØªÙŠ Ù„Ø¹Ù…Ù„ÙŠØ© ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Token Classification', 'api_call': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-ws')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': 'from transformers import BertTokenizerFast, AutoModel\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\\\'bert-base-chinese\\\\')\\\\nmodel = AutoModel.from_pretrained(\\\\'ckiplab/bert-base-chinese-ws\\\\')\\\\nchinese_text = \\\"è¿™æ˜¯ä¸€ç¯‡ä¸­æ–‡æ–‡ç« ã€‚\\\"\\\\ntokenized_input = tokenizer(chinese_text, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**tokenized_input)', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Transformers', 'functionality': 'Token Classification', 'api_name': 'ckiplab/bert-base-chinese-ws', 'api_call': 'from transformers import BertTokenizerFast, AutoModel\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\\\'bert-base-chinese\\\\')\\\\nmodel = AutoModel.from_pretrained(\\\\'ckiplab/bert-base-chinese-ws\\\\')\\\\nchinese_text = \\\"è¿™æ˜¯ä¸€ç¯‡ä¸­æ–‡æ–‡ç« ã€‚\\\"\\\\ntokenized_input = tokenizer(chinese_text, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**tokenized_input)', 'api_arguments': {'pretrained_model': 'ckiplab/bert-base-chinese-ws'}, 'python_environment_requirements': {'transformers': 'BertTokenizerFast, AutoModel'}, 'example_code': \\\"from transformers import (\\\\n BertTokenizerFast,\\\\n AutoModel,\\\\n)\\\\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\\\\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-ws')\\\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ: Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù…. ØªØ£ÙƒØ¯ Ù…Ù† ØªÙˆÙÙŠØ± Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¶Ø¨Ø· Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØª ÙˆØ§Ù„Ù„ÙƒÙ†Ø© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-es-css10')\\\", '<<<api_provider>>>': 'Fairseq', '<<<explanation>>>': \\\"1. Import the necessary modules from fairseq and IPython.display.\\\\n2. Use the load_model_ensemble_and_task_from_hf_hub function to load the 'facebook/tts_transformer-es-css10' model. This is a Spanish male voice text-to-speech model trained on the CSS10 dataset.\\\\n3. Set up the TTSHubInterface with the model configuration and task data configuration.\\\\n4. Create a task-specific generator using the task object and model configuration.\\\\n5. In order to generate speech from text, simply pass the text to the TTSHubInterface methods and receive a waveform and sampling rate as the output.\\\\n6. Use IPython.display to play the generated audio.\\\", '<<<code>>>': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\'facebook/tts_transformer-es-css10\\\\', arg_overrides={\\\\'vocoder\\\\': \\\\'hifigan\\\\', \\\\'fp16\\\\': False})\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\ntext = \\\"Hola, esta es una prueba.\\\"\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-es-css10')\\\", 'provider': 'Fairseq', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/tts_transformer-es-css10', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-es-css10')\\\", 'api_arguments': {'arg_overrides': {'vocoder': 'hifigan', 'fp16': False}}, 'python_environment_requirements': [['fairseq', 'IPython']], 'example_code': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n facebook/tts_transformer-es-css10,\\\\n arg_overrides={vocoder: hifigan, fp16: False}\\\\n)\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\ntext = Hola, esta es una prueba.\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)', 'performance': {'dataset': 'CSS10', 'accuracy': None}, 'description': 'Transformer text-to-speech model from fairseq S^2. Spanish single-speaker male voice trained on CSS10.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ØªØ±Ø¬Ù…Ø© Ù†ØµÙŠØ© Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. We import the necessary classes from the transformers package. This includes AutoModelForSeq2SeqLM for the translation model and AutoTokenizer for tokenizing inputs.\\\\n2. We first load the Helsinki-NLP/opus-mt-en-de model by using the from_pretrained method of the AutoModelForSeq2SeqLM class. This model has been trained on a large corpus of text and can translate English text to German.\\\\n3. We create an instance of the AutoTokenizer, specifying the same Hugging Face model.\\\\n4. We use the tokenizer to convert the input English text into a format suitable for the model, and then use the model to generate the translated German text.\\\\n5. Finally, we tokenize and reconstruct the translated German text to make it human-readable.', '<<<code>>>': 'from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-de\\\\')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-de\\\\')\\\\ninput_text = \\\"response in English\\\"\\\\ninput_tokenized = tokenizer.prepare_seq2seq_batch(src_texts=[input_text], return_tensors=\\\\'pt\\\\')\\\\noutput_tokenized = model.generate(**input_tokenized)\\\\ntranslated_text = tokenizer.batch_decode(output_tokenized, skip_special_tokens=True)[0]', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-en-de', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'Helsinki-NLP/opus-mt-en-de'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'performance': {'dataset': 'newstest2018-ende.en.de', 'accuracy': {'BLEU': 45.2, 'chr-F': 0.69}}, 'description': 'The Helsinki-NLP/opus-mt-en-de model is a translation model developed by the Language Technology Research Group at the University of Helsinki. It translates English text to German using the Hugging Face Transformers library. The model is trained on the OPUS dataset and has a BLEU score of 45.2 on the newstest2018-ende.en.de dataset.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ­Ø¯ÙŠØ¯ Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØª ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù„ØªÙ†ÙÙŠØ° Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ©ØŸ? [/INST]{'domain': 'Audio Voice Activity Detection', 'api_call': \\\"pipeline('voice-activity-detection', model='Eklavya/ZFF_VAD')\\\", 'api_provider': 'Hugging Face', 'explanation': \\\"1. First, you will need to import the 'pipeline' function from the transformers package provided by Hugging Face.\\\", 'code': \\\"from transformers import pipeline\\\\nvad = pipeline('voice-activity-detection', model='Eklavya/ZFF_VAD')\\\\nspeech_segments = vad(audio_file_path)\\\", 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face', 'functionality': 'Voice Activity Detection', 'api_name': 'Eklavya/ZFF_VAD', 'api_call': \\\"from transformers import pipeline\\\\nvad = pipeline('voice-activity-detection', model='Eklavya/ZFF_VAD')\\\\nspeech_segments = vad(audio_file_path)\\\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers, torch', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A Voice Activity Detection model by Eklavya, using the Hugging Face framework.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ø®Ø§Øµ Ø¨Ø¹Ù…ÙŠÙ„Ù†Ø§ Ø§Ù„Ø°ÙŠ ÙŠØ¹Ù…Ù„ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_en_to_es', model='Helsinki-NLP/opus-mt-en-es')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library.\\\\n2. Use the pipeline function to create a translation model taking 'translation_en_to_es' as the task and the model 'Helsinki-NLP/opus-mt-en-es' for the translation.\\\\n3. The created translation model can be used to translate your website's text from English to Spanish using the translation function of the pipeline.\\\\n4. The translated text can then be incorporated into your website for a bilingual experience.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_en_to_es\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-es\\\\')\\\\nwebsite_text = \\\"Welcome to our travel company! We offer amazing packages and experiences.\\\"\\\\ntranslated_text = translator(website_text)', 'api_call': \\\"pipeline('translation_en_to_es', model='Helsinki-NLP/opus-mt-en-es')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-en-es', 'api_call': \\\"pipeline('translation_en_to_es', model='Helsinki-NLP/opus-mt-en-es')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_es', model='Helsinki-NLP/opus-mt-en-es')\\\\ntranslated_text = translation('Hello, how are you?')\\\", 'performance': {'dataset': 'Tatoeba-test.eng.spa', 'accuracy': 54.9}, 'description': 'This model is a translation model from English to Spanish using the Hugging Face Transformers library. It is based on the Marian framework and trained on the OPUS dataset. The model achieves a BLEU score of 54.9 on the Tatoeba test set.' </s>\"\n"}
{"text": "<s>[INST]Ø£ÙƒØªØ¨ Ø¬Ù…Ù„Ø© ØªØ³ØªØ®Ø¯Ù… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø¹Ø¨Ø§Ø±Ø© ØªÙˆØ¬ÙŠÙ‡ÙŠØ© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='gpt2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class and function from the transformers package. This includes the pipeline function for creating a text-generation model.\\\\n2. We then create a text-generation model using the pipeline function and specifying the model as 'gpt2'.\\\\n3. We can now use the generated model to create ten sentences about climate change and its impact on the environment.\\\", '<<<code>>>': 'from transformers import pipeline, set_seed\\\\n\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'gpt2\\\\')\\\\nset_seed(42)\\\\ngenerated_text = generator(\\\\n    \\\"Climate change affects the environment because\\\",\\\\n    max_length=50,\\\\n    num_return_sequences=10,\\\\n)\\\\n\\\\n# Extract sentences from the generated text\\\\nsentences = [result[\\\\'generated_text\\\\'] for result in generated_text]\\\\nfor i, sentence in enumerate(sentences, start=1):\\\\n    print(f\\\"{i}. {sentence}\\\")', 'api_call': \\\"pipeline('text-generation', model='gpt2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'gpt2', 'api_call': \\\"pipeline('text-generation', model='gpt2')\\\", 'api_arguments': ['text', 'max_length', 'num_return_sequences'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline, set_seed\\\\ngenerator = pipeline('text-generation', model='gpt2')\\\\nset_seed(42)\\\\ngenerator(Hello, I'm a language model,, max_length=30, num_return_sequences=5)\\\", 'performance': {'dataset': {'LAMBADA': {'accuracy': '35.13'}, 'CBT-CN': {'accuracy': '45.99'}, 'CBT-NE': {'accuracy': '87.65'}, 'WikiText2': {'accuracy': '83.4'}, 'PTB': {'accuracy': '29.41'}, 'enwiki8': {'accuracy': '65.85'}, 'text8': {'accuracy': '1.16'}, 'WikiText103': {'accuracy': '1.17'}, '1BW': {'accuracy': '37.50'}}}, 'description': 'GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø£ÙˆÙ„Ø§Ù‹ ÙÙ‡Ù… Ø¹Ù„Ù… Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ÙˆÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± ÙˆØ§Ù‚Ø¹ÙŠØ© Ø¨Ø¯ÙˆÙ† Ø´Ø±ÙˆØ· Ù„Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Image-to-Image Ù…Ù† Ø£Ø¬Ù„ ØªØ­Ù‚ÙŠÙ‚ Ù‡Ø°Ø§ Ø§Ù„Ù‡Ø¯Ù. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙØ±ÙŠ Ù„ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© ÙˆØ¯Ù‚Ø© Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"!pip install diffusers\\\\nfrom diffusers import DDPMPipeline\\\\nddpm = DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\\ngenerated_bedroom_image = ddpm().images[0]\\\\ngenerated_bedroom_image.save('virtual_bedroom_image.png')\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-bedroom-256', 'api_call': \\\"!pip install diffusers\\\\nfrom diffusers import DDPMPipeline\\\\nddpm = DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\\ngenerated_bedroom_image = ddpm().images[0]\\\\ngenerated_bedroom_image.save('virtual_bedroom_image.png')\\\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ø¹ØµØ±ÙŠ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù†Ø³Ø® Ø¥Ù„Ù‰ Ø§Ù„Ù†ØµØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ù†Ø§Ø³Ø¨Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¯Ø§Ø®Ù„Ø© ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙØ± Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"CrossEncoder('cross-encoder/nli-deberta-v3-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the CrossEncoder class from the sentence_transformers package.\\\\n2. Create a model instance using: model = CrossEncoder('cross-encoder/nli-deberta-v3-base'), loading the mentioned model, which is trained on natural language inference tasks using the SNLI and MultiNLI datasets.\\\\n3. To infer the relationship between two sentences, provide the pair of sentences as input to the predict() method of the model in the form of a tuple.\\\\n4. The model will return an array of scores corresponding to three labels: contradiction, entailment, and neutral. The highest score indicates the relationship between the two sentences.\\\", '<<<code>>>': 'from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder(\\\\'cross-encoder/nli-deberta-v3-base\\\\')\\\\nsentence_pair = (\\\"A man is eating pizza\\\", \\\"A man eats something\\\")\\\\nscores = model.predict([sentence_pair])', 'api_call': \\\"CrossEncoder('cross-encoder/nli-deberta-v3-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cross-encoder/nli-deberta-v3-base', 'api_call': \\\"CrossEncoder('cross-encoder/nli-deberta-v3-base')\\\", 'api_arguments': ['sentence_pairs'], 'python_environment_requirements': [['sentence_transformers', 'transformers']], 'example_code': \\\"from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-base')\\\\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])\\\", 'performance': {'dataset': {'SNLI-test': '92.38', 'MNLI mismatched set': '90.04'}}, 'description': 'This model is based on microsoft/deberta-v3-base and was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ù…Ø¯Ù‰ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, install the sentence-transformers library using 'pip install -U sentence-transformers'.\\\\n2. Import the SentenceTransformer class from the sentence_transformers library.\\\\n3. Load the pre-trained model 'sentence-transformers/all-MiniLM-L12-v2' using the SentenceTransformer class.\\\\n4. Extract lines of dialogue from movie characters and store them as a list of sentences.\\\\n5. Encode the sentences using the model's 'encode' method, which will generate embeddings for each sentence.\\\\n6. Compare the embeddings using similarity measures like cosine similarity to analyze the relationship between characters based on their lines.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\n\\\\n# Example character lines\\\\ncharacter1_lines = [\\\"We have to do something!\\\", \\\"I think we should split up.\\\"]\\\\ncharacter2_lines = [\\\"We need a plan.\\\", \\\"It might be safer if we split up.\\\"]\\\\n\\\\n# Load the pre-trained model\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-MiniLM-L12-v2\\\\')\\\\n\\\\n# Encode the sentences\\\\ncharacter1_embeddings = model.encode(character1_lines)\\\\ncharacter2_embeddings = model.encode(character2_lines)\\\\n\\\\n# Calculate the similarity between character lines\\\\nsimilarity_matrix = cosine_similarity(character1_embeddings, character2_embeddings)\\\\nprint(similarity_matrix)', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/all-MiniLM-L12-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': '1,170,060,424 training pairs', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø§Ù„ØªØ·ÙˆÙŠØ± ÙÙŠ Ù…Ø¬Ø§Ù„ ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ Ø­ÙŠØ« Ù†Ù‡Ø¯Ù Ù„ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§ØªØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠØ³ØªØ·ÙŠØ¹ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø«Ù„Ø§Ø«ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ø±Ø¦ÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø§Ù„ØªÙŠ Ù†Ù…Ù„ÙƒÙ‡Ø§.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required AutoModel class from the transformers package provided by Hugging Face.\\\\n2. Use the from_pretrained method of the AutoModel class to load the pre-trained model 'sayakpaul/glpn-nyu-finetuned-diode-221116-110652'. This model is specifically fine-tuned for depth estimation tasks, making it ideal for estimating the depth of objects in an image of a room.\\\\n3. Pass an input image into the model for inference. The model will return the depth estimation map corresponding to the input image. This map can be useful in understanding the depth and distance between objects in the room.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nimport torch\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\\\\ninput_image = your_preprocessed_image_tensor  # Preprocess the input image to match the model's input format\\\\nimage_depth_map = model(torch.unsqueeze(input_image, 0)).squeeze()  # Add a batch dimension to the image tensor and remove it from the output\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221116-110652', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4018, 'Mae': 0.3272, 'Rmse': 0.4546, 'Abs Rel': 0.3934, 'Log Mae': 0.138, 'Log Rmse': 0.1907, 'Delta1': 0.4598, 'Delta2': 0.7659, 'Delta3': 0.9082}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>\"\n"}
{"text": "<s>[INST]Ù„ØªÙ†ÙÙŠØ° ÙƒØ´Ù Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØª ÙÙŠ Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ©. Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø±ÙØ© ÙˆØªÙ‚ÙŠÙŠÙ… Ù…Ø¯Ù‰ ØªÙˆØ§Ø¬Ø¯ Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØªÙŠ. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© ÙƒØ´Ù Ø§Ù„Ù†Ø´Ø§Ø· ÙˆØªØ­Ø¯ÙŠØ¯ ÙØªØ±Ø§Øª Ø§Ù„ØµÙ…Øª Ø¨Ø¯Ù‚Ø© Ø£Ø¹Ù„Ù‰. ÙƒÙ…Ø§ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù…Ø· Ø§Ù„ØµÙˆØªÙŠ Ù„Ø²ÙŠØ§Ø¯Ø© ÙƒÙØ§Ø¡Ø© ÙƒØ´Ù Ø§Ù„Ù†Ø´Ø§Ø· Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„.? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the Model class from the pyannote.audio library.\\\\n2. Use the Model.from_pretrained() method to load the pre-trained model 'popcornell/pyannote-segmentation-chime6-mixer6'. This model is fine-tuned for voice activity detection and can distinguish between noise and human voice in audio recordings.\\\\n3. The loaded model can then be used to analyze a meeting recording file to detect segments with human voice and separate them from the noise.\\\\n4. Using this information, it's possible to create an application that filters out noise and only focuses on the valuable content of the human voice.\\\", '<<<code>>>': \\\"from pyannote.audio import Model\\\\nvoice_detection_model = Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\\nsegmentation_result = voice_detection_model.apply_to(audio_file_path)\\\", 'api_call': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'popcornell/pyannote-segmentation-chime6-mixer6', 'api_call': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['pyannote.audio 2.1.1'], 'example_code': 'from pyannote.audio import Model\\\\nmodel = Model.from_pretrained(popcornell/pyannote-segmentation-chime6-mixer6)', 'performance': {'dataset': 'ami', 'accuracy': 'N/A'}, 'description': 'Pyannote Segmentation model fine-tuned on data from CHiME-7 DASR Challenge. Used to perform diarization in the CHiME-7 DASR diarization baseline.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ ØµÙˆØ±Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· ØªØµÙˆØ± Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ø¬Ù…Ù„Ø© \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required 'pipeline' function from the transformers library.\\\\n2. Initialize the model and tokenizer using the 'pipeline' function, specifying the model and tokenizer with the identifier 'seungwon12/layoutlmv2-base-uncased_finetuned_docvqa' and the task type as 'question-answering'.\\\\n3. Once initialized, you can provide the context (a document in text form) and the question to the model, and it will return the answer to the question.\\\\n4. Make sure that the document text and questions are appropriately formatted and in plain text to achieve better results.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\'question-answering\\\\', model=\\\\'seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\\\\', tokenizer=\\\\'seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\\\\')\\\\ndocument = \\\"\\\"\\\"Put your document text here.\\\"\\\"\\\"\\\\nquestion = \\\"Your question here\\\"\\\\nanswer = qa_pipeline({\\\\'context\\\\': document, \\\\'question\\\\': question})\\\\nprint(answer)', 'api_call': \\\"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \\\"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'A document question answering model finetuned on the DocVQA dataset using LayoutLMv2-base-uncased.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ù…ØªØ·ÙˆØ±Ø© ÙÙŠ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù†Ø±ÙŠØ¯ Ù…Ø¹Ø±ÙØ© ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø­Ø´Ùˆ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Initialize the pipeline with 'fill-mask', 'camembert-base' model, and 'camembert-base' tokenizer. These specifications are designed to work with French text and the model focuses on filling in the missing words in the given sentences.\\\\n3. Pass the sentence containing the masked word '<mask>' as input to the model, and it will return the completed sentence with the most suitable word in place of the mask.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ncamembert_fill_mask = pipeline(\\\\'fill-mask\\\\', model=\\\\'camembert-base\\\\', tokenizer=\\\\'camembert-base\\\\')\\\\nresults = camembert_fill_mask(\\\"Le camembert est <mask> :)\\\")', 'api_call': \\\"pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'camembert-base', 'api_call': \\\"pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')\\\", 'api_arguments': ['model', 'tokenizer'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline; camembert_fill_mask = pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base'); results = camembert_fill_mask('Le camembert est <mask> :)')\\\", 'performance': {'dataset': 'oscar', 'accuracy': 'N/A'}, 'description': 'CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. It is available on Hugging Face in 6 different versions with varying number of parameters, amount of pretraining data, and pretraining data source domains. It can be used for Fill-Mask tasks.' </s>\"\n"}
{"text": "<s>[INST]Ù„ØªØ­Ø³ÙŠÙ† Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµØŒ Ø£Ù‚ØªØ±Ø­ ØªÙƒÙˆÙŠÙ† Ù†Ø§Ø¯Ù Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ Ø¨Ø§Ù†ØªØ¸Ø§Ù… Ù„Ù…Ù†Ø§Ù‚Ø´Ø© Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª ÙˆØ­Ù„Ù‡Ø§. ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆÙÙŠØ± Ø£ÙØ¶Ù„ Ø§Ù„Ø£Ø¯ÙˆØ§Øª ÙˆØ§Ù„Ù…ÙˆØ§Ø±Ø¯ Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„ÙŠÙ‚ÙˆÙ…ÙˆØ§ Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ù„ØºÙˆÙŠØ© ÙˆØªØ·Ø¨ÙŠÙ‚Ù‡Ø§ Ø¹Ù„Ù‰ ÙˆÙƒÙ„Ø§Ø¡ ØªØ¹Ù„Ù… Ø¢Ù„ÙŠ.? [/INST]{'domain': 'Natural Language Processing Text Classification', 'api_call': \\\"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We import the pipeline function from the transformers library by Hugging Face.', 'code': '', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'sentiment_analysis_generic_dataset', 'api_call': '', 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': \\\"sentiment_analysis('I love this product!')\\\", 'performance': {'dataset': 'generic_dataset', 'accuracy': 'Not specified'}, 'description': 'This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text classification.'}} </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØ·ÙˆÙŠØ± ÙˆØªØ­Ø³ÙŠÙ† ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø®Ø§Øµ Ø¨ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoClassificationPipeline(model='hf-tiny-model-private/tiny-random-VideoMAEForVideoClassification')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary components from the 'transformers' library, which includes 'VideoClassificationPipeline'.\\\\n2. Instantiate a video classification pipeline using the 'hf-tiny-model-private/tiny-random-VideoMAEForVideoClassification' model. This is a small model designed for video classification tasks.\\\\n3. Use the created pipeline to classify short meeting videos into three categories: 'productive', 'unproductive', or 'neutral'.\\\", '<<<code>>>': \\\"from transformers import VideoClassificationPipeline\\\\nvideo_classifier = VideoClassificationPipeline(model='hf-tiny-model-private/tiny-random-VideoMAEForVideoClassification')\\\\nclassification_result = video_classifier(video_input, candidate_labels=['productive', 'unproductive', 'neutral'])\\\", 'api_call': \\\"VideoClassificationPipeline(model='hf-tiny-model-private/tiny-random-VideoMAEForVideoClassification')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tiny-random-VideoMAEForVideoClassification', 'api_call': \\\"VideoClassificationPipeline(model='hf-tiny-model-private/tiny-random-VideoMAEForVideoClassification')\\\", 'api_arguments': 'model', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random VideoMAE model for video classification.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª (API) ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© (Natural Language Processing) Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆÙ…Ù„Ø¦ Ø§Ù„ÙØ¬ÙˆØ§Øª (Fill-Mask). Ù‚Ù… Ø¨ØªÙˆÙÙŠØ± Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙØ¬ÙˆØ§Øª Ø§Ù„ØªÙŠ ØªØ±ØºØ¨ ÙÙŠ Ù…Ù„Ø¦Ù‡Ø§ØŒ ÙˆØ³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨ØªØ­Ù„ÙŠÙ„Ù‡Ø§ ÙˆØ¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØªÙ‡Ø§ Ø¨Ù…Ù„Ø¦ Ø§Ù„ÙØ¬ÙˆØ§Øª.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is `pipeline` from transformers.\\\\n2. Use the `pipeline` function from transformers to load the model, which in this case is 'bert-base-cased'.\\\\n3. The loaded model will be used for completing the masked sentence, filling in the word at the position of the mask token `[MASK]`.\\\\n4. Provide the sentence with the `[MASK]` token as input to the model, and the model will predict and return the most likely word to fill in the mask.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'bert-base-cased\\\\')\\\\ninput_text = \\\"I have always wanted to [MASK] a marathon.\\\"\\\\nresult = unmasker(input_text)', 'api_call': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-base-cased', 'api_call': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", 'api_arguments': ['model'], 'python_environment_requirements': [['transformers']], 'example_code': ['from transformers import pipeline', \\\"unmasker = pipeline('fill-mask', model='bert-base-cased')\\\", \\\"unmasker(Hello I'm a [MASK] model.)\\\"], 'performance': {'dataset': 'GLUE', 'accuracy': 79.6}, 'description': \\\"BERT base model (cased) is a pre-trained transformer model on English language using a masked language modeling (MLM) objective. It was introduced in a paper and first released in a repository. This model is case-sensitive, which means it can differentiate between 'english' and 'English'. The model can be used for masked language modeling or next sentence prediction, but it's mainly intended to be fine-tuned on a downstream task.\\\" </s>\"\n"}
{"text": "<s>[INST]ÙŠØ±Ø¬Ù‰ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„ØªÙŠ ØªØ±ØºØ¨ ÙÙŠ ØªØµÙ†ÙŠÙÙ‡Ø§ Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ØŒ Ø«Ù… Ø§Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Classification Zero-Shot Ù…Ù† Ø­ÙˆØ§Ø³Ø¨ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the pipeline function from the transformers library.\\\\n2. Create a pipeline object for zero-shot image classification by specifying the model 'Bingsu/clip-vit-base-patch32-ko'. This is a pre-trained model designed to recognize objects in images without prior knowledge of the specific classes.\\\\n3. Provide the image URL and a list of candidate labels (e.g. names of Korean foods or drinks) to the pipeline. The candidate_labels parameter should contain an array of strings representing the potential food or drink categories.\\\\n4. Run the pipeline, and the model will return the most likely category for the image based on the provided candidate labels.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nurl = 'https://example.com/korean_food_image.jpg'\\\\n# replace with a valid image URL containing a Korean food or drink\\\\ncandidate_labels = ['kimchi', 'bulgogi', 'bibimbap', 'soju', 'makgeolli']\\\\npipe = pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\\\\nresult = pipe(images=url, candidate_labels=candidate_labels)\\\", 'api_call': \\\"pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'clip-vit-base-patch32-ko', 'api_call': \\\"pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\\\", 'api_arguments': {'images': 'url', 'candidate_labels': 'Array of strings', 'hypothesis_template': 'String'}, 'python_environment_requirements': [['transformers', 'torch', 'PIL', 'requests']], 'example_code': \\\"from transformers import pipeline\\\\nrepo = 'Bingsu/clip-vit-base-patch32-ko'\\\\npipe = pipeline('zero-shot-image-classification', model=repo)\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nresult = pipe(images=url, candidate_labels=[], hypothesis_template='{}')\\\\nresult\\\", 'performance': {'dataset': 'AIHUB', 'accuracy': 'Not provided'}, 'description': 'Korean CLIP model trained by Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. It is a zero-shot image classification model that can be used to classify images without any training data.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†ØµÙˆØµ Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø±Ù‹Ø§ Ø¹Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø¯Ø®Ù„Ø© Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø© ØªØ¬Ø¹Ù„Ù‡Ø§ Ø¨ÙŠÙ†ØªØ§ØºØ§Ù…Ù…ÙŠØ© ÙˆØ³Ù„Ø³Ø©. Ø¹Ù„ÙŠÙƒ Ø£Ù† ØªØ¶Ù…Ù† Ø£Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙÙ†ØªÙØ¬Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Ø·Ù‚ ØµØ­ÙŠØ­ Ù…Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø¬ÙŠÙ„ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù†ØµÙˆØµ ÙˆØ°Ù„Ùƒ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ù‡Ø¯Ø§Ù ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚Ø§Øª ØªÙ‚Ù†ÙŠØ© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"XLNetModel.from_pretrained('xlnet-base-cased')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the XLNetTokenizer and XLNetModel classes from the transformers library.\\\\n2. Load the pre-trained 'xlnet-base-cased' model, which is capable of generating coherent text given a context.\\\\n3. Tokenize the text from the news article about floods using the tokenizer's from_pretrained method.\\\\n4. Use the model to generate a summary of the input text by passing the tokenized input to the model.\\\\n5. Format and output the generated summary.\\\", '<<<code>>>': 'from transformers import XLNetTokenizer, XLNetModel\\\\narticle_text = \\\"News article about floods here...\\\"\\\\ntokenizer = XLNetTokenizer.from_pretrained(\\\\'xlnet-base-cased\\\\')\\\\nmodel = XLNetModel.from_pretrained(\\\\'xlnet-base-cased\\\\')\\\\ninputs = tokenizer(article_text, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nlast_hidden_states = outputs.last_hidden_state\\\\n# Post-processing to obtain the summary from the last_hidden_state', 'api_call': \\\"XLNetModel.from_pretrained('xlnet-base-cased')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'xlnet-base-cased', 'api_call': \\\"XLNetModel.from_pretrained('xlnet-base-cased')\\\", 'api_arguments': {'pretrained_model_name': 'xlnet-base-cased'}, 'python_environment_requirements': [{'library': 'transformers', 'version': '4.0.0+'}], 'example_code': \\\"from transformers import XLNetTokenizer, XLNetModel\\\\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\\\\nmodel = XLNetModel.from_pretrained('xlnet-base-cased')\\\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlast_hidden_states = outputs.last_hidden_state\\\", 'performance': {'dataset': 'bookcorpus, wikipedia', 'accuracy': 'state-of-the-art (SOTA) results on various downstream language tasks'}, 'description': 'XLNet model pre-trained on English language. It was introduced in the paper XLNet: Generalized Autoregressive Pretraining for Language Understanding by Yang et al. and first released in this repository. XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± ØºÙŠØ± Ø§Ù„Ù…Ø±Ø¦ÙŠØ© ÙÙŠ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ø¯ÙˆÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ Ø¨Ø¯Ù‚Ø© ÙˆÙØ¹Ø§Ù„ÙŠØ© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create an image classification model and specify the model to be used: 'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup'.\\\\n3. This model is trained for zero-shot image classification and can be used to classify images without any prior knowledge of the exact category.\\\\n4. To categorize the mushrooms in the given image, provide the image file path and the categories (edible, poisonous) to the classifier.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')\\\\nmushroom_classification = image_classifier(image_path, class_names='edible, poisonous')\\\", 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup', 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')\\\", 'api_arguments': 'image_path, class_names', 'python_environment_requirements': ['transformers'], 'example_code': \\\"results = model(image_path, class_names='cat, dog, bird')\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '76.9'}, 'description': 'A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Large model (convnext_large) as the image tower, a MLP (fc - gelu - drop - fc) head in vision tower instead of the single projection of other CLIP models, and a text tower with same width but 4 layers more depth than ViT-L / RN50x16 models (depth 16, embed dim 768).' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙØ¹ÙØ¯Ù‘Ù Ø§Ù„ØªÙ‘ÙØµÙ’Ù†ÙÙŠÙÙ Ø§Ù„ØµÙ‘ÙÙÙ’Ø±ÙÙŠ Ù„ÙÙ…ÙØ¹ÙØ§Ù„ÙØ¬ÙØ©Ù Ø§Ù„Ù„Ù‘ÙØºÙØ©Ù Ø§Ù„Ø·Ù‘ÙØ¨ÙÙŠØ¹ÙÙŠÙ‘ÙØ©Ù Ù…ÙÙ† Ù…ÙØ¬ÙØ§Ù„Ù ØªÙØ·Ù’ÙˆÙÙŠØ±Ù ØªÙÙ‚Ù’Ù†ÙÙŠÙ‘ÙØ§ØªÙ Ø§Ù„Ù’ØªÙØ¹ÙÙ„Ù‘ÙÙ…Ù Ø§Ù„Ø°Ù‘ÙÙƒÙÙŠÙ‘Ù Ø­ÙÙŠÙÙˆÙÙŠÙ‹Ø§. Ù†ÙÙ‚ÙÙˆÙ…Ù Ø¨ÙØªÙØ·Ù’Ø¨ÙÙŠÙ‚Ù Ù†ÙÙ…ÙØ°ÙØ¬ÙÙ†ÙØ§ Ø¹ÙÙ„ÙÙ‰ Ø§Ù„Ù’Ø¨ÙÙŠÙØ§Ù†ÙØ§ØªÙ Ù…ÙÙ† Ù…ÙØ¬ÙØ§Ù„ÙØ§ØªÙ Ù…ÙØªÙÙ†ÙÙˆÙ‘ÙØ¹ÙØ©Ù Ø¯ÙÙˆÙ†Ù Ø§Ù„Ù’Ø­ÙØ§Ø¬ÙØ©Ù Ø¥ÙÙ„ÙÙ‰ ØªÙØµÙ’Ù†ÙÙŠÙÙ Ù…ÙØ³ÙØ¨Ù‘ÙÙ‚Ù Ø£ÙÙˆ ØªÙØ¯Ù’Ø±ÙÙŠØ¨Ù.? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"YOLO('keremberke/yolov8s-pothole-segmentation')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary libraries from ultralyticsplus, which are YOLO and render_result.', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'URL or local path to the image'\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pothole-segmentation', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'URL or local path to the image'\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.928, 'mAP@0.5(mask)': 0.928}}, 'description': 'A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ ØªØµÙ…ÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§Øµ Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆØ°Ù„Ùƒ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ø¯ÙŠØ¯ ØªØµÙ†ÙŠÙ ÙƒÙ„ Ø±Ù…Ø² Ø¶Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­. Ø¹Ù„ÙŠÙƒ ØªÙˆØ¶ÙŠØ­ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙˆØ¶Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙƒÙˆÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙˆØ«ÙˆÙ‚Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"SequenceTagger.load('flair/ner-english-ontonotes-large')\\\", '<<<api_provider>>>': 'Flair', '<<<explanation>>>': \\\"1. Import the SequenceTagger model and Sentence class from the Flair library.\\\\n2. To extract named entities such as names, dates, and other relevant information, load the Flair pre-trained model, 'flair/ner-english-ontonotes-large'.\\\\n3. Create a Sentence object containing a user message.\\\\n4. Use the SequenceTagger.predict() method on the message to predict named entities.\\\\n5. The predicted entities can be accessed by iterating over the named entity spans in the Sentence object.\\\", '<<<code>>>': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\ntagger = SequenceTagger.load('flair/ner-english-ontonotes-large')\\\\nmessage = Sentence('I need support for my order number 12345 shipped on May 5th, 2021.')\\\\n\\\\ntagger.predict(message)\\\\n\\\\nentities = {}\\\\nfor entity in message.get_spans('ner'):\\\\n    entities[entity.tag] = entity.text\\\\n\\\\nprint(entities)\\\", 'api_call': \\\"SequenceTagger.load('flair/ner-english-ontonotes-large')\\\", 'provider': 'Flair', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'flair/ner-english-ontonotes-large', 'api_call': \\\"SequenceTagger.load('flair/ner-english-ontonotes-large')\\\", 'api_arguments': ['sentence'], 'python_environment_requirements': [['flair']], 'example_code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\n# load tagger\\\\ntagger = SequenceTagger.load('flair/ner-english-ontonotes-large')\\\\n\\\\n# make example sentence\\\\nsentence = Sentence('On September 1st George won 1 dollar while watching Game of Thrones.')\\\\n\\\\n# predict NER tags\\\\ntagger.predict(sentence)\\\\n\\\\n# print sentence\\\\nprint(sentence)\\\\n\\\\n# print predicted NER spans\\\\nprint('The following NER tags are found:')\\\\n\\\\n# iterate over entities and print\\\\nfor entity in sentence.get_spans('ner'):\\\\n    print(entity)\\\", 'performance': {'dataset': 'Ontonotes', 'accuracy': 90.93}, 'description': 'English NER in Flair (Ontonotes large model). This is the large 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on document-level XLM-R embeddings and FLERT.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ù…Ù„Ø£ Ø§Ù„ÙØ±Ø§Øº ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Natural Language Processing: \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©ØŒ ØªÙ…Ø± Ø¹ØµØ§ÙÙŠØ± Ø§Ù„Ù‰ Ø§Ù„Ø³Ù…Ø§Ø¡ Ø¨Ù‡Ø¯ÙˆØ¡ ÙˆØ¬Ù…Ø§Ù„\\\". Ø§Ø®ØªØ± Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø§Ù„Ù†Øµ.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Firstly, import 'pipeline' from the transformers package.\\\\n2. Instantiate the masked language model by calling the 'pipeline' function with the 'fill-mask' task and the model name 'bert-base-cased'.\\\\n3. Pass a sentence containing a masked token [MASK] to the model as input, which will predict and return the most likely word to fill the gap.\\\\n4. This model can be used to create statements with missing words filled in based on the context.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nfill_mask = pipeline(\\\\'fill-mask\\\\', model=\\\\'bert-base-cased\\\\')\\\\ninput_sentence = \\\"The media company specializes in creating [MASK] content.\\\"\\\\nfilled_statement = fill_mask(input_sentence)', 'api_call': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-base-cased', 'api_call': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", 'api_arguments': ['model'], 'python_environment_requirements': [['transformers']], 'example_code': ['from transformers import pipeline', \\\"unmasker = pipeline('fill-mask', model='bert-base-cased')\\\", \\\"unmasker(Hello I'm a [MASK] model.)\\\"], 'performance': {'dataset': 'GLUE', 'accuracy': 79.6}, 'description': \\\"BERT base model (cased) is a pre-trained transformer model on English language using a masked language modeling (MLM) objective. It was introduced in a paper and first released in a repository. This model is case-sensitive, which means it can differentiate between 'english' and 'English'. The model can be used for masked language modeling or next sentence prediction, but it's mainly intended to be fine-tuned on a downstream task.\\\" </s>\"\n"}
{"text": "<s>[INST]Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ù‹Ø§ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø®Ù„Ø§ØµØ§Øª. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ø®Ù„Ø§ØµØ§Øª Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required libraries - BartForConditionalGeneration, and BartTokenizer from transformers.\\\\n2. Load the pre-trained 'sshleifer/distilbart-cnn-6-6' model using BartForConditionalGeneration.from_pretrained() method. The loaded model is a distilled version of BART which is fine-tuned for text summarization.\\\\n3. Load the tokenizer using the BartTokenizer.from_pretrained() method with the same model name.\\\\n4. Use the tokenizer to encode the given article text and then use the model to generate a summary.\\\\n5. Decode the generated summary using the tokenizer's decode() method.\\\", '<<<code>>>': 'from transformers import BartForConditionalGeneration, BartTokenizer\\\\nmodel = BartForConditionalGeneration.from_pretrained(\\\\'sshleifer/distilbart-cnn-6-6\\\\')\\\\ntokenizer = BartTokenizer.from_pretrained(\\\\'sshleifer/distilbart-cnn-6-6\\\\')\\\\ninputs = tokenizer.encode(\\\"Your long news article here...\\\", return_tensors=\\\\'pt\\\\', max_length=1024, truncation=True)\\\\nsummary_ids = model.generate(inputs, num_beams=4, max_length=100, early_stopping=True)\\\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sshleifer/distilbart-cnn-6-6', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': {'cnn_dailymail': {'Rouge 2': 20.17, 'Rouge-L': 29.7}, 'xsum': {'Rouge 2': 20.92, 'Rouge-L': 35.73}}}, 'description': 'DistilBART model for text summarization, trained on the CNN/Daily Mail and XSum datasets. It is a smaller and faster version of BART, suitable for summarizing English text.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ù†Ø·Ù„Ø¨ Ù…Ù† Ø¹Ù…ÙŠÙ„Ù†Ø§ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø©. Ø§Ù„Ù‡Ø¯Ù Ù‡Ùˆ Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… ÙŠØ³ØªØ·ÙŠØ¹ ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆØ§Ù‡Ø§ ÙˆÙÙ‡Ù… Ù…Ø­ØªÙˆØ§Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ø£Ø¹Ù…Ù‚. Ø¹Ù„ÙŠÙƒ Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ·ÙŠØ¹ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ù…Ø¶Ù…ÙˆÙ† Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø¯Ù‚Ø© ÙˆÙÙ‡Ù… Ù…Ø­ØªÙˆØ§Ù‡.? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries: VideoMAEFeatureExtractor and VideoMAEForPreTraining from the transformers package.', 'code': \\\"from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(16, 3, 224, 224))  # Replace with a list of frames from the actual video\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\n\\\\npixel_values = feature_extractor(video, return_tensors='pt').pixel_values\\\\n\\\\nnum_frames = 16\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss\\\\n\\\\n# Perform classification and intrusion detection by adding a linear layer on top of the pre-trained encoder\\\", 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'videomae-base-ssv2', 'api_call': \\\"from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(16, 3, 224, 224))  # Replace with a list of frames from the actual video\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\n\\\\npixel_values = feature_extractor(video, return_tensors='pt').pixel_values\\\\n\\\\nnum_frames = 16\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss\\\\n\\\\n# Perform classification and intrusion detection by adding a linear layer on top of the pre-trained encoder\\\", 'api_arguments': 'video', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\\\npixel_values = feature_extractor(video, return_tensors=pt).pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss', 'performance': {'dataset': 'Something-Something-v2', 'accuracy': ''}, 'description': 'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠ Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ©. Ø§Ø®ØªØ± Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø´Ø§Ù…Ù„Ø© ÙˆÙ…ØªÙ†ÙˆØ¹Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ø¯Ø§Ø¡ Ø¹Ø§Ù„ÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': '1. Import the required libraries joblib and pandas. Joblib is used to load the pre-trained KNN model, and pandas is used to process the input data.\\\\n2. Load the pre-trained KNN model for multi-class classification of Iris flower species using joblib.load().\\\\n3. Read the input data, which contains sepal and petal lengths and widths, into a pandas DataFrame. Ensure the column names match the expected format.\\\\n4. Use the loaded KNN model to predict the species of the Iris flowers in the input data.', '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\ndata = pd.DataFrame({'feat_sepal_length': [5.1, 6.3],\\\\n                     'feat_sepal_width': [3.5, 2.8],\\\\n                     'feat_petal_length': [1.4, 5.1],\\\\n                     'feat_petal_width': [0.2, 1.5]})\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'abhishek/autotrain-iris-knn', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'scikit-learn/iris', 'accuracy': 0.9}, 'description': 'A K-Nearest Neighbors (KNN) model trained on the Iris dataset for multi-class classification. The model is trained using AutoTrain and has an accuracy of 0.9.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø¬Ù‡Ø§Ø² Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, in this case, transformers and pipeline from the Hugging Face library.\\\\n2. Set up the conversational pipeline using the 'ingen51/DialoGPT-medium-GPT4' model. This model has been specifically trained for handling conversational tasks.\\\\n3. Pass the customer questions as input to the conversational_pipeline.\\\\n4. The model generates human-like responses to the customer questions that can be used for managing customer support inquiries.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nconversational_pipeline = pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\\\\nresponse = conversational_pipeline('Hello, how do I change my password?')\\\", 'api_call': \\\"pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ingen51/DialoGPT-medium-GPT4', 'api_call': \\\"pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\\\", 'api_arguments': ['message'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"conversational_pipeline('Hello, how are you?')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A GPT-4 model for generating conversational responses in a dialogue setting.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\" ØªØªØ­Ø¯Ø« Ø¹Ù† Ø¹Ù…Ù„ÙŠØ© ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± ÙÙŠ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers and PIL libraries. This includes SegformerForSemanticSegmentation for the semantic segmentation model, SegformerImageProcessor for processing the image, and Image for handling image data.\\\\n2. Load the required pre-trained model 'nvidia/segformer-b0-finetuned-ade-512-512', which can identify components and segment them based on their labels.\\\\n3. Create an instance of SegformerImageProcessor and SegformerForSemanticSegmentation with this pre-trained model.\\\\n4. Load the image from a file or a provided URL, and then pass the image to SegformerImageProcessor to prepare the input for the model.\\\\n5. Finally, forward the processed image through the model to obtain segmentation results.\\\", '<<<code>>>': \\\"from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nprocessor = SegformerImageProcessor.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\", 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nvidia/segformer-b0-finetuned-ade-512-512', 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\\\", 'api_arguments': {'images': 'Image', 'return_tensors': 'pt'}, 'python_environment_requirements': [{'transformers': 'SegformerImageProcessor, SegformerForSemanticSegmentation', 'PIL': 'Image', 'requests': 'requests'}], 'example_code': 'from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nprocessor = SegformerImageProcessor.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… ÙØ±ÙŠÙ‚ Ø§Ù„Ø¹Ù…Ù„ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± ÙˆØªØ¬Ø³ÙŠØ¯ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙˆØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ§Ù‚Ø¹Ù‡Ø§ØŒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ ØªÙ†ÙÙŠØ° Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©.? [/INST]{'domain': 'Computer Vision Object Detection', 'api_call': \\\"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\\\nimport torch\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'your_image_url_here'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\\n\\\\ninputs = processor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\n\\\\n# Extract object detection results from 'outputs'\\\", 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'deformable-detr', 'api_call': \\\"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\\\nimport torch\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'your_image_url_here'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\\n\\\\ninputs = processor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\n\\\\n# Extract object detection results from 'outputs'\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': \\\"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\\\nimport torch\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\\ninputs = processor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'performance': {'dataset': 'COCO 2017', 'accuracy': 'Not provided'}, 'description': 'Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]Ø§Ù„Ù…Ø·ÙˆØ±ÙˆÙ† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØµÙ…ÙŠÙ… Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠÙ†ØµØ­ Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙØªØ±Ø§Øª ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('typeform/mobilebert-uncased-mnli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes AutoTokenizer and AutoModelForSequenceClassification from transformers library by Hugging Face.\\\\n2. Initialize the tokenizer and the model by calling the from_pretrained method with the pretrained model 'typeform/mobilebert-uncased-mnli'. This is a sequence classification model specifically fine-tuned for the task of zero-shot classification.\\\\n3. Use the tokenizer to tokenize the input text (social media post) and create the input tensors required by the model.\\\\n4. Make predictions using the model by feeding it the input tensors.\\\\n5. Obtain the highest-scoring category label for the input text from the model's output, which can be one of the three categories: positive, negative, or neutral.\\\", '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\ntokenizer = AutoTokenizer.from_pretrained('typeform/mobilebert-uncased-mnli')\\\\nmodel = AutoModelForSequenceClassification.from_pretrained('typeform/mobilebert-uncased-mnli')\\\\ninput_texts = [post]  # Replace `post` with the actual post content\\\\ninputs = tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True)\\\\noutputs = model(**inputs)\\\\ncategory_ids = outputs.logits.argmax(dim=-1)\\\\ncategory_labels = ['positive', 'negative', 'neutral']\\\\ncategory = category_labels[category_ids[0].item()]\\\", 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('typeform/mobilebert-uncased-mnli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'typeform/mobilebert-uncased-mnli', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('typeform/mobilebert-uncased-mnli')\\\", 'api_arguments': {'pretrained_model': 'typeform/mobilebert-uncased-mnli'}, 'python_environment_requirements': [{'transformers': 'AutoTokenizer, AutoModelForSequenceClassification'}], 'example_code': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\ntokenizer = AutoTokenizer.from_pretrained(typeform/mobilebert-uncased-mnli)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(typeform/mobilebert-uncased-mnli)', 'performance': {'dataset': 'multi_nli', 'accuracy': 'More information needed'}, 'description': 'This model is the Multi-Genre Natural Language Inference (MNLI) fine-turned version of the uncased MobileBERT model. It can be used for the task of zero-shot classification.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ø§Ø³ØªØ´Ø¹Ø§Ø± Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØªÙŠØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ù…ØªÙ‰ ÙŠØ­Ø¯Ø« Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØªÙŠ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ©.? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To detect and monitor speakers in an audio recording, we need to use a Voice Activity Detection model.\\\\n2. Import the necessary libraries, which are 'Model' from pyannote.audio.\\\\n3. Load the pretrained 'popcornell/pyannote-segmentation-chime6-mixer6' model using the 'Model.from_pretrained' method.\\\\n4. Apply the model on the audio recording to segment it and identify the speakers.\\\\n5. The model will return timestamps and speaker labels, which can be used to assess individual speaker contributions to the meeting.\\\", '<<<code>>>': \\\"from pyannote.audio import Model\\\\nmodel = Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\\n# Then apply the model on the audio file to perform voice activity detection\\\", 'api_call': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'popcornell/pyannote-segmentation-chime6-mixer6', 'api_call': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['pyannote.audio 2.1.1'], 'example_code': 'from pyannote.audio import Model\\\\nmodel = Model.from_pretrained(popcornell/pyannote-segmentation-chime6-mixer6)', 'performance': {'dataset': 'ami', 'accuracy': 'N/A'}, 'description': 'Pyannote Segmentation model fine-tuned on data from CHiME-7 DASR Challenge. Used to perform diarization in the CHiME-7 DASR diarization baseline.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ†Ùƒ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ‚Ø¹ Ù‚ÙŠÙ… Ø±Ù‚Ù…ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„ØªØ§Ø¨Ø¹Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ©. Ø¶Ø¹ ÙÙŠ Ø§Ø¹ØªØ¨Ø§Ø±Ùƒ ØªØ­Ù„ÙŠÙ„ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù‚Ø¨Ù„ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\", '<<<api_provider>>>': 'Keras', '<<<explanation>>>': \\\"1. We first import the necessary libraries such as tensorflow and keras.\\\\n2. We use the from_pretrained method of the TFAutoModelForSequenceClassification class to load the pre-trained model 'keras-io/timeseries-anomaly-detection'. This model has been designed for anomaly detection tasks in timeseries data.\\\\n3. As we're working with HVAC units data, we preprocess the timeseries data to make it compatible with the model.\\\\n4. The anomaly detection model will be used to identify anomalous behavior in the HVAC system by analyzing the timeseries data from the HVAC units.\\\\n5. After processing the data and detecting anomalies, the building management team can take necessary actions to fix the problem and prevent further complications.\\\", '<<<code>>>': \\\"import tensorflow as tf\\\\nfrom transformers import TFAutoModelForSequenceClassification\\\\n\\\\nmodel = TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\\n\\\\n# Preprocess the timeseries data from the HVAC units\\\\n# ...\\\\n\\\\n# Feed the preprocessed data to the model to detect anomalies\\\\n# anomaly_results = model(...)\\\", 'api_call': \\\"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\", 'provider': 'Keras', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keras-io/timeseries-anomaly-detection', 'api_call': \\\"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\", 'api_arguments': {'optimizer': {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'training_precision': 'float32'}, 'python_environment_requirements': [['tensorflow', 'keras']], 'example_code': '', 'performance': {'dataset': 'Numenta Anomaly Benchmark(NAB)', 'accuracy': {'Train Loss': 0.006, 'Validation Loss': 0.008}}, 'description': 'This script demonstrates how you can use a reconstruction convolutional autoencoder model to detect anomalies in timeseries data. We will use the Numenta Anomaly Benchmark(NAB) dataset. It provides artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered, timestamped, single-valued metrics.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø§Ù„Ø´Ø±ÙƒØ© Ø¨ØªØ·ÙˆÙŠØ± Ù…Ù†ØªØ¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ ÙˆÙØ¹Ø§Ù„.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the 'pipeline' function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a question-answering model, specifying the model 'sultan/BioM-ELECTRA-Large-SQuAD2'.\\\\n3. This model is trained on the BioASQ dataset and fine-tuned on SQuAD2.0, making it suitable for understanding complex biological information.\\\\n4. The created model can be used to answer questions from students within a given context (text passage) covering biological topics.\\\\n5. To use the model, simply provide the context (text) and the question, and the model will return the most likely answer.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\'question-answering\\\\', model=\\\\'sultan/BioM-ELECTRA-Large-SQuAD2\\\\')\\\\ncontext = \\\"Biological context here...\\\"\\\\nquestion = \\\"Student question here...\\\"\\\\nresult = qa_pipeline({\\\\'context\\\\': context, \\\\'question\\\\': question})', 'api_call': \\\"pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sultan/BioM-ELECTRA-Large-SQuAD2', 'api_call': \\\"pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\", 'api_arguments': None, 'python_environment_requirements': [['transformers', 'sentencepiece']], 'example_code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\\nresult = qa_pipeline({'context': 'your_context', 'question': 'your_question'})\\\", 'performance': {'dataset': 'SQuAD2.0 Dev', 'accuracy': {'exact': 84.33420365535248, 'f1': 87.49354241889522}}, 'description': 'BioM-ELECTRA-Large-SQuAD2 is a fine-tuned version of BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts, on the SQuAD2.0 dataset. Fine-tuning the biomedical language model on the SQuAD dataset helps improve the score on the BioASQ challenge. This model is suitable for working with BioASQ or biomedical QA tasks.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ØªØ±Ø¬Ù…Ø© Ø¢Ù„ÙŠ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ØªÙ†ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù„ØºØ§Øª Ø¨Ø¯Ù‚Ø© ÙˆÙØ¹Ø§Ù„ÙŠØ© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import necessary libraries, including the translation_pipeline from the transformers package.\\\\n2. Create a 'translation_pipeline' object, specifying the specific translation model you want to use, 'Helsinki-NLP/opus-mt-sv-en'.\\\\n3. Use the 'translation_pipeline' function to translate the email text from Swedish to English.\\\\n4. Then use the translated text to derive the appropriate response or perform further analysis.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_sv_to_en\\\\', model=\\\\'Helsinki-NLP/opus-mt-sv-en\\\\')\\\\nswedish_text = \\\"Swedish email text here...\\\"\\\\nenglish_translation = translator(swedish_text)\\\\nenglish_text = english_translation[0][\\\\'translation_text\\\\']\\\\n\\\\n# Process the English text and generate a response', 'api_call': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-sv-en', 'api_call': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", 'api_arguments': ['inputs'], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'Tatoeba.sv.en', 'accuracy': 'BLEU: 64.5, chr-F: 0.763'}, 'description': 'A Swedish to English translation model trained on the OPUS dataset using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece.' </s>\"\n"}
{"text": "<s>[INST]ØªØ´Ù…Ù„ Ø£Ø¹Ù…Ø§Ù„ Ø´Ø±ÙƒØªÙ†Ø§ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØµÙˆØ±ØŒ Ø­ÙŠØ« Ù†Ø±ØºØ¨ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ù†ØªØ¬Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… Ø¬Ù…Ø¹Ù‡Ø§.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0', torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. We import the necessary libraries, including Torch and StableDiffusionPipeline from the diffusers package.\\\\n2. We load the pretrained \\\\'andite/anything-v4.0\\\\' model, specifying it should use float16 data type for improved speed and memory efficiency.\\\\n3. The model is sent to GPU (if available) for faster processing.\\\\n4. To generate an image, we provide a description, such as \\\"a warrior with a sword and shield\\\", as the input prompt for the model.\\\\n5. The model generates the image, which can then be saved as a file or further processed as needed.', '<<<code>>>': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\n\\\\nmodel_id = \\\\'andite/anything-v4.0\\\\'\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(\\\\'cuda\\\\')  # If GPU is available\\\\nprompt = \\\"a warrior with a sword and shield\\\"\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(\\\"warrior_with_sword_and_shield.png\\\")', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0', torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'andite/anything-v4.0', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0', torch_dtype=torch.float16)\\\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': [{'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}], 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary libraries and classes, including BeitImageProcessor and BeitForImageClassification from the transformers package, and the Image class from the PIL package.\\\\n2. We then use the from_pretrained method of the BeitForImageClassification class to load the pre-trained model 'microsoft/beit-base-patch16-224-pt22k-ft22k'. This model has been trained for image classification tasks and can identify objects in a variety of categories.\\\\n3. We load the image data from an uploaded file or it can be obtained directly from your online store.\\\\n4. We process the image using the BeitImageProcessor before passing it to the BeitForImageClassification model.\\\\n5. The model can then analyze the image and classify it into one of the pre-defined categories.\\\", '<<<code>>>': 'from transformers import BeitImageProcessor, BeitForImageClassification\\\\nfrom PIL import Image\\\\nimage = Image.open(\\\\'product_image_path.jpg\\\\')\\\\n# replace \\\\'product_image_path.jpg\\\\' with the path to your product image\\\\nprocessor = BeitImageProcessor.from_pretrained(\\\\'microsoft/beit-base-patch16-224-pt22k-ft22k\\\\')\\\\nmodel = BeitForImageClassification.from_pretrained(\\\\'microsoft/beit-base-patch16-224-pt22k-ft22k\\\\')\\\\ninputs = processor(images=image, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(f\\\"Predicted class: {model.config.id2label[predicted_class_idx]}\\\")', 'api_call': \\\"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/beit-base-patch16-224-pt22k-ft22k', 'api_call': \\\"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import BeitImageProcessor, BeitForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\\\ninputs = processor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\\", 'performance': {'dataset': 'ImageNet-22k', 'accuracy': 'Not specified'}, 'description': 'BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø¥Ù„Ù‰ Ù†ØµØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ†Ø¨Ø¤ Ø¨Ù†ÙˆØ¹ÙŠØ© Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ø§ØªØ¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØªÙˆÙØ±Ø©.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes and functions from the transformers package. This includes the 'pipeline' function for setting up the model and 'AutoTokenizer' for tokenizing the input images.\\\\n2. We then use the 'pipeline' function from transformers to load the 'microsoft/git-large-r-textcaps' model. This model is specifically designed for generating text descriptions from images, which is what we need to automatically label products on an e-commerce website.\\\\n3. To use this model, we first need to tokenize the input image using the 'AutoTokenizer' class from transformers.\\\\n4. The model can then be used to generate a text description for the input image, which can be used as a label for the product.\\\", '<<<code>>>': \\\"from transformers import pipeline, AutoTokenizer\\\\ncaption_generator = pipeline('text-generation', model='microsoft/git-large-r-textcaps')\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/git-large-r-textcaps')\\\\nimage_path = 'path/to/image.jpg'  # Replace with the path to your product image\\\\ninputs = tokenizer(image_path)  # Tokenize the input image\\\\noutputs = caption_generator(inputs)  # Generate a text description for the image\\\\nlabel = outputs[0]['generated_text']\\\", 'api_call': \\\"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'git-large-r-textcaps', 'api_call': \\\"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\\\", 'api_arguments': 'image', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'TextCaps', 'accuracy': ''}, 'description': \\\"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\\\" </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙÙŠ ÙˆØ«ÙŠÙ‚Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·.? [/INST]{'domain': 'Multimodal Document Question Answer', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary classes from the transformers library, which includes the AutoModelForDocumentQuestionAnswering class for document question answering tasks.', 'code': 'from transformers import AutoModelForDocumentQuestionAnswering\\\\noursales_model = AutoModelForDocumentQuestionAnswering.from_pretrained(\\\\'impira/layoutlm-invoices\\\\')\\\\n\\\\ndef find_total_sales(receipts):\\\\n    total_sales = 0\\\\n    for receipt in receipts:\\\\n        result = oursales_model(question=\\\"What is the total amount?\\\", context=receipt)\\\\n        total_sales += float(result[\\\\'answer\\\\'])\\\\n    return total_sales', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-invoices', 'api_call': 'from transformers import AutoModelForDocumentQuestionAnswering\\\\noursales_model = AutoModelForDocumentQuestionAnswering.from_pretrained(\\\\'impira/layoutlm-invoices\\\\')\\\\n\\\\ndef find_total_sales(receipts):\\\\n    total_sales = 0\\\\n    for receipt in receipts:\\\\n        result = oursales_model(question=\\\"What is the total amount?\\\", context=receipt)\\\\n        total_sales += float(result[\\\\'answer\\\\'])\\\\n    return total_sales', 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \\\"nlp(question='What is the total amount?', context='your_invoice_text')\\\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‡Ø¯Ù Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ÙŠØ³ØªØ·ÙŠØ¹ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙØ§Øª Ù…Ø³Ø¨Ù‚Ø©. Ù†Ø±ÙŠØ¯ ØªØµÙ…ÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø°Ø§ØªÙŠØ© ÙˆØªØ­Ø¯ÙŠØ¯ ÙØ¦Ø© ÙƒÙ„ Ù†Øµ Ø§Ø³ØªÙ†Ø§Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¥Ø´Ø±Ø§Ù Ø¨Ø´Ø±ÙŠ. ÙŠØªØ·Ù„Ø¨ Ø°Ù„Ùƒ ÙÙ‡Ù…Ù‹Ø§ Ø¹Ù…ÙŠÙ‚Ù‹Ø§ Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ù„ØºØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the 'pipeline' function from the transformers library.\\\\n2. Initialize the classification model by loading the 'svalabs/gbert-large-zeroshot-nli' model using the 'pipeline' function. This model is designed for zero-shot classification tasks in German language.\\\\n3. Define the input sentence and the labels to classify the sentence, in this case, ['Computer', 'Handy', 'Tablet', 'dringend', 'nicht dringend'].\\\\n4. Run the text classification by providing the input sentence and labels.\\\\n5. The result will be a category that best describes the context of the input sentence.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsequence = \\\"Ich habe ein Problem mit meinem Iphone das so schnell wie mÃ¶glich gelÃ¶st werden muss\\\"\\\\nlabels = [\\\\'Computer\\\\', \\\\'Handy\\\\', \\\\'Tablet\\\\', \\\\'dringend\\\\', \\\\'nicht dringend\\\\']\\\\nhypothesis_template = \\\"In diesem Satz geht es um das Thema {}.\\\"\\\\nzershot_pipeline = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'svalabs/gbert-large-zeroshot-nli\\\\')\\\\npredictions = zershot_pipeline(sequence, labels, hypothesis_template=hypothesis_template)\\\\ncategory = predictions[\\\\'labels\\\\'][0]', 'api_call': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'svalabs/gbert-large-zeroshot-nli', 'api_call': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", 'api_arguments': ['sequence', 'labels', 'hypothesis_template'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\nzershot_pipeline = pipeline(zero-shot-classification, model=svalabs/gbert-large-zeroshot-nli)\\\\nsequence = Ich habe ein Problem mit meinem Iphone das so schnell wie mÃ¶glich gelÃ¶st werden muss\\\\nlabels = [Computer, Handy, Tablet, dringend, nicht dringend]\\\\nhypothesis_template = In diesem Satz geht es um das Thema {}. ', 'performance': {'dataset': 'XNLI TEST-Set', 'accuracy': '85.6%'}, 'description': 'A German zeroshot classification model based on the German BERT large model from deepset.ai and finetuned for natural language inference using machine-translated nli sentence pairs from mnli, anli, and snli datasets.' </s>\"\n"}
{"text": "<s>[INST]Ø¨ØªØµÙ…ÙŠÙ… Ù†Ø¸Ù… Ø§Ù„ØªØµÙˆØ± Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ±ØŒ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ÙÙ‡Ù… Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ Ù†ØªØ­Ù„Ùˆ Ø¨ØªØµÙˆÙŠØ±Ù‡Ø§ Ù„ØªØ­Ù‚ÙŠÙ‚ ØªØµÙ†ÙŠÙ Ø¯Ù‚ÙŠÙ‚ ÙˆÙØ¹Ø§Ù„.? [/INST]{'domain': 'Computer Vision Image Classification', 'api_call': \\\"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimport torch\\\\nimg = Image.open(urlopen('https://image_url_here'))\\\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))\\\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\\\", 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'convnextv2_huge.fcmae_ft_in1k', 'api_call': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimport torch\\\\nimg = Image.open(urlopen('https://image_url_here'))\\\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))\\\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\\\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': ['timm'], 'example_code': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimg = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))\\\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\\\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 86.256}, 'description': 'A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨ÙˆØ±Ø´Ø© Ø¹Ù…Ù„ Ø­ÙˆÙ„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ. ÙŠÙˆØµÙ‰ Ø¨ØªÙˆÙÙŠØ± ÙˆØµÙˆÙ„ Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù†ØµÙŠØ© Ø¶Ø®Ù…Ø© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙƒÙ…ØµØ¯Ø± Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù†Ø¯ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ„Ø®ÙŠØµ.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('csebuetnlp/mT5_multilingual_XLSum')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('csebuetnlp/mT5_multilingual_XLSum')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'csebuetnlp/mT5_multilingual_XLSum', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('csebuetnlp/mT5_multilingual_XLSum')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers==4.11.0.dev0']], 'example_code': \\\"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub('\\\\\\\\s+', ' ', re.sub('\\\\\\\\n+', ' ', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\\\", 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 36.5, 'ROUGE-2': 13.934, 'ROUGE-L': 28.988, 'ROUGE-LSUM': 28.996, 'loss': 2.067, 'gen_len': 26.973}}, 'description': 'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¨Ù…Ø³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØµÙ†ÙŠÙ Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ø´Ø§Ù‡Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø¨Ø±ÙŠØ©ØŸ? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224)) # replace with actual video frames\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\\nmodel = TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\\ninputs = processor(images=video, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n    predicted_class_idx = logits.argmax(-1).item()\\\\n    print('Predicted class:', model.config.id2label[predicted_class_idx])\\\", 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'facebook/timesformer-base-finetuned-k600', 'api_call': \\\"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224)) # replace with actual video frames\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\\nmodel = TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\\ninputs = processor(images=video, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n    predicted_class_idx = logits.argmax(-1).item()\\\\n    print('Predicted class:', model.config.id2label[predicted_class_idx])\\\", 'api_arguments': ['images'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k600)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k600)\\\\ninputs = processor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-600', 'accuracy': None}, 'description': 'TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù…Ù‡ Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ù‚ØµÙŠØ±Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/tinyroberta-squad2')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/tinyroberta-squad2')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'deepset/tinyroberta-squad2', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/tinyroberta-squad2')\\\", 'api_arguments': {'model_name_or_path': 'deepset/tinyroberta-squad2', 'question': 'Why is model conversion important?', 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel_name = deepset/tinyroberta-squad2\\\\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\\\\nQA_input = {\\\\n 'question': 'Why is model conversion important?',\\\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\\\n}\\\\nres = nlp(QA_input)\\\", 'performance': {'dataset': 'squad_v2', 'accuracy': {'exact': 78.69114798281817, 'f1': 81.9198998536977}}, 'description': 'This is the distilled version of the deepset/roberta-base-squad2 model. This model has a comparable prediction quality and runs at twice the speed of the base model.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø¬Ø¯ÙˆÙ„ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ ÙˆØ§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary classes from the transformers package. This includes TapasTokenizer to tokenize the input text and TapasForQuestionAnswering for the question-answering model.\\\\n2. We then use the from_pretrained method of the TapasForQuestionAnswering class to load the pre-trained model 'google/tapas-small-finetuned-wikisql-supervised'.\\\\n3. Utilize this model to analyze the text input (renters' questions) and the table representing the car selection, and provide answers to the queries.\\\", '<<<code>>>': 'from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\ntokenizer = TapasTokenizer.from_pretrained(\\\\'google/tapas-small-finetuned-wikisql-supervised\\\\')\\\\nmodel = TapasForQuestionAnswering.from_pretrained(\\\\'google/tapas-small-finetuned-wikisql-supervised\\\\')\\\\nquestions = [\\\"Which car has the lowest rental price?\\\", \\\"What is the price of the SUV?\\\"]\\\\n# replace with renters\\\\' real questions\\\\ninputs = tokenizer(questions, tables=table_data, return_tensors=\\\"pt\\\")\\\\noutputs = model(**inputs)', 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/tapas-small-finetuned-wikisql-supervised', 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\\\", 'api_arguments': \\\"model = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\\\", 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\ntokenizer = TapasTokenizer.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\\\\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\\\", 'performance': {'dataset': 'wikisql', 'accuracy': 'Not specified'}, 'description': 'TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. This model is fine-tuned on WikiSQL and can be used for answering questions related to a table.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø·Ù„Ø§Ø¨ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙŠØ²Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…ÙŠÙ† Ø£Ø¯Ø§Ø© Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡? [/INST]{'domain': 'Multimodal Feature Extraction', 'api_call': \\\"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import necessary libraries, including 'AutoTokenizer' and 'AutoModel' from the transformers package.\\\", 'code': 'import torch\\\\nfrom transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/dragon-plus-query-encoder\\\\')\\\\nquery_encoder = AutoModel.from_pretrained(\\\\'facebook/dragon-plus-query-encoder\\\\')\\\\ncontext_encoder = AutoModel.from_pretrained(\\\\'facebook/dragon-plus-context-encoder\\\\')\\\\nquery = \\\\'Where was Marie Curie born?\\\\'\\\\ncontexts = [\\\\n  \\\"Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\\\",\\\\n  \\\"Born in Paris on 15 May 1859, Pierre Curie was the son of EugÃ¨ne Curie, a doctor of French Catholic origin from Alsace.\\\"\\\\n]\\\\nquery_input = tokenizer(query, return_tensors=\\\\'pt\\\\')\\\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors=\\\\'pt\\\\')\\\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\\\nscores = torch.matmul(query_emb, ctx_emb.T).squeeze()\\\\nmost_relevant_answer = contexts[torch.argmax(scores).item()]', 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': 'import torch\\\\nfrom transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/dragon-plus-query-encoder\\\\')\\\\nquery_encoder = AutoModel.from_pretrained(\\\\'facebook/dragon-plus-query-encoder\\\\')\\\\ncontext_encoder = AutoModel.from_pretrained(\\\\'facebook/dragon-plus-context-encoder\\\\')\\\\nquery = \\\\'Where was Marie Curie born?\\\\'\\\\ncontexts = [\\\\n  \\\"Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\\\",\\\\n  \\\"Born in Paris on 15 May 1859, Pierre Curie was the son of EugÃ¨ne Curie, a doctor of French Catholic origin from Alsace.\\\"\\\\n]\\\\nquery_input = tokenizer(query, return_tensors=\\\\'pt\\\\')\\\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors=\\\\'pt\\\\')\\\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\\\nscores = torch.matmul(query_emb, ctx_emb.T).squeeze()\\\\nmost_relevant_answer = contexts[torch.argmax(scores).item()]', 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \\\"import torch\\\\nfrom transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\\\nquery = 'Where was Marie Curie born?'\\\\ncontexts = [\\\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of EugÃ¨ne Curie, a doctor of French Catholic origin from Alsace.'\\\\n]\\\\nquery_input = tokenizer(query, return_tensors='pt')\\\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\\\nscore1 = query_emb @ ctx_emb[0]\\\\nscore2 = query_emb @ ctx_emb[1]\\\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ Ù„Ù„Ù†Øµ.? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/fastspeech2-en-200_speaker-cv4', arg_overrides={'vocoder': 'hifigan', 'fp16': False})\\\", 'api_provider': 'Fairseq', 'explanation': '1. We first import the required libraries and functions from fairseq, the checkpoint_utils module, and the IPython library for audio playback.', 'code': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\'facebook/fastspeech2-en-200_speaker-cv4\\\\', arg_overrides={\\\\'vocoder\\\\': \\\\'hifigan\\\\', \\\\'fp16\\\\': False})\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(models[0], cfg)\\\\n\\\\ntext = \\\"Hello, this is a news article.\\\"\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, models[0], generator, sample)\\\\n\\\\nipd.Audio(wav, rate=rate)', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Fairseq', 'functionality': 'Text-to-Speech', 'api_name': 'fastspeech2-en-male1', 'api_call': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\'facebook/fastspeech2-en-200_speaker-cv4\\\\', arg_overrides={\\\\'vocoder\\\\': \\\\'hifigan\\\\', \\\\'fp16\\\\': False})\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(models[0], cfg)\\\\n\\\\ntext = \\\"Hello, this is a news article.\\\"\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, models[0], generator, sample)\\\\n\\\\nipd.Audio(wav, rate=rate)', 'api_arguments': ['text'], 'python_environment_requirements': ['fairseq', 'IPython'], 'example_code': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n facebook/fastspeech2-en-200_speaker-cv4,\\\\n arg_overrides={vocoder: hifigan, fp16: False}\\\\n)\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\ntext = Hello, this is a test run.\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)', 'performance': {'dataset': 'common_voice', 'accuracy': None}, 'description': 'FastSpeech 2 text-to-speech model from fairseq S^2. English, 200 male/female voices, trained on Common Voice v4.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ ØªØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ØªØ±ØºØ¨ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ù‡Ø¯ØŸ? [/INST]'<<<domain>>>': 'Multimodal Feature Extraction', '<<<api_call>>>': \\\"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary libraries to create a feature extraction model. We will use the 'AutoModel' from the transformers library.\\\\n2. We load the 'DeepPavlov/rubert-base-cased' model using the 'from_pretrained()' method. This model has been trained on a large corpus of Russian text composed of Wikipedia and news data.\\\\n3. The loaded model can then be used to analyze and extract features from the input text.\\\\n4. With the feature extraction, you can study the grammatical structures, relations between sentences, and other language-related patterns within the text.\\\", '<<<code>>>': 'from transformers import AutoModel, AutoTokenizer\\\\n\\\\nrubert_model = AutoModel.from_pretrained(\\\\'DeepPavlov/rubert-base-cased\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'DeepPavlov/rubert-base-cased\\\\')\\\\n\\\\ninput_text = \\\"Russian text goes here...\\\"\\\\nencoded_input = tokenizer.encode(input_text, return_tensors=\\\\'pt\\\\')\\\\nfeature_extracted = rubert_model(encoded_input)\\\\n\\\\n# Now analyze the extracted features to reveal patterns and insights within the text', 'api_call': \\\"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \\\"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12â€‘layer, 768â€‘hidden, 12â€‘heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERTâ€‘base as an initialization for RuBERT[1].' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ±Ø¬Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ø§Ø³ØªÙ†Ø§Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©. Ù‡Ù„ Ø¨Ø¥Ù…ÙƒØ§Ù†Ùƒ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include AutoFeatureExtractor from transformers, Image from PIL, and requests.\\\\n2. Load the image from the provided URL using the Image library and the requests module.\\\\n3. Use the AutoFeatureExtractor to obtain features of the image.\\\\n4. Load the SwinForImageClassification model using the specific pretrained model 'microsoft/swin-tiny-patch4-window7-224'.\\\\n5. Pass the features extracted from the image to the Swin Transformer model.\\\\n6. The output 'logits' from the model are used to get the index of the predicted class.\\\\n7. Retrieve the name of the object in the image using the 'id2label' dictionary provided in the model's configuration.\\\", '<<<code>>>': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = \\\"http://images.cocodataset.org/val2017/000000039769.jpg\\\"\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\\\\'microsoft/swin-tiny-patch4-window7-224\\\\')\\\\nmodel = SwinForImageClassification.from_pretrained(\\\\'microsoft/swin-tiny-patch4-window7-224\\\\')\\\\ninputs = feature_extractor(images=image, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\n\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/swin-tiny-patch4-window7-224', 'api_call': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': [{'transformers': 'AutoFeatureExtractor', 'PIL': 'Image', 'requests': 'requests'}], 'example_code': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.' </s>\"\n"}
{"text": "<s>[INST]Ù„ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ù…ØªØ·ÙˆØ±Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ù‚Ù… Ø¨ØªÙˆÙÙŠØ± Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…ÙˆÙ„ÙÙ‘Ø¯ Ù†ØµÙˆØµ ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„ØªØ¹Ù„ÙÙ… Ø§Ù„Ø¢Ù„ÙŠ. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø§Ø¯Ø±Ù‹Ø§ Ø¹Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ ÙˆÙ…ÙÙ‡ÙˆÙ…ØŒ ÙˆÙ‚Ù… Ø¨ØªØ¬Ù‡ÙŠØ² ÙØ±ÙŠÙ‚Ùƒ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù‚ØµÙˆÙ‰ Ø¶Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø®Ø±Ù‰ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù„Ù‘ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library: AutoTokenizer and AutoModelForCausalLM.\\\\n2. Load the tokenizer and the pre-trained model 'Salesforce/codegen-2B-multi'. This model has been specifically designed for program synthesis tasks and can generate code in multiple programming languages based on human-readable descriptions.\\\\n3. Tokenize the input human-readable description using the tokenizer's encode method, which converts the description into a sequence of input_ids that the model can understand.\\\\n4. Pass the input_ids to the model's generate method, specifying the maximum length of the generated code. This will return the generated_ids corresponding to the predicted code.\\\\n5. Finally, decode the generated_ids using the tokenizer's decode method to obtain the generated code as text.\\\", '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained('Salesforce/codegen-2B-multi')\\\\nmodel = AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\\\\ntext = 'Create a function that prints Hello, World!'\\\\ninput_ids = tokenizer.encode(text, return_tensors='pt').input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\ngenerated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\\\", 'api_call': \\\"AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Salesforce/codegen-2B-multi', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\\\", 'api_arguments': {'input_ids': 'input_ids', 'max_length': 128}, 'python_environment_requirements': [{'transformers': 'AutoTokenizer, AutoModelForCausalLM'}], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-2B-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-2B-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval, MTPB'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B). The checkpoint included in this repository is denoted as CodeGen-Multi 2B, where Multi means the model is initialized with CodeGen-NL 2B and further pre-trained on a dataset of multiple programming languages, and 2B refers to the number of trainable parameters.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø¨ØªØ¹Ù„Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ¥Ù†Ø´Ø§Ø¡ Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ. Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ù„ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. This includes the AutoModelForCausalLM for the text generation model and AutoTokenizer for tokenizing input text.\\\\n2. Use the from_pretrained method of the AutoModelForCausalLM class to load the pre-trained DialoGPT model 'microsoft/DialoGPT-large'. This model has been trained for large-scale conversational interactions, which is exactly what we need for a chatbot doing customer service for a bank.\\\\n3. Also, load the appropriate tokenizer for the DialoGPT model using the from_pretrained method of the AutoTokenizer class.\\\\n4. Now, when a customer asks a question, tokenize the input, generate a response using the DialoGPT model, and then decode the output to produce a human-readable response.\\\", '<<<code>>>': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-large')\\\\nmodel = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/DialoGPT-large', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_arguments': ['bot_input_ids', 'max_length', 'pad_token_id'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-large)\\\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-large)', 'performance': {'dataset': 'Reddit discussion thread', 'accuracy': 'Comparable to human response quality under a single-turn conversation Turing test'}, 'description': 'DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multi-turn conversations. The model is trained on 147M multi-turn dialogues from Reddit discussion threads.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ¹ÙŠÙ† Ø¹Ù„ÙŠÙƒ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙØ§Øª Ø³Ø§Ø¨Ù‚Ø©. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø­Ù„ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙØ§Øª Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'pipeline' function from the transformers library.\\\\n2. Create a translation pipeline with the 'translation_de_to_es' task using the 'Helsinki-NLP/opus-mt-de-es' model to handle German to Spanish translation.\\\\n3. Pass the text in German from the online chat as an input to the pipeline to get the translated text in Spanish.\\\\n4. The translated text can then be used for further communication in the chat.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\\ngerman_text = 'Guten Morgen! Wie geht es Ihnen?'\\\\n# Replace this with the text from the online chat\\\\ntranslated_text = translation(text=german_text)\\\", 'api_call': \\\"pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-de-es', 'api_call': \\\"pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\\ntranslated_text = translation('Guten Tag')\\\", 'performance': {'dataset': 'Tatoeba.de.es', 'accuracy': {'BLEU': 48.5, 'chr-F': 0.676}}, 'description': 'A German to Spanish translation model based on the OPUS dataset and trained using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece tokenization.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ø³ØªØ®Ø¯Ù… ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬ÙŠØ§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± ÙˆØªØ¹ÙŠÙŠÙ† Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©ØŒ Ø«Ù… Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± ØªÙ„Ù‚Ø§Ø¦ÙŠØ§ ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries including requests, PIL, and transformers.\\\\n2. Use the requests library to fetch the image from the provided URL.\\\\n3. Open the image using the Image class from the PIL package.\\\\n4. Load the pre-trained model 'nateraw/vit-age-classifier' using the ViTForImageClassification.from_pretrained() method, designed for age classification.\\\\n5. Load the transforms using the ViTFeatureExtractor.from_pretrained() method.\\\\n6. Apply the transforms to the input image and convert the result into a PyTorch tensor.\\\\n7. Pass the transformed input to the model and obtain the output logits.\\\\n8. Get the probabilities and predictions of the age by applying softmax on the logits and finding the index with the highest probability.\\\\n9. Finally, return the predicted age of the person in the image.\\\", '<<<code>>>': 'import requests\\\\nfrom PIL import Image\\\\nfrom io import BytesIO\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\n\\\\nurl = \\\"<url>\\\"\\\\nr = requests.get(url)\\\\nim = Image.open(BytesIO(r.content))\\\\nmodel = ViTForImageClassification.from_pretrained(\\\\'nateraw/vit-age-classifier\\\\')\\\\ntransforms = ViTFeatureExtractor.from_pretrained(\\\\'nateraw/vit-age-classifier\\\\')\\\\ninputs = transforms(im, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**inputs)\\\\nproba = output.logits.softmax(1)\\\\npreds = proba.argmax(1)\\\\npredicted_age = preds.item()', 'api_call': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nateraw/vit-age-classifier', 'api_call': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/vit-age-classifier'}, 'python_environment_requirements': [['requests', 'PIL', 'transformers']], 'example_code': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom io import BytesIO\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\n\\\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\\\nim = Image.open(BytesIO(r.content))\\\\n\\\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\\\n\\\\ninputs = transforms(im, return_tensors='pt')\\\\noutput = model(**inputs)\\\\n\\\\nproba = output.logits.softmax(1)\\\\npreds = proba.argmax(1)\\\", 'performance': {'dataset': 'fairface', 'accuracy': None}, 'description': \\\"A vision transformer finetuned to classify the age of a given person's face.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù†Ø·Ø§Ù‚ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨. ÙŠØ­ØªØ§Ø¬ Ø¹Ù…ÙŠÙ„Ù†Ø§ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ù…Ø¹Ø±ÙÙŠØ©. ÙŠØªØ¹ÙŠÙ† Ø£Ù† ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import DetrForSegmentation, DetrFeatureExtractor\\\\nfrom PIL import Image\\\\nimport io, requests\\\\n\\\\nurl = 'your_image_url'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50-panoptic')\\\\nmodel = DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nprocessed_sizes = torch.as_tensor(inputs['pixel_values'].shape[-2:]).unsqueeze(0)\\\\nresult = feature_extractor.post_process_panoptic(outputs, processed_sizes)[0]\\\\npanoptic_seg = Image.open(io.BytesIO(result['png_string']))\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'facebook/detr-resnet-50-panoptic', 'api_call': \\\"from transformers import DetrForSegmentation, DetrFeatureExtractor\\\\nfrom PIL import Image\\\\nimport io, requests\\\\n\\\\nurl = 'your_image_url'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50-panoptic')\\\\nmodel = DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nprocessed_sizes = torch.as_tensor(inputs['pixel_values'].shape[-2:]).unsqueeze(0)\\\\nresult = feature_extractor.post_process_panoptic(outputs, processed_sizes)[0]\\\\npanoptic_seg = Image.open(io.BytesIO(result['png_string']))\\\", 'api_arguments': ['image'], 'python_environment_requirements': ['torch', 'numpy', 'transformers', 'PIL', 'requests', 'io'], 'example_code': ['import io', 'import requests', 'from PIL import Image', 'import torch', 'import numpy', 'from transformers import DetrFeatureExtractor, DetrForSegmentation', 'from transformers.models.detr.feature_extraction_detr import rgb_to_id', \\\"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\", 'image = Image.open(requests.get(url, stream=True).raw)', \\\"feature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50-panoptic')\\\", \\\"model = DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\\\", \\\"inputs = feature_extractor(images=image, return_tensors='pt')\\\", 'outputs = model(**inputs)', \\\"processed_sizes = torch.as_tensor(inputs['pixel_values'].shape[-2:]).unsqueeze(0)\\\", 'result = feature_extractor.post_process_panoptic(outputs, processed_sizes)[0]', \\\"panoptic_seg = Image.open(io.BytesIO(result['png_string']))\\\", 'panoptic_seg = numpy.array(panoptic_seg, dtype=numpy.uint8)', 'panoptic_seg_id = rgb_to_id(panoptic_seg)'], 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': {'box_AP': 38.8, 'segmentation_AP': 31.1, 'PQ': 43.4}}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø¯ÙˆÙ† Ø´Ø±ÙˆØ· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, install the 'diffusers' package using !pip install diffusers.\\\\n2. Then import the DiffusionPipeline from the diffusers package.\\\\n3. Load the 'google/ncsnpp-celebahq-256' model using the from_pretrained() method of DiffusionPipeline. This is a score-based generative model pre-trained on human faces (CelebA-HQ dataset) and specifically optimized for generating high-fidelity images of human faces.\\\\n4. Call the loaded model object to generate a new sample image.\\\\n5. Save the generated image as a PNG file to your local storage or display it directly.\\\", '<<<code>>>': \\\"!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = 'google/ncsnpp-celebahq-256'\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\n\\\\ndef generate_random_face():\\\\n    image = sde_ve()[0]\\\\n    image.save('sde_ve_generated_face.png')\\\", 'api_call': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/ncsnpp-celebahq-256', 'api_call': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\\\", 'api_arguments': {'model_id': 'google/ncsnpp-celebahq-256'}, 'python_environment_requirements': [['diffusers']], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = google/ncsnpp-celebahq-256\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[sample]\\\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¬Ø¯ÙŠØ¯Ø© Ø¯ÙˆÙ† Ø´Ø±ÙˆØ· Ø¨ÙˆØ§Ø³Ø·Ø© ØªÙ‚Ù†ÙŠØ© Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± ÙˆØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('google/ddpm-ema-cat-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, install the 'diffusers' package to use the DDPMPipeline class.\\\\n2. Import DDPMPipeline from the diffusers package.\\\\n3. Load the pre-trained DDPM-EMA cat generator model using the from_pretrained method and the provided model id 'google/ddpm-ema-cat-256'.\\\\n4. Generate an image of a cat using the loaded model by calling the loaded model.\\\\n5. Save the generated image to a file.\\\\n6. Using this code, you will be able to generate high-quality images of cats.\\\", '<<<code>>>': \\\"!pip install diffusers\\\\nfrom diffusers import DDPMPipeline\\\\nmodel_id = 'google/ddpm-ema-cat-256'\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save('ddpm_generated_image.png')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-ema-cat-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/ddpm-ema-cat-256', 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-ema-cat-256')\\\", 'api_arguments': ['model_id'], 'python_environment_requirements': [['!pip install diffusers']], 'example_code': 'from diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-ema-cat-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images, and supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. On the unconditional CIFAR10 dataset, it achieves an Inception score of 9.46 and a state-of-the-art FID score of 3.17.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù†ØµÙŠØ© ÙˆØ§Ù„Ø®Ø±ÙˆØ¬ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù…Ù‡Ù…Ø© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù†Øµ Ù…Ù† Ù‚Ø¨Ù„ Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ø«Ù…ØŒ ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ØªÙ†ÙÙŠØ° Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø°ÙŠ ÙŠÙÙŠ Ø¨Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We import the required T5ForConditionalGeneration class for text-to-text generation from the transformers package.\\\\n2. Load the required pretrained model 'castorini/doc2query-t5-base-msmarco', which has been specifically trained on the MS MARCO dataset for generating search queries from document content.\\\\n3. Use the model to generate search queries by passing the article text as input, and setting the max_length parameter, which defines the maximum length of generated search queries.\\\", '<<<code>>>': \\\"from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\n\\\\nmodel = T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\\\\ntokenizer = T5Tokenizer.from_pretrained('castorini/doc2query-t5-base-msmarco')\\\\ninput_ids = tokenizer.encode('Article text here...', return_tensors='pt')\\\\noutputs = model.generate(input_ids, max_length=10, num_return_sequences=5)\\\\n\\\\ngenerated_queries = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\\\", 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'castorini/doc2query-t5-base-msmarco', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\\\", 'api_arguments': 'text, max_length', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'MS MARCO', 'accuracy': 'Not specified'}, 'description': 'A T5 model trained on the MS MARCO dataset for generating queries from documents.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙØ´Ø¯Ø¯ Ø¹Ù„Ù‰ ÙØ±ÙŠÙ‚ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¢Ù„ÙŠ ØªØ¹Ù‚Ø¨ Ø§Ù„Ù†Øµ Ø§Ù„ØµÙˆØªÙŠ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø³Ù…ÙˆØ¹Ø© ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ù…Ø³Ù…ÙˆØ¹Ø© Ø¨ØµÙˆØª Ø·Ø¨ÙŠØ¹ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù….? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and classes such as SpeechT5Processor, SpeechT5ForTextToSpeech, and SpeechT5HifiGan.\\\\n2. Load the pre-trained SpeechT5 model 'microsoft/speecht5_tts', which has been fine-tuned for text-to-speech tasks.\\\\n3. Use an appropriate processor to convert the input text to a format that the model can interpret.\\\\n4. Generate the synthesized speech for the given input text using the model and a suitable feature extractor.\\\\n5. Convert the generated output to an audio file that can be played back.\\\", '<<<code>>>': 'from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\\\\nimport soundfile as sf\\\\ntext = \\\"Hello, I am your AI voice assistant.\\\"\\\\nprocessor = SpeechT5Processor.from_pretrained(\\\\'microsoft/speecht5_tts\\\\')\\\\nmodel = SpeechT5ForTextToSpeech.from_pretrained(\\\\'microsoft/speecht5_tts\\\\')\\\\nvocoder = SpeechT5HifiGan.from_pretrained(\\\\'microsoft/speecht5_hifigan\\\\')\\\\ninputs = processor(text=text, return_tensors=\\\\'pt\\\\')\\\\nspeech = model.generate_speech(inputs[\\\\'input_ids\\\\'], speaker_embeddings=None, vocoder=vocoder)\\\\nsf.write(\\\\'response.wav\\\\', speech.numpy(), samplerate=16000)', 'api_call': \\\"SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/speecht5_tts', 'api_call': \\\"SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\\\", 'api_arguments': ['text', 'return_tensors', 'input_ids', 'speaker_embeddings', 'vocoder'], 'python_environment_requirements': ['!pip install git+https://github.com/huggingface/transformers sentencepiece datasets'], 'example_code': \\\"from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\nimport soundfile as sf\\\\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_tts')\\\\nmodel = SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\\\\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\\\\ninputs = processor(text='Hello, my dog is cute', return_tensors='pt')\\\\nembeddings_dataset = load_dataset('Matthijs/cmu-arctic-xvectors', split='validation')\\\\nspeaker_embeddings = torch.tensor(embeddings_dataset[7306]['xvector']).unsqueeze(0)\\\\nspeech = model.generate_speech(inputs['input_ids'], speaker_embeddings, vocoder=vocoder)\\\\nsf.write('speech.wav', speech.numpy(), samplerate=16000)\\\", 'performance': {'dataset': 'LibriTTS', 'accuracy': 'Not specified'}, 'description': 'SpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS. It is a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. It can be used for a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ø³ØªØ®Ø¯Ù… Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¥Ù„Ù‰ Ù†Øµ Ù…ÙƒØªÙˆØ¨: \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary packages from the transformers and datasets library. This includes WhisperProcessor and WhisperForConditionalGeneration for the Automatic Speech Recognition (ASR) model.\\\\n2. We use the from_pretrained method to load the pre-trained model 'openai/whisper-large-v2'. This model has been trained to recognize and transcribe human speech to text.\\\\n3. We need to preprocess the audio using the WhisperProcessor. It'll take the raw audio file and decode the waveform into some input features that the model can use.\\\\n4. We forward the preprocessed audio to the WhisperForConditionalGeneration model.\\\\n5. The model returns the IDs of the predicted tokens, which we can convert back into text using the processor.\\\", '<<<code>>>': \\\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\\naudio_path = 'path/to/audio_file.wav' # replace with the path to your audio file\\\\ninput_features = processor.load_and_process_audio_file(audio_path, return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\\", 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai/whisper-large-v2', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", 'api_arguments': {'forced_decoder_ids': \\\"WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')\\\"}, 'python_environment_requirements': [['transformers', 'datasets']], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', \\\"processor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\\\", \\\"model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", 'model.config.forced_decoder_ids = None', \\\"ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')\\\", \\\"sample = ds[0]['audio']\\\", \\\"input_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\\\", 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)'], 'performance': {'dataset': 'LibriSpeech test-clean', 'accuracy': 3.0003583080317573}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\\\", '<<<api_provider>>>': 'Keras', '<<<explanation>>>': \\\"1. Import the necessary libraries, such as Image from PIL, from_pretrained_keras from huggingface_hub, and TensorFlow, numpy as well as requests.\\\\n2. Load the blurry image using the Image.open() method provided by PIL. The image can be a file on disk, or it can be fetched from a URL.\\\\n3. Cast and resize the image as a TensorFlow tensor with the required dimensions (256, 256).\\\\n4. Import the deblurring model 'google/maxim-s3-deblurring-gopro' using the from_pretrained_keras method from Hugging Face.\\\\n5. Predict the deblurred image by calling the model.predict method with input image tensor.\\\", '<<<code>>>': 'from huggingface_hub import from_pretrained_keras\\\\nfrom PIL import Image\\\\nimport tensorflow as tf\\\\nimport numpy as np\\\\nimport requests\\\\n\\\\nurl = \\\\'image_path_or_url\\\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw if \\\"http\\\" in url else url)\\\\nimage = np.array(image)\\\\nimage = tf.convert_to_tensor(image)\\\\nimage = tf.image.resize(image, (256, 256))\\\\nmodel = from_pretrained_keras(\\\\'google/maxim-s3-deblurring-gopro\\\\')\\\\ndeblurred_image = model.predict(tf.expand_dims(image, 0))', 'api_call': \\\"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\\\", 'provider': 'Keras', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/maxim-s3-deblurring-gopro', 'api_call': \\\"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\\\", 'api_arguments': ['image'], 'python_environment_requirements': [['huggingface_hub', 'PIL', 'tensorflow', 'numpy', 'requests']], 'example_code': 'from huggingface_hub import from_pretrained_keras\\\\nfrom PIL import Image\\\\nimport tensorflow as tf\\\\nimport numpy as np\\\\nimport requests\\\\nurl = https://github.com/sayakpaul/maxim-tf/raw/main/images/Deblurring/input/1fromGOPR0950.png\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nimage = np.array(image)\\\\nimage = tf.convert_to_tensor(image)\\\\nimage = tf.image.resize(image, (256, 256))\\\\nmodel = from_pretrained_keras(google/maxim-s3-deblurring-gopro)\\\\npredictions = model.predict(tf.expand_dims(image, 0))', 'performance': {'dataset': 'GoPro', 'accuracy': {'PSNR': 32.86, 'SSIM': 0.961}}, 'description': 'MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªÙˆÙ„ÙŠØ¯ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø£Ùˆ ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ù„Ø¹Ù…Ù„ÙŠØ© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Fill-Mask Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ù†Ø¸Ø§Ù… ÙŠØ³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ù„Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ§Øª ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø¨ÙƒÙ„Ù…Ø§Øª Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¹Ù„Ù‰ Ø´ÙƒÙ„ ÙÙŠØ¯ÙŠÙˆ ØªÙˆØ¶Ø­ ØªØ·Ø¨ÙŠÙ‚ Ù‡Ø°Ù‡ Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„ ÙˆØ¯Ù‚ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"AutoModel.from_pretrained('nlpaueb/legal-bert-small-uncased')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the AutoTokenizer and AutoModel classes from the transformers library provided by Hugging Face.\\\\n2. Load the tokenizer and model 'nlpaueb/legal-bert-small-uncased' using the from_pretrained() method for AutoTokenizer and AutoModel, respectively. This model is specifically tuned for legal documents, contracts, agreements, and rights.\\\\n3. Create an input sentence with a masked token to be filled. Use the tokenizer to tokenize the input sentence with the masked token.\\\\n4. Feed the tokenized input to the model to get a prediction. Retrieve the top predicted token(s) for the masked position.\\\\n5. Use the tokenizer to decode the predicted token(s) and obtain the completed sentence.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModel\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'nlpaueb/legal-bert-small-uncased\\\\')\\\\nmodel = AutoModel.from_pretrained(\\\\'nlpaueb/legal-bert-small-uncased\\\\')\\\\n\\\\nmasked_sentence = \\\"The party agrees to pay a penalty if the contract is not <mask> within a specified time.\\\"\\\\ntokens = tokenizer(masked_sentence, return_tensors=\\\\'pt\\\\')\\\\nprediction = model(**tokens)\\\\npredicted_index = prediction.logits.argmax(-1).item()\\\\npredicted_token = tokenizer.convert_ids_to_tokens(predicted_index)\\\\n\\\\nfilled_sentence = masked_sentence.replace(\\\\'<mask>\\\\', predicted_token)', 'api_call': \\\"AutoModel.from_pretrained('nlpaueb/legal-bert-small-uncased')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nlpaueb/legal-bert-small-uncased', 'api_call': \\\"AutoModel.from_pretrained('nlpaueb/legal-bert-small-uncased')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nlpaueb/legal-bert-small-uncased'}, 'python_environment_requirements': [{'transformers': 'AutoTokenizer, AutoModel'}], 'example_code': \\\"from transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-small-uncased')\\\\nmodel = AutoModel.from_pretrained('nlpaueb/legal-bert-small-uncased')\\\", 'performance': {'dataset': 'Legal Corpora', 'accuracy': 'Comparable to larger models'}, 'description': 'LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications. This is the light-weight version of BERT-BASE (33% the size of BERT-BASE) pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ·Ù„Ø¨ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ø¹Ù„Ù‰ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ù…Ø®ØªÙ„ÙØ©. ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ Ù…Ø³Ø¨Ù‚ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. ÙŠØ¬Ø¨ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„ØµÙˆØ± ÙƒØ¥Ø¯Ø®Ø§Ù„ Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØ¹Ù„Ù… ÙƒÙŠÙÙŠØ© ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± Ø¨Ø¯Ù‚Ø©. Ø§Ø®ØªØ¨Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙˆØ± Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙƒÙØ§Ø¡ØªÙ‡ ÙÙŠ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± Ø¨Ø¯Ù‚Ø© ÙˆÙØ§Ø¹Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"'OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)'\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, including classes from the transformers package and Image from the PIL package. This includes OneFormerForUniversalSegmentation for the segmentation model and Image for processing image data.\\\\n2. Use the from_pretrained method of the OneFormerForUniversalSegmentation class to load the pre-trained model 'shi-labs/oneformer_coco_swin_large'. This model has been trained for multiple segmentation tasks, which is exactly what we need for identifying road signs, pedestrians, and vehicles in images.\\\\n3. Load the image data from a file or acquire it in real-time from the autonomous vehicle's camera.\\\\n4. This model can then be used to perform universal image segmentation upon the provided image, resulting in the identification of various objects in the image.\\\", '<<<code>>>': \\\"from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimage = Image.open('image_path.jpg') # replace 'image_path.jpg' with path to your image\\\\nprocessor = OneFormerProcessor.from_pretrained('shi-labs/oneformer_coco_swin_large')\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_coco_swin_large')\\\\nsemantic_inputs = processor(images=image, task_inputs=['semantic'], return_tensors='pt')\\\\nsemantic_outputs = model(**semantic_inputs)\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\", 'api_call': \\\"'OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)'\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'shi-labs/oneformer_coco_swin_large', 'api_call': \\\"'OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)'\\\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/coco.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_coco_swin_large)\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)\\\\n\\\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\\\nsemantic_outputs = model(**semantic_inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ydshieh/coco_dataset_script', 'accuracy': 'Not provided'}, 'description': 'OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ´Ù…Ù„ Ø§Ù„ØªØ­Ø¯ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù…Ù† Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ù…Ù‡Ù…Ø© Ø¬Ø¯ÙŠØ¯Ø©. Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Load the required libraries and utilities such as torch, Image from PIL, MLSDdetector from controlnet_aux, and functions from diffusers.\\\\n2. Set the checkpoint and prompt information as needed.\\\\n3. Load the source image from the provided URL and your prompt text; then, process the image using MLSDdetector.\\\\n4. Create a ControlNetModel by loading the pretrained model from the checkpoint while specifying the desired torch_dtype, which is torch.float16.\\\\n5. Create a diffusion pipeline by loading a pretrained StableDiffusionControlNetPipeline with the controlnet model, torch_dtype, and any other required configurations.\\\\n6. Generate the image using the pipeline and the provided prompt, and then save the generated image to a specified output file.', '<<<code>>>': 'import torch\\\\nfrom PIL import Image\\\\nfrom controlnet_aux import MLSDdetector\\\\nfrom diffusers import ControlNetModel, StableDiffusionControlNetPipeline, UniPCMultistepScheduler\\\\nfrom diffusers.utils import load_image\\\\n\\\\ncheckpoint = \\\\'lllyasviel/control_v11p_sd15_mlsd\\\\'\\\\nimage_url = \\\"https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\\\"\\\\nprompt = \\\"royal chamber with fancy bed\\\"\\\\n\\\\nimage = load_image(image_url)\\\\nprocessor = MLSDdetector.from_pretrained(\\\\'lllyasviel/ControlNet\\\\')\\\\ncontrol_image = processor(image)\\\\ncontrol_image.save(\\\\'./images/control.png\\\\')\\\\n\\\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n    \\\\'runwayml/stable-diffusion-v1-5\\\\', controlnet=controlnet, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(0)\\\\n\\\\ngenerated_image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\ngenerated_image.save(\\\\'images/image_out.png\\\\')', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/control_v11p_sd15_mlsd', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\\\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate', 'controlnet_aux']], 'example_code': \\\"import torch\\\\nimport os\\\\nfrom huggingface_hub import HfApi\\\\nfrom pathlib import Path\\\\nfrom diffusers.utils import load_image\\\\nfrom PIL import Image\\\\nimport numpy as np\\\\nfrom controlnet_aux import MLSDdetector\\\\nfrom diffusers import (\\\\n ControlNetModel,\\\\n StableDiffusionControlNetPipeline,\\\\n UniPCMultistepScheduler,\\\\n)\\\\ncheckpoint = lllyasviel/control_v11p_sd15_mlsd\\\\nimage = load_image(\\\\n https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\\\\n)\\\\nprompt = royal chamber with fancy bed\\\\nprocessor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\\\ncontrol_image = processor(image)\\\\ncontrol_image.save(./images/control.png)\\\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(0)\\\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\nimage.save('images/image_out.png')\\\", 'performance': {'dataset': 'MLSD', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© ÙˆØ¥Ø¬Ø§Ø¨Ø§Øª Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠØ¬Ø¨ Ø£Ù† ØªØ¨Ø¯Ø£ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ø¨Ø¹Ø¨Ø§Ø±Ø© \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the TapasTokenizer and TapasForQuestionAnswering classes from the transformers library.', 'code': 'from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\n\\\\nmodel = TapasForQuestionAnswering.from_pretrained(\\\\'google/tapas-base-finetuned-wtq\\\\')\\\\ntokenizer = TapasTokenizer.from_pretrained(\\\\'google/tapas-base-finetuned-wtq\\\\')\\\\n\\\\ntable_data = \\\"\\\"\\\"\\\\nTeam | Goals | Assists\\\\n-------|-------|---------\\\\nTeam A | 5     | 3\\\\nTeam B | 2     | 1\\\\n\\\"\\\"\\\"\\\\n\\\\nquestions = [\\\\n    \\\"Which team scored more goals?\\\",\\\\n    \\\"How many assists were there in total?\\\",\\\\n]\\\\n\\\\nfor question in questions:\\\\n    inputs = tokenizer(question, table_data, return_tensors=\\\"pt\\\")\\\\n    outputs = model(**inputs)\\\\n    answer = tokenizer.convert_logits_to_predictions(inputs, outputs.logits)\\\\n    print(f\\\"Question: {question}\\\\\\\\nAnswer: {answer[0]}\\\\\\\\n\\\")', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-base-finetuned-wtq', 'api_call': 'from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\n\\\\nmodel = TapasForQuestionAnswering.from_pretrained(\\\\'google/tapas-base-finetuned-wtq\\\\')\\\\ntokenizer = TapasTokenizer.from_pretrained(\\\\'google/tapas-base-finetuned-wtq\\\\')\\\\n\\\\ntable_data = \\\"\\\"\\\"\\\\nTeam | Goals | Assists\\\\n-------|-------|---------\\\\nTeam A | 5     | 3\\\\nTeam B | 2     | 1\\\\n\\\"\\\"\\\"\\\\n\\\\nquestions = [\\\\n    \\\"Which team scored more goals?\\\",\\\\n    \\\"How many assists were there in total?\\\",\\\\n]\\\\n\\\\nfor question in questions:\\\\n    inputs = tokenizer(question, table_data, return_tensors=\\\"pt\\\")\\\\n    outputs = model(**inputs)\\\\n    answer = tokenizer.convert_logits_to_predictions(inputs, outputs.logits)\\\\n    print(f\\\"Question: {question}\\\\\\\\nAnswer: {answer[0]}\\\\\\\\n\\\")', 'api_arguments': {'model_name': 'google/tapas-base-finetuned-wtq'}, 'python_environment_requirements': {'transformers': '4.12.0'}, 'example_code': \\\"from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\n\\\\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')\\\\ntokenizer = TapasTokenizer.from_pretrained('google/tapas-base-finetuned-wtq')\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.4638}, 'description': 'TAPAS base model fine-tuned on WikiTable Questions (WTQ). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion, and then fine-tuned on SQA, WikiSQL, and finally WTQ. It can be used for answering questions related to a table.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ø¬Ø¯ÙˆÙ„ ØªØµÙ†ÙŠÙÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„ØªØ±Ø¬Ù…Ø©ØŒ Ù…Ø«Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙˆØ§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø¬Ù…Ù„ØŒ Ø«Ù… Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù‚Ù… Ø¨ØªØµÙ†ÙŠÙÙ‡Ø§ Ø­Ø³Ø¨ ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª: [Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø¹Ù†Ø§ØµØ± Ø£Ùˆ ÙƒØ§Ø¦Ù†Ø§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ØªÙ†ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø®ØµØ§Ø¦Øµ]? [/INST]{'domain': 'Natural Language Processing Token Classification', 'api_call': \\\"SequenceTagger.load('flair/pos-english')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load(\\\\'flair/pos-english\\\\')\\\\nsentence = Sentence(\\\"Your english sentence\\\")\\\\ntagger.predict(sentence)\\\\nfor entity in sentence.get_spans(\\\\'pos\\\\'):\\\\n    print(entity)', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Part-of-Speech Tagging', 'api_name': 'flair/pos-english', 'api_call': 'from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load(\\\\'flair/pos-english\\\\')\\\\nsentence = Sentence(\\\"Your english sentence\\\")\\\\ntagger.predict(sentence)\\\\nfor entity in sentence.get_spans(\\\\'pos\\\\'):\\\\n    print(entity)', 'api_arguments': 'sentence', 'python_environment_requirements': 'flair (pip install flair)', 'example_code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\ntagger = SequenceTagger.load(flair/pos-english)\\\\n\\\\nsentence = Sentence(I love Berlin.)\\\\n\\\\ntagger.predict(sentence)\\\\n\\\\nprint(sentence)\\\\n\\\\nfor entity in sentence.get_spans('pos'):\\\\n print(entity)\\\", 'performance': {'dataset': 'Ontonotes', 'accuracy': '98.19'}, 'description': 'This is the standard part-of-speech tagging model for English that ships with Flair. It predicts fine-grained POS tags based on Flair embeddings and LSTM-CRF.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ±Ø¬Ù‰ ØªØµÙ…ÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø£Ø³Ø¦Ù„Ø© Ø¹Ù† Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-1024shot')\\\", '<<<api_provider>>>': 'PyTorch Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'AutoTokenizer', 'AutoModelForSeq2SeqLM' from transformers, and 'pandas' for working with the data.\\\\n2. Load the table-based QA model 'neulab/omnitab-large-1024shot', which has been pretrained for few-shot table-based question answering.\\\\n3. Create the data as a pandas DataFrame containing the electronic devices information.\\\\n4. Specify the query, which will be used to compare the devices and find out the one with the highest battery capacity.\\\\n5. Encode the table and query using the tokenizer and generate the answer using the model.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport pandas as pd\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'neulab/omnitab-large-1024shot\\\\')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'neulab/omnitab-large-1024shot\\\\')\\\\n\\\\ndata = {\\\\n    \\\\'Device\\\\': [\\\\'Device1\\\\', \\\\'Device2\\\\', \\\\'Device3\\\\'],\\\\n    \\\\'Battery\\\\': [4000, 5000, 4500]\\\\n}\\\\ntable = pd.DataFrame(data)\\\\nquery = \\\"Which device has the highest battery capacity?\\\"\\\\n\\\\nencoding = tokenizer(table=table, query=query, return_tensors=\\\\'pt\\\\')\\\\noutputs = model.generate(**encoding)\\\\nanswer = tokenizer.batch_decode(outputs, skip_special_tokens=True)\\\\n\\\\nprint(answer)', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-1024shot')\\\", 'provider': 'PyTorch Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'neulab/omnitab-large-1024shot', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-1024shot')\\\", 'api_arguments': {'table': 'pd.DataFrame.from_dict(data)', 'query': 'str'}, 'python_environment_requirements': [['transformers', 'pandas']], 'example_code': 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport pandas as pd\\\\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-1024shot)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-1024shot)\\\\ndata = {\\\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\\\n city: [athens, paris, st. louis, athens, beijing, london]\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = In which year did beijing host the Olympic Games?\\\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))', 'performance': {'dataset': 'wikitablequestions', 'accuracy': 'Not provided'}, 'description': 'OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. neulab/omnitab-large-1024shot (based on BART architecture) is initialized with microsoft/tapex-large and continuously pretrained on natural and synthetic data (SQL2NL model trained in the 1024-shot setting).' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ§Øª Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ‚Ø¯Ù… ÙÙŠ Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªÙ†Ù…ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø¸Ù… Ø§Ù„Ø°ÙƒÙŠØ© Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Multimodal Feature Extraction', '<<<api_call>>>': \\\"AutoModel.from_pretrained('microsoft/unixcoder-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries 'AutoTokenizer' and 'AutoModel' from the transformers package.\\\\n2. Create a tokenizer using the 'AutoTokenizer.from_pretrained()' function with the model name 'microsoft/unixcoder-base'.\\\\n3. Load the 'microsoft/unixcoder-base' model using 'AutoModel.from_pretrained()'.\\\\n4. Tokenize the code comments and abstract syntax trees (AST) using the tokenizer object created earlier.\\\\n5. Use the model to generate embeddings and feature representation of the code comments and AST.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'microsoft/unixcoder-base\\\\')\\\\nmodel = AutoModel.from_pretrained(\\\\'microsoft/unixcoder-base\\\\')\\\\ninputs = tokenizer([\\\"code comment here\\\", \\\"AST representation here\\\"], return_tensors=\\\"pt\\\", padding=True, truncation=True)\\\\noutputs = model(**inputs)\\\\nfeatures = outputs[0]', 'api_call': \\\"AutoModel.from_pretrained('microsoft/unixcoder-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/unixcoder-base', 'api_call': \\\"AutoModel.from_pretrained('microsoft/unixcoder-base')\\\", 'api_arguments': {'tokenizer': \\\"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\\"}, 'python_environment_requirements': [{'transformers': 'from transformers import AutoTokenizer, AutoModel'}], 'example_code': \\\"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\\\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù†Ø¯Ù…Ø§ ØªØ£ØªÙˆÙ† Ø¥Ù„Ù‰ Ù…ÙŠØ¯Ø§Ù† Ø§Ù„Ù„Ø¹Ø¨ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ Ø§Ø³ØªØ®Ø¯Ù…ÙˆØ§ Ø£Ø¯Ø§Ø© Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ¶Ø¹ÙŠØ§Øª Ù„Ù„Ø§Ù†Ø¯Ù…Ø§Ø¬ Ø¨ÙŠÙ† Ø§Ù„Ù†Øµ ÙˆØ§Ù„ØµÙˆØ±Ø©. Ù‚ÙˆÙ…ÙˆØ§ Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø³ÙˆÙ…Ø§Øª ØªØ¹Ø¨ÙŠØ±ÙŠØ© ØªØ¹ÙƒØ³ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚Ø±ÙˆØ¡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù…ÙƒØ§Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø³Ù… ÙˆØ§Ù„ØªÙ„ÙˆÙŠÙ† Ø§Ù„Ù…Ø®ØªÙ„ÙØ©.? [/INST]{'domain': 'Multimodal Text-to-Image', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0', torch_dtype=torch.float16)\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': \\\"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = 'andite/anything-v4.0'\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to('cuda')\\\\nprompt = 'futuristic city landscape'\\\\nimage = pipe(prompt).images[0]\\\\nimage.save('futuristic_city_landscape.png')\\\", 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \\\"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = 'andite/anything-v4.0'\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to('cuda')\\\\nprompt = 'futuristic city landscape'\\\\nimage = pipe(prompt).images[0]\\\\nimage.save('futuristic_city_landscape.png')\\\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø¥Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„Ø§Øª ØµÙˆØªÙŠØ© Ø¨Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ©. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø±Ù‹Ø§ Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ø§Ù„ØµÙˆØª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"pipeline('audio-to-audio', model='JorisCos/DPTNet_Libri1Mix_enhsingle_16k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the required pipeline function from the transformers library provided by Hugging Face.\\\\n2. Utilize the pipeline function to create an audio-to-audio model, which is designed to enhance the quality of audio files.\\\\n3. Specify the model \\\\'JorisCos/DPTNet_Libri1Mix_enhsingle_16k\\\\' to be loaded. This model is trained for the \\\"enh_single\\\" task of the Libri1Mix dataset, which involves enhancing audio quality for files with multiple speakers.\\\\n4. The created audio quality enhancement model can be used to process audio files and produce clean audio files with enhanced quality.', '<<<code>>>': \\\"from transformers import pipeline\\\\naudio_enhancer = pipeline('audio-to-audio', model='JorisCos/DPTNet_Libri1Mix_enhsingle_16k')\\\\nenhanced_audio = audio_enhancer(input_audio_file)\\\", 'api_call': \\\"pipeline('audio-to-audio', model='JorisCos/DPTNet_Libri1Mix_enhsingle_16k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'JorisCos/DPTNet_Libri1Mix_enhsingle_16k', 'api_call': \\\"pipeline('audio-to-audio', model='JorisCos/DPTNet_Libri1Mix_enhsingle_16k')\\\", 'api_arguments': {'model': 'JorisCos/DPTNet_Libri1Mix_enhsingle_16k'}, 'python_environment_requirements': [['transformers', 'asteroid']], 'example_code': '', 'performance': {'dataset': 'Libri1Mix', 'si_sdr': 14.829670037349064, 'si_sdr_imp': 11.379888731489366, 'sdr': 15.395712644737149, 'sdr_imp': 11.893049845524112, 'sir': 'Infinity', 'sir_imp': 'NaN', 'sar': 15.395712644737149, 'sar_imp': 11.893049845524112, 'stoi': 0.9301948391058859, 'stoi_imp': 0.13427501556534832}, 'description': 'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ÙŠØ³ØªØ®Ø¯Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†ØµÙˆØµØŒ Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† ØªÙˆÙ„ÙŠØ¯ ÙˆØµÙ Ù†ØµÙŠ Ø¯Ù‚ÙŠÙ‚ Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙˆØ± Ø¨Ø´ÙƒÙ„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© ÙˆÙƒØ¨ÙŠØ±Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ø¯Ø§Ø¡ Ù…ØªÙ…ÙŠØ² ÙˆØ¯Ù‚ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the Pix2StructForConditionalGeneration and Pix2StructProcessor classes from the transformers library provided by Hugging Face.\\\\n2. We load the pretrained model 'google/pix2struct-base' that is specially designed for generating textual descriptions for images. This model has been trained on various tasks, including image captioning.\\\\n3. Using the model and the processor, we can convert the input image data into a textual description.\\\\n4. The generated description can be used as an alternative text for images on websites to provide better accessibility to users with visual impairments.\\\", '<<<code>>>': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(\\\\'google/pix2struct-base\\\\')\\\\nprocessor = Pix2StructProcessor.from_pretrained(\\\\'google/pix2struct-base\\\\')\\\\ninputs = processor(images=input_image, return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(**inputs)\\\\ndescription = processor.batch_decode(outputs, skip_special_tokens=True)[0]', 'api_call': \\\"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/pix2struct-base', 'api_call': \\\"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\\\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': [{'transformers': '4.15.0', 'torch': '1.10.1'}], 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ·ÙŠØ¹ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ù„ØªØ¯Ø®Ù„ ÙŠØ¯ÙˆÙŠ Ù…Ù† Ø§Ù„Ø®Ø§Ø±Ø¬.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include DistilBertTokenizer and DistilBertForSequenceClassification from the transformers package.\\\\n2. Use the from_pretrained method to load both the tokenizer and the pretrained model with the given model name 'distilbert-base-uncased-finetuned-sst-2-english'. This model is specifically trained to predict sentiment for binary sentiment classification.\\\\n3. Given a movie review, use the tokenizer to convert it into a set of tokens that are input to the model.\\\\n4. The model's output logits can be used to determine the predicted class label, which can be either positive or negative sentiment.\\\\n5. Classifying movie reviews this way allows us to sort movies based on user preferences.\\\", '<<<code>>>': \\\"from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\\\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\\\\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\\\\ninputs = tokenizer('Movie review text here...', return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_class_id = logits.argmax().item()\\\\nsentiment_label = model.config.id2label[predicted_class_id]\\\", 'api_call': \\\"DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilbert-base-uncased-finetuned-sst-2-english', 'api_call': \\\"DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\\\", 'api_arguments': ['inputs'], 'python_environment_requirements': [['torch', 'transformers']], 'example_code': \\\"import torch\\\\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\\\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\\\\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\\\\ninputs = tokenizer('Hello, my dog is cute', return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_class_id = logits.argmax().item()\\\\nmodel.config.id2label[predicted_class_id]\\\", 'performance': {'dataset': 'glue', 'accuracy': 0.911}, 'description': 'This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©. Ø«Ù… Ù‚Ù… Ø¨ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ø®ØªØ¨Ø§Ø± Ù…Ù† Ø§Ù„ØµÙˆØ± ÙˆÙ‚ÙŠØ§Ø³ Ø¯Ù‚ØªÙ‡. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙÙŠ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries for working with images and loading the pretrained model. This includes the 'timm' library, which makes it easy to load the model, and the 'Image' class from PIL for working with image data.\\\\n2. Use the 'timm.create_model()' function to load the pretrained 'convnextv2_huge.fcmae_ft_in1k' model. This model has been trained on a wide range of image classes and should be able to identify various types of food in images.\\\\n3. Load an image, which could be taken by the robot's camera or from a file.\\\\n4. Apply any necessary preprocessing steps to the image, set the model to evaluation mode, and then use the model to classify the image.\\\\n5. Extract the top 5 most probable class indices and their corresponding class probabilities.\\\", '<<<code>>>': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimg = Image.open(urlopen('https://some_host.com/your_image_url_here.jpg'))\\\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\", 'api_call': \\\"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'convnextv2_huge.fcmae_ft_in1k', 'api_call': \\\"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': [['timm']], 'example_code': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimg = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))\\\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\\\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 86.256}, 'description': 'A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…ØªØ®ØµØµ ÙÙŠ ØªÙ…Ø«ÙŠÙ„ Ù‚Ø¯Ø±Ø§Øª Ù„ØºÙˆÙŠØ©. Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØµÙ†ÙØ© Ù…Ù† Ø¯ÙˆÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥Ø´Ø§Ø±Ø§ØªØŸ? [/INST]{'domain': 'Computer Vision Zero-Shot Image Classification', 'api_call': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import ChineseCLIPModel, ChineseCLIPProcessor\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = \\\\'https://example.com/product_image.jpg\\\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nmodel = ChineseCLIPModel.from_pretrained(\\\\'OFA-Sys/chinese-clip-vit-large-patch14\\\\')\\\\nprocessor = ChineseCLIPProcessor.from_pretrained(\\\\'OFA-Sys/chinese-clip-vit-large-patch14\\\\')\\\\ntexts = [\\\"ç”µå­è®¾å¤‡\\\", \\\"æœè£…\\\", \\\"å¨æˆ¿ç”¨å“\\\", \\\"æ±½è½¦é…ä»¶\\\"]\\\\ninputs = processor(text=texts, images=image, return_tensors=\\\\'pt\\\\', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'chinese-clip-vit-large-patch14', 'api_call': 'from transformers import ChineseCLIPModel, ChineseCLIPProcessor\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = \\\\'https://example.com/product_image.jpg\\\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nmodel = ChineseCLIPModel.from_pretrained(\\\\'OFA-Sys/chinese-clip-vit-large-patch14\\\\')\\\\nprocessor = ChineseCLIPProcessor.from_pretrained(\\\\'OFA-Sys/chinese-clip-vit-large-patch14\\\\')\\\\ntexts = [\\\"ç”µå­è®¾å¤‡\\\", \\\"æœè£…\\\", \\\"å¨æˆ¿ç”¨å“\\\", \\\"æ±½è½¦é…ä»¶\\\"]\\\\ninputs = processor(text=texts, images=image, return_tensors=\\\\'pt\\\\', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)', 'api_arguments': {'model_name': 'OFA-Sys/chinese-clip-vit-large-patch14'}, 'python_environment_requirements': {'libraries': ['transformers', 'PIL', 'requests']}, 'example_code': 'from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\\\\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\\\\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = []\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nimage_features = model.get_image_features(**inputs)\\\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True) # normalize\\\\ninputs = processor(text=texts, padding=True, return_tensors=pt)\\\\ntext_features = model.get_text_features(**inputs)\\\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True) # normalize\\\\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\\\\nprobs = logits_per_image.softmax(dim=1) # probs: [[0.0066, 0.0211, 0.0031, 0.9692]]', 'performance': {'dataset': 'MUGE Text-to-Image Retrieval, Flickr30K-CN Retrieval, COCO-CN Retrieval, CIFAR10, CIFAR100, DTD, EuroSAT, FER, FGV, KITTI, MNIST, PASCAL VOC', 'accuracy': 'Varies depending on the dataset'}, 'description': 'Chinese-CLIP-ViT-Large-Patch14 is a large version of the Chinese CLIP model, with ViT-L/14 as the image encoder and RoBERTa-wwm-base as the text encoder. Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It is designed for zero-shot image classification tasks.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ØµÙˆØªÙŠØ© Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('nguyenvulebinh/wav2vec2-base-vietnamese-250h')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We import the necessary classes from the transformers and soundfile packages. This includes the Wav2Vec2Processor for processing audio data and the Wav2Vec2ForCTC for the speech recognition model.', 'code': 'from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nimport soundfile as sf\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained(\\\\'nguyenvulebinh/wav2vec2-base-vietnamese-250h\\\\')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\\\'nguyenvulebinh/wav2vec2-base-vietnamese-250h\\\\')\\\\n\\\\n# Replace \\\\'audio_path.wav\\\\' with the location of your audio file\\\\nspeech, _ = sf.read(\\\\'audio_path.wav\\\\')\\\\ninput_values = processor(speech, return_tensors=\\\"pt\\\").input_values\\\\nlogits = model(input_values).logits\\\\n\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'nguyenvulebinh/wav2vec2-base-vietnamese-250h', 'api_call': 'from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nimport soundfile as sf\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained(\\\\'nguyenvulebinh/wav2vec2-base-vietnamese-250h\\\\')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\\\'nguyenvulebinh/wav2vec2-base-vietnamese-250h\\\\')\\\\n\\\\n# Replace \\\\'audio_path.wav\\\\' with the location of your audio file\\\\nspeech, _ = sf.read(\\\\'audio_path.wav\\\\')\\\\ninput_values = processor(speech, return_tensors=\\\"pt\\\").input_values\\\\nlogits = model(input_values).logits\\\\n\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'api_arguments': ['file'], 'python_environment_requirements': ['transformers', 'datasets', 'soundfile', 'torch'], 'example_code': \\\"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nfrom datasets import load_dataset\\\\nimport soundfile as sf\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained(nguyenvulebinh/wav2vec2-base-vietnamese-250h)\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(nguyenvulebinh/wav2vec2-base-vietnamese-250h)\\\\n\\\\ndef map_to_array(batch):\\\\n speech, _ = sf.read(batch[file])\\\\n batch[speech] = speech\\\\n return batch\\\\n\\\\nds = map_to_array({\\\\n file: 'audio-test/t1_0001-00010.wav'\\\\n})\\\\n\\\\ninput_values = processor(ds[speech], return_tensors=pt, padding=longest).input_values\\\\n\\\\nlogits = model(input_values).logits\\\\n\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'performance': {'dataset': [{'name': 'VIVOS', 'accuracy': 6.15}, {'name': 'Common Voice vi', 'accuracy': 11.52}]}, 'description': 'Vietnamese end-to-end speech recognition using wav2vec 2.0. Pre-trained on 13k hours of Vietnamese youtube audio (un-label data) and fine-tuned on 250 hours labeled of VLSP ASR dataset on 16kHz sampled speech audio.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚ÙÙ… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹ÙŠÙ†Ø§Øª ØµÙˆØªÙŠØ© Ù„Ù…Ù‡Ù…Ø© ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ ØµÙˆØª.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': 'ConvTasNet_Libri3Mix_sepclean_8k()', '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': 'ConvTasNet_Libri3Mix_sepclean_8k()', 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ConvTasNet_Libri3Mix_sepclean_8k', 'api_call': 'ConvTasNet_Libri3Mix_sepclean_8k()', 'api_arguments': {'n_src': 3, 'sample_rate': 8000, 'segment': 3, 'task': 'sep_clean', 'train_dir': 'data/wav8k/min/train-360', 'valid_dir': 'data/wav8k/min/dev', 'kernel_size': 16, 'n_filters': 512, 'stride': 8, 'bn_chan': 128, 'hid_chan': 512, 'mask_act': 'relu', 'n_blocks': 8, 'n_repeats': 3, 'skip_chan': 128, 'lr': 0.001, 'optimizer': 'adam', 'weight_decay': 0.0, 'batch_size': 24, 'early_stop': True, 'epochs': 200, 'half_lr': True, 'num_workers': 4}, 'python_environment_requirements': ['Asteroid'], 'example_code': '', 'performance': {'dataset': 'Libri3Mix', 'accuracy': {'si_sdr': 8.581797049575108, 'si_sdr_imp': 11.977037288467368, 'sdr': 9.305885208641385, 'sdr_imp': 12.3943409734845, 'sir': 16.42030534048559, 'sir_imp': 19.508759460400984, 'sar': 10.641943911079238, 'sar_imp': -56.4345187842095, 'stoi': 0.8365148408724333, 'stoi_imp': 0.24401766199806396}}, 'description': 'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri3Mix dataset.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© ØªØ³ØªØ®Ø¯Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†ØµÙˆØµ ÙˆÙˆØµÙ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries from transformers, such as AutoImageProcessor and TimesformerForVideoClassification.\\\\n2. Load the pretrained model 'facebook/timesformer-base-finetuned-k600' using TimesformerForVideoClassification.from_pretrained() method.\\\\n3. Process the video frames using AutoImageProcessor and create a torch tensor of the frames.\\\\n4. Pass the processed video tensor into the model to get the logits.\\\\n5. Find the predicted class index with the highest probability in the logits.\\\\n6. Finally, use the predicted class index to obtain the label of the action that is classified by the model.\\\", '<<<code>>>': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(\\\\'facebook/timesformer-base-finetuned-k600\\\\')\\\\nmodel = TimesformerForVideoClassification.from_pretrained(\\\\'facebook/timesformer-base-finetuned-k600\\\\')\\\\ninputs = processor(images=video, return_tensors=\\\\'pt\\\\')\\\\n\\\\n with torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n predicted_class_idx = logits.argmax(-1).item()\\\\n print(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/timesformer-base-finetuned-k600', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\", 'api_arguments': ['images'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k600)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k600)\\\\ninputs = processor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-600', 'accuracy': None}, 'description': 'TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ ØªØµÙ…ÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ØŒ ÙˆÙ‡Ùˆ Ø£Ù…Ø± Ø­Ø§Ø³Ù… Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ¯Ù‚Ø© Ø§Ù„Ù†Ø¸Ø§Ù….? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('google/tapas-large-finetuned-sqa')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': \\\"from transformers import TapasForQuestionAnswering\\\\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-large-finetuned-sqa')\\\\n# Use the model to answer questions based on the provided financial data\\\\n# Replace `financial_data_table` and `user_question` with the relevant input\\\\nanswer = model(question=user_question, table=financial_data_table)\\\", 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-large-finetuned-sqa', 'api_call': \\\"from transformers import TapasForQuestionAnswering\\\\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-large-finetuned-sqa')\\\\n# Use the model to answer questions based on the provided financial data\\\\n# Replace `financial_data_table` and `user_question` with the relevant input\\\\nanswer = model(question=user_question, table=financial_data_table)\\\", 'api_arguments': ['question', 'table'], 'python_environment_requirements': ['transformers'], 'example_code': 'https://huggingface.co/google/tapas-large-finetuned-sqa', 'performance': {'dataset': 'msr_sqa', 'accuracy': 0.7289}, 'description': 'TAPAS large model fine-tuned on Sequential Question Answering (SQA). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned on SQA. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø³Ø¤Ø§Ù„ Ù…ÙƒØªÙˆØ¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… ØªØ³Ù…ÙŠØ© Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ±Ø¬Ù…Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·.? [/INST]{'domain': 'Multimodal Visual Question Answering', 'api_call': \\\"ViltModel.from_pretrained('tufa15nik/vilt-finetuned-vqasi')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import ViltModel, ViltTokenizer\\\\nmodel = ViltModel.from_pretrained(\\\\'tufa15nik/vilt-finetuned-vqasi\\\\')\\\\ntokenizer = ViltTokenizer.from_pretrained(\\\\'tufa15nik/vilt-finetuned-vqasi\\\\')\\\\nquestion = \\\"<insert_question_here>\\\"\\\\ninputs = tokenizer(question, return_tensors=\\\"pt\\\")\\\\nimage_inputs = {\\\"image\\\": image}  # Load image content\\\\noutputs = model([inputs, image_inputs], return_dict=True)\\\\nanswer = outputs[\\\"answer\\\"][0]', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'vilt-finetuned-vqasi', 'api_call': 'from transformers import ViltModel, ViltTokenizer\\\\nmodel = ViltModel.from_pretrained(\\\\'tufa15nik/vilt-finetuned-vqasi\\\\')\\\\ntokenizer = ViltTokenizer.from_pretrained(\\\\'tufa15nik/vilt-finetuned-vqasi\\\\')\\\\nquestion = \\\"<insert_question_here>\\\"\\\\ninputs = tokenizer(question, return_tensors=\\\"pt\\\")\\\\nimage_inputs = {\\\"image\\\": image}  # Load image content\\\\noutputs = model([inputs, image_inputs], return_dict=True)\\\\nanswer = outputs[\\\"answer\\\"][0]', 'api_arguments': {'model': 'tufa15nik/vilt-finetuned-vqasi', 'tokenizer': 'tufa15nik/vilt-finetuned-vqasi'}, 'python_environment_requirements': {'transformers': '>=4.11.3'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the VQASI dataset by tufa15nik using the ViLT architecture. The model is designed to answer questions based on the content of an input image.'}} </s>\"\n"}
{"text": "<s>[INST]Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ±ØºØ¨ ÙÙŠ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ØªØ¹Ø±Ù Ø§Ù„ØµÙˆØª Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø­ÙˆØ§Ø³ÙŠØ¨. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØµÙˆØªÙŠØ© Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ø¯Ù‚Ø© ÙˆÙØ¹Ø§Ù„ÙŠØ©.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\", 'api_provider': 'PyTorch Transformers', 'explanation': '1. First, import the necessary classes from the transformers package: Wav2Vec2Processor and Data2VecForCTC.', 'code': \\\"from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/data2vec-audio-base-960h')\\\\nmodel = Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\\ninput_values = processor(conference_audio, return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'PyTorch Transformers', 'functionality': 'Automatic Speech Recognition', 'api_name': 'data2vec-audio-base-960h', 'api_call': \\\"from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/data2vec-audio-base-960h')\\\\nmodel = Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\\ninput_values = processor(conference_audio, return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers', 'datasets', 'torch', 'jiwer'], 'example_code': 'from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\n# load model and processor\\\\nprocessor = Wav2Vec2Processor.from_pretrained(facebook/data2vec-audio-base-960h)\\\\nmodel = Data2VecForCTC.from_pretrained(facebook/data2vec-audio-base-960h)\\\\n\\\\n# load dummy dataset and read soundfiles\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\n\\\\n# tokenize\\\\ninput_values = processor(ds[0][audio][array],, return_tensors=pt, padding=longest).input_values # Batch size 1\\\\n\\\\n# retrieve logits\\\\nlogits = model(input_values).logits\\\\n\\\\n# take argmax and decode\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'performance': {'dataset': 'librispeech_asr', 'accuracy': {'clean': 2.77, 'other': 7.08}}, 'description': \\\"Facebook's Data2Vec-Audio-Base-960h model is an Automatic Speech Recognition model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It can be used for transcribing audio files and achieves competitive performance on major benchmarks of speech recognition. The model is based on the Data2Vec framework which uses the same learning method for either speech, NLP, or computer vision.\\\"}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù…Ù„ØŒ ÙˆØ°Ù„Ùƒ Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠÙ„Ø²Ù…Ù†Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¶Ø¨Ø·Ù‡ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø².? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v1')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. First, import the SentenceTransformer class from the sentence-transformers library.', 'code': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nimport numpy as np\\\\n\\\\nquery = \\\"What do customers say about the hotel\\\\'s breakfast?\\\"\\\\nreviews = [...] # List of customer reviews\\\\n\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-MiniLM-L12-v1\\\\')\\\\nembeddings = model.encode([query] + reviews)\\\\nquery_embedding = embeddings[0]\\\\nreview_embeddings = embeddings[1:]\\\\n\\\\nsimilarities = cosine_similarity([query_embedding], review_embeddings)[0]\\\\ntop_n_indices = np.argsort(-similarities)[:n]\\\\ntop_n_reviews = [reviews[idx] for idx in top_n_indices]', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Sentence Transformers', 'api_name': 'sentence-transformers/all-MiniLM-L12-v1', 'api_call': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nimport numpy as np\\\\n\\\\nquery = \\\"What do customers say about the hotel\\\\'s breakfast?\\\"\\\\nreviews = [...] # List of customer reviews\\\\n\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-MiniLM-L12-v1\\\\')\\\\nembeddings = model.encode([query] + reviews)\\\\nquery_embedding = embeddings[0]\\\\nreview_embeddings = embeddings[1:]\\\\n\\\\nsimilarities = cosine_similarity([query_embedding], review_embeddings)[0]\\\\ntop_n_indices = np.argsort(-similarities)[:n]\\\\ntop_n_reviews = [reviews[idx] for idx in top_n_indices]', 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v1')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': [{'name': 'Sentence Embeddings Benchmark', 'url': 'https://seb.sbert.net'}], 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model that maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‡Ù„ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØ¹Ù„Ù… ØªØ¹Ø²ÙŠØ²ÙŠ ÙŠØ³ØªØ·ÙŠØ¹ ØªÙ‚Ø¯ÙŠÙ… ÙÙŠØ¯ÙŠÙˆ ÙŠØµÙˆØ± Ù…Ø´Ù‡Ø¯Ù‹Ø§ Ù„Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©ØŸ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø°Ù„Ùƒ.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"PPO.load_from_hub('araffin/ppo-LunarLander-v2', 'ppo-LunarLander-v2.zip')\\\", '<<<api_provider>>>': 'Stable-Baselines3', '<<<explanation>>>': '1. Import the required libraries load_from_hub from huggingface_sb3, PPO from stable_baselines3, make_vec_env from stable_baselines3.common.env_util and evaluate_policy from stable_baselines3.common.evaluation.\\\\n2. Load the trained model from the Hugging Face model hub using the load_from_hub function and the provided model name and file. The model is a PPO (Proximal Policy Optimization) trained on the LunarLander-v2 environment.\\\\n3. Pass the loaded model into PPO.load() method to create the PPO model for the LunarLander-v2 game.\\\\n4. Create the LunarLander-v2 environment using the make_vec_env function.\\\\n5. You can now use this model to make decisions in the LunarLander-v2 environment and beat your friend at the game.', '<<<code>>>': \\\"from huggingface_sb3 import load_from_hub\\\\nfrom stable_baselines3 import PPO\\\\nfrom stable_baselines3.common.env_util import make_vec_env\\\\ncheckpoint = load_from_hub('araffin/ppo-LunarLander-v2', 'ppo-LunarLander-v2.zip')\\\\nmodel = PPO.load(checkpoint)\\\\nenv = make_vec_env('LunarLander-v2', n_envs=1)\\\", 'api_call': \\\"PPO.load_from_hub('araffin/ppo-LunarLander-v2', 'ppo-LunarLander-v2.zip')\\\", 'provider': 'Stable-Baselines3', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'araffin/ppo-LunarLander-v2', 'api_call': \\\"PPO.load_from_hub('araffin/ppo-LunarLander-v2', 'ppo-LunarLander-v2.zip')\\\", 'api_arguments': {'checkpoint': 'araffin/ppo-LunarLander-v2.zip'}, 'python_environment_requirements': [['huggingface_sb3', 'stable_baselines3']], 'example_code': {'load_model': 'from huggingface_sb3 import load_from_hub\\\\ncheckpoint = load_from_hub(araffin/ppo-LunarLander-v2, ppo-LunarLander-v2.zip)\\\\nmodel = PPO.load(checkpoint)', 'evaluate': 'from stable_baselines3.common.env_util import make_vec_env\\\\nfrom stable_baselines3.common.evaluation import evaluate_policy\\\\nenv = make_vec_env(LunarLander-v2, n_envs=1)\\\\nmean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\\\\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})'}, 'performance': {'dataset': 'LunarLander-v2', 'accuracy': '283.49 +/- 13.74'}, 'description': 'This is a trained model of a PPO agent playing LunarLander-v2 using the stable-baselines3 library.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„ØµÙˆØªÙŠØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¨Ø¯Ù‚Ø© ÙˆØ³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"Wav2Vec2ForCTC.from_pretrained('nguyenvulebinh/wav2vec2-base-vietnamese-250h')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary libraries, like Wav2Vec2Processor and Wav2Vec2ForCTC from the transformers package for speech recognition. We should also import soundfile for audio file reading and torch for computation.\\\\n2. We then load the pre-trained Vietnamese speech recognition model using the 'nguyenvulebinh/wav2vec2-base-vietnamese-250h' identifier.\\\\n3. We read the Vietnamese audio file and convert it into a suitable format for further processing.\\\\n4. Having chosen the audio file, the model processes the audio and outputs a list of transcribed texts.\\\\n5. These transcriptions can then be used to compile an audio summary of the latest Vietnamese news.\\\", '<<<code>>>': \\\"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nimport soundfile as sf\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained('nguyenvulebinh/wav2vec2-base-vietnamese-250h')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained('nguyenvulebinh/wav2vec2-base-vietnamese-250h')\\\\n\\\\nspeech, _ = sf.read('your_audio_file.wav')\\\\n# replace 'your_audio_file.wav' with the path to your audio file\\\\ninput_values = processor(speech, return_tensors='pt').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('nguyenvulebinh/wav2vec2-base-vietnamese-250h')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nguyenvulebinh/wav2vec2-base-vietnamese-250h', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('nguyenvulebinh/wav2vec2-base-vietnamese-250h')\\\", 'api_arguments': ['file'], 'python_environment_requirements': [['transformers', 'datasets', 'soundfile', 'torch']], 'example_code': \\\"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nfrom datasets import load_dataset\\\\nimport soundfile as sf\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained(nguyenvulebinh/wav2vec2-base-vietnamese-250h)\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(nguyenvulebinh/wav2vec2-base-vietnamese-250h)\\\\n\\\\ndef map_to_array(batch):\\\\n speech, _ = sf.read(batch[file])\\\\n batch[speech] = speech\\\\n return batch\\\\n\\\\nds = map_to_array({\\\\n file: 'audio-test/t1_0001-00010.wav'\\\\n})\\\\n\\\\ninput_values = processor(ds[speech], return_tensors=pt, padding=longest).input_values\\\\n\\\\nlogits = model(input_values).logits\\\\n\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'performance': {'dataset': [{'name': 'VIVOS', 'accuracy': 6.15}, {'name': 'Common Voice vi', 'accuracy': 11.52}]}, 'description': 'Vietnamese end-to-end speech recognition using wav2vec 2.0. Pre-trained on 13k hours of Vietnamese youtube audio (un-label data) and fine-tuned on 250 hours labeled of VLSP ASR dataset on 16kHz sampled speech audio.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"SequenceTagger.load('flair/ner-english')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries from the flair package including Sentence and SequenceTagger.\\\\n2. Load the pre-trained model 'flair/ner-english' using the SequenceTagger.load method.\\\\n3. Create a Sentence object with the input text. The Sentence class is used for handling sentence-level operations, such as tokenization and named entity recognition.\\\\n4. Use the predict method of the SequenceTagger to identify the named entities in the input text.\\\\n5. Iterate over the identified entities and classify them as person, location, organization, or other, based on their detected tags.\\\", '<<<code>>>': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\ntagger = SequenceTagger.load('flair/ner-english')\\\\nsentence = Sentence('Input text here...')\\\\ntagger.predict(sentence)\\\\n\\\\nentities = {}\\\\nfor entity in sentence.get_spans('ner'):\\\\n    entities[entity.text] = entity.tag\\\\n\\\\nprint(entities)\\\", 'api_call': \\\"SequenceTagger.load('flair/ner-english')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'flair/ner-english', 'api_call': \\\"SequenceTagger.load('flair/ner-english')\\\", 'api_arguments': ['sentence'], 'python_environment_requirements': [['flair']], 'example_code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\n# load tagger\\\\ntagger = SequenceTagger.load('flair/ner-english')\\\\n\\\\n# make example sentence\\\\nsentence = Sentence('George Washington went to Washington')\\\\n\\\\n# predict NER tags\\\\ntagger.predict(sentence)\\\\n\\\\n# print sentence\\\\nprint(sentence)\\\\n\\\\n# print predicted NER spans\\\\nprint('The following NER tags are found:')\\\\n\\\\n# iterate over entities and print\\\\nfor entity in sentence.get_spans('ner'):\\\\n    print(entity)\\\", 'performance': {'dataset': 'conll2003', 'accuracy': '93.06'}, 'description': 'This is the standard 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯Ù‚Ø© ÙˆÙØ¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Feature Extraction', '<<<api_call>>>': \\\"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary modules from the transformers library, specifically AutoTokenizer and AutoModel.\\\\n2. Load the pre-trained 'dmis-lab/biobert-v1.1' model for feature extraction. This model is particularly suitable for handling biomedical texts since it has been specifically designed and trained for that purpose.\\\\n3. Tokenize the input medical text using the appropriate tokenizer, which can be obtained from the AutoTokenizer module.\\\\n4. Use the resulting tokenized input to feed the pre-trained model, which will output the text's features that can be used to represent the medical concepts and relationships present in the text.\\\", '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-v1.1')\\\\nmodel = AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\\\\ninput_text = 'Medical text content...'\\\\ninputs = tokenizer(input_text, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nfeatures = outputs.last_hidden_state\\\", 'api_call': \\\"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \\\"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"AutoModelForVideoClassification.from_pretrained('dangle124/videomae-base-finetuned-RealLifeViolenceSituations-subset')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary class, 'AutoModelForVideoClassification', from the transformers library.\\\\n2. Load the pre-trained model 'dangle124/videomae-base-finetuned-RealLifeViolenceSituations-subset'. This model is specifically tuned for detecting real-life violence in videos.\\\\n3. The loaded model can then be used to process video files or video streams.\\\\n4. As the model is trained to classify videos containing real-life violence events, it will be able to accurately identify whether the input video contains any violent situations or not.\\\", '<<<code>>>': \\\"from transformers import AutoModelForVideoClassification\\\\nmodel = AutoModelForVideoClassification.from_pretrained('dangle124/videomae-base-finetuned-RealLifeViolenceSituations-subset')\\\\n\\\\n# Code to load video, preprocess it and predict using the 'model' object should be added here.\\\", 'api_call': \\\"AutoModelForVideoClassification.from_pretrained('dangle124/videomae-base-finetuned-RealLifeViolenceSituations-subset')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'videomae-base-finetuned-RealLifeViolenceSituations-subset', 'api_call': \\\"AutoModelForVideoClassification.from_pretrained('dangle124/videomae-base-finetuned-RealLifeViolenceSituations-subset')\\\", 'api_arguments': {'model_name': 'dangle124/videomae-base-finetuned-RealLifeViolenceSituations-subset'}, 'python_environment_requirements': [{'transformers': '4.27.2', 'pytorch': '1.13.1', 'datasets': '2.10.1', 'tokenizers': '0.13.2'}], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': 0.9533}, 'description': 'This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It is trained for video classification task, specifically for RealLifeViolenceSituations.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ¹Ù„Ù‚ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØª Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù„Ù… Ø¢Ù„ÙŠ Ù„ØªØ¹Ø±ÙŠÙ ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ø£ØµÙˆØ§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·. Ø³ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØª ÙÙŠ Ø¨ÙŠØ¦Ø§Øª Ù…Ø®ØªÙ„ÙØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¯Ù‚Ø© ÙˆÙØ¹Ø§Ù„ÙŠØ© Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬.? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'https://path_to_image.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'https://path_to_image.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØ­Ø¯ÙŠØ¯ ØµÙˆØ± Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"ViTModel.from_pretrained('facebook/dino-vitb16')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We use the Vision Transformer (ViT) model 'facebook/dino-vitb16', which is pretrained on a large collection of images in a self-supervised fashion. This model can extract features from images to perform classification tasks.\\\\n2. We first import the necessary libraries: ViTFeatureExtractor from transformers, ViTModel from transformers, Image from PIL and requests.\\\\n3. We load an image from a URL and convert it to a PIL Image object.\\\\n4. We instantiate the ViTFeatureExtractor using the from_pretrained method with the 'facebook/dino-vitb16' model.\\\\n5. We also load the main ViTModel using the from_pretrained method with the 'facebook/dino-vitb16' model.\\\\n6. The feature extraction is performed by passing the image to the feature_extractor object, and this is then passed to the model, obtaining the last_hidden_state as the extracted image features.\\\\n7. These features can then be used for further classification tasks.\\\", '<<<code>>>': \\\"from transformers import ViTFeatureExtractor, ViTModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlast_hidden_states = outputs.last_hidden_state\\\", 'api_call': \\\"ViTModel.from_pretrained('facebook/dino-vitb16')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \\\"ViTModel.from_pretrained('facebook/dino-vitb16')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': [{'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}], 'example_code': \\\"from transformers import ViTFeatureExtractor, ViTModel\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlast_hidden_states = outputs.last_hidden_state\\\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© ÙˆÙ‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"pipeline('audio-classification', model='MIT/ast-finetuned-audioset-10-10-0.4593')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' function provided by the transformers library from Hugging Face.\\\\n2. Load the MIT/ast-finetuned-audioset-10-10-0.4593 model using the 'pipeline' function with 'audio-classification' as the task identifier. This model is fine-tuned on the AudioSet dataset and can perform audio classification tasks.\\\\n3. To detect bird sounds among the ambient noises, you can pass your audio recordings as spectrograms (essentially, an image representing the audio information) to the loaded model. The model will identify the presence of bird sounds within the input audio data.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\naudio_classifier = pipeline('audio-classification', model='MIT/ast-finetuned-audioset-10-10-0.4593')\\\\nresult = audio_classifier(audio_data_as_spectrogram)\\\", 'api_call': \\\"pipeline('audio-classification', model='MIT/ast-finetuned-audioset-10-10-0.4593')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ast-finetuned-audioset-10-10-0.4593', 'api_call': \\\"pipeline('audio-classification', model='MIT/ast-finetuned-audioset-10-10-0.4593')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'AudioSet', 'accuracy': ''}, 'description': 'Audio Spectrogram Transformer (AST) model fine-tuned on AudioSet. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ø¬Ø¯ÙˆÙ„ ØªØµÙ†ÙŠÙÙŠ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù…Ù‡Ù…Ø© ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙˆÙ‚Ù… Ø¨ØªØµÙ†ÙŠÙÙ‡Ø§ ÙˆÙÙ‚Ø§ Ù„Ù…Ø¹Ø§ÙŠÙŠØ± ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('mrm8488-bert2bert_shared-spanish-finetuned-summarization')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary libraries, which are 'torch' and 'transformers'. Make sure to import 'AutoTokenizer' and 'AutoModelForSeq2SeqLM' from the 'transformers' package.\\\", 'code': \\\"from torch import device, cuda\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nmodel_checkpoint = 'mrm8488/bert2bert_shared-spanish-finetuned-summarization'\\\\ndevice = 'cuda' if cuda.is_available() else 'cpu'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\\\\ntext = 'Texto en espaÃ±ol en el que necesitas resumir aquÃ­...'\\\\ninputs = tokenizer([text], padding=True, truncation=True, max_length=512, return_tensors=pt)\\\\ninput_ids = inputs.input_ids.to(device)\\\\nattention_mask = inputs.attention_mask.to(device)\\\\noutput = model.generate(input_ids, attention_mask=attention_mask)\\\\nsummary = tokenizer.decode(output[0], skip_special_tokens=True)\\\", 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Text2Text Generation', 'api_name': 'mrm8488/bert2bert_shared-spanish-finetuned-summarization', 'api_call': \\\"from torch import device, cuda\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nmodel_checkpoint = 'mrm8488/bert2bert_shared-spanish-finetuned-summarization'\\\\ndevice = 'cuda' if cuda.is_available() else 'cpu'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\\\\ntext = 'Texto en espaÃ±ol en el que necesitas resumir aquÃ­...'\\\\ninputs = tokenizer([text], padding=True, truncation=True, max_length=512, return_tensors=pt)\\\\ninput_ids = inputs.input_ids.to(device)\\\\nattention_mask = inputs.attention_mask.to(device)\\\\noutput = model.generate(input_ids, attention_mask=attention_mask)\\\\nsummary = tokenizer.decode(output[0], skip_special_tokens=True)\\\", 'api_arguments': ['text'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': 'def generate_summary(text):\\\\n  inputs = tokenizer([text], padding=max_length, truncation=True, max_length=512, return_tensors=pt)\\\\n  input_ids = inputs.input_ids.to(device)\\\\n  attention_mask = inputs.attention_mask.to(device)\\\\n  output = model.generate(input_ids, attention_mask=attention_mask)\\\\n  return tokenizer.decode(output[0], skip_special_tokens=True)\\\\ntext = Your text here...\\\\ngenerate_summary(text)', 'performance': {'dataset': 'mlsum', 'accuracy': {'Rouge1': 26.24, 'Rouge2': 8.9, 'RougeL': 21.01, 'RougeLsum': 21.02}}, 'description': 'Spanish BERT2BERT (BETO) fine-tuned on MLSUM ES for summarization'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù†ØµÙŠØ© Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. This includes TapasTokenizer and TapasForQuestionAnswering for tackling the table question answering task.\\\\n2. Use the from_pretrained method of the TapasTokenizer and TapasForQuestionAnswering classes to load the pre-trained model 'lysandre/tapas-temporary-repo'.\\\\n3. This model is capable of handling questions related to a table in a conversational setting, making it ideal for your startup's project of extracting relevant information based on a user's question.\\\\n4. Provide the table and the user's question to the model, and it will return the predicted answer coordinates and aggregation indices, which you can use to extract the desired information.\\\", '<<<code>>>': \\\"from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\ntokenizer = TapasTokenizer.from_pretrained('lysandre/tapas-temporary-repo')\\\\nmodel = TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\\ninputs = tokenizer(table=table, queries=question, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lysandre/tapas-temporary-repo', 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\", 'api_arguments': ['model_name', 'question', 'table'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"tokenizer = TapasTokenizer.from_pretrained('lysandre/tapas-temporary-repo')\\\\nmodel = TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\\ninputs = tokenizer(table=table, queries=question, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())\\\", 'performance': {'dataset': 'SQA', 'accuracy': 'Not provided'}, 'description': 'TAPAS base model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion and can be used for answering questions related to a table in a conversational set-up.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„ØµÙˆØ± Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒÙ… Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªØµÙ†ÙŠÙÙŠ Ù„Ù„ØµÙˆØ± ÙˆØ§Ù„ØµÙˆØ± Ø§Ù„ÙÙˆØªÙˆØºØ±Ø§ÙÙŠØ© Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙƒÙ….? [/INST]{'domain': 'Computer Vision Image Classification', 'api_call': \\\"timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimg = Image.open(urlopen('https://your-image-url-here'))\\\\nmodel = timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))\\\", 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification, Feature Map Extraction, Image Embeddings', 'api_name': 'convnext_base.fb_in1k', 'api_call': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimg = Image.open(urlopen('https://your-image-url-here'))\\\\nmodel = timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))\\\", 'api_arguments': {'pretrained': 'True', 'features_only': 'True', 'num_classes': '0'}, 'python_environment_requirements': ['timm'], 'example_code': ['from urllib.request import urlopen', 'from PIL import Image', 'import timm', \\\"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\\", \\\"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\", 'model = model.eval()', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'output = model(transforms(img).unsqueeze(0))'], 'performance': {'dataset': 'imagenet-1k', 'accuracy': '83.82%'}, 'description': 'A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…Ø´Ø±ÙˆØ· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ù„Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§ØªØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù„Ù„ØµÙˆØ± Ø§Ù„Ø­Ø§Ù„ÙŠØ©.? [/INST]{'domain': 'Computer Vision Object Detection', 'api_call': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'in_game_image.jpg'  # Replace this with the path to your image\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'in_game_image.jpg'  # Replace this with the path to your image\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \\\"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\\\"}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªÙˆÙ„ÙŠØ¯ ØªØ±Ø¬Ù…Ø© Ø¢Ù„ÙŠØ© Ù„Ù„Ù†ØµÙˆØµ Ù…Ù† Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes and methods from the transformers library, such as pipeline and the AutoModelForSeq2SeqLM.\\\\n2. Load the 'Helsinki-NLP/opus-mt-en-ru' model using the from_pretrained method of the AutoModelForSeq2SeqLM class. This model has been trained to translate English text to Russian.\\\\n3. Create a pipeline object using the pipeline function and specify 'translation_en_to_ru' as the task and provide the model obtained in the previous step.\\\\n4. Use the created pipeline object to translate English text on your website to Russian.\\\", '<<<code>>>': \\\"from transformers import pipeline, AutoModelForSeq2SeqLM\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\\ntranslation = pipeline('translation_en_to_ru', model=model)\\\\ntranslated_text = translation('Your English website content here')\\\", 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-en-ru', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers', 'torch', 'tensorflow']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ru', model='Helsinki-NLP/opus-mt-en-ru')\\\\ntranslated_text = translation('Hello, how are you?')\\\", 'performance': {'dataset': 'newstest2019-enru', 'accuracy': '27.1'}, 'description': 'Helsinki-NLP/opus-mt-en-ru is a translation model trained on the OPUS dataset, which translates English text to Russian. It is based on the Marian NMT framework and can be used with Hugging Face Transformers.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙˆØ§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To build the object detection application, we import the necessary packages, namely YolosFeatureExtractor and YolosForObjectDetection from the transformers library, Image from PIL, and requests.\\\\n2. We use the YolosFeatureExtractor.from_pretrained method to load the feature extractor for the 'hustvl/yolos-tiny' model.\\\\n3. We use the YolosForObjectDetection.from_pretrained method to create an instance of the object detection model with the same pretrained model name 'hustvl/yolos-tiny'.\\\\n4. Then, we open an image (loaded from an URL) and convert it to the appropriate format using the feature extractor.\\\\n5. We pass the processed input image to the model and obtain the outputs (logits and bounding boxes) which can be used to identify objects in the image.\\\", '<<<code>>>': \\\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\n\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\", 'api_call': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hustvl/yolos-tiny', 'api_call': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': \\\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '28.7 AP'}, 'description': 'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ·Ù„Ø¨ Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… ØªØ¬Ø³ÙŠØ± Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø­ÙˆØ§Ø³ÙŠØ¨ Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ù…ÙØµÙ„Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ø¨Ø¯ÙˆÙ† Ù‚ÙŠÙˆØ¯.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\\ngenerated_image = pipeline().images[0]\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'myunus1/diffmodels_galaxies_scratchbook', 'api_call': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\\ngenerated_image = pipeline().images[0]\\\", 'api_arguments': {'from_pretrained': 'myunus1/diffmodels_galaxies_scratchbook'}, 'python_environment_requirements': {'package': 'diffusers', 'import': 'from diffusers import DDPMPipeline'}, 'example_code': {'initialize_pipeline': \\\"pipeline = DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", 'generate_image': 'image = pipeline().images[0]', 'display_image': 'image'}, 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'This model is a diffusion model for unconditional image generation of cute ğŸ¦‹.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ¶Ù…ÙŠÙ† Ø­Ù‚Ù„ <mask> ÙÙŠ Ø§Ù„Ù†Øµ Ù„Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ù†ØµØ§Ø¦Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØªÙ‚Ø¯ÙŠÙ… Ø¬Ù…Ù„Ø© Ù…Ø¹Ù†ÙˆÙŠØ© Ù…ØªÙƒØ§Ù…Ù„Ø©.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We will first use the transformers library to create a pipeline to translate the Dutch text to English.', 'code': \\\"from transformers import pipeline\\\\ndutch_text = 'Hallo, Ik hoop dat alles goed gaat met jou en je team. We zijn erg blij met de vooruitgang van ons project en waarderen uw inspanningen. Laten we een vergadering plannen om onze volgende stappen te bespreken. Kunt u de komende week een geschikte tijd voorstellen? Met vriendelijke groet.'\\\\ntranslation_pipeline = pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\\ntranslated_text = translation_pipeline(dutch_text)[0]['translation_text']\\\", 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Translation', 'api_name': 'Helsinki-NLP/opus-mt-nl-en', 'api_call': \\\"from transformers import pipeline\\\\ndutch_text = 'Hallo, Ik hoop dat alles goed gaat met jou en je team. We zijn erg blij met de vooruitgang van ons project en waarderen uw inspanningen. Laten we een vergadering plannen om onze volgende stappen te bespreken. Kunt u de komende week een geschikte tijd voorstellen? Met vriendelijke groet.'\\\\ntranslation_pipeline = pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\\ntranslated_text = translation_pipeline(dutch_text)[0]['translation_text']\\\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\\ntranslated_text = translation('Hallo, hoe gaat het met je?')[0]['translation_text']\\\", 'performance': {'dataset': 'Tatoeba.nl.en', 'accuracy': {'BLEU': 60.9, 'chr-F': 0.749}}, 'description': 'A Dutch to English translation model based on the OPUS dataset, using a transformer-align architecture with normalization and SentencePiece pre-processing.'}} </s>\"\n"}
{"text": "<s>[INST]Ø§Ø³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø¬ÙˆØ§Ø¨ Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø¹Ù† Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ù€ \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"TapasTokenizer.from_pretrained('google/tapas-base-finetuned-sqa')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\ntapas_pipeline = pipeline('table-question-answering', model='google/tapas-base-finetuned-sqa')\\\\nquestion = 'How many goals did player A score?'\\\\ntable = [['Player', 'Goals'], ['Player A', 5], ['Player B', 3]]\\\\nresult = tapas_pipeline(question=question, table=table)\\\", 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-base-finetuned-sqa', 'api_call': \\\"from transformers import pipeline\\\\ntapas_pipeline = pipeline('table-question-answering', model='google/tapas-base-finetuned-sqa')\\\\nquestion = 'How many goals did player A score?'\\\\ntable = [['Player', 'Goals'], ['Player A', 5], ['Player B', 3]]\\\\nresult = tapas_pipeline(question=question, table=table)\\\", 'api_arguments': ['question', 'table'], 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntapas_pipeline = pipeline('table-question-answering', model='google/tapas-base-finetuned-sqa')\\\\n\\\\n# Define the table and question\\\\nquestion = 'How many goals did player A score?'\\\\ntable = [['Player', 'Goals'], ['Player A', 5], ['Player B', 3]]\\\\n\\\\n# Get the answer\\\\nresult = tapas_pipeline(question=question, table=table)\\\\nprint(result)\\\", 'performance': {'dataset': 'msr_sqa', 'accuracy': 0.6874}, 'description': 'TAPAS base model fine-tuned on Sequential Question Answering (SQA). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia and fine-tuned on SQA. It can be used for answering questions related to a table in a conversational set-up.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙ†ÙÙŠØ° Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"pipeline('text-classification', model='roberta-base-openai-detector')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We first import the necessary libraries, which are 'pipeline' from transformers.\\\\n2. We then use the 'pipeline' function to load the model, which in this case is 'roberta-base-openai-detector'. This model has been fine-tuned to detect GPT-2 generated text.\\\\n3. The loaded model will be used for text classification, where it will identify whether the given text is influenced by AI-generated content.\\\\n4. The model will evaluate the text and return either positive or negative, depending on whether the content seems to be AI-generated or not.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'text-classification\\\\', model=\\\\'roberta-base-openai-detector\\\\')\\\\narticle_text = \\\"Text from the news article...\\\"\\\\nresult = classifier(article_text)', 'api_call': \\\"pipeline('text-classification', model='roberta-base-openai-detector')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'roberta-base-openai-detector', 'api_call': \\\"pipeline('text-classification', model='roberta-base-openai-detector')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\npipe = pipeline(text-classification, model=roberta-base-openai-detector)\\\\nprint(pipe(Hello world! Is this content AI-generated?))', 'performance': {'dataset': 'WebText', 'accuracy': '95%'}, 'description': 'RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¨ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¥Ù„Ù‰ Ù†Øµ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': 'TF_Decision_Trees(input_features, target)', '<<<api_provider>>>': 'Keras', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': 'TF_Decision_Trees(input_features, target)', 'provider': 'Keras', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'TF_Decision_Trees', 'api_call': 'TF_Decision_Trees(input_features, target)', 'api_arguments': ['input_features', 'target'], 'python_environment_requirements': [['tensorflow >= 7.0']], 'example_code': 'https://github.com/tdubon/TF-GB-Forest/blob/c0cf4c7e3e29d819b996cfe4eecc1f2728115e52/TFDecisionTrees_Final.ipynb', 'performance': {'dataset': 'Census-Income Data Set', 'accuracy': 96.57}, 'description': \\\"Use TensorFlow's Gradient Boosted Trees model in binary classification of structured data. Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is trained on the US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables. The task is to determine whether a person makes over 50k a year.\\\" </s>\"\n"}
{"text": "<s>[INST]ÙŠØ±ÙƒØ² Ø§Ø¨Ø­Ø§Ø«Ù†Ø§ Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ Ù„ØªÙ‚Ù†ÙŠØ§Øª ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ ØµÙˆØª ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Audio Audio-to-Audio', 'api_call': \\\"pipeline('translation', model='facebook/xm_transformer_sm_all-en')\\\", 'api_provider': 'Fairseq', 'explanation': '1. Import the pipeline function from the transformers library.', 'code': 'from transformers import pipeline\\\\ntranslation_pipeline = pipeline(\\\\'translation\\\\', model=\\\\'facebook/xm_transformer_sm_all-en\\\\')\\\\ninput_sentence = \\\"Hello, how are you?\\\"\\\\ntarget_language = \\\"fr\\\" # French\\\\ntranslated_sentence = translation_pipeline(f\\\"{input_sentence} <text_to_translate> {target_language}\\\")', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Fairseq', 'functionality': 'speech-to-speech-translation', 'api_name': 'facebook/xm_transformer_sm_all-en', 'api_call': 'from transformers import pipeline\\\\ntranslation_pipeline = pipeline(\\\\'translation\\\\', model=\\\\'facebook/xm_transformer_sm_all-en\\\\')\\\\ninput_sentence = \\\"Hello, how are you?\\\"\\\\ntarget_language = \\\"fr\\\" # French\\\\ntranslated_sentence = translation_pipeline(f\\\"{input_sentence} <text_to_translate> {target_language}\\\")', 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \\\"translation_pipeline('Hello, how are you?')\\\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A speech-to-speech translation model that can be loaded on the Inference API on-demand.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆ Ù„ØªØ­ÙˆÙŠÙ„ ØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ Ù…Ø­Ø³Ù†. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary packages, including transformers and torch.\\\\n2. Load the 'lllyasviel/sd-controlnet-seg' pre-trained ControlNetModel from Hugging Face, which is trained for image segmentation tasks that can help in understanding images within your platformer game.\\\\n3. Preprocess the image data obtained from the game according to the model's requirements (e.g., resizing, normalization).\\\\n4. Feed the preprocessed image data into the trained ControlNetModel to generate the segmented image.\\\\n5. Use the segmented output image to better understand the game environment.\\\", '<<<code>>>': \\\"from transformers import ControlNetModel\\\\nimport torch\\\\n\\\\nmodel = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\\npreprocessed_image = preprocess_game_image(game_image) # Replace with a function to preprocess your game images\\\\nseg_output = model(preprocessed_image.to(torch_dtype))\\\", 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/sd-controlnet-seg', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", 'api_arguments': ['torch_dtype'], 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate']], 'example_code': \\\"image = pipe(house, image, num_inference_steps=20).images[0]\\\\nimage.save('./images/house_seg_out.png')\\\", 'performance': {'dataset': 'ADE20K', 'accuracy': 'Trained on 164K segmentation-image, caption pairs'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ø³ØªØ®Ø¯Ù… ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§ØªÙ†Ø§ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø¬ÙˆØ¯Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©. ÙŠÙ…ÙƒÙ†Ùƒ ØªÙƒØ§Ù…Ù„ Ø§Ù„ØµÙˆØª ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ø¹Ø¨Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù€ API ÙˆØ¥Ø¹Ø·Ø§Ø¡ ØªØ¬Ø±Ø¨Ø© ØµÙˆØªÙŠØ© Ø±Ø§Ø¦Ø¹Ø© Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙƒ.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/bert-base-cased-squad2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. We use the AutoModelForQuestionAnswering class provided by the transformers library and load the pre-trained model 'deepset/bert-base-cased-squad2'. This model is designed for question answering tasks and can provide relevant answers to users' questions.\\\", 'code': \\\"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained('deepset/bert-base-cased-squad2')\\\\ntokenizer = AutoTokenizer.from_pretrained('deepset/bert-base-cased-squad2')\\\\nqa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\\\\ncontext = 'The restaurant is located at 123 Main Street, offers Italian cuisine, and accepts reservations online.'\\\\nquestion = 'Where is the restaurant located?'\\\\nanswer = qa_pipeline({'context': context, 'question': question})\\\\nanswer_text = answer['answer']\\\", 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'deepset/bert-base-cased-squad2', 'api_call': \\\"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained('deepset/bert-base-cased-squad2')\\\\ntokenizer = AutoTokenizer.from_pretrained('deepset/bert-base-cased-squad2')\\\\nqa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\\\\ncontext = 'The restaurant is located at 123 Main Street, offers Italian cuisine, and accepts reservations online.'\\\\nquestion = 'Where is the restaurant located?'\\\\nanswer = qa_pipeline({'context': context, 'question': question})\\\\nanswer_text = answer['answer']\\\", 'api_arguments': 'context, question', 'python_environment_requirements': 'transformers', 'example_code': \\\"qa_pipeline({'context': 'This model can be loaded on the Inference API on-demand.', 'question': 'Where can this model be loaded?'})\\\", 'performance': {'dataset': 'squad_v2', 'accuracy': {'exact_match': 71.152, 'f1': 74.671}}, 'description': 'This is a BERT base cased model trained on SQuAD v2'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªÙ†ÙÙŠØ° Ù†Ù…ÙˆØ°Ø¬ ØªÙƒÙˆÙŠÙ† ØµÙˆØ± Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ØªÙ‚Ù†ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ø­ÙŠØ« ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ÙˆØ§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± ØªØ§Ù…Ø© Ø§Ù„Ø´ÙƒÙ„ Ø¨Ø´ÙƒÙ„ Ø¢Ù„ÙŠ Ø¨Ø¯ÙˆÙ† Ø¥Ø´Ø§Ø±Ø§Øª Ø¯Ù„Ø§Ù„ÙŠØ©ØŒ Ø«Ù… Ù‚Ù… Ø¨ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø­ÙŠØ« Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØµÙˆÙŠØ± ÙˆØ§Ù„Ø§Ù…ØªÙŠØ§Ø² ÙÙŠ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØµÙˆØ±.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary DDPMPipeline class from the diffusers package to work with unconditional image generation models.\\\\n2. We then use the from_pretrained method of the DDPMPipeline class to load the pre-trained model 'myunus1/diffmodels_galaxies_scratchbook'. This model has been trained for unconditional image synthesis tasks, with a focus on generating galaxy-like shapes.\\\\n3. Once the model is loaded, we can use it to generate an image of a galaxy-like shape, which can be used by the astrophysicist for their research project.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\\nimage = pipeline().images[0]\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'myunus1/diffmodels_galaxies_scratchbook', 'api_call': \\\"DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", 'api_arguments': {'from_pretrained': 'myunus1/diffmodels_galaxies_scratchbook'}, 'python_environment_requirements': [{'package': 'diffusers', 'import': 'from diffusers import DDPMPipeline'}], 'example_code': {'initialize_pipeline': \\\"pipeline = DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", 'generate_image': 'image = pipeline().images[0]', 'display_image': 'image'}, 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'This model is a diffusion model for unconditional image generation of cute ğŸ¦‹.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Then, we use the pipeline function to create a sentiment analysis model, and we specify the model 'ProsusAI/finbert' to be loaded. This model has been fine-tuned for financial sentiment classification tasks, making it suitable for analyzing the sentiment of a cryptocurrency news article.\\\\n3. We can now use the loaded model to analyze the sentiment of the given news article. The model will classify the sentiment as 'positive', 'negative', or 'neutral'.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'sentiment-analysis\\\\', model=\\\\'ProsusAI/finbert\\\\')\\\\nresult = classifier(\\\"Bitcoin has recently seen an impressive surge in value, leading to a renewed interest in cryptocurrencies. This has created a lot of excitement among investors, with many predicting a profitable future. However, the market remains volatile, and it would be wise for investors to approach with caution, as sudden fluctuations can result in significant losses.\\\")', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ProsusAI/finbert', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline; classifier = pipeline('sentiment-analysis', model='ProsusAI/finbert'); classifier('your_text_here')\\\", 'performance': {'dataset': 'Financial PhraseBank', 'accuracy': 'Not provided'}, 'description': 'FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ù†ØµÙˆØµ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙØ§Ø¹Ù„ Ø¨ÙŠÙ† Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ù†ØµÙˆØµØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„ØªÙØ§Ø¹Ù„.? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary libraries, such as TrOCRProcessor and VisionEncoderDecoderModel from transformers, Image from PIL, requests, and torch.', 'code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\nimport torch\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\\\n\\\\npixel_values = processor(image, return_tensors='pt').pixel_values\\\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\\\", 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-stage1', 'api_call': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\nimport torch\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\\\n\\\\npixel_values = processor(image, return_tensors='pt').pixel_values\\\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\\\", 'api_arguments': {'url': 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'torch'], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\nimport torch\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\\\n\\\\npixel_values = processor(image, return_tensors='pt').pixel_values\\\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\\\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ù…Ù‡ØªÙ…ÙˆÙ† Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø¹Ù…Ù‚ Ø§Ù„ØµÙˆØ± ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­ÙˆØ§Ø³ÙŠØ¨. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙ†Ø§ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries by importing the pipeline function from the transformers library.\\\\n2. Use the pipeline function to create a depth estimation model, specifying 'sayakpaul/glpn-nyu-finetuned-diode-221221-102136' as the model to be loaded.\\\\n3. The loaded model will estimate depth values for a given input image.\\\\n4. The robot's camera system can use the output from this model to identify the distance between the robot and objects on the floor.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ndepth_estimator = pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\\\\ndepth_map = depth_estimator(image)\\\", 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221221-102136', 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\\\", 'api_arguments': [], 'python_environment_requirements': [['Transformers 4.24.0', 'Pytorch 1.12.1+cu116', 'Datasets 2.8.0', 'Tokenizers 0.13.2']], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4222, 'Mae': 0.411, 'Rmse': 0.6292, 'Abs Rel': 0.3778, 'Log Mae': 0.1636, 'Log Rmse': 0.224, 'Delta1': 0.432, 'Delta2': 0.6806, 'Delta3': 0.8068}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]{'domain': 'Multimodal Text-to-Image', 'api_call': \\\"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\ntext_to_image = pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\\\\ngenerated_image = text_to_image('analog style landscape')\\\", 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \\\"from transformers import pipeline\\\\ntext_to_image = pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\\\\ngenerated_image = text_to_image('analog style landscape')\\\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \\\"text_to_image('analog style landscape')\\\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \\\"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\\\"}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ ØªØµÙ†ÙŠÙÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù†ØµÙŠØ© ÙˆØ§Ù„ØµÙˆØ± ÙˆÙ‚Ù… Ø¨ØªØµÙ†ÙŠÙ ÙƒÙ„ Ø¹Ù†ØµØ± ØªØ¨Ø¹Ø§Ù‹ Ù„Ù„ÙˆØ³ÙŠØ· Ø§Ù„Ù…Ø¹Ø¨Ø± Ø§Ù„Ù…Ù†Ø§Ø³Ø¨.? [/INST]{'domain': 'Multimodal Text-to-Image', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion', torch_dtype=torch.float16)\\\", 'api_provider': 'Hugging Face', 'explanation': '1. We first import the required class and tools from the diffusers package and torch.', 'code': \\\"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\npipe = StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion', torch_dtype=torch.float16)\\\\npipe = pipe.to('cuda')\\\\nprompt = 'archer arcane style magical princess with golden hair'\\\\nimage = pipe(prompt).images[0]\\\\nimage.save('./magical_princess.png')\\\", 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \\\"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\npipe = StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion', torch_dtype=torch.float16)\\\\npipe = pipe.to('cuda')\\\\nprompt = 'archer arcane style magical princess with golden hair'\\\\nimage = pipe(prompt).images[0]\\\\nimage.save('./magical_princess.png')\\\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = nitrosocke/nitro-diffusion\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = archer arcane style magical princess with golden hair\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ù„ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ù„Ø§ Ø´Ø±Ø·ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('ddpm-celebahq-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. First, install the necessary diffusers package using \\\"!pip install diffusers\\\".\\\\n2. Import the DDPMPipeline class from the diffusers library.\\\\n3. Use the DDPMPipeline.from_pretrained method to load the pre-trained model \\\\'google/ddpm-celebahq-256\\\\', which has been trained on high-quality human face images.\\\\n4. Call the loaded model to generate a human-like face image.\\\\n5. Save the generated image as \\\"movie_poster.png\\\" to use it as the main movie poster background.', '<<<code>>>': \\\"!pip install diffusers\\\\nfrom diffusers import DDPMPipeline\\\\nmodel_id = 'google/ddpm-celebahq-256'\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().sample\\\\nimage[0].save('movie_poster.png')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('ddpm-celebahq-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/ddpm-celebahq-256', 'api_call': \\\"DDPMPipeline.from_pretrained('ddpm-celebahq-256')\\\", 'api_arguments': ['model_id'], 'python_environment_requirements': [['diffusers']], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-celebahq-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm()[sample]\\\\nimage[0].save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙ†ÙÙŠØ° Ù‡Ø°Ø§ Ø§Ù„Ù…Ù‡Ù…Ø©ØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-large-seq2seq')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are AutoTokenizer and AutoModelForSeq2SeqLM from the transformers library.\\\\n2. Load the GODEL-v1_1-large-seq2seq model and its tokenizer using the AutoModelForSeq2SeqLM.from_pretrained and AutoTokenizer.from_pretrained functions respectively.\\\\n3. Define a function called 'generate' which takes instruction, knowledge, and dialog as input. The function should combine the dialog using 'EOS' as a separator and create a query with the instruction, knowledge, and dialog.\\\\n4. Convert the query into input_ids using the tokenizer.\\\\n5. Generate a response using the model's 'generate' function, specifying the desired output length and sampling parameters.\\\\n6. Decode the output ids using the tokenizer to get the final empathic response.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'microsoft/GODEL-v1_1-large-seq2seq\\\\')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'microsoft/GODEL-v1_1-large-seq2seq\\\\')\\\\n\\\\ndef generate(instruction, knowledge, dialog):\\\\n    if knowledge != \\\\'\\\\':\\\\n        knowledge = \\\\'[KNOWLEDGE] \\\\' + knowledge\\\\n    dialog = \\\\' EOS \\\\'.join(dialog)\\\\n    query = f\\\"{instruction} [CONTEXT] {dialog} {knowledge}\\\"\\\\n    input_ids = tokenizer(query, return_tensors=\\\\'pt\\\\').input_ids\\\\n    outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n    return output\\\\n\\\\ninstruction = \\\"Instruction: given a dialog context, you need to response empathically.\\\"\\\\nknowledge = \\\"\\\"\\\\ndialog = [\\\\n    \\\\'Does money buy happiness?\\\\',\\\\n    \\\\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\\\',\\\\n    \\\\'What is the best way to buy happiness ?\\\\'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-large-seq2seq')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/GODEL-v1_1-large-seq2seq', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-large-seq2seq')\\\", 'api_arguments': {'instruction': 'Instruction: given a dialog context, you need to response empathically.', 'knowledge': '', 'dialog': ['Does money buy happiness?', 'It is a question. Money buys you a lot of things, but not enough to buy happiness.', 'What is the best way to buy happiness ?']}, 'python_environment_requirements': [{'transformers': 'AutoTokenizer, AutoModelForSeq2SeqLM'}], 'example_code': \\\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != '':\\\\n knowledge = '[KNOWLEDGE] ' + knowledge\\\\n dialog = ' EOS '.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f'Instruction: given a dialog context, you need to response empathically.'\\\\nknowledge = ''\\\\ndialog = [\\\\n 'Does money buy happiness?',\\\\n 'It is a question. Money buys you a lot of things, but not enough to buy happiness.',\\\\n 'What is the best way to buy happiness ?'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\\\", 'performance': {'dataset': 'Reddit discussion thread, instruction and knowledge grounded dialogs', 'accuracy': 'Not provided'}, 'description': 'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ø²ÙŠØ²ÙŠ Ø§Ù„Ø·Ø§Ù„Ø¨ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ØªØ¹Ù„Ù… Ø¢Ù„ÙŠ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨ØµÙˆØª Ù…Ø³Ù…ÙˆØ¹. ÙŠØ¬ØªÙ…Ø¹ Ù†Ø§Ø¯ÙŠÙ†Ø§ Ø§Ù„Ø·Ù„Ø§Ø¨ÙŠ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø¸Ø§Ù… \\\"Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù…\\\" Ø§Ù„Ø°ÙŠ Ù‚Ù…Øª Ø¨Ø¥Ù†Ø´Ø§Ø¦Ù‡. Ø³ØªÙ‚ÙˆÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù„Ù„ØªÙ†Ø§ÙØ³ ÙÙŠ Ù…Ø³Ø§Ø¨Ù‚Ø© \\\"ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©\\\". Ù†ØªÙ…Ù†Ù‰ Ù„Ùƒ Ø§Ù„ØªÙˆÙÙŠÙ‚!? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", '<<<api_provider>>>': 'speechbrain', '<<<explanation>>>': \\\"1. Import the necessary libraries and modules, including 'speechbrain', 'torchaudio', 'Tacotron2', and 'HIFIGAN'.\\\\n2. Use the 'from_hparams' method of the Tacotron2 class to load the pre-trained Tacotron2 model with a German source 'padmalcom/tts-tacotron2-german'.\\\\n3. Use the 'from_hparams' method of the HIFIGAN class to load the pre-trained HiFIGAN vocoder with a German dataset under the source 'padmalcom/tts-hifigan-german'.\\\\n4. Encode the factory tour script text using the Tacotron2 model to obtain the mel spectrogram (mel_output, mel_length, and alignment).\\\\n5. Decode the mel spectrogram using the HiFIGAN vocoder to generate the speech waveforms.\\\\n6. Save the generated audio to a .wav file using 'torchaudio.save'.\\\", '<<<code>>>': 'import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2\\\\nfrom speechbrain.pretrained import HIFIGAN\\\\n\\\\ntacotron2 = Tacotron2.from_hparams(source=\\\\'padmalcom/tts-tacotron2-german\\\\')\\\\nhifi_gan = HIFIGAN.from_hparams(source=\\\\'padmalcom/tts-hifigan-german\\\\')\\\\n\\\\ntour_script = \\\"Factory tour script in German...\\\"\\\\nmel_output, mel_length, alignment = tacotron2.encode_text(tour_script)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\n\\\\ntorchaudio.save(\\\\'factory_tour_audio.wav\\\\', waveforms.squeeze(1), 22050)', 'api_call': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", 'provider': 'speechbrain', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tts-hifigan-german', 'api_call': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", 'api_arguments': ['mel_output'], 'python_environment_requirements': [['speechbrain']], 'example_code': \\\"import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2\\\\nfrom speechbrain.pretrained import HIFIGAN\\\\ntacotron2 = Tacotron2.from_hparams(source=padmalcom/tts-tacotron2-german, savedir=tmpdir_tts)\\\\nhifi_gan = HIFIGAN.from_hparams(source=padmalcom/tts-hifigan-german, savedir=tmpdir_vocoder)\\\\nmel_output, mel_length, alignment = tacotron2.encode_text(Mary had a little lamb)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\\\", 'performance': {'dataset': 'custom German dataset', 'accuracy': 'Not specified'}, 'description': 'A HiFIGAN vocoder trained on a generated German dataset using mp3_to_training_data. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ Ø­ÙŠØ« Ù†Ø±ØºØ¨ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ ÙØ¦Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers and PIL packages. This includes SwinForImageClassification for the image classification model and Image for processing image data.\\\\n2. We then use the from_pretrained method of the SwinForImageClassification class to load the pre-trained model 'microsoft/swin-tiny-patch4-window7-224'. This model has been trained on a large dataset (ImageNet-1k) and can be useful for scene classification tasks.\\\\n3. We load the image data from a file or a set of consecutive image frames from a TV show.\\\\n4. This model can then be used to analyze an image and classify the scene content in it.\\\", '<<<code>>>': \\\"from transformers import SwinForImageClassification\\\\nfrom PIL import Image\\\\nimage = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with path to your image\\\\nmodel = SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'api_call': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/swin-tiny-patch4-window7-224', 'api_call': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': [{'transformers': 'AutoFeatureExtractor', 'PIL': 'Image', 'requests': 'requests'}], 'example_code': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø¬ÙˆØ¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ÙŠØ³ØªØ·ÙŠØ¹ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ ÙˆÙˆØ§Ø¶Ø­.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-es-css10')\\\", '<<<api_provider>>>': 'Fairseq', '<<<explanation>>>': \\\"1. We first import the necessary libraries, such as fairseq, IPython and the methods load_model_ensemble_and_task_from_hf_hub, TTSHubInterface from fairseq.\\\\n2. We then use the load_model_ensemble_and_task_from_hf_hub method to load the pre-trained model 'facebook/tts_transformer-es-css10'. This model has been trained for text-to-speech tasks in Spanish.\\\\n3. This model can then be used to convert the text of the article (in Spanish) into synthesized speech. The synthesized audio is then played using IPython.display.Audio.\\\", '<<<code>>>': \\\"from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n    'facebook/tts_transformer-es-css10',\\\\n    arg_overrides={'vocoder': 'hifigan', 'fp16': False}\\\\n)\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\n\\\\ntext = 'Hola, esta es una prueba.'\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)\\\", 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-es-css10')\\\", 'provider': 'Fairseq', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/tts_transformer-es-css10', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-es-css10')\\\", 'api_arguments': {'arg_overrides': {'vocoder': 'hifigan', 'fp16': False}}, 'python_environment_requirements': [['fairseq', 'IPython']], 'example_code': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n facebook/tts_transformer-es-css10,\\\\n arg_overrides={vocoder: hifigan, fp16: False}\\\\n)\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\ntext = Hola, esta es una prueba.\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)', 'performance': {'dataset': 'CSS10', 'accuracy': None}, 'description': 'Transformer text-to-speech model from fairseq S^2. Spanish single-speaker male voice trained on CSS10.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… ÙØ±ÙŠÙ‚Ù†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ ÙˆØªÙ‚Ù†ÙŠØ§Øª Ù„ÙØµÙ„ Ø§Ù„Ø£Ø¬Ø³Ø§Ù… ÙÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ù„ØªÙ‚Ø·Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='Filosofas/DialoGPT-medium-PALPATINE2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create a text generation pipeline using the 'text-generation' and the  pre-trained DialoGPT model 'Filosofas/DialoGPT-medium-PALPATINE2'. This model is specifically designed to hold conversational interactions which makes it a good choice for building a chatbot.\\\\n3. Now, you will have a conversational agent initialized that can provide human-like conversational responses based on user input.\\\\n4. Call the pipeline with user input to generate the chatbot's response.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nchatbot = pipeline(\\\\'text-generation\\\\', model=\\\\'Filosofas/DialoGPT-medium-PALPATINE2\\\\')\\\\nuser_input = \\\"I\\\\'m feeling really stressed and overwhelmed.\\\"\\\\nresponse = chatbot(user_input)[0][\\\\'generated_text\\\\']\\\\nprint(response)', 'api_call': \\\"pipeline('text-generation', model='Filosofas/DialoGPT-medium-PALPATINE2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DialoGPT-medium-PALPATINE2', 'api_call': \\\"pipeline('text-generation', model='Filosofas/DialoGPT-medium-PALPATINE2')\\\", 'api_arguments': 'message', 'python_environment_requirements': ['transformers'], 'example_code': \\\"api('Input a message to start chatting with Filosofas/DialoGPT-medium-PALPATINE2.')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A DialoGPT model trained for generating human-like conversational responses.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙŠØ³Ø§Ø¹Ø¯ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…ÙŠØ²Ø© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø¥Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ù„ÙŠØªÙ… Ø§Ù„Ø±Ø¯ Ø¹Ù„ÙŠÙ‡Ø§ Ù…Ù† Ù‚Ø¨Ù„ Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø§Ù„Ù†Øµ.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline method to set the task to 'question-answering' and load the pre-trained model 'xhyi/layoutlmv3_docvqa_t11c5000'.\\\\n3. This model has been trained on financial documents and can answer questions based on those documents.\\\\n4. Once the model is created, you can query the document by passing the context (the document text) and the question you want to be answered.\\\\n5. The model will return an answer for the query, which can be helpful in finding the dividends paid by the company 'RenewX'.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\'question-answering\\\\', model=\\\\'xhyi/layoutlmv3_docvqa_t11c5000\\\\')\\\\nquestion = \\\"What are the dividends paid by RenewX?\\\"\\\\ncontext = \\\"Financial document text here...\\\"\\\\nresult = qa_pipeline({\\\"question\\\": question, \\\"context\\\": context})', 'api_call': \\\"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'xhyi/layoutlmv3_docvqa_t11c5000', 'api_call': \\\"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\\\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'LayoutLMv3 model trained for document question answering task.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø§Ù„Ø´Ø±ÙƒØ© Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠØ³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ.? [/INST]{'domain': 'Computer Vision Depth Estimation', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import AutoModel\\\\nfrom PIL import Image\\\\nimport torch\\\\n\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\\\\nimage = Image.open('image_path.jpg') # replace 'image_path.jpg' with path to your image\\\\ninput_tensor = torch.tensor(image).unsqueeze(0) # Convert image to tensor and add batch dimension\\\\ndepth_estimate = model(input_tensor)\\\", 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-014502', 'api_call': \\\"from transformers import AutoModel\\\\nfrom PIL import Image\\\\nimport torch\\\\n\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\\\\nimage = Image.open('image_path.jpg') # replace 'image_path.jpg' with path to your image\\\\ninput_tensor = torch.tensor(image).unsqueeze(0) # Convert image to tensor and add batch dimension\\\\ndepth_estimate = model(input_tensor)\\\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3476, 'Mae': 0.2763, 'Rmse': 0.4088, 'Abs Rel': 0.3308, 'Log Mae': 0.1161, 'Log Rmse': 0.17, 'Delta1': 0.5682, 'Delta2': 0.8301, 'Delta3': 0.9279}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù„ØªØµÙ†ÙŠÙ Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø¨ØµØ±ÙŠØ© Ù„Ù„Ø­Ø§Ø³ÙˆØ¨. Ø¨Ø¥Ù…ÙƒØ§Ù†Ùƒ ØªØ­Ø¯ÙŠØ¯ Ù„ØºØ© Ø§Ù„Ù†Øµ ÙˆØ§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ø¨Ø±Ø© ÙˆØ§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù†Ø·Ù‚.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the required classes from the transformers package, including VideoMAEFeatureExtractor and VideoMAEForPreTraining. Import numpy for handling input arrays and torch for tensors.\\\\n2. Then, use the from_pretrained method of the VideoMAEForPreTraining class to load the pre-trained model 'MCG-NJU/videomae-base-short-ssv2'. This model is designed for video classification tasks and will be used to classify the type of exercise in a given video.\\\\n3. Extract the features using VideoMAEFeatureExtractor class and prepare the video for input to the model.\\\\n4. Using the model, classify the video into different categories corresponding to the type of exercise being performed.\\\", '<<<code>>>': \\\"from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = load_video_data('exercise_video.mp4') \\\\n# Replace 'load_video_data' function with appropriate way to read video data.\\\\n\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\n\\\\npixel_values = feature_extractor(video, return_tensors='pt').pixel_values\\\\n\\\\nnum_frames = len(video)\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\n\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss\\\", 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'videomae-base-ssv2', 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\", 'api_arguments': 'video', 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\\\npixel_values = feature_extractor(video, return_tensors=pt).pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss', 'performance': {'dataset': 'Something-Something-v2', 'accuracy': ''}, 'description': 'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ù„ÙØ¦Ø© ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨.? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import AutoModelForVideoClassification\\\\nmodel = AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb')\\\\n# Preprocessing video(s) and extracting features is not shown in this example.\\\", 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb', 'api_call': \\\"from transformers import AutoModelForVideoClassification\\\\nmodel = AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb')\\\\n# Preprocessing video(s) and extracting features is not shown in this example.\\\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch, tokenizers, datasets', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': 0.7298}, 'description': 'This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.5482, Accuracy: 0.7298.'}} </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='xlm-roberta-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a masked language modeling model using the 'xlm-roberta-base' model. This model is pre-trained on 100 languages and can generate suitable replacements for masked words without fine-tuning.\\\\n3. Create a unmasker object that can be used to fill in masked words in text inputs.\\\\n4. The unmasker can be utilized in a conversational AI bot whenever a masked word is detected in the input sentence.\\\\n5. Let the AI bot use the unmasker object to complete the sentences by replacing the masked words with suitable choices.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'xlm-roberta-base\\\\')\\\\nunmasked_result = unmasker(\\\"Hello, I\\\\'m a <mask> model.\\\")', 'api_call': \\\"pipeline('fill-mask', model='xlm-roberta-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'xlm-roberta-base', 'api_call': \\\"pipeline('fill-mask', model='xlm-roberta-base')\\\", 'api_arguments': {'model': 'xlm-roberta-base'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='xlm-roberta-base')\\\\nunmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': 'CommonCrawl', 'accuracy': 'N/A'}, 'description': 'XLM-RoBERTa is a multilingual version of RoBERTa pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It can be used for masked language modeling and is intended to be fine-tuned on a downstream task.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ØªØ¹Ø±Ù ØµÙˆØªÙŠ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ù„ÙØ¸ÙŠ Ø¥Ù„Ù‰ Ù†Øµ Ù…ÙƒØªÙˆØ¨ØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ¯Ù‚Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ÙŠØ© ÙˆØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'AutoModel' from transformers and 'Image' from PIL.\\\\n2. Use the 'from_pretrained' method of the AutoModel class to load the model, which in this case is 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'.\\\\n3. This model has been fine-tuned for depth estimation tasks, which can help you estimate the depth of various rooms from an image.\\\\n4. Load an image of the room, and then apply the model to that image to get an estimate of the room's depth.\\\\n5. This will assist the customers in visualizing the space on your real estate platform.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nfrom PIL import Image\\\\nroom_image = Image.open('path_to_room_image.jpg')\\\\n# replace 'path_to_room_image.jpg' with the actual path to your room image\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\\\\ninputs = feature_extractor(images=room_image, return_tensors='pt')\\\\ndepth_estimation = model(**inputs)\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221116-054332', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\\\", 'api_arguments': {'model_name': 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'}, 'python_environment_requirements': [{'transformers': '4.24.0', 'pytorch': '1.13.0+cu117', 'tokenizers': '0.13.2'}], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.6028, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…Ø²ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes, such as AutoTokenizer and AutoModelForTokenClassification, from the transformers library provided by Hugging Face.\\\\n2. Load the pre-trained model 'd4data/biomedical-ner-all', which is capable of recognizing 107 bio-medical entities from text, using AutoModelForTokenClassification.\\\\n3. Load the corresponding tokenizer for the model using AutoTokenizer.\\\\n4. Create a pipeline using the loaded model, tokenizer, and the pipeline function from the transformers library.\\\\n5. Use the created pipeline to analyze the biomedical paper and extract bio-medical entities from the text.\\\", '<<<code>>>': \\\"from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\\\\nmodel = AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\\ntokenizer = AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\\\\nbiomedical_ner = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy='simple')\\\\nentities = biomedical_ner(biomedical_paper_text)\\\", 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'd4data/biomedical-ner-all', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\", 'api_arguments': {'model': 'AutoModelForTokenClassification.from_pretrained(d4data/biomedical-ner-all)', 'tokenizer': 'AutoTokenizer.from_pretrained(d4data/biomedical-ner-all)', 'aggregation_strategy': 'simple'}, 'python_environment_requirements': [{'transformers': 'from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification'}], 'example_code': 'pipe(The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.)', 'performance': {'dataset': 'Maccrobat', 'accuracy': 'Not provided'}, 'description': 'An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Øµ ØªÙ‚Ù†ÙŠ ÙŠØ´Ø±Ø­ ÙƒÙŠÙÙŠØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ Ø¨Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"translation_pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library to create the translation pipeline.\\\\n2. Load the German to English translation model 'Helsinki-NLP/opus-mt-de-en' for translating the German news article.\\\\n3. Use the translation pipeline to convert the text from German to English, in order to cater to the company's international website.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_de_to_en\\\\', model=\\\\'Helsinki-NLP/opus-mt-de-en\\\\')\\\\ngerman_text = \\\"Der Bundesrat hat heute neue MaÃŸnahmen fÃ¼r Unternehmen angekÃ¼ndigt, um sie aufgrund der wirtschaftlichen Auswirkungen von COVID-19 zu unterstÃ¼tzen. Die Firmen, die am meisten betroffen sind, erhalten Finanzhilfe von der Regierung, um den Arbeitsplatzverlust zu verhindern. Diese MaÃŸnahmen halten die Wirtschaft des Landes am Laufen und schÃ¼tzen die ArbeitsplÃ¤tze.\\\"\\\\ntranslated_text = translator(german_text)\\\\nenglish_text = translated_text[0][\\\\'translation_text\\\\']', 'api_call': \\\"translation_pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-de-en', 'api_call': \\\"translation_pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\\\", 'api_arguments': ['source languages: de', 'target languages: en'], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'opus', 'accuracy': {'newssyscomb2009.de.en': 29.4, 'news-test2008.de.en': 27.8, 'newstest2009.de.en': 26.8, 'newstest2010.de.en': 30.2, 'newstest2011.de.en': 27.4, 'newstest2012.de.en': 29.1, 'newstest2013.de.en': 32.1, 'newstest2014-deen.de.en': 34.0, 'newstest2015-ende.de.en': 34.2, 'newstest2016-ende.de.en': 40.4, 'newstest2017-ende.de.en': 35.7, 'newstest2018-ende.de.en': 43.7, 'newstest2019-deen.de.en': 40.1, 'Tatoeba.de.en': 55.4}}, 'description': 'A German to English translation model trained on the OPUS dataset using the Hugging Face Transformers library.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØµÙˆØ± Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù†Ø¸Ø§Ù… Ù„ÙÙ‡Ù… ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ ÙˆØ¯Ù‚ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler), torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We import the necessary classes from the diffusers library, including StableDiffusionPipeline and EulerDiscreteScheduler. We also import torch.\\\\n2. We load the pre-trained model 'stabilityai/stable-diffusion-2-1-base', which is designed for text-to-image generation tasks, along with the EulerDiscreteScheduler model loader.\\\\n3. We provide the text prompt 'an elephant flying with balloons over the plains of Africa' to guide the image generation.\\\\n4. The created pipeline processes the prompt to generate an image illustrating the described scenario. The image can then be saved to a file.\\\", '<<<code>>>': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nimport torch\\\\nmodel_id = \\\"stabilityai/stable-diffusion-2-1-base\\\"\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\\\"scheduler\\\")\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(\\\"cuda\\\")\\\\nprompt = \\\"an elephant flying with balloons over the plains of Africa\\\"\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(\\\"elephant_flying.png\\\")', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler), torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler), torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors']], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nimport torch\\\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>\"\n"}
{"text": "<s>[INST]Ù†ÙˆØµÙŠ Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø§Ø¯Ù Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù…Ù† Ø£Ø¬Ù„ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ù‚Ø¯Ù… Ù„Ù‡Ù… Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ù…Ù‚Ø§Ø¨Ù„ ÙˆÙƒÙŠÙ„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªÙˆÙ„ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-large-seq2seq')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required AutoTokenizer and AutoModelForSeq2SeqLM classes from the transformers library provided by Hugging Face.\\\\n2. Use AutoTokenizer.from_pretrained and AutoModelForSeq2SeqLM.from_pretrained on 'microsoft/GODEL-v1_1-large-seq2seq' to obtain the pre-trained tokenizer and model.\\\\n3. Define a function to generate a response using the pre-trained model. This function takes the instruction of responding empathically, dialog context, and any knowledge to be used for grounding the response.\\\\n4. Prepare input by joining dialog context using the 'EOS' token, and then format the input string with instruction, dialog, and knowledge.\\\\n5. Tokenize the input string, obtain input_ids, and generate a response from the model.\\\\n6. Decode the output to get the empathic response to the user's query.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'microsoft/GODEL-v1_1-large-seq2seq\\\\')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'microsoft/GODEL-v1_1-large-seq2seq\\\\')\\\\n\\\\ndef generate_empathic_response(instruction, knowledge, dialog):\\\\n    if knowledge != \\\\'\\\\':\\\\n        knowledge = \\\\'[KNOWLEDGE] \\\\' + knowledge\\\\n    dialog_str = \\\\' EOS \\\\'.join(dialog)\\\\n    query = f\\\"{instruction} [CONTEXT] {dialog_str} {knowledge}\\\"\\\\n    input_ids = tokenizer(query, return_tensors=\\\\'pt\\\\').input_ids\\\\n    outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n    return response\\\\n\\\\ninstruction = \\\\'Instruction: given a dialog context, you need to respond empathically.\\\\'\\\\nknowledge = \\\\'\\\\'\\\\ndialog_context = [\\\\n    \\\\'Does money buy happiness?\\\\',\\\\n    \\\\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\\\',\\\\n    \\\\'What is the best way to buy happiness ?\\\\'\\\\n]\\\\n\\\\nempathic_response = generate_empathic_response(instruction, knowledge, dialog_context)\\\\nprint(empathic_response)', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-large-seq2seq')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/GODEL-v1_1-large-seq2seq', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-large-seq2seq')\\\", 'api_arguments': {'instruction': 'Instruction: given a dialog context, you need to response empathically.', 'knowledge': '', 'dialog': ['Does money buy happiness?', 'It is a question. Money buys you a lot of things, but not enough to buy happiness.', 'What is the best way to buy happiness ?']}, 'python_environment_requirements': [{'transformers': 'AutoTokenizer, AutoModelForSeq2SeqLM'}], 'example_code': \\\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != '':\\\\n knowledge = '[KNOWLEDGE] ' + knowledge\\\\n dialog = ' EOS '.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f'Instruction: given a dialog context, you need to response empathically.'\\\\nknowledge = ''\\\\ndialog = [\\\\n 'Does money buy happiness?',\\\\n 'It is a question. Money buys you a lot of things, but not enough to buy happiness.',\\\\n 'What is the best way to buy happiness ?'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\\\", 'performance': {'dataset': 'Reddit discussion thread, instruction and knowledge grounded dialogs', 'accuracy': 'Not provided'}, 'description': 'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© ØªÙØ§Ø¹Ù„ÙŠØ© ØªØ³Ù…Ø­ Ø¨Ø¥Ø¯Ø®Ø§Ù„ Ù†ØµÙˆØµ ÙˆØµÙˆØ± Ù…ØªØ¹Ø¯Ø¯Ø© Ù„ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠÙ Ù…Ø­ØªÙˆÙ‰ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·.? [/INST]{'domain': 'Multimodal Text-to-Image', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\\\", 'api_provider': 'Hugging Face', 'explanation': '1. Import the necessary classes from the diffusers package, which includes StableDiffusionPipeline and DPMSolverMultistepScheduler.', 'code': \\\"from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\\\nimport torch\\\\n\\\\nprompt = 'an autumn landscape with trees shedding leaves along a peaceful river at sunset'\\\\nmodel_id = 'darkstorm2150/Protogen_v2.2_Official_Release'\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to('cuda')\\\\n\\\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\\\nimage.save('./result.jpg')\\\", 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \\\"from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\\\nimport torch\\\\n\\\\nprompt = 'an autumn landscape with trees shedding leaves along a peaceful river at sunset'\\\\nmodel_id = 'darkstorm2150/Protogen_v2.2_Official_Release'\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to('cuda')\\\\n\\\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\\\nimage.save('./result.jpg')\\\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\\\nimport torch\\\\nprompt = (\\\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\\\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\\\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\\\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\\\n)\\\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to(cuda)\\\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \\\"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\\\"}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ØªÙ…Ù‡ÙŠØ¯Ø§Ù‹ Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø³Ø¤Ø§Ù„ ÙŠØªØ¹Ù„Ù‚ Ø¨ÙˆØ«ÙŠÙ‚Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„ÙŠÙ‡.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the pipeline function from the transformers package.\\\\n2. Create an instance of OCR-DocVQA-Donut pipeline using the provided model \\\\'jinhybr/OCR-DocVQA-Donut\\\\'. This model uses both vision and text processing capabilities to answer questions based on visual documents.\\\\n3. You now have a document-question-answering pipeline tailored for OCR and can use it for extracting titles from different documents.\\\\n4. To use the pipeline, pass in the image path of the document you wish to process and the question \\\"What is the title?\\\".', '<<<code>>>': \\\"from transformers import pipeline\\\\ndoc_vqa = pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\\ntitle = doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\\\", 'api_call': \\\"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': \\\"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\", 'api_arguments': 'image_path, question', 'python_environment_requirements': ['transformers'], 'example_code': \\\"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\\\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>\"\n"}
{"text": "<s>[INST]Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø¹ÙŠÙ†Ø©. Ø«Ù…Ù‘ØŒ Ù‚Ù… Ø¨ØªÙ‚Ø¯ÙŠÙ… ØªØ­Ø¯ÙŠØ§ Ù„Ù„Ø·Ù„Ø§Ø¨ Ø¨Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ ÙˆØ¥Ø¶Ø§ÙØ© Ø¹Ù†ØµØ± Ù…Ù†Ø§ÙØ³ÙŠØ© Ø¨ØªÙˆØ¸ÙŠÙ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙƒØ®ØµÙ….? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='saltacc/anime-ai-detect')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create an image classification pipeline by specifying the required model 'saltacc/anime-ai-detect'. This model is a BEiT classifier trained to differentiate between AI-generated and human-created anime artworks.\\\\n3. Use the created pipeline object to classify the anime artwork based on the input image.\\\\n4. The model will return a classification result that indicates if the anime art was created by an AI or a human artist.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimport cv2\\\\n\\\\nimg = cv2.imread('your_anime_art_image.jpg')\\\\nclassifier = pipeline('image-classification', model='saltacc/anime-ai-detect')\\\\nresult = classifier(img)\\\", 'api_call': \\\"pipeline('image-classification', model='saltacc/anime-ai-detect')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'saltacc/anime-ai-detect', 'api_call': \\\"pipeline('image-classification', model='saltacc/anime-ai-detect')\\\", 'api_arguments': ['image'], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'aibooru and imageboard sites', 'accuracy': '96%'}, 'description': 'A BEiT classifier to see if anime art was made by an AI or a human.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© ÙˆØ£Ø¬ÙˆØ¨Ø© Ø°Ø§ØªÙŠØ© Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹ÙŠÙ†Ø© Ù…Ù† Ø£Ø¬Ù„ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ÙØ¹Ø§Ù„ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"AutoModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' function from the transformers library.\\\\n2. Create a question-answering pipeline using the 'bert-large-uncased-whole-word-masking-finetuned-squad' model, which is a BERT large model specifically fine-tuned for question-answering tasks.\\\\n3. Pass the question and the context (text passage) to the pipeline, which will then return the answer.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad')\\\\nresult = qa_pipeline({'question': 'What is the capital of France?', 'context': 'Paris is the capital of France.'})\\\\nprint(result['answer'])\\\", 'api_call': \\\"AutoModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-large-uncased-whole-word-masking-finetuned-squad', 'api_call': \\\"AutoModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\\\", 'api_arguments': {'model_name_or_path': 'bert-large-uncased-whole-word-masking', 'dataset_name': 'squad', 'do_train': True, 'do_eval': True, 'learning_rate': 3e-05, 'num_train_epochs': 2, 'max_seq_length': 384, 'doc_stride': 128, 'output_dir': './examples/models/wwm_uncased_finetuned_squad/', 'per_device_eval_batch_size': 3, 'per_device_train_batch_size': 3}, 'python_environment_requirements': [{'torch': '1.9.0', 'transformers': '4.9.2'}], 'example_code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad')\\\\nresult = qa_pipeline({'question': 'What is the capital of France?', 'context': 'Paris is the capital of France.'})\\\\nprint(result)\\\", 'performance': {'dataset': 'SQuAD', 'accuracy': {'f1': 93.15, 'exact_match': 86.91}}, 'description': 'BERT large model (uncased) whole word masking finetuned on SQuAD. The model was pretrained on BookCorpus and English Wikipedia. It was trained with two objectives: Masked language modeling (MLM) and Next sentence prediction (NSP). This model should be used as a question-answering model.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ±ØºØ¨ ÙÙŠ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙŠØ³ØªØ®Ø¯Ù… Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ ØµÙˆØ± Ø£Ø®Ø±Ù‰. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù…Ù† Ù†ÙˆØ¹ Ø¥Ù„Ù‰ Ù†ÙˆØ¹ Ø¢Ø®Ø± Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]{'domain': 'Computer Vision Image-to-Image', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-hed')\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': \\\"from PIL import Image\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom controlnet_aux import HEDdetector, load_image\\\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png')\\\\nimage = hed(image)\\\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-hed', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\noil_painting_image = pipe('oil painting of handsome old man, masterpiece', image, num_inference_steps=20).images[0]\\\\noil_painting_image.save('images/man_hed_out.png')\\\", 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-hed', 'api_call': \\\"from PIL import Image\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom controlnet_aux import HEDdetector, load_image\\\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png')\\\\nimage = hed(image)\\\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-hed', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\noil_painting_image = pipe('oil painting of handsome old man, masterpiece', image, num_inference_steps=20).images[0]\\\\noil_painting_image.save('images/man_hed_out.png')\\\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \\\"from PIL import Image\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom controlnet_aux import HEDdetector\\\\nfrom diffusers.utils import load_image\\\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png)\\\\nimage = hed(image)\\\\ncontrolnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-hed, torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\nimage = pipe(oil painting of handsome old man, masterpiece, image, num_inference_steps=20).images[0]\\\\nimage.save('images/man_hed_out.png')\\\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.'}} </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ¹Ù…Ù„ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ø§Ù„Ø¨ØµØ±ÙŠ ÙˆØ§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ù„Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© ÙˆØ§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙˆØ§ØµÙ„ Ø¨ÙŠÙ† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†.? [/INST]'<<<domain>>>': 'Multimodal Graph Machine Learning', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary libraries and modules, including the transformers library provided by Hugging Face.\\\\n2. We then use the from_pretrained method to load the pre-trained model 'janpase97/codeformer-pretrained'. This model can generate code that can be used to control an NPC in a game like GTA5.\\\\n3. We need to design a neural network for the model. In this case, it could be as simple as a 2-layer fully connected network to control the driving actions of the NPC based on inputs such as traffic conditions and vehicle speed.\\\\n4. The model is then trained with a data cap on the selected dataset (e.g., MNIST) using the PyTorch framework.\\\\n5. We check if the targeted game process is running and attach the trained model to the game.\\\\n6. The AI model can then control the NPC van and make it drive properly in traffic.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport torch\\\\nimport torch.optim as optim\\\\nimport torch.nn as nn\\\\nfrom torchvision import datasets, transforms\\\\n\\\\ndef train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):\\\\n    data_processed = 0\\\\n    data_cap_bytes = data_cap_gb * (1024 ** 3)\\\\n    epoch = 0\\\\n    while data_processed < data_cap_bytes:\\\\n        running_loss = 0.0\\\\n        for i, data in enumerate(data_loader, 0):\\\\n            inputs, labels = data\\\\n            inputs, labels = inputs.to(device), labels.to(device)\\\\n            data_processed += inputs.nelement() * inputs.element_size()\\\\n            if data_processed >= data_cap_bytes:\\\\n                break\\\\n            optimizer.zero_grad()\\\\n            outputs = model(inputs.view(-1, 28 * 28))\\\\n            loss = criterion(outputs, labels)\\\\n            loss.backward()\\\\n            optimizer.step()\\\\n            running_loss += loss.item()\\\\n        epoch += 1\\\\n    print(f\\\\'Epoch {epoch}, Loss: {running_loss / (i + 1)}\\\\')\\\\n    print(f\\\\'Data processed: {data_processed / (1024 ** 3):.2f} GB\\\\')\\\\n    return model\\\\n\\\\n# Load the model\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\"janpase97/codeformer-pretrained\\\")\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\"janpase97/codeformer-pretrained\\\")\\\\ndevice = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\\\n\\\\n# Load training data\\\\ntrain_loader = torch.utils.data.DataLoader(\\\\n    datasets.MNIST(\\\"mnist_data\\\", train=True, download=True, transform=transforms.ToTensor()),\\\\n    batch_size=64, shuffle=True)\\\\n\\\\n# Neural network\\\\nclass NanoCircuit(nn.Module):\\\\n    def __init__(self):\\\\n        super(NanoCircuit, self).__init__()\\\\n        self.fc1 = nn.Linear(784, 128)\\\\n        self.fc2 = nn.Linear(128, 10)\\\\n\\\\n    def forward(self, x):\\\\n        x = x.view(-1, 784)\\\\n        x = torch.relu(self.fc1(x))\\\\n        x = self.fc2(x)\\\\n        return x\\\\n\\\\n# Create and train the model\\\\nmy_model = NanoCircuit()\\\\nmy_model.to(device)\\\\ncriterion = nn.CrossEntropyLoss()\\\\noptimizer = optim.SGD(my_model.parameters(), lr=0.001, momentum=0.9)\\\\ntrained_model = train_with_data_cap(my_model, train_loader, criterion, optimizer, device, data_cap_gb=10)', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GTA5_PROCESS_LEARNING_AI', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\\\", 'api_arguments': {'model': 'NanoCircuit', 'data_loader': 'train_loader', 'criterion': 'nn.CrossEntropyLoss', 'optimizer': 'optim.SGD', 'device': 'torch.device', 'data_cap_gb': 10}, 'python_environment_requirements': [['contextlib', 'os', 'matplotlib', 'numpy', 'torch', 'torch.nn', 'torch.optim', 'requests', 'torchvision', 'psutil', 'time', 'subprocess', 'onnxruntime', 'numexpr', 'transformers']], 'example_code': {'import_libraries': ['import contextlib', 'import os', 'from matplotlib import pyplot as plt', 'import numpy as np', 'import torch', 'import torch.nn as nn', 'import torch.optim as optim', 'import requests', 'from torchvision import datasets, transforms', 'import psutil', 'import time', 'import subprocess', 'import onnxruntime as ort', 'import matplotlib.pyplot as plt', 'import numpy as np', 'import numexpr as ne', 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM'], 'define_neural_network': ['class NanoCircuit(nn.Module):', ' def init(self):', ' super(NanoCircuit, self).init()', ' self.fc1 = nn.Linear(784, 128)', ' self.fc2 = nn.Linear(128, 10)', 'def forward(self, x):', ' x = x.view(-1, 784)', ' x = torch.relu(self.fc1(x))', ' x = self.fc2(x)', ' return x'], 'train_with_data_cap': ['def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):', ' data_processed = 0', ' data_cap_bytes = data_cap_gb * (1024 ** 3)', ' epoch = 0', 'while data_processed < data_cap_bytes:', ' running_loss = 0.0', ' for i, data in enumerate(data_loader, 0):', ' inputs, labels = data', ' inputs, labels = inputs.to(device), labels.to(device)', ' data_processed += inputs.nelement() * inputs.element_size()', ' if data_processed >= data_cap_bytes:', ' break', ' optimizer.zero_grad()', ' outputs = model(inputs.view(-1, 28 * 28))', ' loss = criterion(outputs, labels)', ' loss.backward()', ' optimizer.step()', ' running_loss += loss.item()', 'epoch += 1', 'print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})', 'print(fData processed: {data_processed / (1024 ** 3):.2f} GB)', 'return model']}, 'performance': {'dataset': 'MNIST', 'accuracy': 'Not specified'}, 'description': 'This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø§Ù„Ø¨Ø¯Ø¡ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ÙˆØ§Ù„Ù†ØµÙˆØµ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©ØŸ? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import Text2ImagePipeline from the transformers package provided by Hugging Face.\\\\n2. Use the Text2ImagePipeline to create a text-to-image model, specifying the 'Linaqruf/anything-v3.0' model.\\\\n3. This model can be used to generate images based on text descriptions, so you provide the model with the client's description to create an image for the book cover.\\\", '<<<code>>>': 'from transformers import Text2ImagePipeline\\\\ntext2image = Text2ImagePipeline(model=\\\\'Linaqruf/anything-v3.0\\\\')\\\\ndescription = \\\"A mysterious forest with a full moon shining through the trees and a dark figure walking in the shadows.\\\"\\\\noutput_image = text2image(description)', 'api_call': \\\"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \\\"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>\"\n"}
{"text": "<s>[INST]Ø³Ù†Ù‚ÙˆÙ… Ø¨Ø¹Ù‚Ø¯ Ø¬Ù„Ø³Ø© ØªØ¯Ø±ÙŠØ¨ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ²ÙŠ. Ù‚Ù… Ø¨Ù‚Ø±Ø§Ø¡Ø© ÙˆÙÙ‡Ù… Ù†Ø¸Ø±ÙŠØ§Øª ÙˆÙ…ÙØ§Ù‡ÙŠÙ… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ²ÙŠ. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ø¨Ø¯Ø£ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ÙˆØ§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ Ø¨Ù‡Ø¯Ù ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡Ù‡Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø¹ÙŠÙ†Ø©.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"load_from_hub(repo_id='sb3/td3-Ant-v3,filename='{MODEL FILENAME}.zip',)\\\", '<<<api_provider>>>': 'Stable-Baselines3', '<<<explanation>>>': '1. We will utilize the Stable-Baselines3 library, which provides a collection of reinforcement learning algorithms and pre-trained agents.\\\\n2. From this library, we will select the TD3 algorithm (Twin Delayed Deep Deterministic Policy Gradient) as it has been proven to be effective in continuous control tasks.\\\\n3. We will use the RL Zoo, a training framework for Stable Baselines3 reinforcement learning agents, to train the TD3 agent in a hospital environment.\\\\n4. After training the agent, it can be deployed in a navigation robot, allowing it to autonomously navigate the hospital.', '<<<code>>>': \\\"from stable_baselines3.common import load_from_hub\\\\nimport gym\\\\n\\\\n# Load pre-trained model\\\\nmodel = load_from_hub(repo_id='sb3/td3-Ant-v3', filename='{MODEL FILENAME}.zip')\\\\n\\\\n# Create the hospital environment (custom environment required)\\\\nenv = gym.make('HospitalNavigation-v0')\\\\n\\\\n# Enjoy the trained agent\\\\nobs = env.reset()\\\\ndone = False\\\\nwhile not done:\\\\n    action, _states = model.predict(obs)\\\\n    obs, rewards, done, info = env.step(action)\\\\n    env.render()\\\", 'api_call': \\\"load_from_hub(repo_id='sb3/td3-Ant-v3,filename='{MODEL FILENAME}.zip',)\\\", 'provider': 'Stable-Baselines3', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'td3-Ant-v3', 'api_call': \\\"load_from_hub(repo_id='sb3/td3-Ant-v3,filename='{MODEL FILENAME}.zip',)\\\", 'api_arguments': ['algo', 'env', 'f'], 'python_environment_requirements': [['RL Zoo: https://github.com/DLR-RM/rl-baselines3-zoo', 'SB3: https://github.com/DLR-RM/stable-baselines3', 'SB3 Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib']], 'example_code': ['python -m rl_zoo3.load_from_hub --algo td3 --env Ant-v3 -orga sb3 -f logs/', 'python enjoy.py --algo td3 --env Ant-v3 -f logs/', 'python train.py --algo td3 --env Ant-v3 -f logs/', 'python -m rl_zoo3.push_to_hub --algo td3 --env Ant-v3 -f logs/ -orga sb3'], 'performance': {'dataset': 'Ant-v3', 'accuracy': '5822.96 +/- 93.33'}, 'description': 'This is a trained model of a TD3 agent playing Ant-v3 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‡Ø¯Ù Ø§Ù„Ø´Ø±ÙƒØ© Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ© Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ÙÙŠ Ø§Ù„ØªØµÙˆÙŠØ±ØŒ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø¯Ù‚Ø© Ø§Ù„ØªÙ‚Ø¯ÙŠØ± ÙˆØ³Ø±Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ù†Ø¸Ø§Ù….? [/INST]{'domain': 'Computer Vision Depth Estimation', 'api_call': \\\"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import DPTForDepthEstimation\\\\ndepth_estimator = DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\\\\ndepth_map = depth_estimator.predict(image)\\\", 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'tiny-random-DPTForDepthEstimation', 'api_call': \\\"from transformers import DPTForDepthEstimation\\\\ndepth_estimator = DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\\\\ndepth_map = depth_estimator.predict(image)\\\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random DPT model for depth estimation using Hugging Face Transformers library.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ§Øª ÙÙŠ Ø§Ù„Ù†ØµÙˆØµØŒ Ø­ÙŠØ« ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„ÙˆØ¶Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù†Ø§Ù‚ØµØ©. Ù†Ù‡Ø¯Ù Ø¥Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ø³ÙŠÙ† ÙˆØªØ·ÙˆÙŠØ± Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ø¯ÙŠÙ†Ø§.? [/INST]{'domain': 'Natural Language Processing Fill-Mask', 'api_call': 'pipeline(\\\\'fill-mask\\\\', model=\\\"cl-tohoku/bert-base-japanese-whole-word-masking\\\")', 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the 'pipeline' function from the transformers package.\\\", 'code': 'from transformers import pipeline\\\\nfill_mask = pipeline(\\\\'fill-mask\\\\', model=\\\"cl-tohoku/bert-base-japanese-whole-word-masking\\\")\\\\ninput_sentence = \\\"å½¼å¥³ã¯[MASK]ã‚’é£²ã‚“ã§ã„ã¾ã™ã€‚\\\"\\\\noutput = fill_mask(input_sentence)\\\\nword_predictions = [prediction[\\\"token_str\\\"] for prediction in output]\\\\nprint(word_predictions)', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Fill-Mask', 'api_name': 'cl-tohoku/bert-base-japanese-whole-word-masking', 'api_call': 'from transformers import pipeline\\\\nfill_mask = pipeline(\\\\'fill-mask\\\\', model=\\\"cl-tohoku/bert-base-japanese-whole-word-masking\\\")\\\\ninput_sentence = \\\"å½¼å¥³ã¯[MASK]ã‚’é£²ã‚“ã§ã„ã¾ã™ã€‚\\\"\\\\noutput = fill_mask(input_sentence)\\\\nword_predictions = [prediction[\\\"token_str\\\"] for prediction in output]\\\\nprint(word_predictions)', 'api_arguments': {'model': 'cl-tohoku/bert-base-japanese-whole-word-masking'}, 'python_environment_requirements': {'transformers': 'latest'}, 'example_code': \\\"fill_mask('[MASK]')\\\", 'performance': {'dataset': 'Japanese Wikipedia', 'accuracy': 'Not provided'}, 'description': 'This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization. Additionally, the model is trained with the whole word masking enabled for the masked language modeling (MLM) objective.'}} </s>\"\n"}
{"text": "<s>[INST]Ø³ÙŠÙ‚ÙˆÙ… ÙØ±ÙŠÙ‚ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø®Ø§Øµ Ø¨Ù†Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ØŒ Ù†Ø±ÙŠØ¯ Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ‚Ø¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø¹Ù† Ø§Ù„Ø±ÙˆØ§ØªØ¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"BartForConditionalGeneration.from_pretrained('microsoft/tapex-base-finetuned-wtq')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include TapexTokenizer and BartForConditionalGeneration from the transformers package, and pandas for creating data tables.\\\\n2. Load the tokenizer and the model using their respective from_pretrained methods.\\\\n3. Convert the provided table_data into a pandas DataFrame.\\\\n4. Use the tokenizer to encode the table and the query.\\\\n5. Call the model's generate method to obtain an answer to the query.\\\\n6. Decode the answer using the tokenizer's batch_decode method.\\\", '<<<code>>>': 'from transformers import TapexTokenizer, BartForConditionalGeneration\\\\nimport pandas as pd\\\\ntokenizer = TapexTokenizer.from_pretrained(\\\\'microsoft/tapex-base-finetuned-wtq\\\\')\\\\nmodel = BartForConditionalGeneration.from_pretrained(\\\\'microsoft/tapex-base-finetuned-wtq\\\\')\\\\ntable_data = [{\\\"year\\\": 1896, \\\"city\\\": \\\"Athens\\\"}, {\\\"year\\\": 1900, \\\"city\\\": \\\"Paris\\\"}, {\\\"year\\\": 1904, \\\"city\\\": \\\"St. Louis\\\"}, {\\\"year\\\": 2004, \\\"city\\\": \\\"Athens\\\"}, {\\\"year\\\": 2008, \\\"city\\\": \\\"Beijing\\\"}, {\\\"year\\\": 2012, \\\"city\\\": \\\"London\\\"}]\\\\ntable = pd.DataFrame(table_data)\\\\nquery = \\\"In which year did Beijing host the Olympic Games?\\\"\\\\nencoding = tokenizer(table=table, query=query, return_tensors=\\\\'pt\\\\')\\\\noutputs = model.generate(**encoding)\\\\nanswer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('microsoft/tapex-base-finetuned-wtq')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/tapex-base-finetuned-wtq', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('microsoft/tapex-base-finetuned-wtq')\\\", 'api_arguments': {'tokenizer': 'TapexTokenizer.from_pretrained(microsoft/tapex-base-finetuned-wtq)', 'model': 'BartForConditionalGeneration.from_pretrained(microsoft/tapex-base-finetuned-wtq)', 'table': 'pd.DataFrame.from_dict(data)', 'query': 'query'}, 'python_environment_requirements': [['transformers', 'pandas']], 'example_code': 'from transformers import TapexTokenizer, BartForConditionalGeneration\\\\nimport pandas as pd\\\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-base-finetuned-wtq)\\\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-base-finetuned-wtq)\\\\ndata = {\\\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\\\n city: [athens, paris, st. louis, athens, beijing, london]\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = In which year did beijing host the Olympic Games?\\\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))', 'performance': {'dataset': 'wikitablequestions', 'accuracy': 'Not provided'}, 'description': 'TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ±Ø¬Ù…Ø© Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries from the transformers package.\\\\n2. Use the 'from_pretrained()' function to load the pre-trained model 'Helsinki-NLP/opus-mt-ROMANCE-en', which translates Romance languages (French, Spanish, Portuguese, Italian, Romanian, etc.) into English.\\\\n3. Create a function that wraps the model, and when given a text input in one of the supported Romance languages, translates it into English.\\\\n4. Integrate this function into the English-speaking toy so that it can receive text inputs and respond accordingly.\\\", '<<<code>>>': 'from transformers import MarianTokenizer, MarianMTModel\\\\nmodel_name = \\\\'Helsinki-NLP/opus-mt-ROMANCE-en\\\\'\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\n\\\\ndef translate(input_text):\\\\n    batch = tokenizer.prepare_translation_batch(src_texts=[input_text])\\\\n    translated = model.generate(**batch)\\\\n    return tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\\\\n\\\\ntranslated_text = translate(\\\"Input the Romance language text here\\\")', 'api_call': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-ROMANCE-en', 'api_call': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", 'api_arguments': ['source languages', 'target languages'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': '', 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': 62.2, 'chr-F': 0.75}}, 'description': 'A model for translating Romance languages to English, trained on the OPUS dataset. It supports multiple source languages such as French, Spanish, Portuguese, Italian, and Romanian, among others. The model is based on the transformer architecture and uses normalization and SentencePiece for pre-processing.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ ÙŠØªÙŠØ­ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ Ù„Ø°Ù„Ùƒ Ù†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨ØªÙˆØµÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model for Optical Character Recognition (OCR), which in this case is 'kha-white/manga-ocr-base'.\\\\n3. This loaded model was specifically designed to work with Japanese text and has been trained primarily for Japanese manga image-to-text tasks.\\\\n4. After loading the OCR model, use the pipeline to provide the manga image and the model will return the recognized text from the manga image.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nocr_pipeline = pipeline(\\\\'ocr\\\\', model=\\\\'kha-white/manga-ocr-base\\\\')\\\\nimage_path = \\\"path/to/your/manga/image.jpg\\\"\\\\nwith open(image_path, \\\\'rb\\\\') as image_file:\\\\n    image = image_file.read()\\\\nrecognized_text = ocr_pipeline(image)', 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'api_arguments': 'image', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ Ø¨ÙŠØ§Ù†Ø§Øª ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠØ© ÙˆØ§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©ØŒ Ø«Ù… Ù‚Ù… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load(hf_hub_download('srg/outhimar_64-Close-regression', 'sklearn_model.joblib'))\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'joblib' from joblib and 'hf_hub_download' from a library like huggingface_hub.\\\\n2. Load the pre-trained tabular regression model 'srg/outhimar_64-Close-regression' from Hugging Face Model Hub using the joblib.load function.\\\\n3. Prepare the input data with the necessary parameters like temperature, time of the day, and humidity.\\\\n4. Use the loaded model to predict the energy consumption for a day based on input data.\\\", '<<<code>>>': \\\"from joblib import load\\\\nfrom huggingface_hub import hf_hub_download\\\\nmodel = load(hf_hub_download('srg/outhimar_64-Close-regression', 'sklearn_model.joblib'))\\\\ninputs = {'temperature': 25, 'time': 12, 'humidity': 60}\\\\n# replace values with your actual data\\\\nprediction = model.predict([inputs])[0]\\\", 'api_call': \\\"joblib.load(hf_hub_download('srg/outhimar_64-Close-regression', 'sklearn_model.joblib'))\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'srg/outhimar_64-Close-regression', 'api_call': \\\"joblib.load(hf_hub_download('srg/outhimar_64-Close-regression', 'sklearn_model.joblib'))\\\", 'api_arguments': ['X_train', 'y_train'], 'python_environment_requirements': [['scikit-learn', 'dabl']], 'example_code': \\\"Pipeline(steps=[('easypreprocessor',EasyPreprocessor(types= continuous dirty_float low_card_int ... date free_string useless\\\\nDate False False False ... True False False\\\\nOpen True False False ... False False False\\\\nHigh True False False ... False False False\\\\nLow True False False ... False False False\\\\nAdj Close True False False ... False False False\\\\nVolume True False False ... False False False[6 rows x 7 columns])),('ridge', Ridge(alpha=10))])\\\", 'performance': {'dataset': 'outhimar_64', 'accuracy': {'r2': 0.999858, 'neg_mean_squared_error': -1.067685}}, 'description': 'Baseline Model trained on outhimar_64 to apply regression on Close. Disclaimer: This model is trained with dabl library as a baseline, for better results, use AutoTrain. Logs of training including the models tried in the process can be found in logs.txt.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù…Ù†ØªØ¬ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¹Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face Transformer.\\\\n2. Use the pipeline function to create a question-answering model.\\\\n3. Specify the model 'sultan/BioM-ELECTRA-Large-SQuAD2' to be loaded. This model is fine-tuned for the SQuAD2.0 dataset and is trained specifically for handling biomedical texts.\\\\n4. The created model can be used to answer questions related to medical findings present in the research text.\\\\n5. Provide the research text as context and ask relevant questions to extract important information.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\\nresult = qa_pipeline({'context': 'your_research_text', 'question': 'your_medical_question'})\\\", 'api_call': \\\"pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sultan/BioM-ELECTRA-Large-SQuAD2', 'api_call': \\\"pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\", 'api_arguments': None, 'python_environment_requirements': [['transformers', 'sentencepiece']], 'example_code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\\nresult = qa_pipeline({'context': 'your_context', 'question': 'your_question'})\\\", 'performance': {'dataset': 'SQuAD2.0 Dev', 'accuracy': {'exact': 84.33420365535248, 'f1': 87.49354241889522}}, 'description': 'BioM-ELECTRA-Large-SQuAD2 is a fine-tuned version of BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts, on the SQuAD2.0 dataset. Fine-tuning the biomedical language model on the SQuAD dataset helps improve the score on the BioASQ challenge. This model is suitable for working with BioASQ or biomedical QA tasks.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ù„ØªØ¹Ø±ÙŠÙ ÙˆØªØ­ÙˆÙŠÙ„ Ù†ØµÙˆØµ Ø§Ù„ÙƒÙ„Ø§Ù… ÙÙŠ Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ù…ÙƒØªÙˆØ¨Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„Ø¢Ù„ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ù‡Ø°Ø§ØŸ? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import necessary libraries like 'WhisperProcessor', 'WhisperForConditionalGeneration' from transformers and 'load_dataset' from datasets.\\\", 'code': '', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Transcription and Translation', 'api_name': 'openai/whisper-medium', 'api_call': '', 'api_arguments': ['sample', 'sampling_rate', 'language', 'task', 'skip_special_tokens'], 'python_environment_requirements': ['transformers', 'datasets'], 'example_code': 'from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\n\\\\nprocessor = WhisperProcessor.from_pretrained(openai/whisper-medium)\\\\nmodel = WhisperForConditionalGeneration.from_pretrained(openai/whisper-medium)\\\\n\\\\nmodel.config.forced_decoder_ids = None\\\\n\\\\nds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\\\\nsample = ds[0][audio]\\\\ninput_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\\\\n\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)', 'performance': {'dataset': [{'name': 'LibriSpeech (clean)', 'accuracy': 2.9}, {'name': 'LibriSpeech (other)', 'accuracy': 5.9}, {'name': 'Common Voice 11.0', 'accuracy': 53.87}]}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and was trained on either English-only data or multilingual data.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¹Ù…Ù„ Ø§Ù„Ø¨Ø§Ø­Ø« Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨ØªÙ‚Ù†ÙŠØ© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ·ÙŠØ¹ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the transformers library provided by Hugging Face.\\\\n2. Import the 'pipeline' function from the transformers library.\\\\n3. Use the pipeline function to create a conversational model with the 'ingen51/DialoGPT-medium-GPT4' model, which is a GPT-4 based model specifically trained for generating human-like conversational responses.\\\\n4. Pass a message or a series of messages to the created pipeline to generate a conversational reply.\\\\n5. Embed the pipeline in your product to generate text based on user inputs while chatting.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nchat_pipeline = pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\\\\nuser_message = 'Hello, how are you?'\\\\ngenerated_response = chat_pipeline(user_message)\\\", 'api_call': \\\"pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ingen51/DialoGPT-medium-GPT4', 'api_call': \\\"pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\\\", 'api_arguments': ['message'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"conversational_pipeline('Hello, how are you?')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A GPT-4 model for generating conversational responses in a dialogue setting.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ÙÙŠ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ Ù„Ù„ØµÙˆØ± Ø¨ÙˆØ§Ø³Ø·Ø© Ø¨Ù†Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ¨ÙŠØ±Ø© ÙˆØªØ­Ø¯ÙŠØ¯ Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù…Ø«Ù„ Ø§Ù„Ø³ÙŠØ§Ø±Ø§ØªØŒ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§ØªØŒ Ø§Ù„Ø£Ø«Ø§Ø« ÙˆØºÙŠØ±Ù‡Ø§.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To create a depth estimation model, import the pipeline function from the transformers library.\\\\n2. Use the pipeline function to create a depth estimation model by specifying the 'depth-estimation' pipeline and the pre-trained model 'sayakpaul/glpn-kitti-finetuned-diode-221214-123047'.\\\\n3. This model has been fine-tuned on the diode-subset dataset and can be used for depth estimation tasks needed in your computer vision application.\\\\n4. To use the model, simply provide an input image, and the model will return depth estimations for the objects present in the image.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ndepth_estimator = pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\\\", 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-kitti-finetuned-diode-221214-123047', 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2']], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3497, 'Mae': 0.2847, 'Rmse': 0.3977, 'Abs Rel': 0.3477, 'Log Mae': 0.1203, 'Log Rmse': 0.1726, 'Delta1': 0.5217, 'Delta2': 0.8246, 'Delta3': 0.9436}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠØªÙ…ÙƒÙ† Ù…Ù† ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨ØµÙˆØª Ø·Ø¨ÙŠØ¹ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ù† ÙŠØ³ØªØ·ÙŠØ¹ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨ØµÙˆØª ÙˆØ§Ø¶Ø­ ÙˆØ¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ¶Ù…Ø§Ù† ÙØ¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¹Ù…Ù„ÙŠØ©.? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': \\\"unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')\\\", 'api_provider': 'Fairseq', 'explanation': '1. First, we need to install and import the necessary packages, including HuggingFace Hub and Fairseq.', 'code': 'from huggingface_hub import ModelHubMixin\\\\nfrom fairseq import unit\\\\nmodel = unit.TTS.from_pretrained(\\\\'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10\\\\')\\\\n# provide a sentence to convert text-to-speech\\\\nsentence = \\\"Hello, how are you?\\\"\\\\naudio = model(sentence)', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Fairseq', 'functionality': 'Text-to-Speech', 'api_name': 'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10', 'api_call': 'from huggingface_hub import ModelHubMixin\\\\nfrom fairseq import unit\\\\nmodel = unit.TTS.from_pretrained(\\\\'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10\\\\')\\\\n# provide a sentence to convert text-to-speech\\\\nsentence = \\\"Hello, how are you?\\\"\\\\naudio = model(sentence)', 'api_arguments': None, 'python_environment_requirements': 'huggingface_hub, fairseq', 'example_code': None, 'performance': {'dataset': 'covost2', 'accuracy': None}, 'description': 'A text-to-speech model trained on multiple datasets including mtedx, covost2, europarl_st, and voxpopuli. Supports English, Spanish, French, and Italian languages.'}} </s>\"\n"}
{"text": "<s>[INST]ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, including SegformerFeatureExtractor, SegformerForSemanticSegmentation, PIL, and requests.\\\\n2. Instantiate the feature extractor and the model using the from_pretrained method. The model we'll be using is 'nvidia/segformer-b5-finetuned-ade-640-640', which is specifically designed for semantic segmentation tasks.\\\\n3. Obtain the image from the drone's camera, and use the PIL library to open and preprocess it.\\\\n4. Use the feature extractor to transform the image into an input tensor that is compatible with the segmentation model.\\\\n5. Pass the input tensor through the model to obtain the segmentation logits, which can then be used to identify various objects in the environment.\\\", '<<<code>>>': \\\"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\\nimage_url = 'http://path_to_drone_image.jpg'\\\\nimage = Image.open(requests.get(image_url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\", 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nvidia/segformer-b5-finetuned-ade-640-640', 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20K', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ†ÙÙŠØ° Ø¹Ù…Ù„ÙŠØ© ØªØµÙ†ÙŠÙ Ø¬Ø¯Ø§ÙˆÙ„ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… Ø¬Ù…Ø¹Ù‡Ø§ ÙˆØªØ¬Ù‡ÙŠØ²Ù‡Ø§ Ù…Ø³Ø¨Ù‚Ù‹Ø§. Ø§Ù„Ù‡Ø¯Ù Ù‡Ùˆ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠØ³ØªØ·ÙŠØ¹ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv')))\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': \\\"1. We use the Scikit-learn library to create a machine learning model for wine quality classification. We load the pre-trained model from the Hugging Face Model Hub, which is stored in a joblib format.\\\\n2. Import the necessary libraries such as huggingface_hub, joblib, and pandas.\\\\n3. Download the pre-trained model and dataset using the Hugging Face Hub's cached_download and hf_hub_url functions.\\\\n4. Load the model using joblib.load and read the data using pandas to create a dataframe that contains the wine samples' features and target values.\\\\n5. We can now use the loaded model to predict the wine quality based on the input features. The model's performance can also be assessed using its score method.\\\", '<<<code>>>': \\\"from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\n\\\\nREPO_ID = 'julien-c/wine-quality'\\\\nFILENAME = 'sklearn_model.joblib'\\\\ndata_file = cached_download(hf_hub_url(REPO_ID, 'winequality-red.csv'))\\\\nmodel = joblib.load(cached_download(hf_hub_url(REPO_ID, FILENAME)))\\\\n\\\\nwinedf = pd.read_csv(data_file, sep=';')\\\\nX = winedf.drop(['quality'], axis=1)\\\\nY = winedf['quality']\\\\nlabels = model.predict(X[:3])\\\\nmodel_score = model.score(X, Y)\\\", 'api_call': \\\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv')))\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'julien-c/wine-quality', 'api_call': \\\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv')))\\\", 'api_arguments': ['X'], 'python_environment_requirements': [['huggingface_hub', 'joblib', 'pandas']], 'example_code': 'from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nREPO_ID = julien-c/wine-quality\\\\nFILENAME = sklearn_model.joblib\\\\nmodel = joblib.load(cached_download(\\\\n hf_hub_url(REPO_ID, FILENAME)\\\\n))\\\\ndata_file = cached_download(\\\\n hf_hub_url(REPO_ID, winequality-red.csv)\\\\n)\\\\nwinedf = pd.read_csv(data_file, sep=;)\\\\nX = winedf.drop([quality], axis=1)\\\\nY = winedf[quality]\\\\nprint(X[:3])\\\\nlabels = model.predict(X[:3])\\\\nmodel.score(X, Y)', 'performance': {'dataset': 'julien-c/wine-quality', 'accuracy': 0.6616635397123202}, 'description': 'A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù„Ù… Ø¢Ù„ÙŠ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ ÙÙˆØ±ÙŠ Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ù‚Ø¯Ø±Ø© Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø¯ÙˆÙ† ØªÙˆØ¬ÙŠÙ‡ Ù…Ø³Ø¨Ù‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('typeform/distilbert-base-uncased-mnli')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the AutoTokenizer and AutoModelForSequenceClassification classes from the transformers library.\\\\n2. Load the 'typeform/distilbert-base-uncased-mnli' pre-trained model, which is optimized for zero-shot classification tasks.\\\\n3. Use the AutoTokenizer to tokenize the customer comments.\\\\n4. Employ the AutoModelForSequenceClassification to make predictions for sentiment categories such as 'positive', 'negative', or 'neutral'.\\\\n5. Determine the sentiment class with the highest probability for each customer comment.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'typeform/distilbert-base-uncased-mnli\\\\')\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\\\'typeform/distilbert-base-uncased-mnli\\\\')\\\\n\\\\ninput_text = \\\"I really love the taste of this product!\\\"\\\\ninputs = tokenizer(input_text, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nlabel_mapping = {0: \\\\'positive\\\\', 1: \\\\'negative\\\\', 2: \\\\'neutral\\\\'}\\\\npredicted_class = label_mapping[outputs.logits.argmax().item()]', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('typeform/distilbert-base-uncased-mnli')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'typeform/distilbert-base-uncased-mnli', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('typeform/distilbert-base-uncased-mnli')\\\", 'api_arguments': {'tokenizer': 'AutoTokenizer.from_pretrained(typeform/distilbert-base-uncased-mnli)', 'model': 'AutoModelForSequenceClassification.from_pretrained(typeform/distilbert-base-uncased-mnli)'}, 'python_environment_requirements': [{'transformers': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification'}], 'example_code': 'tokenizer = AutoTokenizer.from_pretrained(typeform/distilbert-base-uncased-mnli)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(typeform/distilbert-base-uncased-mnli)', 'performance': {'dataset': 'multi_nli', 'accuracy': 0.8206875508543532}, 'description': 'This is the uncased DistilBERT model fine-tuned on Multi-Genre Natural Language Inference (MNLI) dataset for the zero-shot classification task.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØºÙŠØ± Ù…Ø­Ø¯Ø¯Ø© Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' function from the transformers library.\\\\n2. Use the 'pipeline' function to create a zero-shot classification model by providing the model name 'valhalla/distilbart-mnli-12-1' as an argument. This model is pretrained on a multi-choice NLI benchmark dataset, making it suitable for zero-shot classification.\\\\n3. Define the categories in which the customer reviews should be classified (e.g., 'customer service', 'product quality', 'shipping', 'pricing').\\\\n4. For each customer review, use the created classifier to perform zero-shot classification and assign the most relevant category to the review.\\\\n5. This way, reviews can be automatically categorized without having an annotated dataset for training.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'valhalla/distilbart-mnli-12-1\\\\')\\\\n\\\\nreview = \\\"The product arrived late and the packaging was damaged.\\\"\\\\ncategories = [\\\\'customer service\\\\', \\\\'product quality\\\\', \\\\'shipping\\\\', \\\\'pricing\\\\']\\\\n\\\\nresult = classifier(review, categories)\\\\nprint(result[\\\\'labels\\\\'][0])  # prints the most relevant category', 'api_call': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'valhalla/distilbart-mnli-12-1', 'api_call': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-1')\\\", 'api_arguments': {'model_name_or_path': 'valhalla/distilbart-mnli-12-1'}, 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'MNLI', 'matched_accuracy': 87.08, 'mismatched_accuracy': 87.5}, 'description': 'distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is designed for zero-shot classification tasks.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± ØºÙŠØ± Ù…Ø´Ø±ÙˆØ·Ø© Ù„Ù…Ø´Ø§Ù‡Ø¯ ØªØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we need to install the diffusers package and import the DDPMPipeline class.\\\\n2. Next, we use the from_pretrained method to load the pre-trained model 'ocariz/universe_1400'. This model has been trained specifically for generating high-quality images of the universe.\\\\n3. We can then use this model to generate a space-themed image that can be used as a desktop wallpaper.\\\\n4. The generated image can be saved to a file or displayed using image manipulation tools.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\\nimage = pipeline().images[0]\\\\nimage.save('space_wallpaper.png')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ocariz/universe_1400', 'api_call': \\\"DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\", 'api_arguments': '', 'python_environment_requirements': ['diffusers'], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is a diffusion model for unconditional image generation of the universe trained for 1400 epochs.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø£Ø¬Ø³Ø§Ù… ÙÙŠ Ø§Ù„ØµÙˆØ± ÙˆÙ…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯ Ø§Ù„ØªÙŠ ØªÙ„ØªÙ‚Ø·Ù‡Ø§ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers and PIL packages, such as YolosForObjectDetection for the object detection model, Image for image processing, and requests for fetching image data.\\\\n2. Use the from_pretrained method of the YolosForObjectDetection class to load the pre-trained model 'hustvl/yolos-small', which is optimized for real-time object detection tasks like detecting vehicles on the streets.\\\\n3. Load image data from an image file or get it in real-time from the traffic surveillance cameras.\\\\n4. Pass the image data to the model, which will analyze the image to identify the vehicles within the frame, giving you the necessary information for traffic monitoring.\\\", '<<<code>>>': \\\"from transformers import YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimage = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with the path to your image\\\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'api_call': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hustvl/yolos-small', 'api_call': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\\\", 'api_arguments': {'model_name': 'hustvl/yolos-small'}, 'python_environment_requirements': [{'packages': ['transformers', 'PIL', 'requests']}], 'example_code': {'import': ['from transformers import YolosFeatureExtractor, YolosForObjectDetection', 'from PIL import Image', 'import requests'], 'url': 'http://images.cocodataset.org/val2017/000000039769.jpg', 'image': 'Image.open(requests.get(url, stream=True).raw)', 'feature_extractor': \\\"YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\\\", 'model': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\\\", 'inputs': \\\"feature_extractor(images=image, return_tensors='pt')\\\", 'outputs': 'model(**inputs)', 'logits': 'outputs.logits', 'bboxes': 'outputs.pred_boxes'}, 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '36.1 AP'}, 'description': 'YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†ØµÙˆØµ Ø¬Ø¯ÙŠØ¯Ø© Ø§Ø³ØªÙ†Ø§Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø¯Ø®Ù„Ø©ØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªÙ‚Ø¯ÙŠØ± Ø¯Ù‚Ø© Ø§Ù„ØªÙˆÙ„ÙŠÙ ÙˆØ§Ù„ØªØ¹Ø¨ÙŠØ± Ø¨Ø§Ù„Ù†Øµ.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the pipeline function from the transformers library provided by Hugging Face.', 'code': '', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Grammar Synthesis', 'api_name': 'pszemraj/flan-t5-large-grammar-synthesis', 'api_call': '', 'api_arguments': ['raw_text'], 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\ncorrector = pipeline(\\\\n 'text2text-generation',\\\\n 'pszemraj/flan-t5-large-grammar-synthesis',\\\\n )\\\\n\\\\nraw_text = 'i can has cheezburger'\\\\nresults = corrector(raw_text)\\\\nprint(results)\\\", 'performance': {'dataset': 'jfleg', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of google/flan-t5-large for grammar correction on an expanded version of the JFLEG dataset.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ø£Ø®Ø±Ù‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('t5-3b')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('t5-3b')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 't5-3b', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('t5-3b')\\\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"input_text = 'translate English to French: The quick brown fox jumps over the lazy dog'; inputs = tokenizer.encode(input_text, return_tensors='pt'); outputs = model.generate(inputs); translated_text = tokenizer.decode(outputs[0])\\\", 'performance': {'dataset': 'c4', 'accuracy': 'See research paper, Table 14'}, 'description': 'T5-3B is a Text-To-Text Transfer Transformer (T5) model with 3 billion parameters. It is designed for various NLP tasks such as translation, summarization, question answering, and sentiment analysis. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on multiple supervised and unsupervised tasks.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library by Hugging Face.\\\\n2. Use the pipeline function to create a text-generation model, specifying the model name 'PygmalionAI/pygmalion-1.3b'.\\\\n3. This model is specially designed for generating conversational responses, ideal for creating a chatbot capable of providing hints to players.\\\\n4. To interact with the chatbot, provide it with an input prompt structured as follows: CHARACTER persona, DIALOGUE history, and USER input.\\\\n5. The model will provide a generated response as the output based on the input prompt.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nhint_chatbot = pipeline(\\\\'text-generation\\\\', \\\\'PygmalionAI/pygmalion-1.3b\\\\')\\\\ninput_prompt = \\\"HintBot\\\\'s Persona: A helpful bot providing hints to players when they are stuck in the game.\\\\\\\\nYou: I\\\\'m stuck in the maze, can you help me?\\\"\\\\nresponse = hint_chatbot(input_prompt)', 'api_call': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pygmalion-1.3b', 'api_call': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", 'api_arguments': 'input_prompt', 'python_environment_requirements': [['transformers']], 'example_code': \\\"[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\\\", 'performance': {'dataset': '56MB of dialogue data', 'accuracy': 'Not provided'}, 'description': \\\"Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù‚ÙŠØ§Ø³ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the required class SentenceTransformer from the package sentence_transformers.\\\\n2. Then, initialize the model using the pre-trained model 'sentence-transformers/distilbert-base-nli-stsb-mean-tokens'.\\\\n3. Encode the given movie plot summaries into 768-dimensional dense vector representations using the model's 'encode' method.\\\\n4. Calculate the pairwise similarities between the embeddings, and find the pair with the highest similarity score.\\\\n5. Return the index of the most similar pair of movie plot summaries.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nimport numpy as np\\\\n\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/distilbert-base-nli-stsb-mean-tokens\\\\')\\\\nmovie_plots = [\\\"A hero with a mysterious past embarks on a journey to save a princess from an evil sorcerer.\\\",\\\\n               \\\"In a post-apocalyptic world, survivors struggle to maintain their humanity as they battle against mutated creatures.\\\",\\\\n               \\\"A young wizard goes on a quest to find a mythical sword, accompanied by a fierce warrior and a skilled archer.\\\"]\\\\nembeddings = model.encode(movie_plots)\\\\nplot_similarity_matrix = cosine_similarity(embeddings)\\\\nnp.fill_diagonal(plot_similarity_matrix, 0)\\\\nmost_similar_pair_idx = np.unravel_index(np.argmax(plot_similarity_matrix, axis=None), plot_similarity_matrix.shape)\\\\n\\\\nmost_similar_pair_idx', 'api_call': \\\"SentenceTransformer('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-stsb-mean-tokens', 'api_call': \\\"SentenceTransformer('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ø£Ø¯Ø§Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØ±Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØµÙ Ø§Ù„Ù†ØµÙŠ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ Ø¨Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨.? [/INST]{'domain': 'Computer Vision Depth Estimation', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import AutoModel\\\\nfrom PIL import Image\\\\nimage = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with path to your image\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\\\\ndepth_map = model(image)\\\", 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-230103-091356', 'api_call': \\\"from transformers import AutoModel\\\\nfrom PIL import Image\\\\nimage = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with path to your image\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\\\\ndepth_map = model(image)\\\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, datasets==2.8.0, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.436, 'Mae': 0.4251, 'Rmse': 0.6169, 'Abs Rel': 0.45, 'Log Mae': 0.1721, 'Log Rmse': 0.2269, 'Delta1': 0.3828, 'Delta2': 0.6326, 'Delta3': 0.8051}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØ¹Ø±Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„ÙƒÙ„Ø§Ù… Ø§Ù„ØµÙˆØªÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© ØªØ¹Ø±Ù Ø§Ù„ÙƒÙ„Ø§Ù… Ù„Ù„ØºØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø©.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-small')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\\\\naudio_path = 'path/to/your/audio_file.wav'\\\\n# replace 'path/to/your/audio_file.wav' with the actual path to your audio file\\\\nsample_rate = 16000\\\\n# replace 16000 with your actual sample rate\\\\naudio_data = read_audio_file(audio_path, sample_rate)\\\\ninput_features = processor(audio_data, sampling_rate=sample_rate, return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\\\nprint(transcription)\\\", 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Transcription and Translation', 'api_name': 'openai/whisper-small', 'api_call': \\\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-small')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\\\\naudio_path = 'path/to/your/audio_file.wav'\\\\n# replace 'path/to/your/audio_file.wav' with the actual path to your audio file\\\\nsample_rate = 16000\\\\n# replace 16000 with your actual sample rate\\\\naudio_data = read_audio_file(audio_path, sample_rate)\\\\ninput_features = processor(audio_data, sampling_rate=sample_rate, return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\\\nprint(transcription)\\\", 'api_arguments': {'language': 'english', 'task': 'transcribe'}, 'python_environment_requirements': {'transformers': 'latest', 'datasets': 'latest'}, 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', 'processor = WhisperProcessor.from_pretrained(openai/whisper-small)', 'model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-small)', 'model.config.forced_decoder_ids = None', 'ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)', 'sample = ds[0][audio]', 'input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features', 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)', 'print(transcription)'], 'performance': {'dataset': 'LibriSpeech (clean) test set', 'accuracy': '3.432 WER'}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and supports transcription and translation in various languages.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù…Ø±ÙƒØ¨Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†ØµØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ ØªÙØ³ÙŠØ± Ø§Ù„ØµÙˆØ± Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø¨ØªÙ‚Ù†ÙŠØ© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Text Classification', 'api_call': \\\"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\nsentiment_analysis = pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\\nresult = sentiment_analysis('I love this product!')\\\", 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'sentiment_analysis_generic_dataset', 'api_call': \\\"from transformers import pipeline\\\\nsentiment_analysis = pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\\nresult = sentiment_analysis('I love this product!')\\\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': \\\"sentiment_analysis('I love this product!')\\\", 'performance': {'dataset': 'generic_dataset', 'accuracy': 'Not specified'}, 'description': 'This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text classification.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨ØµÙ†Ù Ø§Ù„Ù†Øµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, classes, and functions.\\\\n2. Load the pre-trained model, 'zhayunduo/roberta-base-stocktwits-finetuned', and its tokenizer specifically fine-tuned for analyzing sentiments in stock-related comments.\\\\n3. Create an inference pipeline using the tokenizer and the model, and pass in the stock-related messages to predict whether they are bullish (optimistic) or bearish (pessimistic).\\\\n4. Print or return the predicted sentiments.\\\", '<<<code>>>': \\\"from transformers import RobertaForSequenceClassification, RobertaTokenizer, pipeline\\\\ntokenizer_loaded = RobertaTokenizer.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\\\nmodel_loaded = RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\\\nnlp = pipeline('text-classification', model=model_loaded, tokenizer=tokenizer_loaded)\\\\nmessages = ['The stock is going to soar!', 'Sell now or regret it!', 'Steady growth in the long term']\\\\nresults = nlp(messages)\\\\nprint(results)\\\", 'api_call': \\\"RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'zhayunduo/roberta-base-stocktwits-finetuned', 'api_call': \\\"RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\\", 'api_arguments': {'model': 'RobertaForSequenceClassification', 'tokenizer': 'RobertaTokenizer'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import RobertaForSequenceClassification, RobertaTokenizer\\\\nfrom transformers import pipeline\\\\nimport pandas as pd\\\\nimport emoji\\\\ntokenizer_loaded = RobertaTokenizer.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\\\nmodel_loaded = RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\\\nnlp = pipeline(text-classification, model=model_loaded, tokenizer=tokenizer_loaded)\\\\nsentences = pd.Series(['just buy','just sell it','entity rocket to the sky!','go down','even though it is going up, I still think it will not keep this trend in the near future'])\\\\nsentences = list(sentences)\\\\nresults = nlp(sentences)\\\\nprint(results)\\\", 'performance': {'dataset': 'stocktwits', 'accuracy': 0.9343}, 'description': \\\"This model is fine-tuned with roberta-base model on 3,200,000 comments from stocktwits, with the user-labeled tags 'Bullish' or 'Bearish'.\\\" </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ø§Ù„ØªÙŠ ØªØ³ØªÙ‡Ø¯Ù ØªØµÙ†ÙŠÙ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ØŒ Ù‚Ù… Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªÙˆÙÙŠØ±Ù‡Ø§ Ø¨Ø¯ÙˆÙ† ØªØµÙ†ÙŠÙ Ù…Ø³Ø¨Ù‚ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. First, import the necessary libraries, which are 'joblib' for loading the trained model and 'pandas' for handling the dataset.\\\\n2. Use the 'joblib.load' function to load the pre-trained model from the file 'model.joblib'.\\\\n3. Read the dataset with the housing price information as a pandas DataFrame.\\\\n4. Select only the required features listed in the config.json file for the model, and then rename the columns according to the feature name format expected by the model.\\\\n5. Finally, use the loaded model to predict the housing prices based on the input dataset.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'jwan2021/autotrain-us-housing-prices-1771761511', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': {'data': 'pandas.DataFrame'}, 'python_environment_requirements': [{'joblib': 'latest', 'pandas': 'latest'}], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'jwan2021/autotrain-data-us-housing-prices', 'accuracy': {'Loss': 134406.507, 'R2': 0.861, 'MSE': 18065109105.27, 'MAE': 103271.843, 'RMSLE': 0.139}}, 'description': 'A model trained using AutoTrain for predicting US housing prices. The model is trained on the jwan2021/autotrain-data-us-housing-prices dataset and is a single column regression model with an ID of 1771761511.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙØ·Ù„Ø¨ Ù…Ù† ÙØ±ÙŠÙ‚ Ø§Ù„ØªØ¹Ù‚Ø¨ Ø¥Ø¬Ø±Ø§Ø¡ ØªØ­Ù‚ÙŠÙ‚ Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­Ø¯Ø§Ø« Ø¹Ù„Ù‰ ØµÙˆØ± Ù„ÙˆØ­Ø© Ù…Ø±Ø³ÙˆÙ…Ø© ÙŠØ¯ÙˆÙŠÙ‹Ø§ ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ ØµÙˆØ± Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù„ÙˆØ­Ø§Øª ÙÙ†ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class AutoModel from the transformers library provided by Hugging Face.\\\\n2. We then use the from_pretrained method of the AutoModel class to load the pre-trained model 'sayakpaul/glpn-nyu-finetuned-diode-221122-082237'. This model has been fine-tuned on the diode-subset dataset and can perform depth estimation tasks.\\\\n3. This model can be used to analyze images taken by the autonomous vehicle in real-time to estimate the depth information of the surrounding objects.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221122-082237', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\\\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': ['transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3421, 'Mae': 0.27, 'Rmse': 0.4042, 'Abs Rel': 0.3279, 'Log Mae': 0.1132, 'Log Rmse': 0.1688, 'Delta1': 0.5839, 'Delta2': 0.8408, 'Delta3': 0.9309}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¹ØªØ¨Ø± Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙØ±ÙŠ Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù…Ø¬Ø§Ù„Ù‹Ø§ Ø­ÙŠÙˆÙŠÙ‹Ø§ ÙÙŠ ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ©. Ù†Ù‚ÙˆÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ Ù…ÙˆØ¯ÙŠÙ„Ø§ØªÙ†Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…Ø¬Ø§Ù„Ø§Øª Ù…ØªÙ†ÙˆØ¹Ø© Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙ Ù…Ø³Ø¨Ù‚ Ø£Ùˆ ØªØ¯Ø±ÙŠØ¨.? [/INST]{'domain': 'Natural Language Processing Fill-Mask', 'api_call': \\\"AutoModelForMaskedLM.from_pretrained('bert-base-chinese')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import AutoTokenizer, AutoModelForMaskedLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'bert-base-chinese\\\\')\\\\nmodel = AutoModelForMaskedLM.from_pretrained(\\\\'bert-base-chinese\\\\')\\\\n\\\\nsentence = \\\"æˆ‘å–œæ¬¢å­¦ä¹ [MASK]ã€‚\\\"\\\\ninputs = tokenizer(sentence, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\npredictions = outputs.logits.argmax(dim=-1)\\\\npredicted_word = tokenizer.decode(predictions[0, -3]).strip()', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Masked Language Modeling', 'api_name': 'bert-base-chinese', 'api_call': 'from transformers import AutoTokenizer, AutoModelForMaskedLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'bert-base-chinese\\\\')\\\\nmodel = AutoModelForMaskedLM.from_pretrained(\\\\'bert-base-chinese\\\\')\\\\n\\\\nsentence = \\\"æˆ‘å–œæ¬¢å­¦ä¹ [MASK]ã€‚\\\"\\\\ninputs = tokenizer(sentence, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\npredictions = outputs.logits.argmax(dim=-1)\\\\npredicted_word = tokenizer.decode(predictions[0, -3]).strip()', 'api_arguments': {'pretrained_model_name': 'bert-base-chinese'}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModelForMaskedLM'}, 'example_code': 'tokenizer = AutoTokenizer.from_pretrained(bert-base-chinese)\\\\nmodel = AutoModelForMaskedLM.from_pretrained(bert-base-chinese)', 'performance': {'dataset': '[More Information Needed]', 'accuracy': '[More Information Needed]'}, 'description': 'This model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper). It can be used for masked language modeling.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ ØµÙˆØª. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŸ? [/INST]{'domain': 'Audio Audio-to-Audio', 'api_call': \\\"separator.from_hparams(source='speechbrain/sepformer-wsj02mix')\\\", 'api_provider': 'SpeechBrain', 'explanation': '1. Install the SpeechBrain package and import the necessary classes and functions.', 'code': \\\"from speechbrain.pretrained import SepformerSeparation as separator\\\\nimport torchaudio\\\\nmodel = separator.from_hparams(source='speechbrain/sepformer-wsj02mix', savedir='pretrained_models/sepformer-wsj02mix')\\\\nest_sources = model.separate_file(path='path_to_mixed_audio_file.wav')\\\\ntorchaudio.save('source1_output.wav', est_sources[:, :, 0].detach().cpu(), 8000)\\\\ntorchaudio.save('source2_output.wav', est_sources[:, :, 1].detach().cpu(), 8000)\\\", 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'SpeechBrain', 'functionality': 'Audio Source Separation', 'api_name': 'sepformer-wsj02mix', 'api_call': \\\"from speechbrain.pretrained import SepformerSeparation as separator\\\\nimport torchaudio\\\\nmodel = separator.from_hparams(source='speechbrain/sepformer-wsj02mix', savedir='pretrained_models/sepformer-wsj02mix')\\\\nest_sources = model.separate_file(path='path_to_mixed_audio_file.wav')\\\\ntorchaudio.save('source1_output.wav', est_sources[:, :, 0].detach().cpu(), 8000)\\\\ntorchaudio.save('source2_output.wav', est_sources[:, :, 1].detach().cpu(), 8000)\\\", 'api_arguments': ['path'], 'python_environment_requirements': ['pip install speechbrain'], 'example_code': ['from speechbrain.pretrained import SepformerSeparation as separator', 'import torchaudio', \\\"model = separator.from_hparams(source=speechbrain/sepformer-wsj02mix, savedir='pretrained_models/sepformer-wsj02mix')\\\", \\\"est_sources = model.separate_file(path='speechbrain/sepformer-wsj02mix/test_mixture.wav')\\\", 'torchaudio.save(source1hat.wav, est_sources[:, :, 0].detach().cpu(), 8000)', 'torchaudio.save(source2hat.wav, est_sources[:, :, 1].detach().cpu(), 8000)'], 'performance': {'dataset': 'WSJ0-2Mix', 'accuracy': '22.4 dB'}, 'description': 'This repository provides all the necessary tools to perform audio source separation with a SepFormer model, implemented with SpeechBrain, and pretrained on WSJ0-2Mix dataset.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ÙƒØ´Ù Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ù…Ù† Ø®Ù„Ø§Ù„ ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù…ÙˆØ§Ù‚Ø¹ ÙˆØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ø£Ø¬Ø³Ø§Ù… ÙÙŠ Ø§Ù„ØµÙˆØ±.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8m-csgo-player-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and classes, including 'YOLO' and 'render_result' from the ultralyticsplus package.\\\\n2. Create an instance of the YOLO class by providing the model name 'keremberke/yolov8m-csgo-player-detection'.\\\\n3. Override some model parameters to adjust the confidence threshold, IoU threshold, non-maximum suppression settings, and maximum detected objects.\\\\n4. Use the 'predict' method of the model to perform object detection on a given image URL.\\\\n5. Print the detected boxes for the CS:GO players.\\\\n6. Create a rendered version of the input image with the detected players, and visualize the results using the 'show' method.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage_url = 'https://example.com/csgo_image.jpg'\\\\nresults = model.predict(image_url)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image_url, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8m-csgo-player-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8m-csgo-player-detection', 'api_call': \\\"YOLO('keremberke/yolov8m-csgo-player-detection')\\\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.892}, 'description': \\\"An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø² Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ÙˆØ§Ø¨Ø¯Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¶Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©. Ø«Ù… Ø§Ø®ØªØ¨Ø± Ø¯Ù‚Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² ÙˆÙ…Ø±Ø§Ø¬Ø¹ØªÙ‡Ø§.? [/INST]{'domain': 'Natural Language Processing Token Classification', 'api_call': \\\"SequenceTagger.load('flair/upos-english')\\\", 'api_provider': 'Flair', 'explanation': '', 'code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load('flair/upos-english')\\\\ninput_text = 'The quick brown fox jumps over the lazy dog.'\\\\nsentence = Sentence(input_text)\\\\ntagger.predict(sentence)\\\\nprint(sentence)\\\\nfor entity in sentence.get_spans('pos'):\\\\n    print(entity)\\\", 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Flair', 'functionality': 'Part-of-Speech Tagging', 'api_name': 'flair/upos-english', 'api_call': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load('flair/upos-english')\\\\ninput_text = 'The quick brown fox jumps over the lazy dog.'\\\\nsentence = Sentence(input_text)\\\\ntagger.predict(sentence)\\\\nprint(sentence)\\\\nfor entity in sentence.get_spans('pos'):\\\\n    print(entity)\\\", 'api_arguments': ['sentence'], 'python_environment_requirements': 'pip install flair', 'example_code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load('flair/upos-english')\\\\nsentence = Sentence('I love Berlin.')\\\\ntagger.predict(sentence)\\\\nprint(sentence)\\\\nfor entity in sentence.get_spans('pos'):\\\\n    print(entity)\\\", 'performance': {'dataset': 'ontonotes', 'accuracy': 98.6}, 'description': 'This is the standard universal part-of-speech tagging model for English that ships with Flair. It predicts universal POS tags such as ADJ, ADP, ADV, AUX, CCONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, SYM, VERB, and X. The model is based on Flair embeddings and LSTM-CRF.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙŠØ¯ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆÙ†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ØªÙ†ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ù‚ØµÙ‰ Ù‚Ø¯Ø± Ù…Ù† Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„ÙØ¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include 'VideoMAEImageProcessor', 'VideoMAEForPreTraining', 'torch' and 'numpy'.\\\\n2. Load the VideoMAE model, which has been pretrained on a large dataset. This model will analyze short video clips and classify the activities within them automatically.\\\\n3. The VideoMAEImageProcessor will preprocess your input video frames, while the VideoMAEForPreTraining model will analyze the frames and predict the activity class.\\\\n4. The model will return a list of probabilities associated with each class. Pick the class with the highest probability for the final classification result.\\\", '<<<code>>>': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-short')\\\\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short')\\\\ninput_tensor = processor(video, return_tensors='pt').pixel_values\\\\n\\\\noutputs = model(input_tensor)\\\\nactivity_classification = outputs['logits'].argmax(dim=-1).item()\\\", 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'MCG-NJU/videomae-base-short', 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'MCG-NJU/videomae-base-short'}, 'python_environment_requirements': [{'packages': ['transformers']}], 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-short)\\\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short)\\\\npixel_values = processor(video, return_tensors=pt).pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss', 'performance': {'dataset': 'Kinetics-400', 'accuracy': 'Not provided'}, 'description': 'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø¯ÙˆÙ† Ø´Ø±ÙˆØ· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­ÙˆØ§Ø³ÙŠØ¨.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class from the diffusers package. This includes DDPMPipeline for the unconditional image generation model.\\\\n2. We then use the from_pretrained method of the DDPMPipeline class to load the pre-trained model 'ocariz/butterfly_200'. This model has been trained for unconditional image synthesis tasks, with a specific focus on generating images of cute butterflies.\\\\n3. This model can then be used to generate butterfly images, which can be utilized in your marketing materials for environmental causes.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\\nimage = pipeline().images[0]\\\\nimage.save('butterfly_generated_image.png')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ocariz/butterfly_200', 'api_call': \\\"DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\", 'api_arguments': '', 'python_environment_requirements': ['diffusers'], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is a diffusion model for unconditional image generation of cute butterflies trained for 200 epochs.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ Ù‚Ù… Ø¨Ø¬Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„ØµÙˆØ± Ø§Ù„ØµØ­ÙŠØ­Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù…Ø«Ù„ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø©. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ù…ÙƒØªØ¨Ø© Ù…ÙØªÙˆØ­Ø© Ø§Ù„Ù…ØµØ¯Ø± Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©. Ø§Ø³ØªØ®Ø¯Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù„ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. Ù‚Ù… Ø¨ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª. Ø³ÙŠØ³Ø§Ø¹Ø¯Ùƒ Ù‡Ø°Ø§ Ø¹Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ± Ø¨Ø¯Ù‚Ø©. Ù„Ø§ ØªÙ†Ø³Ù‰ ØªÙˆØ«ÙŠÙ‚ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ Ø§Ù„ØµÙŠØ§Ù†Ø© ÙˆØ§Ù„ØªØ·ÙˆÙŠØ±.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, including timm, PIL, and urllib.request.\\\\n2. Load an image sample or an image from the dataset, using the Image.open() method from the PIL library and the urlopen() method from urllib.request.\\\\n3. Create the model 'convnext_base.fb_in1k' pretrained on the ImageNet-1k dataset using the timm.create_model() function. This model is suitable for image classification, feature map extraction, and image embeddings.\\\\n4. Evaluate the model using the model.eval() method.\\\\n5. Resolve the model data configuration using the timm.data.resolve_model_data_config() function.\\\\n6. Create image transforms using the timm.data.create_transform() function. These transforms will help preprocess images for the model.\\\\n7. Using the model and transforms, obtain the embeddings for all images in your dataset.\\\\n8. Compute the similarity between embeddings of each pair of images to cluster similar images together.\\\", '<<<code>>>': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\n\\\\nimg = Image.open(urlopen('https://your-image-url-here'))\\\\nmodel = timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\nimage_embeddings = model(transforms(img).unsqueeze(0))\\\\n\\\\n# Calculate similarity between image_embeddings and cluster similar images together\\\", 'api_call': \\\"timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'convnext_base.fb_in1k', 'api_call': \\\"timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\", 'api_arguments': {'pretrained': 'True', 'features_only': 'True', 'num_classes': '0'}, 'python_environment_requirements': [['timm']], 'example_code': ['from urllib.request import urlopen', 'from PIL import Image', 'import timm', \\\"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\\", \\\"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\", 'model = model.eval()', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'output = model(transforms(img).unsqueeze(0))'], 'performance': {'dataset': 'imagenet-1k', 'accuracy': '83.82%'}, 'description': 'A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªÙ†Ø¨Ø¤ Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø£Ø¬Ø³Ø§Ù… ÙÙŠ Ø§Ù„ØµÙˆØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ØªÙ†ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import required classes - OneFormerProcessor and OneFormerForUniversalSegmentation for processing and segmentation, and Image for handling image data.\\\\n2. Use the from_pretrained method of OneFormerForUniversalSegmentation class to load the pre-trained model 'shi-labs/oneformer_ade20k_swin_tiny'. This model can perform semantic, instance, and panoptic segmentation tasks.\\\\n3. Load the image data from the file or obtain it in real-time from a surveillance camera feed.\\\\n4. Using the processor, create inputs for each segmentation task and use the model to generate outputs with the specific task conditional tokens (semantic, instance, panoptic).\\\\n5. Process the output according to the specific task using the corresponding post-processing method in the processor (post_process_semantic_segmentation, post_process_instance_segmentation, post_process_panoptic_segmentation).\\\", '<<<code>>>': \\\"from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimage = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with path to your image\\\\nprocessor = OneFormerProcessor.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\\nsemantic_inputs = processor(images=image, task_inputs=['semantic'], return_tensors='pt')\\\\nsemantic_outputs = model(**semantic_inputs)\\\\ninstance_inputs = processor(images=image, task_inputs=['instance'], return_tensors='pt')\\\\ninstance_outputs = model(**instance_inputs)\\\\npanoptic_inputs = processor(images=image, task_inputs=['panoptic'], return_tensors='pt')\\\\npanoptic_outputs = model(**panoptic_inputs)\\\", 'api_call': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'shi-labs/oneformer_ade20k_swin_tiny', 'api_call': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\\\n\\\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\\\nsemantic_outputs = model(**semantic_inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\n\\\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\\\ninstance_outputs = model(**instance_inputs)\\\\n\\\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\\\n\\\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\\\npanoptic_outputs = model(**panoptic_inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù†ØµÙˆØµ ÙˆØ¥Ù†Ø´Ø§Ø¡ ØªÙˆØµÙŠÙØ§Øª Ù†ØµÙŠØ© Ù„Ù„ØµÙˆØ±. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ù…Ù† ØµÙˆØ±Ø©ØŸ? [/INST]{'domain': 'Multimodal Text-to-Image', 'api_call': \\\"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\\\", 'api_provider': 'Hugging Face', 'explanation': '1. We first import the pipeline function from the transformers library provided by Hugging Face.', 'code': \\\"from transformers import pipeline\\\\ntext_to_image = pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\\\\ngenerated_image = text_to_image('analog style futuristic cityscape')\\\", 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \\\"from transformers import pipeline\\\\ntext_to_image = pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\\\\ngenerated_image = text_to_image('analog style futuristic cityscape')\\\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \\\"text_to_image('analog style landscape')\\\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \\\"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\\\"}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚Ø¯Ù… Ù„ÙƒÙ… Ø®Ø¯Ù…Ø© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø³Ù…ÙˆØ¹ Ø¹Ø¨Ø± Ù…Ù†ØªØ¬ Ù…Ø¨ØªÙƒØ± ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ØªÙ‚Ù†ÙŠØ§Øª Ø­Ø¯ÙŠØ«Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØªÙŠØ©ØŒ ÙˆÙŠÙ…ÙƒÙ†Ù†Ø§ ØªÙ†Ø¨Ø¤ Ø¨Ø¬ÙˆØ¯Ø© ØµÙˆØª Ø§Ù„Ù†Ø§ØªØ¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"pipeline('text-to-speech', model='mio/Artoria')\\\", '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a text-to-speech model with the specified model, which is 'mio/Artoria'.\\\\n3. This model has been trained on the 'fate' dataset and is capable of converting text input into speech output.\\\\n4. Provide the input text that you want the model to convert into speech.\\\\n5. The model will generate an audio representation of the input text, which can be played back as speech.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntts = pipeline(\\\\'text-to-speech\\\\', model=\\\\'mio/Artoria\\\\')\\\\ntext_to_read_aloud = \\\"This is the sample text to be read out loud by the TTS model.\\\"\\\\nspeech_audio = tts(text_to_read_aloud)', 'api_call': \\\"pipeline('text-to-speech', model='mio/Artoria')\\\", 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mio/Artoria', 'api_call': \\\"pipeline('text-to-speech', model='mio/Artoria')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; tts = pipeline('text-to-speech', model='mio/Artoria'); tts('s')\\\", 'performance': {'dataset': 'fate', 'accuracy': 'Not provided'}, 'description': 'This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ¬Ù…ÙŠØ¹ Ù…Ù„Ø®Øµ Ù†ØµÙŠ Ø§Ø­ØªØ±Ø§ÙÙŠ Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('cointegrated/rut5-base-absum')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the necessary classes 'T5ForConditionalGeneration' and 'T5Tokenizer' from the transformers library.\\\\n2. Then, use 'T5ForConditionalGeneration.from_pretrained()' to load the pre-trained model 'cointegrated/rut5-base-absum', which is specifically designed for abstractive Russian summarization.\\\\n3. Load the tokenizer with 'T5Tokenizer.from_pretrained()'.\\\\n4. Use the tokenizer to tokenize the input text from the Russian newspaper article.\\\\n5. Generate the summary using the model.generate() method with the appropriate parameters, such as max_length, num_beams, etc.\\\\n6. Decode the generated summary using tokenizer.decode() and return the summarized text.\\\", '<<<code>>>': 'from transformers import T5ForConditionalGeneration, T5Tokenizer\\\\n\\\\nMODEL_NAME = \\\\'cointegrated/rut5-base-absum\\\\'\\\\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\\\\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\\\\n\\\\ndef summarize(text, max_length=1000, num_beams=3, do_sample=False, repetition_penalty=10.0):\\\\n    input_text = \\\"summarize: \\\" + text\\\\n    input_tokens = tokenizer(input_text, return_tensors=\\\\'pt\\\\', padding=True)\\\\n    with torch.no_grad():\\\\n        summary_tokens = model.generate(\\\\n            input_tokens,\\\\n            max_length=max_length,\\\\n            num_beams=num_beams,\\\\n            do_sample=do_sample,\\\\n            repetition_penalty=repetition_penalty\\\\n        )\\\\n    summary_text = tokenizer.decode(summary_tokens[0], skip_special_tokens=True)\\\\n    return summary_text\\\\n\\\\narticle_text = \\\"Russian newspaper article text here...\\\"\\\\nsummarized_text = summarize(article_text)', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('cointegrated/rut5-base-absum')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cointegrated/rut5-base-absum', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('cointegrated/rut5-base-absum')\\\", 'api_arguments': {'n_words': 'int', 'compression': 'float', 'max_length': 'int', 'num_beams': 'int', 'do_sample': 'bool', 'repetition_penalty': 'float'}, 'python_environment_requirements': [{'transformers': 'latest', 'torch': 'latest'}], 'example_code': \\\"import torch\\\\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\\\\nMODEL_NAME = 'cointegrated/rut5-base-absum'\\\\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\\\\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\\\\nmodel.cuda();\\\\nmodel.eval();\\\\ndef summarize(\\\\n text, n_words=None, compression=None,\\\\n max_length=1000, num_beams=3, do_sample=False, repetition_penalty=10.0, \\\\n <strong>kwargs\\\\n):\\\\n \\\\n Summarize the text\\\\n The following parameters are mutually exclusive:\\\\n - n_words (int) is an approximate number of words to generate.\\\\n - compression (float) is an approximate length ratio of summary and original text.\\\\n \\\\n if n_words:\\\\n text = '[{}] '.format(n_words) + text\\\\n elif compression:\\\\n text = '[{0:.1g}] '.format(compression) + text\\\\n x = tokenizer(text, return_tensors='pt', padding=True).to(model.device)\\\\n with torch.inference_mode():\\\\n out = model.generate(\\\\n </strong>x, \\\\n max_length=max_length, num_beams=num_beams, \\\\n do_sample=do_sample, repetition_penalty=repetition_penalty, \\\\n **kwargs\\\\n )\\\\n return tokenizer.decode(out[0], skip_special_tokens=True)\\\", 'performance': {'dataset': ['csebuetnlp/xlsum', 'IlyaGusev/gazeta', 'mlsum'], 'accuracy': 'Not provided'}, 'description': 'This is a model for abstractive Russian summarization, based on cointegrated/rut5-base-multitask and fine-tuned on 4 datasets.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªØµÙ…ÙŠÙ… Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚. Ù†Ø­ØªØ§Ø¬ Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„Ù…ÙÙ‡ÙˆÙ… ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('bigcode/santacoder', trust_remote_code=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. This includes 'AutoModelForCausalLM' for the text generation model and 'AutoTokenizer' for tokenization.\\\\n2. Load the 'bigcode/santacoder' model using the 'AutoModelForCausalLM.from_pretrained()' function.\\\\n3. Tokenize and encode the input Python code snippet using the 'AutoTokenizer.from_pretrained()' function.\\\\n4. Generate the completed code snippet using the 'model.generate()' function and decode the generated tokens with the tokenizer.\\\\n5. The completed code snippet will be output.\\\", '<<<code>>>': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ncheckpoint = \\\\'bigcode/santacoder\\\\'\\\\ndevice = \\\\'cuda\\\\' if torch.cuda.is_available() else \\\\'cpu\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\\\\n\\\\ninputs = tokenizer.encode(\\\"def print_hello_world():\\\", return_tensors=\\\\'pt\\\\').to(device)\\\\noutputs = model.generate(inputs)\\\\ncompleted_code = tokenizer.decode(outputs[0])\\\\nprint(completed_code)', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('bigcode/santacoder', trust_remote_code=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bigcode/santacoder', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('bigcode/santacoder', trust_remote_code=True)\\\", 'api_arguments': ['inputs'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ncheckpoint = bigcode/santacoder\\\\ndevice = cuda # for GPU usage or cpu for CPU usage\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\\\\ninputs = tokenizer.encode(def print_hello_world():, return_tensors=pt).to(device)\\\\noutputs = model.generate(inputs)\\\\nprint(tokenizer.decode(outputs[0]))', 'performance': {'dataset': 'bigcode/the-stack', 'accuracy': {'pass@1 on MultiPL HumanEval (Python)': 0.18, 'pass@10 on MultiPL HumanEval (Python)': 0.29, 'pass@100 on MultiPL HumanEval (Python)': 0.49, 'pass@1 on MultiPL MBPP (Python)': 0.35, 'pass@10 on MultiPL MBPP (Python)': 0.58, 'pass@100 on MultiPL MBPP (Python)': 0.77, 'pass@1 on MultiPL HumanEval (JavaScript)': 0.16, 'pass@10 on MultiPL HumanEval (JavaScript)': 0.27, 'pass@100 on MultiPL HumanEval (JavaScript)': 0.47, 'pass@1 on MultiPL MBPP (Javascript)': 0.28, 'pass@10 on MultiPL MBPP (Javascript)': 0.51, 'pass@100 on MultiPL MBPP (Javascript)': 0.7, 'pass@1 on MultiPL HumanEval (Java)': 0.15, 'pass@10 on MultiPL HumanEval (Java)': 0.26, 'pass@100 on MultiPL HumanEval (Java)': 0.41, 'pass@1 on MultiPL MBPP (Java)': 0.28, 'pass@10 on MultiPL MBPP (Java)': 0.44, 'pass@100 on MultiPL MBPP (Java)': 0.59, 'single_line on HumanEval FIM (Python)': 0.44, 'single_line on MultiPL HumanEval FIM (Java)': 0.62, 'single_line on MultiPL HumanEval FIM (JavaScript)': 0.6, 'BLEU on CodeXGLUE code-to-text (Python)': 18.13}}, 'description': 'The SantaCoder models are a series of 1.1B parameter models trained on the Python, Java, and JavaScript subset of The Stack (v1.1) (which excluded opt-out requests). The main model uses Multi Query Attention, was trained using near-deduplication and comment-to-code ratio as filtering criteria and using the Fill-in-the-Middle objective. In addition there are several models that were trained on datasets with different filter parameters and with architecture and objective variations.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ÙØªØ­ ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆÙ‚Ù… Ø¨Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©: \\\"Ø£Ø­Ù†Ø§ Ø¹Ù†Ø¯Ù†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ Ø¨ÙŠØªØ¬Ù…Ø¹ ÙƒÙ„ ØªÙˆØªÙŠÙ† Ø¹Ø´Ø§Ù† Ù†Ù„Ø¹Ø¨ ÙƒÙˆØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© ØªÙ‚Ø¯Ø± ØªÙ„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ\\\". Ø«Ù… Ø§Ø®ØªØ± Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ÙØ¶Ù„Ø© ÙˆØ§Ø³ØªÙ…Ø¹ Ù„Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù….? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the SentenceTransformer from the sentence_transformers package.', 'code': \\\"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\\nsentences = ['This is the English message in the ad campaign',\\\\n             'Ceci est le message franÃ§ais dans la campagne publicitaire',\\\\n             'Dies ist deutscher Text in der Werbekampagne']\\\\nembeddings = model.encode(sentences)\\\\n(similarity_matrix)\\\", 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Sentence Embeddings', 'api_name': 'sentence-transformers/nli-mpnet-base-v2', 'api_call': \\\"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\\nsentences = ['This is the English message in the ad campaign',\\\\n             'Ceci est le message franÃ§ais dans la campagne publicitaire',\\\\n             'Dies ist deutscher Text in der Werbekampagne']\\\\nembeddings = model.encode(sentences)\\\\n(similarity_matrix)\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ù‚Ø³Ù…Ø© Ø§Ù„ØµÙˆØ± Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙŠØªÙ†Ø¨Ø£ Ø¨Ø§Ù„Ø£Ø´ÙƒØ§Ù„ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙÙŠ Ø§Ù„ØµÙˆØ± Ø¨Ø¯Ù‚Ø© ÙˆØ³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8s-building-segmentation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary Python packages, like YOLO and render_result from the ultralyticsplus library.\\\\n2. Load the YOLOv8 model for building segmentation in satellite images using 'keremberke/yolov8s-building-segmentation'.\\\\n3. Configure the model by setting values for 'conf', 'iou', 'agnostic_nms', and 'max_det' parameters. These parameters influence the confidence threshold, intersection over union, non-maximum suppression, and maximum allowed detections.\\\\n4. Prepare the input satellite image by providing a URL or local image file.\\\\n5. Use the predict method of the loaded model to analyze the image and generate segmentation results.\\\\n6. Extract the bounding boxes and masks from the results.\\\\n7. Render the segmented image using the render_result function.\\\\n8. Display the segmented image with buildings highlighted.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-building-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://path/to/satellite/image.jpg'  # replace with your URL or local path\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8s-building-segmentation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8s-building-segmentation', 'api_call': \\\"YOLO('keremberke/yolov8s-building-segmentation')\\\", 'api_arguments': ['conf', 'iou', 'agnostic_nms', 'max_det', 'image'], 'python_environment_requirements': [['ultralyticsplus==0.0.21']], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8s-building-segmentation')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'print(results[0].masks)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.661, 'mAP@0.5(mask)': 0.651}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ ÙÙŠ Ø´Ø±ÙƒØªÙ†Ø§ Ø§Ù„Ù…ØªØ®ØµØµØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØªØµÙ†ÙŠÙ Ø§Ù„ØµÙØ± Ø¥Ù„Ù‰ ØªØµÙ…ÙŠÙ… Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„Ù…ÙÙ‡ÙˆÙ… ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆÙ…Ø´Ø§Ø¨Ù‡Ø© Ø§Ù„Ø¬Ù…Ù„.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the SentenceTransformer class from the sentence_transformers package.\\\\n2. Initialize the SentenceTransformer model using the 'sentence-transformers/all-roberta-large-v1' pre-trained model. This model has been designed to map sentences into a high-dimensional dense vector space.\\\\n3. Encode the headlines using the model, which will generate embeddings for each headline.\\\\n4. Compare the generated embeddings to measure the similarity between news headlines.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nheadlines = [\\\"Breaking: Scientists Discover Cure for Cancer\\\", \\\"Stunning Discovery: New Cancer Treatment Found\\\"]\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-roberta-large-v1\\\\')\\\\nembeddings = model.encode(headlines)', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/all-roberta-large-v1', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 1024 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…Ù† Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Feature Extraction', '<<<api_call>>>': \\\"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To extract features from the biomedical text, you'll need the 'AutoModel' and 'AutoTokenizer' classes from the Hugging Face Transformers library.\\\\n2. Load the BioBERT model ('dmis-lab/biobert-v1.1') using the 'AutoModel.from_pretrained()' function.\\\\n3. Load the tokenizer associated with the BioBERT model, using 'AutoTokenizer.from_pretrained()'.\\\\n4. Tokenize the input biomedical text using the tokenizer with 'tokenizer.encode()'.\\\\n5. Pass the tokenized input to the model to extract features using 'model(tensor_input)'.\\\\n6. The output tensor contains the features extracted from the biomedical text.\\\", '<<<code>>>': 'from transformers import AutoModel, AutoTokenizer\\\\n\\\\nmodel_checkpoint = \\\\'dmis-lab/biobert-v1-1\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\\\nmodel = AutoModel.from_pretrained(model_checkpoint)\\\\n\\\\ntext = \\\"Biomedical text to be analyzed.\\\"\\\\ntokenized_text = tokenizer.encode(text, return_tensors=\\\\'pt\\\\')\\\\n\\\\nwith torch.no_grad():\\\\n    features = model(tokenized_text)[0]', 'api_call': \\\"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \\\"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„Ø§ÙƒØªØ´Ø§Ù ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8s-hard-hat-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the required Python packages: ultralyticsplus and ultralytics.\\\\n2. Import the YOLO model and render_result function from the ultralyticsplus library.\\\\n3. Load the 'keremberke/yolov8s-hard-hat-detection' model to detect hard hats in the image provided.\\\\n4. Configure the model with the appropriate confidence level, IoU threshold, object categories, and maximum number of detections.\\\\n5. Provide an image URL or local image file that contains workers on the construction site.\\\\n6. Use the predict method to detect hard hats in the provided image and then utilize the render_result function to visualize the detected objects.\\\\n7. Finally, display the rendered image to evaluate the safety measures taken by the workers.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-hard-hat-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'construction_site_image_url.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0]);\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8s-hard-hat-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8s-hard-hat-detection', 'api_call': \\\"YOLO('keremberke/yolov8s-hard-hat-detection')\\\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-hard-hat-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.834}, 'description': 'An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ø£Ø®Ø±Ù‰ Ù„ØºØ±Ø¶ Ø¥Ù†ØªØ§Ø¬ Ù†Øµ ÙŠÙ…ÙƒÙ† ÙÙ‡Ù…Ù‡ Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø´Ø±ØŒ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø®ØµØµ Ù„ØªÙˆÙÙŠØ± Ø­Ù„Ø§Ù‹ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨Ø¥Ø¹Ø§Ù‚Ø© Ø¨ØµØ±ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('t5-3b')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Firstly, import T5ForConditionalGeneration and a suitable tokenizer class from the transformers library.\\\\n2. We load the 't5-3b' model using the from_pretrained method, which is designed for various NLP tasks such as translation, summarization, question answering, and sentiment analysis.\\\\n3. Next, we tokenize and encode the given input text using the appropriate tokenizer for the model.\\\\n4. Then, we generate the French translation of the English text using the model's generate method.\\\\n5. Finally, the translated text is obtained by decoding the model's output.\\\", '<<<code>>>': 'from transformers import T5ForConditionalGeneration, T5Tokenizer\\\\n\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'t5-3b\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'t5-3b\\\\')\\\\n\\\\ninput_text = \\\"translate English to French: The quick brown fox jumps over the lazy dog\\\"\\\\ninputs = tokenizer(input_text, return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs.input_ids)\\\\ntranslated_text = tokenizer.decode(outputs[0])', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('t5-3b')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 't5-3b', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('t5-3b')\\\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"input_text = 'translate English to French: The quick brown fox jumps over the lazy dog'; inputs = tokenizer.encode(input_text, return_tensors='pt'); outputs = model.generate(inputs); translated_text = tokenizer.decode(outputs[0])\\\", 'performance': {'dataset': 'c4', 'accuracy': 'See research paper, Table 14'}, 'description': 'T5-3B is a Text-To-Text Transfer Transformer (T5) model with 3 billion parameters. It is designed for various NLP tasks such as translation, summarization, question answering, and sentiment analysis. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on multiple supervised and unsupervised tasks.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ø§Ù†ØªØ§Ø¬ ØµÙˆØ± Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…Ø´Ø±ÙˆØ· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ Ø®Ù„Ù‚ ØµÙˆØ± ÙˆØ§Ù‚Ø¹ÙŠØ© ÙˆØ¬Ø¯ÙŠØ¯Ø© ØªÙ…Ø§Ù…Ø§Ù‹ Ù„ØªØ­Ø³ÙŠÙ† ÙƒÙØ§Ø¡Ø© Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ø¬Ø³Ø§Ù… ÙÙŠ Ø§Ù„ØµÙˆØ±.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the ultralyticsplus library, which includes the YOLO object detector and render_result function.\\\\n2. Instantiate the YOLO object detection model using the 'keremberke/yolov8n-table-extraction' checkpoint. This model is trained to detect tables from images.\\\\n3. Configure the model's detection parameters like confidence threshold, Intersection over Union (IoU) threshold, Non-Maximum Suppression (NMS), and maximum detections.\\\\n4. Load the image of the document containing tables.\\\\n5. Use the .predict() method to perform the table detection in the image.\\\\n6. The resulting object will contain detected bounding boxes and class labels for detected tables.\\\\n7. Use the render_result function to visualize the detection results.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-table-extraction')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path/to/image_with_tables.jpg'\\\\nresults = model.predict(image)\\\\ndetected_tables = results[0].boxes\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8n-table-extraction', 'api_call': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8n-table-extraction')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'table-extraction', 'accuracy': 0.967}, 'description': \\\"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\\\" </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù…Ù†ØªØ¬ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ ØªÙˆÙÙŠØ± ØªÙˆÙ‚Ø¹Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø¥ÙŠØ¬Ø§Ø¯ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ù…Ù†ØªØ¬ Ø§Ù„Ø¬Ø¯ÙŠØ¯.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes LayoutXLMForQuestionAnswering and LayoutXLMTokenizer from the transformers library provided by Hugging Face.\\\\n2. Load the pretrained model 'fimu-docproc-research/CZ_DVQA_layoutxlm-base' using the LayoutXLMForQuestionAnswering.from_pretrained() function. This model is designed to answer questions based on a document containing a mix of text and tables.\\\\n3. Similarly, load the tokenizer using the LayoutXLMTokenizer.from_pretrained() function.\\\\n4. Use the tokenizer to encode the document text and the question. Pass the tokenized inputs to the model for inference.\\\\n5. The model will return an answer based on the document contents.\\\", '<<<code>>>': \\\"from transformers import LayoutXLMForQuestionAnswering, LayoutXLMTokenizer\\\\nmodel = LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\\\\ntokenizer = LayoutXLMTokenizer.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\\\\ninputs = tokenizer(document_text, question, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nanswer = tokenizer.decode(outputs.start_logits.argmax(), outputs.end_logits.argmax() + 1)\\\", 'api_call': \\\"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'CZ_DVQA_layoutxlm-base', 'api_call': \\\"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Document Question Answering model based on LayoutXLM.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØ¹Ù„Ù… Ø¢Ù„ÙŠ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† Ø¥Ø´Ø±Ø§Ù ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠØ¬Ø¨ ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø®ØªØ¨Ø§Ø±Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ØªÙ†ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"CLIPModel.from_pretrained('laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create an instance of the pipeline for the zero-shot-classification task.\\\\n3. Load the specified model 'laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind'. This model is designed for zero-shot image classification and can identify objects in photos without requiring training images.\\\\n4. Use the created classifier with an input image and a list of potential class names to identify the objects in the photo.\\\\n5. The model will predict the class with the highest probability score for the given image.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nclip = pipeline('zero-shot-classification', model='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind')\\\\nclassification_results = clip(image, class_names=['cat', 'dog', 'fish'])\\\", 'api_call': \\\"CLIPModel.from_pretrained('laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind', 'api_call': \\\"CLIPModel.from_pretrained('laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind')\\\", 'api_arguments': 'image, class_names', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline; clip = pipeline('zero-shot-classification', model='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind'); clip(image, class_names=['cat', 'dog', 'fish'])\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '79.1 - 79.4'}, 'description': 'A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ØŸ? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')\\\", 'api_provider': 'Transformers', 'explanation': '1. Install the transformers library, provided by Hugging Face.', 'code': \\\"from transformers import AutoTokenizer, AutoModelForTableQuestionAnswering, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained('google/tapas-mini-finetuned-wtq')\\\\nmodel = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')\\\\ntable_qa_pipeline = pipeline('table-question-answering', model=model, tokenizer=tokenizer)\\\", 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-mini-finetuned-wtq', 'api_call': \\\"from transformers import AutoTokenizer, AutoModelForTableQuestionAnswering, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained('google/tapas-mini-finetuned-wtq')\\\\nmodel = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')\\\\ntable_qa_pipeline = pipeline('table-question-answering', model=model, tokenizer=tokenizer)\\\", 'api_arguments': \\\"tokenizer = AutoTokenizer.from_pretrained('google/tapas-mini-finetuned-wtq'); model = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')\\\", 'python_environment_requirements': 'transformers', 'example_code': \\\"from transformers import pipeline\\\\n\\\\nnlp = pipeline('table-question-answering', model='google/tapas-mini-finetuned-wtq', tokenizer='google/tapas-mini-finetuned-wtq')\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.2854}, 'description': 'TAPAS mini model fine-tuned on WikiTable Questions (WTQ). It is pretrained on a large corpus of English data from Wikipedia and can be used for answering questions related to a table.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚ ÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¹Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ù…Ù‚Ø§Ø·Ø¹ ØµÙˆØªÙŠØ© Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ ØµÙˆØªÙŠØ© Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙˆØ³Ù‡ÙˆÙ„Ø©. ÙŠØ±Ø¬Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ§Ø¬Ù‡Ø© Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø¯ÙŠÙ‡ÙŠØ© ÙˆØ¹Ù…Ù„ÙŠØ© Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ© ÙˆØ§Ø³ØªÙŠØ±Ø§Ø¯Ù‡Ø§ ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¨Ù†Ù‚Ø±Ø© ÙˆØ§Ø­Ø¯Ø©.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the 'pipeline' function from the transformers library.\\\\n2. Use the 'pipeline' function to create an audio source separation model by specifying the model name 'Awais/Audio_Source_Separation'.\\\\n3. This model is trained to perform source separation tasks, meaning it can separate different sources of audio (e.g., vocals and instruments) from a given audio file.\\\\n4. Apply the model on an audio file provided by the musician to separate the vocals from the instrumental sounds.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\naudio_separator = pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\\\\nseparated_sources = audio_separator(music_audio_file_path)\\\", 'api_call': \\\"pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Awais/Audio_Source_Separation', 'api_call': \\\"pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\\\", 'api_arguments': 'audio', 'python_environment_requirements': ['Asteroid'], 'example_code': '', 'performance': {'dataset': 'Libri2Mix', 'accuracy': {'si_sdr': 14.764543634468069, 'si_sdr_imp': 14.764029375607246, 'sdr': 15.29337970745095, 'sdr_imp': 15.114146605113111, 'sir': 24.092904661115366, 'sir_imp': 23.913669683141528, 'sar': 16.06055906916849, 'sar_imp': -51.980784441287454, 'stoi': 0.9311142440593033, 'stoi_imp': 0.21817376142710482}}, 'description': 'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·Ø¨ÙŠÙ‚ ÙŠØ³ØªØ®Ø¯Ù… ØªØ¹Ù„Ù… ØªØ¹Ø²ÙŠØ²ÙŠ ÙÙŠ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ ØµÙˆØ± Ø£Ø®Ø±Ù‰ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø°ÙŠÙ† ÙŠØ¹Ø§Ù†ÙˆÙ† Ù…Ù† Ø¥Ø¹Ø§Ù‚Ø© Ø¨ØµØ±ÙŠØ©ØŒ Ù…Ù…Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ù… Ù…Ù† ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨ØªÙ‡Ù… ÙˆØ§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ù„Ù„ØµÙˆØ± Ø§Ù„Ù…Ø·ÙˆØ±Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the necessary libraries, including ControlNetModel from diffusers, LineartAnimeDetector from controlnet_aux, and CLIPTextModel from transformers.\\\\n2. Load the ControlNet model for creating anime-style images. This model will be used to generate high-quality lineart anime images based on text descriptions.\\\\n3. Define a prompt containing the description of the desired image, like \\\"A warrior girl in the jungle\\\".\\\\n4. Preprocess the image with LineartAnimeDetector, which is used to detect lineart anime in the loaded image.\\\\n5. Load the text encoder from the Hugging Face model hub, and initialize the ControlNetModel.\\\\n6. Set up a diffusion pipeline, using StableDiffusionControlNetPipeline, with the loaded text encoder and control network.\\\\n7. Generate the anime-style image based on the prompt, the image, and the configured pipeline.\\\\n8. Save the generated image as a file or display it.', '<<<code>>>': 'from diffusers import ControlNetModel\\\\nfrom controlnet_aux import LineartAnimeDetector\\\\nfrom transformers import CLIPTextModel\\\\nimport torch\\\\nfrom PIL import Image\\\\nprompt = \\\"A warrior girl in the jungle\\\"\\\\ntext_encoder = CLIPTextModel.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', subfolder=\\\\'text_encoder\\\\', num_hidden_layers=11, torch_dtype=torch.float16)\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/control_v11p_sd15s2_lineart_anime\\\\', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(0)\\\\nresult_image = pipe(prompt, num_inference_steps=30, generator=generator).images[0]\\\\nresult_image.save(\\\\'output_image.png\\\\')', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\\\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': [['pip install diffusers transformers accelerate', 'pip install controlnet_aux==0.3.0']], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import LineartAnimeDetector', 'from transformers import CLIPTextModel', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15s2_lineart_anime', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15s2_lineart_anime/resolve/main/images/input.png', ')', 'image = image.resize((512, 512))', 'prompt = A warrior girl in the jungle', 'processor = LineartAnimeDetector.from_pretrained(lllyasviel/Annotators)', 'control_image = processor(image)', 'control_image.save(./images/control.png)', 'text_encoder = CLIPTextModel.from_pretrained(runwayml/stable-diffusion-v1-5, subfolder=text_encoder, num_hidden_layers=11, torch_dtype=torch.float16)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \\\"image.save('images/image_out.png')\\\"], 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚Ø¯Ù… Ø§Ù„Ø¹Ø§Ù…Ù„ÙˆÙ† ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØµÙˆØ± ØµÙˆØªÙŠØ© Ø®Ø¯Ù…Ø§Øª ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ Ù…ØªÙ‚Ù† ÙˆÙØ¹Ø§Ù„ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¯Ø±Ø¬Ø© ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯Ù‚Ø© ÙˆØ¥ØªÙ‚Ø§Ù† Ø¨Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"MarianMTModel.from_pretrained('pytorch-models/opus-mt-tc-big-en-pt')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library, which includes MarianMTModel and MarianTokenizer.\\\\n2. Define the source text as a list containing multiple English sentences.\\\\n3. Load the tokenizer and model using the model name 'pytorch-models/opus-mt-tc-big-en-pt'.\\\\n4. Use the tokenizer to preprocess the source text and pass it to the model for generating the translated text.\\\\n5. Decode the translated text using the tokenizer and skip the special tokens.\\\", '<<<code>>>': 'from transformers import MarianMTModel, MarianTokenizer\\\\n\\\\nsrc_text = [\\\\n    \\\"Tom tried to stab me.\\\",\\\\n    \\\"He has been to Hawaii several times.\\\"\\\\n]\\\\n\\\\nmodel_name = \\\\'pytorch-models/opus-mt-tc-big-en-pt\\\\'\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\n\\\\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\\\\'pt\\\\', padding=True))\\\\n\\\\nfor t in translated:\\\\n    print(tokenizer.decode(t, skip_special_tokens=True))', 'api_call': \\\"MarianMTModel.from_pretrained('pytorch-models/opus-mt-tc-big-en-pt')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-tc-big-en-pt', 'api_call': \\\"MarianMTModel.from_pretrained('pytorch-models/opus-mt-tc-big-en-pt')\\\", 'api_arguments': {'src_text': 'list of text strings with language tokens'}, 'python_environment_requirements': [{'transformers': '4.16.2'}], 'example_code': 'from transformers import MarianMTModel, MarianTokenizer\\\\nsrc_text = [\\\\n &gt;&gt;por&lt;&lt; Tom tried to stab me.,\\\\n &gt;&gt;por&lt;&lt; He has been to Hawaii several times.\\\\n]\\\\nmodel_name = pytorch-models/opus-mt-tc-big-en-pt\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\ntranslated = model.generate(**tokenizer(src_text, return_tensors=pt, padding=True))\\\\nfor t in translated:\\\\n print( tokenizer.decode(t, skip_special_tokens=True) )', 'performance': {'dataset': [{'name': 'flores101-devtest', 'accuracy': 50.4}, {'name': 'tatoeba-test-v2021-08-07', 'accuracy': 49.6}]}, 'description': 'Neural machine translation model for translating from English (en) to Portuguese (pt). This model is part of the OPUS-MT project, an effort to make neural machine translation models widely available and accessible for many languages in the world.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù…Ù„Ø®ØµÙ‹Ø§ Ù„Ù†Øµ Ù‚ØµÙŠØ± Ø¨ØªÙ‚Ù†ÙŠØ© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙŠ Ù„ØªÙ„Ø®ÙŠØµ Ù†Ù‚Ø§Ø· Ø±Ø¦ÙŠØ³ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"pipeline('summarization', model='philschmid/bart-large-cnn-samsum')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a summarization model. Specify the model 'philschmid/bart-large-cnn-samsum' to be loaded. This model is trained to generate abstractive summaries of conversations.\\\\n3. The created summarizer can be used to provide a summarized version of the conversations in the app.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nsummarizer = pipeline('summarization', model='philschmid/bart-large-cnn-samsum')\\\\nconversation_summary = summarizer(conversation_text)\\\", 'api_call': \\\"pipeline('summarization', model='philschmid/bart-large-cnn-samsum')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'philschmid/bart-large-cnn-samsum', 'api_call': \\\"pipeline('summarization', model='philschmid/bart-large-cnn-samsum')\\\", 'api_arguments': {'model': 'philschmid/bart-large-cnn-samsum'}, 'python_environment_requirements': [{'transformers': 'latest'}], 'example_code': \\\"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\\\nconversation = '''Jeff: Can I train a ğŸ¤— Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n'''\\\\nsummarizer(conversation)\\\", 'performance': {'dataset': 'samsum', 'accuracy': {'eval_rouge1': 42.621, 'eval_rouge2': 21.9825, 'eval_rougeL': 33.034, 'eval_rougeLsum': 39.6783, 'test_rouge1': 41.3174, 'test_rouge2': 20.8716, 'test_rougeL': 32.1337, 'test_rougeLsum': 38.4149}}, 'description': 'philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ§Øª ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ø¥Ù†Ø´Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚Ø§Øª ØµÙˆØªÙŠØ© Ù…Ø¨ØªÙƒØ±Ø© ÙˆØ°Ø§Øª Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ©ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ Ø¯Ù…Ø¬ Ù‡Ø°Ù‡ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ù„ØªØ­Ù‚ÙŠÙ‚ ØªØ¬Ø±Ø¨Ø© ØµÙˆØªÙŠØ© Ù…Ù…ÙŠØ²Ø© ÙˆÙ…Ù„Ù‡Ù…Ø©.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\\\", '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. We import the necessary classes from the espnet2.bin.tts_inference package. This includes Text2Speech for the text-to-speech model.\\\\n2. We then use the from_pretrained method of the Text2Speech class to load the pre-trained model 'espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best'. This model has been trained to convert Chinese text input into speech output in Chinese.\\\\n3. The created text-to-speech system can be used to process the input text and generate the corresponding audio file.\\\\n4. For saving the audio file, we use the soundfile library, which provides a convenient way to write the generated speech to a file.\\\", '<<<code>>>': \\\"import soundfile\\\\nfrom espnet2.bin.tts_inference import Text2Speech\\\\ntext2speech = Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\\\\ntext = 'æ˜¥æ±Ÿæ½®æ°´è¿æµ·å¹³ï¼Œæµ·ä¸Šæ˜æœˆå…±æ½®ç”Ÿ'\\\\nspeech = text2speech(text)['wav']\\\\nsoundfile.write('output.wav', speech.numpy(), text2speech.fs, 'PCM_16')\\\", 'api_call': \\\"Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\\\", 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best', 'api_call': \\\"Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['torch', 'espnet_model_zoo']], 'example_code': 'import soundfile\\\\nfrom espnet2.bin.tts_inference import Text2Speech\\\\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\\\\ntext = æ˜¥æ±Ÿæ½®æ°´è¿æµ·å¹³ï¼Œæµ·ä¸Šæ˜æœˆå…±æ½®ç”Ÿ\\\\nspeech = text2speech(text)[wav]\\\\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)', 'performance': {'dataset': 'csmsc', 'accuracy': 'Not specified'}, 'description': 'A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ ØªØ¬Ø±Ø¨ÙŠØ§Øª Ø¬ÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¬ÙŠÙ„ Ø§Ù„Ù†ØµÙŠØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© Ù„Ø¯Ø¹Ù… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Ø¸Ø§Ù….? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the necessary libraries, which include the T5ForConditionalGeneration class and the T5Tokenizer from the transformers library.', 'code': 'from transformers import T5ForConditionalGeneration, T5Tokenizer\\\\n\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'castorini/doc2query-t5-base-msmarco\\\\')\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'castorini/doc2query-t5-base-msmarco\\\\')\\\\n\\\\ninput_text = \\\"<organization\\\\'s technical document>\\\"\\\\nencoded_input = tokenizer.encode(input_text, return_tensors=\\\"pt\\\")\\\\ngenerated_queries = model.generate(encoded_input, max_length=50)\\\\ndecoded_queries = tokenizer.batch_decode(generated_queries, skip_special_tokens=True)', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Text2Text Generation', 'api_name': 'castorini/doc2query-t5-base-msmarco', 'api_call': 'from transformers import T5ForConditionalGeneration, T5Tokenizer\\\\n\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'castorini/doc2query-t5-base-msmarco\\\\')\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'castorini/doc2query-t5-base-msmarco\\\\')\\\\n\\\\ninput_text = \\\"<organization\\\\'s technical document>\\\"\\\\nencoded_input = tokenizer.encode(input_text, return_tensors=\\\"pt\\\")\\\\ngenerated_queries = model.generate(encoded_input, max_length=50)\\\\ndecoded_queries = tokenizer.batch_decode(generated_queries, skip_special_tokens=True)', 'api_arguments': 'text, max_length', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'MS MARCO', 'accuracy': 'Not specified'}, 'description': 'A T5 model trained on the MS MARCO dataset for generating queries from documents.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†ÙˆØ¯ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ Ø¨ÙŠÙ† Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„ ÙˆØ³Ù„Ø³.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"translation_pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create a translation pipeline for translating German text to English text using the 'Helsinki-NLP/opus-mt-de-en' model. This model is specifically trained for German to English translations and achieved reasonable performance scores on various translation benchmarks.\\\\n3. Use the created translation pipeline to translate the sales emails from German to English.\\\\n4. The translated text can then be used by the sales team to address the client's needs expressed in the email.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntranslation_pipeline = pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\\\\ntranslated_text = translation_pipeline(german_sales_email)\\\", 'api_call': \\\"translation_pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-de-en', 'api_call': \\\"translation_pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\\\", 'api_arguments': ['source languages: de', 'target languages: en'], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'opus', 'accuracy': {'newssyscomb2009.de.en': 29.4, 'news-test2008.de.en': 27.8, 'newstest2009.de.en': 26.8, 'newstest2010.de.en': 30.2, 'newstest2011.de.en': 27.4, 'newstest2012.de.en': 29.1, 'newstest2013.de.en': 32.1, 'newstest2014-deen.de.en': 34.0, 'newstest2015-ende.de.en': 34.2, 'newstest2016-ende.de.en': 40.4, 'newstest2017-ende.de.en': 35.7, 'newstest2018-ende.de.en': 43.7, 'newstest2019-deen.de.en': 40.1, 'Tatoeba.de.en': 55.4}}, 'description': 'A German to English translation model trained on the OPUS dataset using the Hugging Face Transformers library.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include AutoFeatureExtractor and RegNetForImageClassification from transformers, torch, and load_dataset from datasets.\\\\n2. Load a dataset containing a collection of images, which can be obtained from sources like Hugging Face's Dataset Hub.\\\\n3. Preprocess the input image data using AutoFeatureExtractor, which helps in preparing the image data to be fed to the RegNet model.\\\\n4. Load the pre-trained image classification model 'zuppif/regnet-y-040' using RegNetForImageClassification.from_pretrained(). This model is trained to categorize images based on their content.\\\\n5. Use the model to classify the images and retrieve the predicted label for each image.\\\", '<<<code>>>': \\\"from transformers import AutoFeatureExtractor, RegNetForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset('<your-dataset>')  # replace with your dataset\\\\nimage = dataset['test']['image'][0]  # sample image from the dataset\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('zuppif/regnet-y-040')\\\\nmodel = RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\\ninputs = feature_extractor(image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/regnet-y-008', 'api_call': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'zuppif/regnet-y-040'}, 'python_environment_requirements': [{'transformers': 'AutoFeatureExtractor, RegNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}], 'example_code': 'from transformers import AutoFeatureExtractor, RegNetForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset(huggingface/cats-image)\\\\nimage = dataset[test][image][0]\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\\\ninputs = feature_extractor(image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n... logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§ÙƒØªØ´Ø§Ù Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØª ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØµÙˆØªÙŠØ§ØªØŒ ÙˆÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ØªÙˆÙÙŠØ± Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¶Ù…Ø§Ù† Ø£Ø¯Ø§Ø¡ Ù…ÙˆØ«ÙˆÙ‚ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬.? [/INST]{'domain': 'Audio Voice Activity Detection', 'api_call': \\\"Pipeline.from_pretrained('pyannote/speaker-diarization@2.1')\\\", 'api_provider': 'pyannote.audio', 'explanation': \\\"1. Import the 'Pipeline' class from the pyannote.audio library.\\\", 'code': \\\"from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained('pyannote/speaker-diarization@2.1')\\\\naudio_file_path = 'path/to/your/audio_file.wav'\\\\ndiarization = pipeline(audio_file_path)\\\", 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'pyannote.audio', 'functionality': 'Speaker diarization', 'api_name': 'johnislarry/cloned-pyannote-speaker-diarization-endpoint', 'api_call': \\\"from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained('pyannote/speaker-diarization@2.1')\\\\naudio_file_path = 'path/to/your/audio_file.wav'\\\\ndiarization = pipeline(audio_file_path)\\\", 'api_arguments': ['num_speakers', 'min_speakers', 'max_speakers', 'segmentation_onset'], 'python_environment_requirements': 'pyannote.audio 2.0', 'example_code': {'load_pipeline': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)', 'apply_pipeline': 'diarization = pipeline(audio.wav)', 'save_output': 'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)'}, 'performance': {'dataset': [{'name': 'AISHELL-4', 'accuracy': {'DER%': 14.61, 'FA%': 3.31, 'Miss%': 4.35, 'Conf%': 6.95}}, {'name': 'AMI Mix-Headset only_words', 'accuracy': {'DER%': 18.21, 'FA%': 3.28, 'Miss%': 11.07, 'Conf%': 3.87}}, {'name': 'AMI Array1-01 only_words', 'accuracy': {'DER%': 29.0, 'FA%': 2.71, 'Miss%': 21.61, 'Conf%': 4.68}}, {'name': 'CALLHOME Part2', 'accuracy': {'DER%': 30.24, 'FA%': 3.71, 'Miss%': 16.86, 'Conf%': 9.66}}, {'name': 'DIHARD 3 Full', 'accuracy': {'DER%': 20.99, 'FA%': 4.25, 'Miss%': 10.74, 'Conf%': 6.0}}, {'name': 'REPERE Phase 2', 'accuracy': {'DER%': 12.62, 'FA%': 1.55, 'Miss%': 3.3, 'Conf%': 7.76}}, {'name': 'VoxConverse v0.0.2', 'accuracy': {'DER%': 12.76, 'FA%': 3.45, 'Miss%': 3.85, 'Conf%': 5.46}}]}, 'description': 'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.'}} </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø§Ù„Ø°ÙŠ ØªÙ… Ù…Ø´Ø§Ø±ÙƒØªÙ‡ØŒ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙˆÙ„ÙŠØ© Ù„ØªÙˆÙ‚Ø¹ Ù‚ÙŠÙ… Ù…ØªØºÙŠØ± Ù…Ø³ØªÙ†ØªØ¬ Ù…Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠØ©. ÙŠØ¬Ø¨ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© ÙˆØ§Ø®ØªØ¨Ø§Ø±Ù‡ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù„Ø¶Ù…Ø§Ù† ÙƒÙØ§Ø¡ØªÙ‡. Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù…ÙŠØ²Ø© Ø­Ø³Ø§Ø³Ø© Ù„Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹ØªÙ…Ø¯Ø©. Ù„Ø°Ù„ÙƒØŒ ÙŠØ¬Ø¨ ØªÙˆÙÙŠØ± Ø¨ÙŠØ¦Ø© Ø§Ø®ØªØ¨Ø§Ø± Ù…Ù†Ø§Ø³Ø¨Ø© ÙˆØ§Ø®ØªØ¨Ø§Ø±Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. First, we need to load the tabular regression model provided by Hugging Face. This can be done using the joblib library.\\\\n2. Next, we need to read and preprocess the data in the format required by the model. This involves selecting the necessary feature columns and renaming them.\\\\n3. Then, we simply use the loaded model to make predictions for CO2 emissions. The predictions are returned as a numeric array, where each element represents the predicted CO2 emission for the corresponding input example.', '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nimport json\\\\n\\\\n# Load the pretrained model\\\\nmodel = joblib.load('model.joblib')\\\\n\\\\n# Load the feature configuration\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\n\\\\n# Load and preprocess the vehicle lineup data\\\\ndata = pd.read_csv('vehicle_lineup.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\n\\\\n# Make predictions for the CO2 emissions\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'farouk97/autotrain-test7-2644pc-linearregr-38619101723', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'farouk97/autotrain-data-test7-2644pc-linearregr', 'accuracy': {'Loss': 0.145, 'R2': 0.0, 'MSE': 0.021, 'MAE': 0.099, 'RMSLE': 0.101}}, 'description': 'A tabular regression model trained using AutoTrain to predict CO2 emissions (in grams).' </s>\"\n"}
{"text": "<s>[INST]Ø¹ÙŠÙÙ‘Ù† Ø³Ø¤Ø§Ù„Ù‹Ø§ Ø­ÙˆÙ„ Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø§Ù„Ù…Ø±Ø¦ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©.? [/INST]{'domain': 'Multimodal Visual Question Answering', 'api_call': \\\"pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\nvqa_pipeline = pipeline(\\\\'visual-question-answering\\\\', model=\\\\'azwierzc/vilt-b32-finetuned-vqa-pl\\\\')\\\\nimage_path = \\\"path/to/image.jpg\\\"\\\\nquestion_polish = \\\"Pytanie po polsku tutaj\\\"\\\\nanswer = vqa_pipeline(image=image_path, question=question_polish)', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'azwierzc/vilt-b32-finetuned-vqa-pl', 'api_call': 'from transformers import pipeline\\\\nvqa_pipeline = pipeline(\\\\'visual-question-answering\\\\', model=\\\\'azwierzc/vilt-b32-finetuned-vqa-pl\\\\')\\\\nimage_path = \\\"path/to/image.jpg\\\"\\\\nquestion_polish = \\\"Pytanie po polsku tutaj\\\"\\\\nanswer = vqa_pipeline(image=image_path, question=question_polish)', 'api_arguments': {'image': 'path_to_image', 'question': 'question_text'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the Polish language.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØµÙˆØ± Ù…ÙÙŠØ¯Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Multimodal Graph Machine Learning Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·. ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ù†Ø´Ø§Ø¡ ØªØ¹Ù„ÙŠÙ…Ø§Øª ØªÙˆØ¬ÙŠÙ‡ÙŠØ© ØªØ´Ø±Ø­ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ© ÙˆØ§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„ØªØ­Ù‚ÙŠÙ‚ ØªÙ‚Ø¯Ù… ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ØªÙ†Ø¨Ø¤ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©.? [/INST]'<<<domain>>>': 'Multimodal Graph Machine Learning', '<<<api_call>>>': \\\"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required library, AutoModel, from the transformers package to use the Graphormer model.\\\\n2. Load the pretrained 'graphormer-base-pcqm4mv1' model to leverage its capabilities in predicting the quantum properties of materials.\\\\n3. The Graphormer model has been trained on the PCQM4M-LSC dataset and demonstrates excellent performance in graph representation tasks and graph classification tasks.\\\\n4. You can now use the Graphormer model for your material science research, such as predicting quantum properties of various materials or fine-tuning the model for downstream specific tasks.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\\\", 'api_call': \\\"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': \\\"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers']], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ±ØºØ¨ ÙÙŠ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙŠØ³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØµÙˆØª ÙˆØ§Ù„ØµÙˆØ± Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Video', '<<<api_call>>>': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import necessary libraries from the Tune-A-Video package, torch library and others.\\\\n2. Load the pre-trained models from the Hugging Face model hub, which include the diffusion model and the UNet3DConditionModel as specified in the API call.\\\\n3. Create a pipeline object 'pipe' with the combination of both models.\\\\n4. Set the textual prompt as 'a bear enjoying a beautiful sunset in the forest'.\\\\n5. Pass the prompt and other parameters like video_length, height, width for generating the video to the 'pipe'.\\\\n6. Save the generated video to a specified location.\\\", '<<<code>>>': \\\"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\\\nfrom tuneavideo.util import save_videos_grid\\\\nimport torch\\\\n\\\\npretrained_model_path = 'nitrosocke/redshift-diffusion'\\\\nunet_model_path = 'Tune-A-Video-library/redshift-man-skiing'\\\\nprompt = 'a bear enjoying a beautiful sunset in the forest'\\\\n\\\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to('cuda')\\\\npipe.enable_xformers_memory_efficient_attention()\\\\n\\\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\\\nsave_videos_grid(video, f'./{prompt}.gif')\\\", 'api_call': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'redshift-man-skiing', 'api_call': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': [['torch', 'tuneavideo']], 'example_code': \\\"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\\\nfrom tuneavideo.util import save_videos_grid\\\\nimport torch\\\\npretrained_model_path = nitrosocke/redshift-diffusion\\\\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\\\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\nprompt = (redshift style) spider man is skiing\\\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\\\nsave_videos_grid(video, f./{prompt}.gif)\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': \\\"Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø­ÙŠØ« ÙŠØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø¨Ø´ÙƒÙ„ Ø°ÙƒÙŠ ÙˆÙ†Ø±ÙŠØ¯ ØªÙˆÙ‚Ø¹ Ø±Ø¯ÙˆØ¯ Ø§Ù„ÙØ¹Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.? [/INST]{'domain': 'Natural Language Processing Conversational', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('Zixtrauce/JohnBot')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We import the necessary class AutoModelForCausalLM from the transformers library provided by Hugging Face.', 'code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nmodel = AutoModelForCausalLM.from_pretrained('Zixtrauce/JohnBot')\\\\ntokenizer = AutoTokenizer.from_pretrained('Zixtrauce/JohnBot')\\\\n\\\\ndef generate_response(customer_message):\\\\n    input_tensor = tokenizer.encode(customer_message + tokenizer.eos_token, return_tensors='pt')\\\\n    reply_tensor = model.generate(input_tensor, max_length=100, pad_token_id=tokenizer.eos_token_id)\\\\n    response_text = tokenizer.decode(reply_tensor[:, input_tensor.shape[-1]:][0], skip_special_tokens=True)\\\\n    return response_text\\\", 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'Zixtrauce/JohnBot', 'api_call': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nmodel = AutoModelForCausalLM.from_pretrained('Zixtrauce/JohnBot')\\\\ntokenizer = AutoTokenizer.from_pretrained('Zixtrauce/JohnBot')\\\\n\\\\ndef generate_response(customer_message):\\\\n    input_tensor = tokenizer.encode(customer_message + tokenizer.eos_token, return_tensors='pt')\\\\n    reply_tensor = model.generate(input_tensor, max_length=100, pad_token_id=tokenizer.eos_token_id)\\\\n    response_text = tokenizer.decode(reply_tensor[:, input_tensor.shape[-1]:][0], skip_special_tokens=True)\\\\n    return response_text\\\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'Input a message to start chatting with Zixtrauce/JohnBot.', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'JohnBot is a conversational model based on the gpt2 architecture and trained using the Hugging Face Transformers library. It can be used for generating text responses in a chat-based interface.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ´ØºÙŠÙ„ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ ÙÙŠ Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ ÙˆØ§Ù„ØµÙˆØ±. Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ù‡Ø°Ø§ØŸ? [/INST]'<<<domain>>>': 'Multimodal Graph Machine Learning', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import necessary libraries, including PyTorch and Transformers.\\\\n2. Create a simple neural network class (NanoCircuit) that takes in the MNIST dataset input and outputs logits for each of the ten possible classes.\\\\n3. Define a function (train_with_data_cap) that takes in the created model, data_loader, criterion, optimizer, device, and a data_cap_gb parameter. This function will train the model on the dataset, running an epoch in a loop until the amount of data processed surpasses the data_cap_gb limit.\\\\n4. Then, use the trained model to perform predictions on game-related processing like GTA5.\\\\n5. To use the model in-game, pass the AI model instance to the game using its PID and make sure the targeted application is running to interact with it.', '<<<code>>>': 'import torch\\\\nimport torch.nn as nn\\\\nimport torch.optim as optim\\\\nfrom torchvision import datasets, transforms\\\\n\\\\nclass NanoCircuit(nn.Module):\\\\n    def __init__(self):\\\\n        super(NanoCircuit, self).__init__()\\\\n        self.fc1 = nn.Linear(784, 128)\\\\n        self.fc2 = nn.Linear(128, 10)\\\\n\\\\n    def forward(self, x):\\\\n        x = x.view(-1, 784)\\\\n        x = torch.relu(self.fc1(x))\\\\n        x = self.fc2(x)\\\\n        return x\\\\n\\\\ndef train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):\\\\n    data_processed = 0\\\\n    data_cap_bytes = data_cap_gb * (1024 ** 3)\\\\n    epoch = 0\\\\n    while data_processed < data_cap_bytes:\\\\n        running_loss = 0.0\\\\n        for i, data in enumerate(data_loader, 0):\\\\n            inputs, labels = data\\\\n            inputs, labels = inputs.to(device), labels.to(device)\\\\n            data_processed += inputs.nelement() * inputs.element_size()\\\\n            if data_processed >= data_cap_bytes:\\\\n                break\\\\n            optimizer.zero_grad()\\\\n            outputs = model(inputs.view(-1, 28 * 28))\\\\n            loss = criterion(outputs, labels)\\\\n            loss.backward()\\\\n            optimizer.step()\\\\n            running_loss += loss.item()\\\\n        epoch += 1\\\\n    return model\\\\n\\\\n# Training the model with a specific data cap\\\\ntrain_loader = torch.utils.data.DataLoader(\\\\n    datasets.MNIST(\\\\'data\\\\', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\\\\n    batch_size=batch_size, shuffle=True)\\\\ndevice = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\\\nmodel = NanoCircuit().to(device)\\\\ncriterion = nn.CrossEntropyLoss()\\\\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\\\\ndata_cap_gb = 10\\\\ntrained_model = train_with_data_cap(model, train_loader, criterion, optimizer, device, data_cap_gb)', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GTA5_PROCESS_LEARNING_AI', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\\\", 'api_arguments': {'model': 'NanoCircuit', 'data_loader': 'train_loader', 'criterion': 'nn.CrossEntropyLoss', 'optimizer': 'optim.SGD', 'device': 'torch.device', 'data_cap_gb': 10}, 'python_environment_requirements': [['contextlib', 'os', 'matplotlib', 'numpy', 'torch', 'torch.nn', 'torch.optim', 'requests', 'torchvision', 'psutil', 'time', 'subprocess', 'onnxruntime', 'numexpr', 'transformers']], 'example_code': {'import_libraries': ['import contextlib', 'import os', 'from matplotlib import pyplot as plt', 'import numpy as np', 'import torch', 'import torch.nn as nn', 'import torch.optim as optim', 'import requests', 'from torchvision import datasets, transforms', 'import psutil', 'import time', 'import subprocess', 'import onnxruntime as ort', 'import matplotlib.pyplot as plt', 'import numpy as np', 'import numexpr as ne', 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM'], 'define_neural_network': ['class NanoCircuit(nn.Module):', ' def init(self):', ' super(NanoCircuit, self).init()', ' self.fc1 = nn.Linear(784, 128)', ' self.fc2 = nn.Linear(128, 10)', 'def forward(self, x):', ' x = x.view(-1, 784)', ' x = torch.relu(self.fc1(x))', ' x = self.fc2(x)', ' return x'], 'train_with_data_cap': ['def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):', ' data_processed = 0', ' data_cap_bytes = data_cap_gb * (1024 ** 3)', ' epoch = 0', 'while data_processed < data_cap_bytes:', ' running_loss = 0.0', ' for i, data in enumerate(data_loader, 0):', ' inputs, labels = data', ' inputs, labels = inputs.to(device), labels.to(device)', ' data_processed += inputs.nelement() * inputs.element_size()', ' if data_processed >= data_cap_bytes:', ' break', ' optimizer.zero_grad()', ' outputs = model(inputs.view(-1, 28 * 28))', ' loss = criterion(outputs, labels)', ' loss.backward()', ' optimizer.step()', ' running_loss += loss.item()', 'epoch += 1', 'print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})', 'print(fData processed: {data_processed / (1024 ** 3):.2f} GB)', 'return model']}, 'performance': {'dataset': 'MNIST', 'accuracy': 'Not specified'}, 'description': 'This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ. Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙØ§Øª ÙˆÙÙ‚Ù‹Ø§ Ù„Ù†ÙˆØ¹Ù‡Ø§ ÙˆÙ…Ø­ØªÙˆØ§Ù‡Ø§.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('madhurjindal/autonlp-Gibberish-Detector-492513457')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries: AutoModelForSequenceClassification and AutoTokenizer from the transformers library provided by Hugging Face.\\\\n2. Load the pretrained model 'madhurjindal/autonlp-Gibberish-Detector-492513457' using AutoModelForSequenceClassification.from_pretrained() method.\\\\n3. Load the tokenizer associated with the model using the AutoTokenizer.from_pretrained() method.\\\\n4. Tokenize the given input text using the tokenizer and generate the input tensors.\\\\n5. Pass the input tensors to the model and obtain the model outputs.\\\\n6. Analyze the model outputs to determine if the given text input is gibberish or not.\\\", '<<<code>>>': 'from transformers import AutoModelForSequenceClassification, AutoTokenizer\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\\\'madhurjindal/autonlp-Gibberish-Detector-492513457\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'madhurjindal/autonlp-Gibberish-Detector-492513457\\\\')\\\\ninputs = tokenizer(\\\"Given text input here\\\", return_tensors=\\\"pt\\\")\\\\noutputs = model(**inputs)\\\\nprediction = outputs.logits.argmax(dim=1)\\\\nif prediction.item() == 1:\\\\n    print(\\\"The given text is gibberish.\\\")\\\\nelse:\\\\n    print(\\\"The given text is not gibberish.\\\")', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('madhurjindal/autonlp-Gibberish-Detector-492513457')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'madhurjindal/autonlp-Gibberish-Detector-492513457', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('madhurjindal/autonlp-Gibberish-Detector-492513457')\\\", 'api_arguments': {'inputs': 'I love AutoNLP'}, 'python_environment_requirements': [{'transformers': 'AutoModelForSequenceClassification', 'AutoTokenizer': 'from_pretrained'}], 'example_code': 'from transformers import AutoModelForSequenceClassification, AutoTokenizer\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\\\\ninputs = tokenizer(I love AutoNLP, return_tensors=pt)\\\\noutputs = model(**inputs)', 'performance': {'dataset': 'madhurjindal/autonlp-data-Gibberish-Detector', 'accuracy': 0.9735624586913417}, 'description': 'A multi-class text classification model for detecting gibberish text. Trained using AutoNLP and DistilBERT.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆØŸ? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the necessary classes from the transformers and numpy packages.', 'code': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\\\\ninputs = processor(video, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\n  predicted_class_idx = logits.argmax(-1).item()\\\\n  print('Predicted gesture:', model.config.id2label[predicted_class_idx])\\\", 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'videomae-small-finetuned-kinetics', 'api_call': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\\\\ninputs = processor(video, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\n  predicted_class_idx = logits.argmax(-1).item()\\\\n  print('Predicted gesture:', model.config.id2label[predicted_class_idx])\\\", 'api_arguments': {'video': 'list(np.random.randn(16, 3, 224, 224))'}, 'python_environment_requirements': ['transformers', 'numpy', 'torch'], 'example_code': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\\\\ninputs = processor(video, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\\\", 'performance': {'dataset': 'Kinetics-400', 'accuracy': {'top-1': 79.0, 'top-5': 93.8}}, 'description': 'VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ± ÙÙŠ Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ±. Ù‡Ù„ ØªØªØ·Ù„Ø¨ Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ù‡Ø°Ø§ØŸ? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary DDPMPipeline class from the diffusers package.\\\\n2. Use the from_pretrained method of the DDPMPipeline class to load the pre-trained model 'schdoel/sd-class-AFHQ-32'. This model has been trained for unconditional image synthesis tasks of cute animals, which is perfect for generating images for our pet daycare marketing campaign.\\\\n3. We then use the pipeline to generate a cute animal image and save it for our campaign purposes.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\\nimage = pipeline().images[0]\\\\nimage.save('cute_animal_image.png')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sd-class-pandas-32', 'api_call': \\\"DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\", 'api_arguments': {'pretrained_model': 'schdoel/sd-class-AFHQ-32'}, 'python_environment_requirements': [{'package': 'diffusers', 'import': 'from diffusers import DDPMPipeline'}], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': 'AFHQ', 'accuracy': 'Not provided'}, 'description': 'This model is a diffusion model for unconditional image generation of cute ğŸ¦‹.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØµÙˆØ± ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø©ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­Ø¯ÙŠØ¯ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØµÙˆØ± Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙˆÙƒÙØ§Ø¡Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a translation model for Italian to English. Specify the model 'Helsinki-NLP/opus-mt-it-en' to be loaded, which has been trained on a large corpus of text to handle the translation task.\\\\n3. The created translation pipeline can be used to translate Italian text to English.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')\\\\ntranslated_email = translation(italian_email_text)\\\", 'api_call': \\\"pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-it-en', 'api_call': \\\"pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')('Ciao mondo!')\\\", 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': {'newssyscomb2009.it.en': 35.3, 'newstest2009.it.en': 34.0, 'Tatoeba.it.en': 70.9}, 'chr-F': {'newssyscomb2009.it.en': 0.6, 'newstest2009.it.en': 0.594, 'Tatoeba.it.en': 0.808}}}, 'description': 'A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙˆÙ†Ø±ÙŠØ¯ ØªØ­Ø³ÙŠÙ† ÙÙ‡Ù… Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø£ÙˆØªÙˆÙ…Ø§ØªÙŠÙƒÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the required pipeline function from the transformers library provided by Hugging Face.\\\\n2. We use the pipeline function to instantiate a text classification model.\\\\n3. Set the model to 'Seethal/sentiment_analysis_generic_dataset', which has been trained for sentiment analysis tasks.\\\\n4. Use the model to analyze the sentiment of text reviews, which can be used to help inform product recommendations to users based on their sentiment.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsentiment_analyzer = pipeline(\\\\'text-classification\\\\', model=\\\\'Seethal/sentiment_analysis_generic_dataset\\\\')\\\\nsentiment = sentiment_analyzer(\\\"I love this product!\\\")', 'api_call': \\\"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentiment_analysis_generic_dataset', 'api_call': \\\"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': \\\"sentiment_analysis('I love this product!')\\\", 'performance': {'dataset': 'generic_dataset', 'accuracy': 'Not specified'}, 'description': 'This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text classification.' </s>\"\n"}
{"text": "<s>[INST]Ø³ÙŠÙ‚ÙˆÙ… ÙØ±ÙŠÙ‚Ù†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ± Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…Ø´Ø±ÙˆØ· ÙÙŠ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. ÙŠØªØ¹ÙŠÙ† Ø¹Ù„ÙŠÙƒ ØªÙˆÙÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù„ÙƒÙ„ Ø¹Ù†ØµØ± ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø£Ùˆ Ø§Ù„Ù…Ø´Ù‡Ø¯.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\\\ngenerated_image = pipeline().images[0]\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'johnowhitaker/sd-class-wikiart-from-bedrooms', 'api_call': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\\\ngenerated_image = pipeline().images[0]\\\", 'api_arguments': '', 'python_environment_requirements': 'diffusers', 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': 'https://huggingface.co/datasets/huggan/wikiart', 'accuracy': 'Not provided'}, 'description': 'This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„ÙˆØµÙÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙ†ÙÙŠØ° Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary packages from the transformers library. This includes T5ForConditionalGeneration for summarization tasks and a pipeline from transformers.\\\\n2. Load the pre-trained model 'pszemraj/long-t5-tglobal-base-16384-book-summary' using the from_pretrained method of the T5ForConditionalGeneration class. This model is specifically designed for summarizing long texts like books.\\\\n3. Set up a summarization pipeline with the loaded model.\\\\n4. Pass the entire long text of the book to the summarization pipeline, and it will generate a condensed summary of the book.\\\\n5. Print or store the summary as needed.\\\", '<<<code>>>': 'from transformers import pipeline, T5ForConditionalGeneration\\\\nimport torch\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\\\')\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model, device=0 if torch.cuda.is_available() else -1)\\\\nlong_text = \\\"Your 300-page book content here\\\"\\\\nresult = summarizer(long_text)\\\\nsummary = result[0][\\\\'summary_text\\\\']\\\\nprint(summary)', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pszemraj/long-t5-tglobal-base-16384-book-summary', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\", 'api_arguments': ['long_text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don't want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\\\", 'performance': {'dataset': 'kmfoda/booksum', 'accuracy': {'ROUGE-1': 36.408, 'ROUGE-2': 6.065, 'ROUGE-L': 16.721, 'ROUGE-LSUM': 33.34}}, 'description': 'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ© ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ØŸ? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary classes and functions from the transformers and torch libraries. This includes AutoImageProcessor for image preprocessing and ResNetForImageClassification for the image classification model.\\\\n2. We then use the from_pretrained method of the ResNetForImageClassification class to load the pre-trained model 'microsoft/resnet-50'. This model has been trained for image classification tasks, which is exactly what we need for determining the categories of items on an e-commerce site.\\\\n3. We load and preprocess the image using the AutoImageProcessor. The processed image is then fed to the model to classify the object in the image.\\\\n4. The model outputs the logits, and we use the argmax function to determine the predicted label for the image.\\\\n5. Finally, we print the predicted label, which corresponds to the item category we're looking for.\\\", '<<<code>>>': \\\"from transformers import AutoImageProcessor, ResNetForImageClassification\\\\nimport torch\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('microsoft/resnet-50')\\\\nmodel = ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\\\\n\\\\nimage = 'path_to_image.jpg'  # replace with the image's file path\\\\ninputs = processor(image, return_tensors='pt')\\\\n\\\\nlogits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/resnet-50', 'api_call': \\\"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\\\", 'api_arguments': {'from_pretrained': 'microsoft/resnet-50'}, 'python_environment_requirements': [{'transformers': 'AutoImageProcessor, ResNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}], 'example_code': 'from transformers import AutoImageProcessor, ResNetForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset(huggingface/cats-image)\\\\nimage = dataset[test][image][0]\\\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/resnet-50)\\\\nmodel = ResNetForImageClassification.from_pretrained(microsoft/resnet-50)\\\\ninputs = processor(image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': '~0.5% top1'}, 'description': 'ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ±Ø¬Ù‰ ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠÙ‡Ø¯Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ù‡Ø°Ø§ØŸ? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"TimesformerForVideoClassification.from_pretrained('fcakyon/timesformer-large-finetuned-k400')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and classes, including AutoImageProcessor, TimesformerForVideoClassification, numpy, and torch.\\\\n2. Load the 'fcakyon/timesformer-large-finetuned-k400' pre-trained model using the TimesformerForVideoClassification.from_pretrained utility.\\\\n3. This model can then be used to analyze a video on a specific topic and classify it into one of the 400 possible Kinetics-400 labels. When using the model for video classification, you would first use the AutoImageProcessor to process the video and then use the TimesformerForVideoClassification model to predict the class of the video content.\\\\n4. The predicted class can help you understand which topic the video is about.\\\", '<<<code>>>': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(96, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(\\\\'fcakyon/timesformer-large-finetuned-k400\\\\')\\\\nmodel = TimesformerForVideoClassification.from_pretrained(\\\\'fcakyon/timesformer-large-finetuned-k400\\\\')\\\\n\\\\ninputs = processor(video, return_tensors=\\\\'pt\\\\')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('fcakyon/timesformer-large-finetuned-k400')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'fcakyon/timesformer-large-finetuned-k400', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('fcakyon/timesformer-large-finetuned-k400')\\\", 'api_arguments': ['video'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(96, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(fcakyon/timesformer-large-finetuned-k400)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(fcakyon/timesformer-large-finetuned-k400)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-400', 'accuracy': 'Not provided'}, 'description': \\\"TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper 'TimeSformer: Is Space-Time Attention All You Need for Video Understanding?' by Tong et al.\\\" </s>\"\n"}
{"text": "<s>[INST]ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø±Ø¦ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·. Ù†Ø·Ù„Ø¨ Ø¨Ø±Ù†Ø§Ù…Ø¬Ù‹Ø§ ÙŠØ­ÙˆÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"ViltModel.from_pretrained('tufa15nik/vilt-finetuned-vqasi')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To achieve this goal, we can use a Visual Question Answering (VQA) model from Hugging Face Transformers. Specifically, we can use `tufa15nik/vilt-finetuned-vqasi`, which is a VQA model that has been fine-tuned on the VQASI dataset and is designed to answer questions about an input image.\\\\n2. First, import the required classes from the transformers package. This includes ViltModel for the visual question answering model and the tokenizer.\\\\n3. Load the VQA model using the from_pretrained method of the ViltModel class and the address 'tufa15nik/vilt-finetuned-vqasi'.\\\\n4. This model can then be used to analyze an image and answer questions based on the content of the image.\\\", '<<<code>>>': \\\"from transformers import ViltModel, ViltTokenizer\\\\nmodel = ViltModel.from_pretrained('tufa15nik/vilt-finetuned-vqasi')\\\\ntokenizer = ViltTokenizer.from_pretrained('tufa15nik/vilt-finetuned-vqasi')\\\", 'api_call': \\\"ViltModel.from_pretrained('tufa15nik/vilt-finetuned-vqasi')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'vilt-finetuned-vqasi', 'api_call': \\\"ViltModel.from_pretrained('tufa15nik/vilt-finetuned-vqasi')\\\", 'api_arguments': {'model': 'tufa15nik/vilt-finetuned-vqasi', 'tokenizer': 'tufa15nik/vilt-finetuned-vqasi'}, 'python_environment_requirements': [{'transformers': '>=4.11.3'}], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the VQASI dataset by tufa15nik using the ViLT architecture. The model is designed to answer questions based on the content of an input image.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¯Ø±Ø§Ø³ÙŠØ© Ù„Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ‹Ø§ ÙˆØ§Ù„Ù†Ù‚Ø§Ø´ Ø­ÙˆÙ„ Ù…Ø¬Ø§Ù„ Ø§Ù„ØµÙˆØªÙŠØ§Øª ÙˆØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ ØµÙˆØª. Ø«Ù… Ù‚Ù… Ø¨ØªÙ‚Ø¯ÙŠÙ… ÙˆØ­Ø¯Ø© Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø³ÙŠØ§Ù‚.? [/INST]{'domain': 'Audio Audio-to-Audio', 'api_call': 'S2THubInterface()', 'api_provider': 'Fairseq', 'explanation': '1. Import the required libraries and modules such as S2THubInterface, torchaudio, and fairseq.', 'code': 'import torchaudio\\\\nfrom fairseq import hub_utils, load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\n\\\\ncache_dir = os.getenv(\\\\'HUGGINGFACE_HUB_CACHE\\\\')\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n    \\\\'facebook/xm_transformer_s2ut_hk-en\\\\',\\\\n    arg_overrides={\\\\'config_yaml\\\\': \\\\'config.yaml\\\\', \\\\'task\\\\': \\\\'speech_to_text\\\\'},\\\\n    cache_dir=cache_dir,\\\\n)\\\\n\\\\nmodel = models[0].cpu()\\\\ncfg[\\\\'task\\\\'].cpu = True\\\\ngenerator = task.build_generator([model], cfg)\\\\n\\\\naudio, _ = torchaudio.load(\\\"/path/to/hokkien_speech/audio/file\\\")\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\ntranslated_speech = S2THubInterface.get_prediction(task, model, generator, sample)', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Fairseq', 'functionality': 'Speech-to-speech translation', 'api_name': 'xm_transformer_s2ut_hk-en', 'api_call': 'import torchaudio\\\\nfrom fairseq import hub_utils, load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\n\\\\ncache_dir = os.getenv(\\\\'HUGGINGFACE_HUB_CACHE\\\\')\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n    \\\\'facebook/xm_transformer_s2ut_hk-en\\\\',\\\\n    arg_overrides={\\\\'config_yaml\\\\': \\\\'config.yaml\\\\', \\\\'task\\\\': \\\\'speech_to_text\\\\'},\\\\n    cache_dir=cache_dir,\\\\n)\\\\n\\\\nmodel = models[0].cpu()\\\\ncfg[\\\\'task\\\\'].cpu = True\\\\ngenerator = task.build_generator([model], cfg)\\\\n\\\\naudio, _ = torchaudio.load(\\\"/path/to/hokkien_speech/audio/file\\\")\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\ntranslated_speech = S2THubInterface.get_prediction(task, model, generator, sample)', 'api_arguments': {'task': 'speech_to_text', 'model': 'facebook/xm_transformer_s2ut_hk-en', 'generator': 'task.build_generator([model], cfg)', 'sample': 'S2THubInterface.get_model_input(task, audio)'}, 'python_environment_requirements': {'fairseq': 'latest', 'torchaudio': 'latest', 'huggingface_hub': 'latest'}, 'example_code': \\\"import json\\\\nimport os\\\\nfrom pathlib import Path\\\\nimport IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n facebook/xm_transformer_s2ut_hk-en,\\\\n arg_overrides={config_yaml: config.yaml, task: speech_to_text},\\\\n cache_dir=cache_dir,\\\\n)\\\\nmodel = models[0].cpu()\\\\ncfg[task].cpu = True\\\\ngenerator = task.build_generator([model], cfg)\\\\naudio, _ = torchaudio.load(/path/to/an/audio/file)\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\\\\nlibrary_name = fairseq\\\\ncache_dir = (\\\\n cache_dir or (Path.home() / .cache / library_name).as_posix()\\\\n)\\\\ncache_dir = snapshot_download(\\\\n ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name\\\\n)\\\\nx = hub_utils.from_pretrained(\\\\n cache_dir,\\\\n model.pt,\\\\n .,\\\\n archive_map=CodeHiFiGANVocoder.hub_models(),\\\\n config_yaml=config.json,\\\\n fp16=False,\\\\n is_vocoder=True,\\\\n)\\\\nwith open(f{x['args']['data']}/config.json) as f:\\\\n vocoder_cfg = json.load(f)\\\\nassert (\\\\n len(x[args][model_path]) == 1\\\\n), Too many vocoder models in the input\\\\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\nipd.Audio(wav, rate=sr)\\\", 'performance': {'dataset': 'TED, drama, TAT domain', 'accuracy': 'Not provided'}, 'description': 'Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq for Hokkien-English. Trained with supervised data in TED, drama, TAT domain, and weakly supervised data in drama domain.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ù‡Ø¯Ù Ø¥Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ù†ØµÙˆØµ Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙˆØ³Ù„Ø³.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='bigscience/test-bloomd-6b3')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the library from transformers, which is 'pipeline'.\\\\n2. Call the 'pipeline' function with the task type 'text-generation' and the pre-trained model 'bigscience/test-bloomd-6b3'. This model will be used for generating the text based on a given input.\\\\n3. To generate the story, provide an initial seed text as input to the model.\\\\n4. The model will output a continuation of the given seed text in the form of a story.\\\\n5. Participants can then use this generated text as a starting point and build their models based on the provided architecture.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ngenerator = pipeline('text-generation', model='bigscience/test-bloomd-6b3')\\\\nstory_seed = 'Once upon a time in a magical land...'\\\\ngenerated_story = generator(story_seed)\\\", 'api_call': \\\"pipeline('text-generation', model='bigscience/test-bloomd-6b3')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bigscience/test-bloomd-6b3', 'api_call': \\\"pipeline('text-generation', model='bigscience/test-bloomd-6b3')\\\", 'api_arguments': ['input_text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; generator = pipeline('text-generation', model='bigscience/test-bloomd-6b3'); generator('Once upon a time')\\\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'A text generation model from Hugging Face, using the bigscience/test-bloomd-6b3 architecture. It can be used for generating text based on a given input.' </s>\"\n"}
{"text": "<s>[INST]ØªÙˆÙØ± Ù…Ù†ØµØªÙ†Ø§ Ø®Ø¯Ù…Ø© Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ±ÙŠ Ø§Ù„Ø¨Ø¯ÙŠÙ‡ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„ØµÙ†Ù. Ø³ØªØ­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø¯Ù‚Ø© ÙˆÙØ¹Ø§Ù„ÙŠØ© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"CLIPModel.from_pretrained('patrickjohncyh/fashion-clip')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. This includes CLIPProcessor for pre-processing the image and text data and CLIPModel for the zero-shot image classification model.\\\\n2. Load the pre-trained model 'patrickjohncyh/fashion-clip' using the from_pretrained method of the CLIPModel class. This model is specifically trained for fashion industry applications.\\\\n3. Load the processor that matches the model using the from_pretrained method of the CLIPProcessor class.\\\\n4. Prepare the inputs to the model, which includes the image and text data. The text data should include names of the clothing categories you wish to classify the image.\\\\n5. Use the model to get logits_per_image, which represent the probability distribution of the image belonging to each of the categories mentioned earlier. The softmax function is used to convert logits into probabilities.\\\", '<<<code>>>': 'from transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(\\\\'patrickjohncyh/fashion-clip\\\\')\\\\nprocessor = CLIPProcessor.from_pretrained(\\\\'patrickjohncyh/fashion-clip\\\\')\\\\nimage_path = \\\"path/to/image.jpg\\\" # replace with the path to the customer\\\\'s image\\\\nimage = Image.open(image_path)\\\\nclass_names = \\\"t-shirt, dress, shoes, pants, hat\\\"\\\\ninputs = processor(text=class_names, images=image, return_tensors=\\\\'pt\\\\', padding=True)\\\\nlogits_per_image = model(**inputs).logits_per_image\\\\nprobs = logits_per_image.softmax(dim=-1).tolist()[0]', 'api_call': \\\"CLIPModel.from_pretrained('patrickjohncyh/fashion-clip')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'patrickjohncyh/fashion-clip', 'api_call': \\\"CLIPModel.from_pretrained('patrickjohncyh/fashion-clip')\\\", 'api_arguments': {'image': 'File', 'class_names': 'String (comma-separated)'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained('patrickjohncyh/fashion-clip'); processor = CLIPProcessor.from_pretrained('patrickjohncyh/fashion-clip'); inputs = processor(text='blue shoes', images=image, return_tensors='pt', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\\\", 'performance': {'dataset': [{'name': 'FMNIST', 'accuracy': 0.83}, {'name': 'KAGL', 'accuracy': 0.73}, {'name': 'DEEP', 'accuracy': 0.62}]}, 'description': 'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ±ØºØ¨ ÙÙŠ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Text2Text Generation. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ·ÙŠØ¹ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØªØµÙ†ÙŠÙÙ‡Ø§ Ø¨Ø¯Ù‚Ø©.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'google/flan-t5-base\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'google/flan-t5-base\\\\')\\\\ninput_text = \\\"respond to message: \\\" + input_message\\\\ninput_ids = tokenizer(input_text, return_tensors=\\\\'pt\\\\').input_ids\\\\noutputs = model.generate(input_ids)\\\\nresponse_text = tokenizer.decode(outputs[0])', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Language model', 'api_name': 'google/flan-t5-base', 'api_call': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'google/flan-t5-base\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'google/flan-t5-base\\\\')\\\\ninput_text = \\\"respond to message: \\\" + input_message\\\\ninput_ids = tokenizer(input_text, return_tensors=\\\\'pt\\\\').input_ids\\\\noutputs = model.generate(input_ids)\\\\nresponse_text = tokenizer.decode(outputs[0])', 'api_arguments': 'input_text', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-base)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-base)\\\\ninput_text = translate English to German: How old are you?\\\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))', 'performance': {'dataset': [{'name': 'MMLU', 'accuracy': '75.2%'}]}, 'description': 'FLAN-T5 is a language model fine-tuned on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… Ø§Ù„Ø®Ø¨ÙŠØ± Ø¨ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ. ÙŠØ±Ø¬Ù‰ Ø¥Ù†Ø´Ø§Ø¡ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù„Ù„Ø¨Ø±Ù…Ø¬Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø³Ù…ÙˆØ¹ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='TehVenom/PPO_Pygway-V8p4_Dev-6b')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Begin by importing the required 'pipeline' function from the transformers library.\\\\n2. Use the 'pipeline' function to create a text generation model with the specified model 'TehVenom/PPO_Pygway-V8p4_Dev-6b'. This is a mixed model which will provide a variety of responses, making the chatbot engaging for users with different interests.\\\\n3. Once the text generation model is created, you can provide user input to the model to generate contextual and diverse responses using the 'generate()' method.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\n\\\\ntext_generator = pipeline('text-generation', model='TehVenom/PPO_Pygway-V8p4_Dev-6b')\\\\nuser_input = 'User input goes here...'\\\\ngenerated_response = text_generator(user_input)\\\", 'api_call': \\\"pipeline('text-generation', model='TehVenom/PPO_Pygway-V8p4_Dev-6b')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'TehVenom/PPO_Pygway-V8p4_Dev-6b', 'api_call': \\\"pipeline('text-generation', model='TehVenom/PPO_Pygway-V8p4_Dev-6b')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'TODO card. Mix of (GPT-J-6B-Janeway + PPO_HH_GPT-J) + Pygmalion-6b-DEV (V8 / Part 4). At a ratio of GPT-J-6B-Janeway - 20%, PPO_HH_GPT-J - 20%, Pygmalion-6b DEV (V8 / Part 4) - 60%.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø·Ø§Ø© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©ØŒ ÙˆØ¶Ø¹ ÙÙŠ Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØµØ­ÙŠØ­ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary libraries (joblib and pandas).\\\\n2. Load the pre-trained binary classification model ('desertdev/autotrain-imdb-sentiment-analysis-44994113085') using the joblib library.\\\\n3. Read and preprocess the input data (movie reviews in CSV format) using pandas. Select the required features and format the column names accordingly.\\\\n4. Use the loaded model to make predictions on the processed data.\\\\n5. Analyze the predictions to categorize movie reviews as positive or negative, and use this information to make movie recommendations.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\ndata = pd.read_csv('data.csv')\\\\nfeatures = config['features']\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'desertdev/autotrain-imdb-sentiment-analysis-44994113085', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'desertdev/autotrain-data-imdb-sentiment-analysis', 'accuracy': 0.565}, 'description': 'A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.' </s>\"\n"}
{"text": "<s>[INST]ØªØ¹Ù…Ù„ Ø´Ø±ÙƒØªÙ†Ø§ Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¹Ù„Ù‰ Ø¬Ø¯Ø§ÙˆÙ„ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆÙ†Ø±ØºØ¨ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©.? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-sqa')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import necessary libraries and modules, such as 'TapasForQuestionAnswering' from transformers.\\\", 'code': 'from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\nimport pandas as pd\\\\n\\\\nmodel_name = \\\\'google/tapas-mini-finetuned-sqa\\\\'\\\\ntokenizer = TapasTokenizer.from_pretrained(model_name)\\\\nmodel = TapasForQuestionAnswering.from_pretrained(model_name)\\\\n\\\\ndata = {\\\\'date\\\\':[\\\\'2023-01-01\\\\', \\\\'2023-01-02\\\\', \\\\'2023-01-03\\\\',],\\\\n        \\\\'city\\\\':[\\\\'San Francisco\\\\', \\\\'San Francisco\\\\', \\\\'San Francisco\\\\',],\\\\n        \\\\'weather\\\\':[\\\\'Rain\\\\', \\\\'Sun\\\\', \\\\'Rain\\\\',]}\\\\n\\\\ntable = pd.DataFrame(data)\\\\n\\\\nqueries = \\\"Which days will it rain in San Francisco?\\\"\\\\n\\\\ninputs = tokenizer(table=table, queries=queries, padding=\\\\'max_length\\\\', return_tensors=\\\"pt\\\")\\\\noutputs = model(**inputs)\\\\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())\\\\n\\\\nanswers = tokenizer.convert_coordinates_to_text(table=table, token_coordinates=predicted_answer_coordinates[0])\\\\n\\\\nprint(answers)', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-mini-finetuned-sqa', 'api_call': 'from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\nimport pandas as pd\\\\n\\\\nmodel_name = \\\\'google/tapas-mini-finetuned-sqa\\\\'\\\\ntokenizer = TapasTokenizer.from_pretrained(model_name)\\\\nmodel = TapasForQuestionAnswering.from_pretrained(model_name)\\\\n\\\\ndata = {\\\\'date\\\\':[\\\\'2023-01-01\\\\', \\\\'2023-01-02\\\\', \\\\'2023-01-03\\\\',],\\\\n        \\\\'city\\\\':[\\\\'San Francisco\\\\', \\\\'San Francisco\\\\', \\\\'San Francisco\\\\',],\\\\n        \\\\'weather\\\\':[\\\\'Rain\\\\', \\\\'Sun\\\\', \\\\'Rain\\\\',]}\\\\n\\\\ntable = pd.DataFrame(data)\\\\n\\\\nqueries = \\\"Which days will it rain in San Francisco?\\\"\\\\n\\\\ninputs = tokenizer(table=table, queries=queries, padding=\\\\'max_length\\\\', return_tensors=\\\"pt\\\")\\\\noutputs = model(**inputs)\\\\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())\\\\n\\\\nanswers = tokenizer.convert_coordinates_to_text(table=table, token_coordinates=predicted_answer_coordinates[0])\\\\n\\\\nprint(answers)', 'api_arguments': ['model_name', 'table', 'queries'], 'python_environment_requirements': ['transformers'], 'example_code': 'N/A', 'performance': {'dataset': 'msr_sqa', 'accuracy': 0.5148}, 'description': 'TAPAS mini model fine-tuned on Sequential Question Answering (SQA)'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ØµØ±ÙŠ Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-ssv2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries: VideoMAEFeatureExtractor and VideoMAEForVideoClassification from the transformers package, numpy as np, and torch.\\\\n2. Load the pre-trained video classification model 'videomae-small-finetuned-ssv2' using the from_pretrained method of VideoMAEForVideoClassification class. This model is trained for video classification tasks and can be utilized for classifying different types of workouts in user-submitted videos.\\\\n3. To classify a workout video, first, extract the features of the video using the VideoMAEFeatureExtractor with the from_pretrained method.\\\\n4. Then, create a list of the frames from the video and input them into the feature extractor to prepare them for the model.\\\\n5. Finally, run the model with the extracted features as inputs to generate the logits (unnormalized probabilities) for each class. Choose the class with the highest logits as the predicted class, and this will determine the type of workout that the user is performing.\\\", '<<<code>>>': \\\"from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained('MCG-NJU/videomae-small-finetuned-ssv2')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-ssv2')\\\\ninputs = feature_extractor(video, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(f'Predicted class: {model.config.id2label[predicted_class_idx]}')\\\", 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-ssv2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'videomae-small-finetuned-ssv2', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-ssv2')\\\", 'api_arguments': {'model_name': 'MCG-NJU/videomae-small-finetuned-ssv2'}, 'python_environment_requirements': [{'transformers': 'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification', 'numpy': 'import numpy as np', 'torch': 'import torch'}], 'example_code': 'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Something-Something V2', 'accuracy': {'top-1': 66.8, 'top-5': 90.3}}, 'description': 'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø§Ø­Ø«ÙŠÙ† ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ØªÙ‚Ø¯ÙŠÙ… ØªÙ‚Ø¯ÙŠØ± Ø¯Ù‚ÙŠÙ‚ Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ù„ØªÙ‚Ø·Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø¢Ù„ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"pipeline(task='question-answering', model='thatdramebaazguy/roberta-base-squad')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. To create a program to answer questions from a legal document, first import the necessary libraries, which are 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\nqa_pipeline = pipeline(task=\\\\'question-answering\\\\', model=\\\\'thatdramebaazguy/roberta-base-squad\\\\'); legal_document = \\\\'Content of the legal document...\\\\'\\\\nquestions = [\\\\'Question 1...\\\\', \\\\'Question 2...\\\\', \\\\'Question 3...\\\\']\\\\n# Replace with your questions\\\\nfor question in questions:\\\\n    context = {\\\\'context\\\\': legal_document, \\\\'question\\\\': question}\\\\n    answer = qa_pipeline(context)\\\\n    print(f\\\"{question}: {answer[\\\\'answer\\\\']}\\\")', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Question Answering', 'api_name': 'thatdramebaazguy/roberta-base-squad', 'api_call': 'from transformers import pipeline\\\\nqa_pipeline = pipeline(task=\\\\'question-answering\\\\', model=\\\\'thatdramebaazguy/roberta-base-squad\\\\'); legal_document = \\\\'Content of the legal document...\\\\'\\\\nquestions = [\\\\'Question 1...\\\\', \\\\'Question 2...\\\\', \\\\'Question 3...\\\\']\\\\n# Replace with your questions\\\\nfor question in questions:\\\\n    context = {\\\\'context\\\\': legal_document, \\\\'question\\\\': question}\\\\n    answer = qa_pipeline(context)\\\\n    print(f\\\"{question}: {answer[\\\\'answer\\\\']}\\\")', 'api_arguments': {'model_name': 'thatdramebaazguy/roberta-base-squad', 'tokenizer': 'thatdramebaazguy/roberta-base-squad', 'revision': 'v1.0', 'task': 'question-answering'}, 'python_environment_requirements': {'huggingface': {'transformers': 'latest'}}, 'example_code': \\\"model_name = 'thatdramebaazguy/roberta-base-squad'\\\\nqa_pipeline = pipeline(model=model_name, tokenizer=model_name, revision='v1.0', task='question-answering')\\\", 'performance': {'dataset': [{'name': 'SQuADv1', 'accuracy': {'exact_match': 83.6045, 'f1': 91.1709}}, {'name': 'MoviesQA', 'accuracy': {'exact_match': 51.6494, 'f1': 68.2615}}]}, 'description': 'This is Roberta Base trained to do the SQuAD Task. This makes a QA model capable of answering questions.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… ÙØ±ÙŠÙ‚ Ø§Ù„Ø¨Ø­Ø« Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-sqa')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\n\\\\ntable_qa_pipeline = pipeline('table-question-answering', model='google/tapas-medium-finetuned-sqa')\\\\n# replace 'table' with revenue data in the required format\\\\nresult = table_qa_pipeline(table=table, query='What is the total revenue?')\\\", 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-medium-finetuned-sqa', 'api_call': \\\"from transformers import pipeline\\\\n\\\\ntable_qa_pipeline = pipeline('table-question-answering', model='google/tapas-medium-finetuned-sqa')\\\\n# replace 'table' with revenue data in the required format\\\\nresult = table_qa_pipeline(table=table, query='What is the total revenue?')\\\", 'api_arguments': 'table, query', 'python_environment_requirements': 'transformers', 'example_code': \\\"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntable_qa_pipeline = pipeline('table-question-answering', model='google/tapas-medium-finetuned-sqa')\\\\n\\\\n# Provide the table and query\\\\nresult = table_qa_pipeline(table=table, query='What is the total revenue?')\\\", 'performance': {'dataset': 'msr_sqa', 'accuracy': 0.6561}, 'description': 'TAPAS medium model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia and uses relative position embeddings. It can be used for answering questions related to a table in a conversational set-up.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„Ù„Ø¬ÙŠÙ„ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù„Ù„ØµÙˆØ± ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨. ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‚Ø§Ø¯Ø±Ø§Ù‹ Ø¹Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ù…ØªÙ†ÙˆØ¹Ø© Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…Ø´Ø±ÙˆØ·. ÙŠÙ†Ø¨ØºÙŠ Ø£Ù† ÙŠØ³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ± ÙÙŠ Ù…Ø¬Ø§Ù„Ø§Øª Ù…Ø®ØªÙ„ÙØ©.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Install the 'diffusers' library, which contains the necessary tools for working with diffusion-based generative models.\\\", 'code': \\\"!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = 'google/ncsnpp-celebahq-256'\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage_sample = sde_ve()\\\\nimage_sample[0].save('sde_ve_generated_image.png')\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ncsnpp-celebahq-256', 'api_call': \\\"!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = 'google/ncsnpp-celebahq-256'\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage_sample = sde_ve()\\\\nimage_sample[0].save('sde_ve_generated_image.png')\\\", 'api_arguments': {'model_id': 'google/ncsnpp-celebahq-256'}, 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = google/ncsnpp-celebahq-256\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[sample]\\\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ø¬Ø¯ÙˆÙ„ ØªØ¨ÙˆÙŠØ¨ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ØªØµÙ†ÙŠÙÙŠØ© Ù„Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø®ØªÙ„ÙØ©.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': '1. Import the necessary libraries such as json, joblib, and pandas.\\\\n2. Load the pre-trained model using joblib.load() with the path of the saved model.\\\\n3. Read the configuration file to get the features needed for the model input.\\\\n4. Using pandas, load the dataset and filter out the required features from the data.\\\\n5. Use the trained model to predict whether companies have high carbon emission or not, based on the input features.', '<<<code>>>': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'jwan2021/autotrain-jwan-autotrain1-1768961489', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['json', 'joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'Validation Metrics', 'accuracy': 0.983}, 'description': 'Binary Classification model for Carbon Emissions prediction' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¨Ø¯ÙˆÙ† Ù…Ø³Ø¨Ù‚ ØªØ¯Ø±ÙŠØ¨ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„ÙˆØµÙÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙ†ÙÙŠØ° Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a zero-shot classification model.\\\\n3. Specify the model 'MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli' to be loaded. This model is trained for the task of natural language inference, which we can leverage for zero-shot classification.\\\\n4. The created classifier can be used to analyze a given news article and classify it into one or multiple categories.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nnews_classifier = pipeline('zero-shot-classification', model='MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli')\\\\nnews_article = 'The news article content goes here...'\\\\ncategories = ['politics', 'economy', 'entertainment', 'environment', 'technology', 'sports']\\\\nresult = news_classifier(news_article, categories, multi_label=True)\\\", 'api_call': \\\"pipeline('zero-shot-classification', model='MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli', 'api_call': \\\"pipeline('zero-shot-classification', model='MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli')\\\", 'api_arguments': ['sequence_to_classify', 'candidate_labels', 'multi_label'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli)\\\\nsequence_to_classify = Angela Merkel is a politician in Germany and leader of the CDU\\\\ncandidate_labels = [politics, economy, entertainment, environment]\\\\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\\\\nprint(output)', 'performance': {'dataset': [{'name': 'mnli_test_m', 'accuracy': 0.912}, {'name': 'mnli_test_mm', 'accuracy': 0.908}, {'name': 'anli_test', 'accuracy': 0.702}, {'name': 'anli_test_r3', 'accuracy': 0.64}, {'name': 'ling_test', 'accuracy': 0.87}, {'name': 'wanli_test', 'accuracy': 0.77}]}, 'description': 'This model was fine-tuned on the MultiNLI, Fever-NLI, Adversarial-NLI (ANLI), LingNLI and WANLI datasets, which comprise 885 242 NLI hypothesis-premise pairs. This model is the best performing NLI model on the Hugging Face Hub as of 06.06.22 and can be used for zero-shot classification. It significantly outperforms all other large models on the ANLI benchmark.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ„ØªØ²Ù… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø±Ø¦ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ Ø­ÙŠØ« Ø³ÙŠØªÙ… Ø¯Ù…Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ù†ØµÙˆØµ Ù„ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø´Ø§Ù…Ù„Ø© ÙˆØ¯Ù‚ÙŠÙ‚Ø©. Ù†Ø³Ø¹Ù‰ Ø¥Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØ­Ù‚ÙŠÙ‚ Ù‡Ø°Ù‡ Ø§Ù„ØºØ§ÙŠØ© ÙˆØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø¨ØµØ±ÙŠ ÙˆØ§Ù„Ù„ØºÙˆÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù†Ù…ÙˆØ°Ø¬.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. We import necessary classes from the transformers package, plus additional packages to handle images and internet requests.\\\\n2. We use the from_pretrained method of the ViltForQuestionAnswering class to load the pre-trained model \\\\'dandelin/vilt-b32-finetuned-vqa\\\\'. This model is trained for visual question answering tasks and will help answer users\\\\' questions based on images.\\\\n3. We collect a live image from the smart home\\\\'s camera, in this example, an image from the internet is used.\\\\n4. The VILT model is then used to analyze the given image along with a user\\\\'s text-based question (such as \\\"What color is the car?\\\").\\\\n5. The model processes the input and provides an answer based on its understanding of both the image and the question.', '<<<code>>>': 'from transformers import ViltProcessor, ViltForQuestionAnswering\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = \\\\'http://images.cocodataset.org/val2017/000000039769.jpg\\\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntext = \\\"What color is the car?\\\"\\\\n\\\\nprocessor = ViltProcessor.from_pretrained(\\\\'dandelin/vilt-b32-finetuned-vqa\\\\')\\\\nmodel = ViltForQuestionAnswering.from_pretrained(\\\\'dandelin/vilt-b32-finetuned-vqa\\\\')\\\\nencoding = processor(image, text, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**encoding)\\\\nlogits = outputs.logits', 'api_call': \\\"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dandelin/vilt-b32-finetuned-vqa', 'api_call': \\\"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\\\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'text': 'How many cats are there?'}, 'python_environment_requirements': [{'transformers': 'ViltProcessor, ViltForQuestionAnswering', 'requests': 'requests', 'PIL': 'Image'}], 'example_code': 'from transformers import ViltProcessor, ViltForQuestionAnswering\\\\nimport requests\\\\nfrom PIL import Image\\\\n\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntext = How many cats are there?\\\\nprocessor = ViltProcessor.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\\\nmodel = ViltForQuestionAnswering.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\\\n\\\\nencoding = processor(image, text, return_tensors=pt)\\\\noutputs = model(**encoding)\\\\nlogits = outputs.logits\\\\nidx = logits.argmax(-1).item()\\\\nprint(Predicted answer:, model.config.id2label[idx])', 'performance': {'dataset': 'VQAv2', 'accuracy': 'to do'}, 'description': 'Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨. Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªÙ†ÙˆØ¹Ø© Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ ÙˆØ§Ù„Ø£Ø´ÙƒØ§Ù„ ÙÙŠ Ø§Ù„ØµÙˆØ±. Ø§Ø¨Ø¯Ø£ Ø¨ØªÙ‚Ø³ÙŠÙ… ØµÙˆØ±Ø© Ù…Ø±Ø¬Ø¹ÙŠØ© Ø¥Ù„Ù‰ Ø¹Ø¯Ø© Ø£Ù‚Ø³Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø©ØŒ Ø«Ù… Ù‚Ù… Ø¨ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨ØªØºÙŠÙŠØ± Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª ÙˆØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©. Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ø¶Ù…Ø§Ù† Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙˆÙƒÙØ§Ø¡Ø© ÙÙŠ Ø¹Ù…Ù„ÙŠØ© ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ±.? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We start by importing the necessary libraries, which include requests, torch, PIL, and transformers.', 'code': \\\"from PIL import Image\\\\nimport requests\\\\nimport torch\\\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\\n\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n\\\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result['segmentation']\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-coco-panoptic', 'api_call': \\\"from PIL import Image\\\\nimport requests\\\\nimport torch\\\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\\n\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n\\\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result['segmentation']\\\", 'api_arguments': ['image'], 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': 'processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result[segmentation]', 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ù…Ù† Ù†ÙˆØ¹ Ø¥Ù„Ù‰ Ù†ÙˆØ¹ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¬ÙŠÙ„ Ø§Ù„Ù†ØµÙŠ.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\\\nsrc_text = \\\"This is an English text.\\\"\\\\nsrc_lang = \\\"en\\\"\\\\ntgt_lang = \\\"fr\\\"\\\\nmodel = M2M100ForConditionalGeneration.from_pretrained(\\\\'facebook/m2m100_1.2B\\\\')\\\\ntokenizer = M2M100Tokenizer.from_pretrained(\\\\'facebook/m2m100_1.2B\\\\')\\\\ntokenizer.src_lang = src_lang\\\\nencoded_input = tokenizer(src_text, return_tensors=\\\"pt\\\")\\\\ngenerated_tokens = model.generate(**encoded_input, forced_bos_token_id=tokenizer.get_lang_id(tgt_lang))\\\\ntranslated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Many-to-Many multilingual translation', 'api_name': 'facebook/m2m100_1.2B', 'api_call': 'from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\\\nsrc_text = \\\"This is an English text.\\\"\\\\nsrc_lang = \\\"en\\\"\\\\ntgt_lang = \\\"fr\\\"\\\\nmodel = M2M100ForConditionalGeneration.from_pretrained(\\\\'facebook/m2m100_1.2B\\\\')\\\\ntokenizer = M2M100Tokenizer.from_pretrained(\\\\'facebook/m2m100_1.2B\\\\')\\\\ntokenizer.src_lang = src_lang\\\\nencoded_input = tokenizer(src_text, return_tensors=\\\"pt\\\")\\\\ngenerated_tokens = model.generate(**encoded_input, forced_bos_token_id=tokenizer.get_lang_id(tgt_lang))\\\\ntranslated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]', 'api_arguments': {'encoded_input': 'encoded text', 'forced_bos_token_id': 'target language id'}, 'python_environment_requirements': 'sentencepiece', 'example_code': 'from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\\\nhi_text = chinese_text = model = M2M100ForConditionalGeneration.from_pretrained(facebook/m2m100_1.2B)\\\\ntokenizer = M2M100Tokenizer.from_pretrained(facebook/m2m100_1.2B)\\\\ntokenizer.src_lang = hi\\\\nencoded_hi = tokenizer(hi_text, return_tensors=pt)\\\\ngenerated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(fr))\\\\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)', 'performance': {'dataset': 'M2M100', 'accuracy': 'Not specified'}, 'description': 'M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ ÙÙŠ Ø´Ø±ÙƒØªÙ†Ø§ Ø§Ù„Ù…ØªØ®ØµØµØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØªØµÙ†ÙŠÙ Ø§Ù„ØµÙØ± Ø¥Ù„Ù‰ ØªØµÙ…ÙŠÙ… Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„Ù…ÙÙ‡ÙˆÙ… ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"pipeline('token-classification', model='kredor/punctuate-all')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We need to import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. The pipeline function is then used to create a token classification model for punctuation prediction by specifying the 'kredor/punctuate-all' model.\\\\n3. The model is trained on 12 languages, including French, so it will work for the French text input.\\\\n4. We can pass the French conversation transcript text as input, and the model will return an updated version of the text with appropriate punctuation marks inserted.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\n\\\\npunctuator = pipeline('token-classification', model='kredor/punctuate-all')\\\\npunctuated_text = punctuator(french_conversation_transcript)\\\", 'api_call': \\\"pipeline('token-classification', model='kredor/punctuate-all')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kredor/punctuate-all', 'api_call': \\\"pipeline('token-classification', model='kredor/punctuate-all')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'multilingual', 'accuracy': 0.98}, 'description': 'A finetuned xlm-roberta-base model for punctuation prediction on twelve languages: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian.' </s>\"\n"}
{"text": "<s>[INST]Ù„ØªØ­Ø³ÙŠÙ† ØªÙ…ÙŠÙŠØ² Ø§Ù„Ø¬Ù…Ù„ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø´Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù…Ù„ØŒ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø§Ù„Ø¨Ø¯Ø¡ Ø¨Ø¯ÙˆØ±Ø§Øª ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ù…Ø®ØµØµØ© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù…Ø«Ù„ Word2Vec ÙˆBERT Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø±Ø¬ÙˆØ©.? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = ['This is an example sentence', 'Each sentence is converted']\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Sentence Transformers', 'api_name': 'sentence-transformers/paraphrase-distilroberta-base-v2', 'api_call': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = ['This is an example sentence', 'Each sentence is converted']\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØ­ÙˆÙŠÙ„ ØµÙˆØ± Ù…Ù† Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨.? [/INST]{'domain': 'Multimodal Document Question Answer', 'api_call': \\\"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa', return_dict=True))\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Install required packages like PIL, pytesseract, PyTorch and transformers.', 'code': 'from transformers import pipeline, LayoutLMForQuestionAnswering\\\\nimport pytesseract\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\n# Get the image\\\\nimage_url = \\\"https://example.com/invoice_image.png\\\"\\\\nresponse = requests.get(image_url)\\\\nimg = Image.open(BytesIO(response.content))\\\\n\\\\n# Prepare query\\\\nquestion = \\\"Which supplier has sent this invoice?\\\"\\\\n\\\\n# Initialize the pipeline\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\\\'impira/layoutlm-document-qa\\\\', return_dict=True))\\\\n\\\\n# Get the answer\\\\nresult = nlp(img, question)', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': 'from transformers import pipeline, LayoutLMForQuestionAnswering\\\\nimport pytesseract\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\n# Get the image\\\\nimage_url = \\\"https://example.com/invoice_image.png\\\"\\\\nresponse = requests.get(image_url)\\\\nimg = Image.open(BytesIO(response.content))\\\\n\\\\n# Prepare query\\\\nquestion = \\\"Which supplier has sent this invoice?\\\"\\\\n\\\\n# Initialize the pipeline\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\\\'impira/layoutlm-document-qa\\\\', return_dict=True))\\\\n\\\\n# Get the answer\\\\nresult = nlp(img, question)', 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': ['SQuAD2.0', 'DocVQA'], 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required libraries: PegasusTokenizer, PegasusForConditionalGeneration from the transformers library provided by Hugging Face.\\\\n2. Use the model_name: 'human-centered-summarization/financial-summarization-pegasus'.\\\\n3. Load the tokenizer and the model using the PegasusTokenizer.from_pretrained() and PegasusForConditionalGeneration.from_pretrained() methods, respectively, with the provided model_name.\\\\n4. To use the model for summarization, first tokenize the input text (financial news) using the tokenizer. This will create the input_ids which are needed for the model.\\\\n5. Use the model.generate() method to obtain the summarized text by specifying input_ids, max_length, num_beams, and early_stopping parameters.\\\\n6. Decode the output using the tokenizer.decode() method, which provides the final summarized text.\\\", '<<<code>>>': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration\\\\nmodel_name = \\\\'human-centered-summarization/financial-summarization-pegasus\\\\'\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = \\\"<your_financial_news>\\\"\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=\\\\'pt\\\\').input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nsummary = tokenizer.decode(output[0], skip_special_tokens=True)', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'financial-summarization-pegasus', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabiaâ€™s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Sambaâ€™s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf regionâ€™s third-largest lender. The entityâ€™s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle Eastâ€™s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))', 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 35.206, 'ROUGE-2': 16.569, 'ROUGE-L': 30.128, 'ROUGE-LSUM': 30.171}}, 'description': 'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth', torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the required libraries and pipeline class, i.e., StableDiffusionDepth2ImgPipeline.\\\\n2. Instantiate the pipeline with the pre-trained model \\\\'stabilityai/stable-diffusion-2-depth\\\\'.\\\\n3. Use the pipeline object to generate an image based on the given prompt, \\\"a calm and peaceful beach with palm trees and white sand, no people or animals.\\\"\\\\n4. Provide negative prompts like \\\"people,\\\" \\\"animals,\\\" \\\"crowded,\\\" etc., to avoid generating images with unwanted elements.\\\\n5. You can adjust the \\\\'strength\\\\' parameter to influence the strength of the prompt in the final image.', '<<<code>>>': 'from diffusers import StableDiffusionDepth2ImgPipeline\\\\nimport torch\\\\n\\\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\\\'stabilityai/stable-diffusion-2-depth\\\\', torch_dtype=torch.float16).to(cuda)\\\\ngenerated_image = pipe(prompt=\\\"a calm and peaceful beach with palm trees and white sand, no people or animals\\\",\\\\n                      negative_prompt=\\\"people, animals, crowded\\\", strength=0.8).images[0]', 'api_call': \\\"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth', torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \\\"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth', torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': [['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors']], 'example_code': 'import torch\\\\nimport requests\\\\nfrom PIL import Image\\\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\\\n\\\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\\\n stabilityai/stable-diffusion-2-depth,\\\\n torch_dtype=torch.float16,\\\\n).to(cuda)\\\\n\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\\\nprompt = two tigers\\\\nn_propmt = bad, deformed, ugly, bad anotomy\\\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†Ù…Ø§Ø°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙØ±ÙŠ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"pipeline('zero-shot-classification', model='MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\", 'code': \\\"from transformers import pipeline\\\\ntext = 'Putin faces pressures on food prices and gas shortages.'\\\\nclassifier = pipeline('zero-shot-classification', model='MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli')\\\\ncandidate_labels = ['politics', 'economy', 'energy', 'crisis']\\\\nresult = classifier(text, candidate_labels)\\\\nprint(result)\\\", 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Classification', 'api_name': 'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli', 'api_call': \\\"from transformers import pipeline\\\\ntext = 'Putin faces pressures on food prices and gas shortages.'\\\\nclassifier = pipeline('zero-shot-classification', model='MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli')\\\\ncandidate_labels = ['politics', 'economy', 'energy', 'crisis']\\\\nresult = classifier(text, candidate_labels)\\\\nprint(result)\\\", 'api_arguments': {'model': 'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli', 'sequence_to_classify': 'Angela Merkel is a politician in Germany and leader of the CDU', 'candidate_labels': ['politics', 'economy', 'entertainment', 'environment'], 'multi_label': 'False'}, 'python_environment_requirements': {'transformers': '>=4.13'}, 'example_code': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli)\\\\nsequence_to_classify = Angela Merkel is a politician in Germany and leader of the CDU\\\\ncandidate_labels = [politics, economy, entertainment, environment]\\\\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\\\\nprint(output)', 'performance': {'dataset': {'mnli-m': 0.903, 'mnli-mm': 0.903, 'fever-nli': 0.777, 'anli-all': 0.579, 'anli-r3': 0.495}, 'accuracy': {'mnli-m': 0.903, 'mnli-mm': 0.903, 'fever-nli': 0.777, 'anli-all': 0.579, 'anli-r3': 0.495}}, 'description': 'This model was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs. This base model outperforms almost all large models on the ANLI benchmark. The base model is DeBERTa-v3-base from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective, see annex 11 of the original DeBERTa paper.'}} </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ Ù…ÙˆÙ„Ø¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Text2Text GenerationØŒ Ù‚Ù… Ø¨ØªØ¬Ù‡ÙŠØ² Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ø§Ù„ØªÙŠ ØªØªØ¶Ù…Ù† Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù†ØµÙŠØ© (Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ ÙˆØ§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯). Ø«Ù…ØŒ Ù‚Ù… Ø¨ØªØ­Ø¯ÙŠØ¯ ÙˆØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø°ÙŠ ÙŠØ³ØªÙ†Ø¯ Ø¥Ù„Ù‰ ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚. ÙƒÙ…Ø§ ÙŠÙÙˆØµÙ‰ Ø¨Ø¶Ø¨Ø· Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ø«Ù„ BLEU Ùˆ ROUGE. ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØªØ¬Ø±Ø¨Ø©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© ÙˆØ¶Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø± Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ø¯Ø§Ø¡ Ø£ÙØ¶Ù„.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import T5Tokenizer and T5ForConditionalGeneration from the transformers library.\\\\n2. Instantiate the tokenizer and model by loading the \\\\'google/flan-t5-large\\\\' pretrained model.\\\\n3. Use the tokenizer to convert the input text into input_ids. The input text should include the translation instruction: \\\"translate English to French: Hello, how are you?\\\"\\\\n4. Feed the input_ids to the model\\\\'s generate() function to obtain the translated text.\\\\n5. Decode the output to get the translated text in French.', '<<<code>>>': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'google/flan-t5-large\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'google/flan-t5-large\\\\')\\\\ninput_text = \\\"translate English to French: Hello, how are you?\\\"\\\\ninput_ids = tokenizer(input_text, return_tensors=\\\"pt\\\").input_ids\\\\noutputs = model.generate(input_ids)\\\\ntranslated_text = tokenizer.decode(outputs[0])', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/flan-t5-large', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-large')\\\", 'api_arguments': ['input_text', 'input_ids', 'outputs'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-large)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-large)\\\\ninput_text = translate English to German: How old are you?\\\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))', 'performance': {'dataset': [{'name': 'MMLU', 'accuracy': '75.2%'}]}, 'description': 'FLAN-T5 large is a language model fine-tuned on over 1000 tasks and multiple languages. It achieves state-of-the-art performance on several benchmarks, including 75.2% on five-shot MMLU. The model is based on pretrained T5 and fine-tuned with instructions for better zero-shot and few-shot performance. It can be used for research on language models, zero-shot NLP tasks, in-context few-shot learning NLP tasks, reasoning, question answering, and advancing fairness and safety research.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Øµ ÙˆØ§Ù„ØµÙˆØ±Ø© Ø§Ù„ÙˆØ§Ø±Ø¯Ø© ÙˆØ¯Ù…Ø¬Ù‡Ù…Ø§ Ù„ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØ±Ø© Ø¨Ù…Ø¬Ø§Ù„ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¨ØµØ±ÙŠØ© Ù„Ù„Ø­Ø§Ø³ÙˆØ¨.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, such as urllib.request for fetching the image URL, PIL to open the image, and timm for the image classification model.\\\\n2. Assign the URL of the image to a variable and fetch the image using the urlopen method from urllib.request. Then, create an Image object using Image.open().\\\\n3. Load the pre-trained ConvNeXt-V2 image classification model through timm.create_model() with 'convnextv2_huge.fcmae_ft_in1k' as the model name and pretrained=True as an argument.\\\\n4. Set the model to evaluation mode, create proper transforms for the image using timm.data, and apply them to the input image after unsqueezing the tensor.\\\\n5. Calculate the top-5 probabilities and the class indices using torch.topk.\\\", '<<<code>>>': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimport torch\\\\nimg_url = 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\\\\nimg = Image.open(urlopen(img_url))\\\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))\\\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\\\", 'api_call': \\\"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'convnextv2_huge.fcmae_ft_in1k', 'api_call': \\\"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': [['timm']], 'example_code': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimg = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))\\\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\\\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 86.256}, 'description': 'A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ÙŠØ³ØªØ®Ø¯Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ø¨Ø·Ø±ÙŠÙ‚Ø© ØªØ¨Ø¯Ùˆ ÙˆÙƒØ£Ù†Ù‡Ø§ Ù…Ù†Ø´Ø£Ø© Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Ø¸Ø§Ù… Ù‚Ø§Ø¯Ø±Ù‹Ø§ Ø¹Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…Ø¹Ø¨Ø±Ø© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ÙˆØ³Ø§Ø¦Ø· Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': 'pipeline(\\\"text-generation\\\", model=\\\\'Salesforce/codegen-2B-multi\\\\')', 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the 'pipeline' class from the transformers package.\\\", 'code': 'from transformers import pipeline\\\\ngenerator = pipeline(\\\"text-generation\\\", model=\\\\'Salesforce/codegen-2B-multi\\\\')\\\\ninstruction = \\\"Create a Python function that calculates an employee\\\\'s salary based on their hourly rate and hours worked.\\\"\\\\ngenerated_code = generator(instruction, max_length=100)\\\\nprint(generated_code[0][\\\\'generated_text\\\\'])', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Program Synthesis', 'api_name': 'Salesforce/codegen-2B-multi', 'api_call': 'from transformers import pipeline\\\\ngenerator = pipeline(\\\"text-generation\\\", model=\\\\'Salesforce/codegen-2B-multi\\\\')\\\\ninstruction = \\\"Create a Python function that calculates an employee\\\\'s salary based on their hourly rate and hours worked.\\\"\\\\ngenerated_code = generator(instruction, max_length=100)\\\\nprint(generated_code[0][\\\\'generated_text\\\\'])', 'api_arguments': {'input_ids': 'input_ids', 'max_length': 128}, 'python_environment_requirements': {'transformers': 'AutoTokenizer, AutoModelForCausalLM'}, 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-2B-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-2B-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval, MTPB'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B). The checkpoint included in this repository is denoted as CodeGen-Multi 2B, where Multi means the model is initialized with CodeGen-NL 2B and further pre-trained on a dataset of multiple programming languages, and 2B refers to the number of trainable parameters.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ ØªØ¹Ù„ÙŠÙ…Ø© ØµÙˆØªÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© ØªØ¨Ø¯Ø£ Ø¨Ø¬Ù…Ù„Ø© \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. Import joblib and pandas library to load the trained model and process data.\\\\n2. Load the pre-trained model 'abhishek/autotrain-iris-xgboost' saved as 'model.joblib'. This model is trained on the Iris dataset for multi-class classification, which can also be applied to classify different plant species in the park.\\\\n3. Load your plant data into a pandas DataFrame and preprocess it according to the requirements specified in the trained model's config file.\\\\n4. Once the data has been prepared, use the 'predict()' method of the loaded model to classify the plant species in the park. The output categories can then be used to schedule maintenance and development for different areas.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\n# Replace 'your_plant_data.csv' with the path to the file containing plant data for the park\\\\ndata = pd.read_csv('your_plant_data.csv')\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'abhishek/autotrain-iris-xgboost', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'scikit-learn/iris', 'accuracy': 0.8666666666666667}, 'description': 'A tabular classification model trained on the Iris dataset using XGBoost and AutoTrain. The model is capable of multi-class classification and has an accuracy of 86.67%.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ù†Ø´Ø§Ø· Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ØµÙˆØª ÙÙŠ Ø§Ù„ØµÙˆØª Ø§Ù„Ø°ÙŠ ØªÙ… ØªÙˆÙÙŠØ±Ù‡.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. We first import the necessary modules from the transformers library, including AutoModelForCausalLM, AutoTokenizer, and set_seed.\\\\n2. We instantiate the pretrained text generation model \\\\'facebook/opt-66b\\\\' for the task.\\\\n3. We create a tokenizer to process the input text and the generated text.\\\\n4. We give the model a starting prompt like \\\"Create a fitness slogan: \\\". The model will then generate possible slogans based on the given prompt.\\\\n5. We can control the generated text using parameters like \\\\'do_sample\\\\', \\\\'num_return_sequences\\\\', and \\\\'max_length\\\\' to get suitable slogans.', '<<<code>>>': 'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\n\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'facebook/opt-66b\\\\', torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/opt-66b\\\\', use_fast=False)\\\\n\\\\nprompt = \\\"Create a fitness slogan: \\\"\\\\ninput_ids = tokenizer(prompt, return_tensors=\\\\'pt\\\\').input_ids.cuda()\\\\n\\\\nset_seed(32)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\\\\nslogans = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/opt-66b', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", 'api_arguments': ['input_ids', 'do_sample', 'num_return_sequences', 'max_length'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-66b, torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-66b, use_fast=False)\\\\nprompt = Hello, I am conscious and\\\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\\\nset_seed(32)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\\\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)', 'performance': {'dataset': 'GPT-3', 'accuracy': 'roughly matched'}, 'description': 'OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. OPT models are trained to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙŠÙ‚ÙˆÙ… Ø¨ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ. Ø§Ù„Ù‡Ø¯Ù Ù‡Ùˆ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù…Ù„Ø®Øµ Ù‚ØµÙŠØ± ÙˆÙ…ÙÙŠØ¯ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ‡.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"PegasusForConditionalGeneration.from_pretrained('google/pegasus-cnn_dailymail')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary classes from the transformers package. This includes PegasusForConditionalGeneration and PegasusTokenizer.\\\\n2. We then use the from_pretrained method of the PegasusForConditionalGeneration class to load the pre-trained model 'google/pegasus-cnn_dailymail'. This model has been trained for abstractive summarization tasks, which is exactly what we need for summarizing key product features and benefits.\\\\n3. To use the model, we first tokenize the input text using the PegasusTokenizer. Next, we pass the tokenized input to the model which generates a summary.\\\\n4. The summary is then decoded back into human-readable text and can be used in press releases.\\\", '<<<code>>>': \\\"from transformers import PegasusForConditionalGeneration, PegasusTokenizer\\\\nmodel_name = 'google/pegasus-cnn_dailymail'\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ninputs = tokenizer.encode('Our latest product...', return_tensors='pt')\\\\n# replace 'Our latest product...' with the description of the product's features and benefits\\\\nsummary_ids = model.generate(inputs)\\\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\\\", 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('google/pegasus-cnn_dailymail')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/pegasus-cnn_dailymail', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('google/pegasus-cnn_dailymail')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import PegasusForConditionalGeneration, PegasusTokenizer\\\\nmodel_name = 'google/pegasus-cnn_dailymail'\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ninputs = tokenizer.encode('input_text', return_tensors='pt')\\\\nsummary_ids = model.generate(inputs)\\\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\\\", 'performance': {'dataset': 'cnn_dailymail', 'accuracy': '44.16/21.56/41.30'}, 'description': 'PEGASUS model for abstractive summarization, pretrained on the CNN/DailyMail dataset.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ÙÙ‡Ù… Ù…Ø¯Ù‰ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„ Ø¨ÙŠÙ† Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠØ±Ø¬Ù‰ ØªØ­Ù„ÙŠÙ„ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø¬Ù…Ù„ØªÙŠÙ† Ù…Ø®ØªÙ„ÙØªÙŠÙ† ÙˆØ¥Ø¹Ø·Ø§Ø¡ Ù†ØªØ§Ø¦Ø¬Ùƒ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, install the 'sentence-transformers' library using pip.\\\\n2. Import 'SentenceTransformer' from the 'sentence_transformers' library.\\\\n3. Instantiate the SentenceTransformer with  pre-trained model 'sentence-transformers/distiluse-base-multilingual-cased-v2'.\\\\n4. Now, you can pass a list of sentences you want to compare as input to the 'encode' method of the SentenceTransformer. This method will return sentence embeddings in a high-dimensional space.\\\\n5. To measure similarity between sentences, you can calculate the cosine similarity between the embeddings, where a higher value indicates more similar sentences, and a lower value indicates less similar sentences.\\\\n6. Using this model, you can quickly identify similarities between the sentences you write and minimize repetition.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nsentences = [\\\"This is an example sentence.\\\", \\\"Each sentence has similarities.\\\"]\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/distiluse-base-multilingual-cased-v2\\\\')\\\\nembeddings = model.encode(sentences)', 'api_call': \\\"SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distiluse-base-multilingual-cased-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªØµÙ†ÙŠÙ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ù„ØªÙ…ÙŠÙŠØ² Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©. ÙŠØ¬Ø¨ ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ø§Ù„Ù…ØµÙ†ÙØ© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ù„ØªØ¯Ø±ÙŠØ¨Ù‡ ÙˆØ§Ø®ØªØ¨Ø§Ø±Ù‡.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"AutoModel.from_pretrained('abhishek/autotrain-adult-census-xgboost')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. Import the necessary libraries (joblib, json, and pandas) to load the model and handle the dataset.\\\\n2. Load the pre-trained model 'abhishek/autotrain-adult-census-xgboost' using joblib.\\\\n3. Load the dataset in a pandas DataFrame by reading the CSV file.\\\\n4. Select and preprocess the necessary features according to the model's configuration file.\\\\n5. Use the pre-trained model to predict income levels ('above $50,000' or 'below $50,000') for each client in the dataset.\\\", '<<<code>>>': \\\"import joblib\\\\nimport json\\\\nimport pandas as pd\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('client_data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\n\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"AutoModel.from_pretrained('abhishek/autotrain-adult-census-xgboost')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'abhishek/autotrain-adult-census-xgboost', 'api_call': \\\"AutoModel.from_pretrained('abhishek/autotrain-adult-census-xgboost')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['json', 'joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'scikit-learn/adult-census-income', 'accuracy': 0.8750191923844618}, 'description': \\\"A binary classification model trained on the Adult Census Income dataset using the XGBoost algorithm. The model predicts whether an individual's income is above or below $50,000 per year.\\\" </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© ØºÙŠØ± Ù…Ø´Ø±ÙˆØ·Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ØµØ±ÙŠØŒ ÙˆÙ†Ø·Ù…Ø­ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø© ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø°Ø§Øª Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ© ÙˆØ§Ù‚Ø¹ÙŠØ© Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ø´Ø±ÙˆØ· Ù…Ø­Ø¯Ø¯Ø©.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-church-256')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Firstly, we need to install the 'diffusers' library, which contains the required classes for using the 'google/ncsnpp-church-256' model.\\\", 'code': \\\"!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = 'google/ncsnpp-church-256'\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage_sample = sde_ve()[0]\\\\nimage_sample.save('generated_church_image.png')\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ncsnpp-church-256', 'api_call': \\\"!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = 'google/ncsnpp-church-256'\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage_sample = sde_ve()[0]\\\\nimage_sample.save('generated_church_image.png')\\\", 'api_arguments': 'model_id', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = google/ncsnpp-church-256\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[sample]\\\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and can generate high fidelity images of size 1024 x 1024.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ÙƒØ´Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙŠÙØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… ØªÙ‚Ø¯ÙŠØ± Ù„Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ø§Ù„Ù…Ø¹ØªØ±Ù Ø¨Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù†Ø¸Ø§Ù….? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8m-forklift-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the required classes and functions from the ultralyticsplus package, which includes YOLO and render_result.\\\\n2. Use the YOLO() function to load the 'keremberke/yolov8m-forklift-detection' model trained for detecting forklifts and people in images.\\\\n3. Set the appropriate confidence level, Intersection over Union (IoU) threshold, and other necessary overrides for the model.\\\\n4. Process the desired input image for detecting forklifts and people using the predict() function of the instantiated model.\\\\n5. The results returned will contain boxes around detected objects, which can be printed.\\\\n6. Using the render_result function, you can visualize the detected objects in the image.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-forklift-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path_to_image.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8m-forklift-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8m-forklift-detection', 'api_call': \\\"YOLO('keremberke/yolov8m-forklift-detection')\\\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8m-forklift-detection')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'forklift-object-detection', 'accuracy': 0.846}, 'description': 'A YOLOv8 model for detecting forklifts and persons in images.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØªØµÙ†ÙŠÙÙ‡Ø§ Ø¨Ø¯ÙˆÙ† ØªØµÙ†ÙŠÙ Ù…Ø³Ø¨Ù‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, including `AutoTokenizer` and `AutoModelForSequenceClassification` from `transformers` as well as `torch`.\\\\n2. Specify the model name, which is `'MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary'` and load its tokenizer and model.\\\\n3. Input your two sentences (premise and hypothesis) in separate variables.\\\\n4. Tokenize these sentences using the tokenizer's `__call__` method, and make sure to set `truncation=True` and `return_tensors='pt'`.\\\\n5. Perform a forward pass through the model by passing the tokenizer's output as input.\\\\n6. Softmax the logits output to transform them into probabilities.\\\\n7. Associate probabilities with their respective labels `entailment` and `not_entailment`.\\\\n8. Determine whether the two sentences contradict information or not using their respective probabilities.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nimport torch\\\\n\\\\ndevice = torch.device(\\\"cuda\\\") if torch.cuda.is_available() else torch.device(\\\"cpu\\\")\\\\nmodel_name = \\\\'MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\\\\n\\\\npremise = \\\"I first thought that I liked the movie, but upon second thought it was actually disappointing.\\\"\\\\nhypothesis = \\\"The movie was good.\\\"\\\\n\\\\ninput = tokenizer(premise, hypothesis, truncation=True, return_tensors=\\\\'pt\\\\')\\\\noutput = model(input[\\\"input_ids\\\"].to(device))\\\\n\\\\nprediction = torch.softmax(output[\\\"logits\\\"][0], -1).tolist()\\\\nlabel_names = [\\\\'entailment\\\\', \\\\'not_entailment\\\\']\\\\nprediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\\\\n\\\\nprint(prediction)', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary')\\\", 'api_arguments': {'premise': 'string', 'hypothesis': 'string'}, 'python_environment_requirements': [{'transformers': '>=4.13', 'torch': 'any'}], 'example_code': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nimport torch\\\\ndevice = torch.device(cuda) if torch.cuda.is_available() else torch.device(cpu)\\\\nmodel_name = MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\\\\npremise = I first thought that I liked the movie, but upon second thought it was actually disappointing.\\\\nhypothesis = The movie was good.\\\\ninput = tokenizer(premise, hypothesis, truncation=True, return_tensors=pt)\\\\noutput = model(input[input_ids].to(device))\\\\nprediction = torch.softmax(output[logits][0], -1).tolist()\\\\nlabel_names = [entailment, not_entailment]\\\\nprediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\\\\nprint(prediction)', 'performance': {'dataset': {'mnli-m-2c': {'accuracy': 0.925}, 'mnli-mm-2c': {'accuracy': 0.922}, 'fever-nli-2c': {'accuracy': 0.892}, 'anli-all-2c': {'accuracy': 0.676}, 'anli-r3-2c': {'accuracy': 0.665}, 'lingnli-2c': {'accuracy': 0.888}}}, 'description': 'This model was trained on 782 357 hypothesis-premise pairs from 4 NLI datasets: MultiNLI, Fever-NLI, LingNLI and ANLI. The base model is DeBERTa-v3-xsmall from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ¬Ù‡ÙŠØ² Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ±ÙŠ Ø¨Ø¯ÙˆÙ† Ø¯Ø¹Ù… Ø­ØªÙ‰ ØªÙƒÙˆÙ† Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ ØªÙ…ÙŠÙŠØ² Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ø§Ù„ØªÙŠ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ ØªØ¯Ø±ÙŠØ¨Ù‡Ø§ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØŒ ÙŠÙÙØ¶Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯ÙˆØ§Øª Ù„ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ ÙƒÙ…Ø§ Ù„Ùˆ Ø£Ù†Ù‡Ù… ÙŠÙ„Ø¹Ø¨ÙˆÙ† ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the necessary libraries: PIL for image manipulation, requests to download the image, and from transformers, we will use ChineseCLIPProcessor and ChineseCLIPModel.\\\\n2. Create instances of the ChineseCLIPProcessor and ChineseCLIPModel using the pre-trained model \\\\'OFA-Sys/chinese-clip-vit-large-patch14\\\\'.\\\\n3. Load the image file that you want to classify using the Image.open() method from PIL library.\\\\n4. Prepare the list of product types for zero-shot classification in variable named \\\"texts\\\" (i.e., [\\\\'electronics\\\\', \\\\'clothing\\\\', \\\\'toys\\\\']).\\\\n5. Use the processor to create inputs for both the image and potential product types.\\\\n6. Calculate image features and text features separately, and normalize them.\\\\n7. Combine image-text pairs and calculate the similarity score between them using the model.\\\\n8. Apply softmax on the logits to obtain the probabilities for each category.\\\\n9. The output provides the model\\\\'s predictions for the image representing one of the product types.', '<<<code>>>': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\nmodel = ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\\nprocessor = ChineseCLIPProcessor.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\\nimage_url = 'https://example.com/path/to/image.jpg'\\\\nimage = Image.open(requests.get(image_url, stream=True).raw)\\\\ntexts = ['electronics', 'clothing', 'toys']\\\\ninputs = processor(images=image, texts=texts, return_tensors='pt', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\", 'api_call': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'chinese-clip-vit-large-patch14', 'api_call': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\", 'api_arguments': {'model_name': 'OFA-Sys/chinese-clip-vit-large-patch14'}, 'python_environment_requirements': [{'libraries': ['transformers', 'PIL', 'requests']}], 'example_code': 'from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\\\\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\\\\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = []\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nimage_features = model.get_image_features(**inputs)\\\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True) # normalize\\\\ninputs = processor(text=texts, padding=True, return_tensors=pt)\\\\ntext_features = model.get_text_features(**inputs)\\\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True) # normalize\\\\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\\\\nprobs = logits_per_image.softmax(dim=1) # probs: [[0.0066, 0.0211, 0.0031, 0.9692]]', 'performance': {'dataset': 'MUGE Text-to-Image Retrieval, Flickr30K-CN Retrieval, COCO-CN Retrieval, CIFAR10, CIFAR100, DTD, EuroSAT, FER, FGV, KITTI, MNIST, PASCAL VOC', 'accuracy': 'Varies depending on the dataset'}, 'description': 'Chinese-CLIP-ViT-Large-Patch14 is a large version of the Chinese CLIP model, with ViT-L/14 as the image encoder and RoBERTa-wwm-base as the text encoder. Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It is designed for zero-shot image classification tasks.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… ÙØ±ÙŠÙ‚Ù†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ ÙˆØªÙ‚Ù†ÙŠØ§Øª Ù„ÙØµÙ„ Ø§Ù„Ø£Ø¬Ø³Ø§Ù… ÙÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ù„ØªÙ‚Ø·Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the required classes from the transformers library (SegformerFeatureExtractor and SegformerForSemanticSegmentation) and other libraries (PIL for image handling and requests for fetching images from URLs).\\\\n2. We load the pretrained 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024' model, which is a SegFormer model fine-tuned on the CityScapes dataset for image segmentation. This model can segment images into different classes like buildings, roads, and green spaces.\\\\n3. Using the SegformerFeatureExtractor, we preprocess the input aerial image and then pass it through the model, which generates the segmented output.\\\\n4. The output can then be analyzed to make decisions about city planning.\\\", '<<<code>>>': \\\"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\\naerial_image_url = 'https://example.com/aerial_image.jpg'\\\\n\\\\nimage = Image.open(requests.get(aerial_image_url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\", 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024', 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ø³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8m-valorant-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the required libraries: ultralyticsplus and ultralytics.\\\\n2. Import YOLO and render_result classes from the ultralyticsplus library.\\\\n3. Load the YOLO object detection model for the Valorant game using the 'keremberke/yolov8m-valorant-detection' identifier.\\\\n4. Configure the model with the desired confidence (conf), intersection over union (iou), agnostic NMS, and maximum detections (max_det) settings.\\\\n5. Provide a screenshot (image) to the model.\\\\n6. The model will detect game objects such as dropped spikes, enemies, planted spikes, and teammates and return the bounding boxes.\\\\n7. Render the results to visualize the detected objects.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path/to/screenshot.jpg'\\\\nresults = model.predict(image)\\\\nbounding_boxes = results[0].boxes\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8m-valorant-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8m-valorant-detection', 'api_call': \\\"YOLO('keremberke/yolov8m-valorant-detection')\\\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'valorant-object-detection', 'accuracy': 0.965}, 'description': 'A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù…Ø±Ø­Ù„Ø© Ù…Ù† Ø§Ù„ØªØ±ÙƒÙŠØ¨ Ø§Ù„Ù†ØµÙŠ Ø§Ù„Ù‰ Ù†Øµ Ù…Ù‚Ø§Ø¨Ù„Ø§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ. Ù‡Ù„ Ø¨Ø¥Ù…ÙƒØ§Ù†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ù‡Ø°Ø§ØŸ? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-400M-distill')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': \\\"from transformers import BlenderbotForConditionalGeneration, Conversation\\\\nchatbot = BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-400M-distill')\\\\nconversation = Conversation(val)\\\\nresponse = chatbot.generate(conversation.tensor())\\\\nresponse_text = chatbot.tokenizer.batch_decode(response, skip_special_tokens=True)\\\", 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Conversational', 'api_name': 'facebook/blenderbot-400M-distill', 'api_call': \\\"from transformers import BlenderbotForConditionalGeneration, Conversation\\\\nchatbot = BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-400M-distill')\\\\nconversation = Conversation(val)\\\\nresponse = chatbot.generate(conversation.tensor())\\\\nresponse_text = chatbot.tokenizer.batch_decode(response, skip_special_tokens=True)\\\", 'api_arguments': \\\"['message']\\\", 'python_environment_requirements': ['transformers', 'torch'], 'example_code': 'Input a message to start chatting with facebook/blenderbot-400M-distill.', 'performance': {'dataset': 'blended_skill_talk', 'accuracy': 'Not specified'}, 'description': 'BlenderBot-400M-distill is a distilled version of the BlenderBot model, trained on the Blended Skill Talk dataset. It is designed for open-domain chatbot tasks and can generate text-to-text responses in a conversational manner. The model is based on the Transformers library and can be used with PyTorch, TensorFlow, and JAX.'}} </s>\"\n"}
{"text": "<s>[INST]Ø³Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ø°ÙŠ ÙŠÙ‚ÙˆÙ… Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„. ÙŠØ¬Ø¨ ØªØµÙ…ÙŠÙ… Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØªÙ…Ø§Ø´Ù‰ Ù…Ø¹ Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ø°ÙŠÙ† ÙŠØ¹Ø§Ù†ÙˆÙ† Ù…Ù† Ø¥Ø¹Ø§Ù‚Ø© Ø¨ØµØ±ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library.\\\\n2. Initialize the pipeline with the 'table-question-answering' task and the model 'google/tapas-medium-finetuned-wtq'.\\\\n3. Define the table containing the employees' information and create a query to find the less talkative people for the one-month project.\\\\n4. Pass the table and query to the table_qa function to extract the appropriate employee(s) who fit the requirements.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntable_qa = pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\\ntable = {...}  # Employees information table\\\\nquery = 'Who are the least talkative employees for a one-month project?'\\\\nrecommended_employees = table_qa(table=table, query=query)\\\", 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/tapas-medium-finetuned-wtq', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", 'api_arguments': 'table, query', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntable_qa = pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\\n\\\\n# Define the table and the query\\\\ntable = {...}\\\\nquery = '...'\\\\n\\\\n# Get the answer\\\\nanswer = table_qa(table=table, query=query)\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.4324}, 'description': 'TAPAS medium model fine-tuned on WikiTable Questions (WTQ). This model is pretrained on a large corpus of English data from Wikipedia and is used for answering questions related to a table.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø£Ø¬ÙˆØ¨Ø© Ø¹Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©. ÙŠØ±ÙƒØ² Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù„Ù‰ Ø§Ù„ÙØ­Øµ Ø§Ù„Ø¨ØµØ±ÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary library which is AutoModel from transformers.\\\\n2. Use the from_pretrained method of the AutoModel class to load the pre-trained model 'sheldonxxxx/OFA_model_weights'. This model has been designed for visual question answering tasks, which is exactly what we need for answering questions on movie scenes.\\\\n3. You can then use this loaded model to perform visual question answering tasks based on the input data (movie scenes and their corresponding questions).\\\\n4. Note that this is an unofficial mirror of the model weights for OFA, and it has been provided to help with faster downloads outside of China.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", 'api_call': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sheldonxxxx/OFA_model_weights', 'api_call': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙˆØ¬Ø¯ ÙØ±ÙŠÙ‚ Ø·Ù„Ø§Ø¨ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙŠÙ„ØªÙ‚ÙŠ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠ Ù„Ø¹Ø¨Ø© ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ù‚Ù… Ø¨ØªØ²ÙˆÙŠØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. Import the required library (pipeline) from the transformers package provided by Hugging Face.\\\\n2. Use the pipeline function to create a translator that translates German text to Spanish.\\\\n3. Pass the German text \\\"Ich mÃ¶chte ein Auto kaufen. Welche Farbe empfehlen Sie?\\\" to the created translator (translation).\\\\n4. The translator will translate the German input text into Spanish.', '<<<code>>>': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_de_to_es\\\\', model=\\\\'Helsinki-NLP/opus-mt-de-es\\\\')\\\\ngerman_text = \\\"Ich mÃ¶chte ein Auto kaufen. Welche Farbe empfehlen Sie?\\\"\\\\nspanish_translation = translator(german_text)\\\\nresult = spanish_translation[0][\\\\'translation_text\\\\']', 'api_call': \\\"pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-de-es', 'api_call': \\\"pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\\ntranslated_text = translation('Guten Tag')\\\", 'performance': {'dataset': 'Tatoeba.de.es', 'accuracy': {'BLEU': 48.5, 'chr-F': 0.676}}, 'description': 'A German to Spanish translation model based on the OPUS dataset and trained using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece tokenization.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙˆØµÙ Ù†ØµÙŠ ÙŠØµÙ Ù…Ø´Ù‡Ø¯Ù‹Ø§ ÙŠØ¸Ù‡Ø± ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ­Ø¯ÙŠØ¯ ÙˆØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± ÙˆÙ…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and classes such as VideoReader from decord, VideoMAEFeatureExtractor and VideoMAEForVideoClassification from transformers, and hf_hub_download from huggingface_hub.\\\\n2. Create a VideoReader object to read the video file using the given file path.\\\\n3. Pick clip_len number of frames from the video with a specified frame_sample_rate.\\\\n4. Use VideoMAEFeatureExtractor to process the video frames, converting them into the appropriate format for the model.\\\\n5. Load the 'nateraw/videomae-base-finetuned-ucf101' pre-trained model using VideoMAEForVideoClassification and use it to analyze the video footage.\\\\n6. The model will produce logits that correspond to the probability of each action. Find the highest probability action and use the model's config.id2label dictionary to find the matching label.\\\\n7. Display the predicted label.\\\", '<<<code>>>': \\\"from decord import VideoReader, cpu\\\\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\\\nfrom huggingface_hub import hf_hub_download\\\\nfile_path = '<YOUR_VIDEO_FILE_PATH>'\\\\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\\\\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\\\\nvideo = videoreader.get_batch(indices).asnumpy()\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\\ninputs = feature_extractor(list(video), return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'videomae-base-finetuned-ucf101', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/videomae-base-finetuned-ucf101'}, 'python_environment_requirements': [['transformers', 'decord', 'huggingface_hub']], 'example_code': 'from decord import VideoReader, cpu\\\\nimport torch\\\\nimport numpy as np\\\\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\\\nfrom huggingface_hub import hf_hub_download\\\\nnp.random.seed(0)\\\\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\\\\n converted_len = int(clip_len * frame_sample_rate)\\\\n end_idx = np.random.randint(converted_len, seg_len)\\\\n start_idx = end_idx - converted_len\\\\n indices = np.linspace(start_idx, end_idx, num=clip_len)\\\\n indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\\\\n return indices\\\\nfile_path = hf_hub_download(\\\\n repo_id=nateraw/dino-clips, filename=archery.mp4, repo_type=space\\\\n)\\\\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\\\\nvideoreader.seek(0)\\\\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\\\\nvideo = videoreader.get_batch(indices).asnumpy()\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\\\\ninputs = feature_extractor(list(video), return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'UCF101', 'accuracy': 0.758209764957428}, 'description': 'VideoMAE Base model fine tuned on UCF101 for Video Action Recognition' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø·ÙˆØ± ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ© Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨Ø§Ø­Ø«ÙŠÙ† ÙˆØ§Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary classes from the transformers library and custom tokenization module. This includes PegasusForConditionalGeneration for the text summarization model and PegasusTokenizer for processing Chinese text.\\\\n2. We then use the from_pretrained method to load the pre-trained model 'IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese'. This model has been fine-tuned on multiple Chinese text summarization datasets and is ideal for generating a summary of a Chinese article.\\\\n3. We load the text in the variable 'text' and tokenize it using the PegasusTokenizer.\\\\n4. The model processes the tokenized text and generates a summary, which can be obtained by decoding the tokens.\\\", '<<<code>>>': \\\"from transformers import PegasusForConditionalGeneration\\\\nfrom tokenizers_pegasus import PegasusTokenizer\\\\nmodel = PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntokenizer = PegasusTokenizer.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntext = 'ä½ çš„ä¸­æ–‡æ–‡ç« å†…å®¹'\\\\ninputs = tokenizer(text, max_length=1024, return_tensors='pt')\\\\nsummary_ids = model.generate(inputs['input_ids'])\\\\nsummary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\\\", 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Randeng-Pegasus-238M-Summary-Chinese', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\", 'api_arguments': {'text': 'string', 'max_length': 'integer'}, 'python_environment_requirements': [['transformers', 'tokenizers_pegasus.py', 'data_utils.py']], 'example_code': \\\"from transformers import PegasusForConditionalGeneration\\\\nfrom tokenizers_pegasus import PegasusTokenizer\\\\nmodel = PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntokenizer = PegasusTokenizer.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntext = '1'\\\\ninputs = tokenizer(text, max_length=1024, return_tensors='pt')\\\\nsummary_ids = model.generate(inputs['input_ids'])\\\\ntokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\\\", 'performance': {'dataset': 'LCSTS', 'accuracy': {'rouge-1': 43.46, 'rouge-2': 29.59, 'rouge-L': 39.76}}, 'description': 'Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus. It is fine-tuned on 7 Chinese text summarization datasets including education, new2016zh, nlpcc, shence, sohu, thucnews, and weibo. The model can be used to generate summaries for Chinese text inputs.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ø¹Ù…Ù„ Ø¨ØªÙ‚Ù†ÙŠØ§Øª ØªØ¹Ø²ÙŠØ² Ø§Ù„ØªØ¹Ù„Ù… ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù„ÙŠÙ…ÙŠØ© ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ¯Ø±ÙŠØ¨ Ø£Ù†Ø¸Ù…Ø© ØªØ­ÙƒÙ… Ø°ÙƒÙŠØ© Ù„Ù„Ø±ÙˆØ¨ÙˆØªØ§ØªØŒ ÙˆØ°Ù„Ùƒ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ø¯Ø§Ø¡ Ù…ØªÙ…ÙŠØ² ÙÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ¦Ø©.? [/INST]'<<<domain>>>': 'Reinforcement Learning Robotics', '<<<api_call>>>': \\\"model_utils.load_model('model_utils.VC1_BASE_NAME')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary module 'model_utils' from the vc_models.models.vit package. This includes the functions required for loading and utilizing the VC-1 vision transformer model.\\\\n2. We then use the load_model function to load the pre-trained VC-1 model. This model is a vision transformer suitable for EmbodiedAI tasks, making it appropriate for office navigation and object manipulation.\\\\n3. With the model loaded, we can preprocess an input image captured by the robot's camera using the model_transforms helper.\\\\n4. Finally, we pass the preprocessed image through the model to obtain an embedding, which can be used for further analysis and action planning.\\\", '<<<code>>>': \\\"from vc_models.models.vit import model_utils\\\\nmodel, embd_size, model_transforms, model_info = model_utils.load_model(model_utils.VC1_BASE_NAME)\\\\nimg = your_image here  # Replace your_image with an actual image from the robot's camera\\\\ntransformed_img = model_transforms(img)\\\\nembedding = model(transformed_img)\\\", 'api_call': \\\"model_utils.load_model('model_utils.VC1_BASE_NAME')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Reinforcement Learning Robotics', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'VC1_BASE_NAME', 'api_call': \\\"model_utils.load_model('model_utils.VC1_BASE_NAME')\\\", 'api_arguments': 'img', 'python_environment_requirements': ['from vc_models.models.vit import model_utils'], 'example_code': 'model,embd_size,model_transforms,model_info = model_utils.load_model(model_utils.VC1_BASE_NAME)\\\\nimg = your_function_here ...\\\\ntransformed_img = model_transforms(img)\\\\nembedding = model(transformed_img)', 'performance': {'dataset': 'CortexBench', 'accuracy': 'Mean Success: 68.7%'}, 'description': 'The VC-1 model is a vision transformer (ViT) pre-trained on over 4,000 hours of egocentric videos from 7 different sources, together with ImageNet. The model is trained using Masked Auto-Encoding (MAE) and is available in two sizes: ViT-B and ViT-L. The model is intended for use for EmbodiedAI tasks, such as object manipulation and indoor navigation.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù†ØµÙˆØµ. ÙŠØ±Ø¬Ù‰ ØªØ²ÙˆÙŠØ¯Ù†Ø§ Ø¨Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯Ø§Øª ÙˆØ§Ù„Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© ØªØ­Ø¯ÙŠØ« ÙˆØªØ­Ø³ÙŠÙ† Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('flexudy/t5-base-multi-sentence-doctor')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the necessary classes from the transformers library, which are AutoTokenizer and AutoModelWithLMHead.', 'code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'flexudy/t5-base-multi-sentence-doctor\\\\')\\\\nmodel = AutoModelWithLMHead.from_pretrained(\\\\'flexudy/t5-base-multi-sentence-doctor\\\\')\\\\ninput_text = \\\"repair_sentence: I hopes you got recieved thes attachmen with the report we discussed in meet yestday and pleas share you\\\\'re opinion.\\\"\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=\\\\'pt\\\\')\\\\noutputs = model.generate(input_ids, max_length=128, num_beams=1)\\\\ncorrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'flexudy/t5-base-multi-sentence-doctor\\\\')\\\\nmodel = AutoModelWithLMHead.from_pretrained(\\\\'flexudy/t5-base-multi-sentence-doctor\\\\')\\\\ninput_text = \\\"repair_sentence: I hopes you got recieved thes attachmen with the report we discussed in meet yestday and pleas share you\\\\'re opinion.\\\"\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=\\\\'pt\\\\')\\\\noutputs = model.generate(input_ids, max_length=128, num_beams=1)\\\\ncorrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ù†Øµ ØŒ ÙˆÙ‚Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¯Ø§Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„ Ù„Ù‡Ø§ Ø¨Ø¯Ù‚Ø©. ÙŠØ¬Ø¨ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù…Ù‡Ø§ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ù…Ù‡Ù…Ø© Ù…Ù†Ù‡Ø§.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"pipeline('text-generation', model='microsoft/git-base-coco')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which includes the 'pipeline' function from the transformers package.\\\\n2. Use the 'pipeline' function to create a text generation model with the pre-trained 'microsoft/git-base-coco' model. This model has been fine-tuned for the image-to-text generation task, making it suitable for generating captions for images.\\\\n3. Load the image that needs to be captioned and convert it into an appropriate format for feeding into the model.\\\\n4. Pass the image to the model to generate a caption.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nfrom PIL import Image\\\\n\\\\ndef generate_caption(image_path):\\\\n    image_to_text_model = pipeline('text-generation', model='microsoft/git-base-coco')\\\\n    image = Image.open(image_path)\\\\n    caption = image_to_text_model(image)\\\\n    return caption\\\\n\\\\n# Usage example\\\\ncaption_result = generate_caption('your_image_path.jpg')\\\", 'api_call': \\\"pipeline('text-generation', model='microsoft/git-base-coco')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/git-base-coco', 'api_call': \\\"pipeline('text-generation', model='microsoft/git-base-coco')\\\", 'api_arguments': 'image', 'python_environment_requirements': ['transformers'], 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ†Ø¨Ø¤ Ø¨Ø¥Ø¬Ø§Ø¨Ø§Øª ØµØ­ÙŠØ­Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model='bigwiz83/sapbert-from-pubmedbert-squad2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' function from transformers.\\\\n2. Load the model 'bigwiz83/sapbert-from-pubmedbert-squad2' using the 'pipeline' function, setting the functionality to 'question-answering'.\\\\n3. For each research paper abstract, provide a context and question to the model.\\\\n4. The model will then give an answer for the question based on the research paper abstract.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\'question-answering\\\\', model=\\\\'bigwiz83/sapbert-from-pubmedbert-squad2\\\\')\\\\nabstract = \\\"Research paper abstract here...\\\"\\\\nquestion = \\\"What is the main finding of the research paper?\\\"\\\\nresult = qa_pipeline({\\\\'context\\\\': abstract, \\\\'question\\\\': question})\\\\nanswer = result[\\\\'answer\\\\']', 'api_call': \\\"pipeline('question-answering', model='bigwiz83/sapbert-from-pubmedbert-squad2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bigwiz83/sapbert-from-pubmedbert-squad2', 'api_call': \\\"pipeline('question-answering', model='bigwiz83/sapbert-from-pubmedbert-squad2')\\\", 'api_arguments': ['context', 'question'], 'python_environment_requirements': [['transformers==4.7.0', 'torch==1.8.0', 'datasets==1.4.1', 'tokenizers==0.10.2']], 'example_code': \\\"qa_pipeline({'context': 'This model can be loaded on the Inference API on-demand.', 'question': 'Where can the model be loaded?'})\\\", 'performance': {'dataset': 'squad_v2', 'accuracy': '1.2582'}, 'description': 'This model is a fine-tuned version of cambridgeltl/SapBERT-from-PubMedBERT-fulltext on the squad_v2 dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø©. ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªØµÙ…ÙŠÙ… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ Ø¨ÙŠÙ† Ø¹Ø¯Ø© Ù„ØºØ§Øª Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙˆØ³Ù„Ø§Ø³Ø©ØŒ ÙˆØ¶Ø¨Ø·Ù‡Ø§ Ø¹Ù„Ù‰ Ø£Ù† ØªØ¹Ù…Ù„ Ø¨ÙƒÙØ§Ø¡Ø© Ø¨Ù…Ø®ØªÙ„Ù Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ. ÙŠØªÙˆØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙÙ‡Ù… Ø¹Ù…Ù‚ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù„ØºÙˆÙŠØ© ÙˆØ§Ù„ØµÙŠØº Ø§Ù„Ù†Ø­ÙˆÙŠØ© Ù„Ù„ØºØ§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØªØ·ÙˆÙŠØ± Ø§Ù„Ø­Ù„ÙˆÙ„ Ù„ØªÙ‚Ø¯ÙŠÙ… ØªØ±Ø¬Ù…Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØ³Ù„Ø³Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the required libraries such as pipeline function and AutoModelForSeq2SeqLM from the transformers library provided by Hugging Face.\\\\n2. Load the pre-trained Helsinki-NLP/opus-mt-en-ru translation model using AutoModelForSeq2SeqLM.from_pretrained() function. This model is designed for translating English text to Russian.\\\\n3. Create a translation pipeline using the pipeline() function with 'translation_en_to_ru' and the loaded model to perform translations.\\\\n4. Pass the customer reviews as input to the pipeline to get the translated text in Russian.\\\", '<<<code>>>': 'from transformers import pipeline, AutoModelForSeq2SeqLM\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-ru\\\\')\\\\ntranslation = pipeline(\\\\'translation_en_to_ru\\\\', model=model)\\\\ntranslated_text = translation(\\\"Hello, how are you?\\\")', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-en-ru', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers', 'torch', 'tensorflow']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ru', model='Helsinki-NLP/opus-mt-en-ru')\\\\ntranslated_text = translation('Hello, how are you?')\\\", 'performance': {'dataset': 'newstest2019-enru', 'accuracy': '27.1'}, 'description': 'Helsinki-NLP/opus-mt-en-ru is a translation model trained on the OPUS dataset, which translates English text to Russian. It is based on the Marian NMT framework and can be used with Hugging Face Transformers.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù„ØªØ¹Ø±Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„ØµÙˆØªÙŠ. ÙŠÙ‡Ø¯Ù Ø§Ù„Ù†Ø¸Ø§Ù… Ø¥Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ³Ø¬ÙŠÙ„Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø¥Ù„Ù‰ Ù†Øµ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¬Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')\\\\naudio_paths = ['/path/to/lecture1.mp3', '/path/to/lecture2.mp3']\\\\n# Replace these file paths with the paths to your own audio files.\\\\ntranscriptions = model.transcribe(audio_paths)\\\", 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Speech Recognition', 'api_name': 'jonatasgrosman/wav2vec2-large-xlsr-53-japanese', 'api_call': \\\"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')\\\\naudio_paths = ['/path/to/lecture1.mp3', '/path/to/lecture2.mp3']\\\\n# Replace these file paths with the paths to your own audio files.\\\\ntranscriptions = model.transcribe(audio_paths)\\\", 'api_arguments': ['audio_paths'], 'python_environment_requirements': ['huggingsound', 'torch', 'librosa', 'datasets', 'transformers'], 'example_code': 'from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel(jonatasgrosman/wav2vec2-large-xlsr-53-japanese)\\\\naudio_paths = [/path/to/file.mp3, /path/to/another_file.wav]\\\\ntranscriptions = model.transcribe(audio_paths)', 'performance': {'dataset': 'common_voice', 'accuracy': {'WER': 81.8, 'CER': 20.16}}, 'description': 'Fine-tuned XLSR-53 large model for speech recognition in Japanese. Trained on Common Voice 6.1, CSS10, and JSUT datasets. Make sure your speech input is sampled at 16kHz.'}} </s>\"\n"}
{"text": "<s>[INST]Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø´Ø±ÙˆØ¹ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ø§Ù„Ù…ØªØ¹Ù„Ù‚ Ø¨ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ·ÙŠØ¹ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ø§Ù„Ù…ØªÙ†ÙˆØ¹Ø© ÙˆØªØµÙ†ÙŠÙÙ‡Ø§ Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ Ø¨Ø¥Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø©. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø±Ù‹Ø§ Ø¹Ù„Ù‰ ØªÙ…ÙŠÙŠØ² Ø¨ÙŠÙ† Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"pipeline('regression', model='julien-c/pokemon-predict-hp')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a regression model for predicting Pokemon HP, loading the specific pretrained model 'julien-c/pokemon-predict-hp'.\\\\n3. With the Pokemon data, the created model is ready to predict the HP of a Pokemon.\\\\n4. The input data should contain features of a Pokemon, and the model will return the predicted HP value.\\\\n5. The model is trained on julien-c/kaggle-rounakbanik-pokemon dataset and predicts HP based on the input data provided.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\npokemon_hp_predictor = pipeline('regression', model='julien-c/pokemon-predict-hp')\\\\ninput_data = {'feature1': value1, 'feature2': value2, ...}\\\\npredicted_hp = pokemon_hp_predictor(input_data)[0]['value']\\\", 'api_call': \\\"pipeline('regression', model='julien-c/pokemon-predict-hp')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'julien-c/pokemon-predict-hp', 'api_call': \\\"pipeline('regression', model='julien-c/pokemon-predict-hp')\\\", 'api_arguments': 'input_data', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'julien-c/kaggle-rounakbanik-pokemon', 'accuracy': {'mean_absolute_error': 15.909, 'model_loss': 647.605}}, 'description': 'A tabular regression model trained on the julien-c/kaggle-rounakbanik-pokemon dataset to predict the HP of Pokemon.' </s>\"\n"}
{"text": "<s>[INST]Ø³Ù†Ù‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø³Ù…ÙˆØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©. ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ ØªØ­ÙˆÙŠÙ„Ù‡ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„Ù‡Ø¬Ø© ÙˆØ§Ù„Ù†Ø¨Ø±Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù„ØµÙˆØª Ø¨Ø®ØµÙˆØµ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\", '<<<api_provider>>>': 'PyTorch Transformers', '<<<explanation>>>': \\\"1. First, import the required libraries - Wav2Vec2Processor and Data2VecForCTC.\\\\n2. Load the pretrained model and processor using the 'facebook/data2vec-audio-base-960h' model.\\\\n3. Before transcribing, preprocess the audio dataset and read the sound files using the load_dataset method.\\\\n4. Tokenize the preprocessed audio input for feeding into the model using the Wav2Vec2Processor.\\\\n5. Obtain the logits for the input audio by passing it through the model.\\\\n6. Decode the logits into the most probable transcription using the argmax function and the batch_decode method from the processor.\\\", '<<<code>>>': \\\"from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/data2vec-audio-base-960h')\\\\nmodel = Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\\n\\\\nds = load_dataset('patrickvonplaten/librispeech_asr_dummy', 'clean', split='validation')\\\\ninput_values = processor(ds[0]['audio']['array'], return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\n\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_call': \\\"Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\", 'provider': 'PyTorch Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'data2vec-audio-base-960h', 'api_call': \\\"Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers', 'datasets', 'torch', 'jiwer']], 'example_code': 'from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\n# load model and processor\\\\nprocessor = Wav2Vec2Processor.from_pretrained(facebook/data2vec-audio-base-960h)\\\\nmodel = Data2VecForCTC.from_pretrained(facebook/data2vec-audio-base-960h)\\\\n\\\\n# load dummy dataset and read soundfiles\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\n\\\\n# tokenize\\\\ninput_values = processor(ds[0][audio][array],, return_tensors=pt, padding=longest).input_values # Batch size 1\\\\n\\\\n# retrieve logits\\\\nlogits = model(input_values).logits\\\\n\\\\n# take argmax and decode\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'performance': {'dataset': 'librispeech_asr', 'accuracy': {'clean': 2.77, 'other': 7.08}}, 'description': \\\"Facebook's Data2Vec-Audio-Base-960h model is an Automatic Speech Recognition model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It can be used for transcribing audio files and achieves competitive performance on major benchmarks of speech recognition. The model is based on the Data2Vec framework which uses the same learning method for either speech, NLP, or computer vision.\\\" </s>\"\n"}
{"text": "<s>[INST]ÙŠØ±Ø¬Ù‰ ØªÙˆÙ„ÙŠØ¯ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ†Ù‚ÙŠØ¨ Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆØ°Ù„Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„ØªÙÙ‡Ù… Ø§Ù„Ø¯Ù‚ÙŠÙ‚ ÙˆØ§Ù„ÙØ¹Ø§Ù„ Ù„Ù„ØµÙˆØ±. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„ÙƒØŸ? [/INST]{'domain': 'Computer Vision Depth Estimation', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': '', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-082237', 'api_call': '', 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3421, 'Mae': 0.27, 'Rmse': 0.4042, 'Abs Rel': 0.3279, 'Log Mae': 0.1132, 'Log Rmse': 0.1688, 'Delta1': 0.5839, 'Delta2': 0.8408, 'Delta3': 0.9309}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ØŒ Ù†Ø¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ÙŠØ­ÙˆÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ ÙÙŠØ¯ÙŠÙˆ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„ØªÙŠ ØªØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡Ø§.? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"pipeline('table-question-answering', model='Meena/table-question-answering-tapas')\\\", 'api_provider': 'PyTorch Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\ntable_qa_pipeline = pipeline('table-question-answering', model='Meena/table-question-answering-tapas')\\\\nquestion = 'Which country has the highest GDP?'\\\\ntable_data = [ ... ] # insert your table data here\\\\nresult = table_qa_pipeline(question=question, table=table_data)\\\", 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'PyTorch Transformers', 'functionality': 'Table Question Answering', 'api_name': 'table-question-answering-tapas', 'api_call': \\\"from transformers import pipeline\\\\ntable_qa_pipeline = pipeline('table-question-answering', model='Meena/table-question-answering-tapas')\\\\nquestion = 'Which country has the highest GDP?'\\\\ntable_data = [ ... ] # insert your table data here\\\\nresult = table_qa_pipeline(question=question, table=table_data)\\\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': [{'name': 'SQA (Sequential Question Answering by Microsoft)', 'accuracy': None}, {'name': 'WTQ (Wiki Table Questions by Stanford University)', 'accuracy': None}, {'name': 'WikiSQL (by Salesforce)', 'accuracy': None}]}, 'description': 'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ Natural Language Processing Fill-Mask Ø¹Ù„Ù‰ Ù†Øµ ÙŠØµÙ Ù…Ø´Ù‡Ø¯ Ù„Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø¥ØªÙ…Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„ØªØ­Ø¯ÙŠØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. The pipeline function is used to create a fill-mask model, which helps users complete sentences by suggesting suitable words to fill in the blanks.\\\\n3. We specify the model 'distilroberta-base' to be loaded. This is a smaller, faster, and lighter version of the RoBERTa-base model, trained to understand the structure of the English language and complete sentences intelligently.\\\\n4. The created unmasker can be used to analyze a sentence containing a masked token and provide suggestions for completing the sentence.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'distilroberta-base\\\\')\\\\nresult = unmasker(\\\"Hello I\\\\'m a <mask> model.\\\")', 'api_call': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilroberta-base', 'api_call': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"unmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': 'openwebtext', 'accuracy': 'Not provided'}, 'description': \\\"DistilRoBERTa is a distilled version of the RoBERTa-base model, designed to be smaller, faster, and lighter. It is a Transformer-based language model trained on the OpenWebTextCorpus, which is a reproduction of OpenAI's WebText dataset. The model has 6 layers, 768 dimensions, and 12 heads, totaling 82M parameters. It is primarily intended for fine-tuning on downstream tasks such as sequence classification, token classification, or question answering.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·Ø¨ÙŠÙ‚ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ù„ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ÙŠÙ…ÙƒÙ†Ù‡ ÙÙ‡Ù… ÙˆØªÙØ³ÙŠØ± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© ÙˆØ§Ù„Ù†ØµÙŠØ© Ø¨Ø´ÙƒÙ„ Ø°ÙƒÙŠ. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Ø¸Ø§Ù… Ù‚Ø§Ø¯Ø±Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆÙØ¹Ø§Ù„Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-medium')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers library provided by Hugging Face. This includes AutoModelForCausalLM for the conversational model and AutoTokenizer for tokenizing the text.\\\\n2. We then use the from_pretrained method of the AutoModelForCausalLM class to load the pre-trained model 'microsoft/DialoGPT-medium'. This model has been trained for conversational text generation tasks, which is exactly what we need for generating responses in a customer support chatbot.\\\\n3. We also load the tokenizer to process the text and convert it into a format that the model can understand.\\\\n4. Given an input text, we can now generate conversational responses using the DialoGPT model.\\\", '<<<code>>>': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-medium')\\\\nmodel = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-medium')\\\\ninput_ids = tokenizer.encode(customer_message, return_tensors='pt')\\\\nresponse_ids = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.pad_token_id)\\\\nresponse = tokenizer.decode(response_ids[0], skip_special_tokens=True)\\\", 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-medium')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/DialoGPT-medium', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-medium')\\\", 'api_arguments': ['bot_input_ids', 'max_length', 'pad_token_id'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-medium)\\\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-medium)', 'performance': {'dataset': 'Reddit', 'accuracy': 'Comparable to human response quality under a single-turn conversation Turing test'}, 'description': 'DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.' </s>\"\n"}
{"text": "<s>[INST]ØªØ­ØªØ§Ø¬ Ø´Ø±ÙƒØªÙ†Ø§ Ù„ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ø³ØªØ®Ø¯Ù…ÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© ÙÙŠ ØªÙ‚Ù†ÙŠØ© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Ø¸Ø§Ù… Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙˆÙØ¹Ø§Ù„Ø§Ù‹ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¥Ù„Ù‰ Ù†Øµ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the required libraries like SentenceTransformer and a method from sklearn to calculate cosine similarity.\\\\n2. Load the pretrained 'sentence-transformers/all-MiniLM-L6-v2' model from Hugging Face.\\\\n3. Encode the user query and the list of news article snippets using the loaded SentenceTransformer model.\\\\n4. Calculate the cosine similarity between the user query vector and the news article vectors.\\\\n5. Rank news articles based on their cosine similarity scores and display the top k results matching the user query.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nquery = \\\"Tech companies planning major acquisitions\\\"\\\\nnews_articles = [\\\"Apple acquires new AI startup\\\", \\\"Tesla announces the opening of new factory\\\", \\\"Microsoft buys game development company\\\"]\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-MiniLM-L6-v2\\\\')\\\\nquery_embedding = model.encode(query)\\\\nnews_embeddings = model.encode(news_articles)\\\\nsimilarities = cosine_similarity(query_embedding.reshape(1, -1), news_embeddings)\\\\ntop_k_indices = similarities.argsort()[::-1][:3]\\\\nprint(\\\"Top 3 matching articles:\\\")\\\\nfor index in top_k_indices[0]:\\\\n    print(f\\\">>{news_articles[index]}\\\")', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/all-MiniLM-L6-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': '1B sentence pairs dataset', 'accuracy': 'https://seb.sbert.net'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©.? [/INST]'<<<domain>>>': 'Multimodal Feature Extraction', '<<<api_call>>>': \\\"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers package. This includes AutoModel for the RuBERT model and the Tokenizer for processing Russian text.\\\\n2. We then use the from_pretrained method of the AutoModel class to load the pre-trained model 'DeepPavlov/rubert-base-cased'. This model has been trained on Russian Wikipedia and news data and is optimized for processing Russian text.\\\\n3. Next, we tokenize the Russian article text and feed it to the model to extract the features from it.\\\\n4. Based on the features, we can generate a short summary of the Russian article, which the literary magazine can use for its readers.\\\", '<<<code>>>': \\\"from transformers import AutoModel, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\\\\nmodel = AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\\\\ninput_text = 'Russian article text...'\\\\ninput_tokens = tokenizer(input_text, return_tensors='pt')\\\\noutput = model(**input_tokens)\\\", 'api_call': \\\"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \\\"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12â€‘layer, 768â€‘hidden, 12â€‘heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERTâ€‘base as an initialization for RuBERT[1].' </s>\"\n"}
{"text": "<s>[INST]Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ ØµÙˆØ± Ø¨ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„Ø¨ØµØ±ÙŠ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠØŒ Ø­ÙŠØ« ÙŠØ³ØªØ·ÙŠØ¹ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙˆØ³Ø±Ø¹Ø©ØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© ÙƒØ¨ÙŠØ±Ø© ÙˆÙ…ØªÙ†ÙˆØ¹Ø© Ù…Ù† Ø§Ù„ØµÙˆØ± Ù„Ø¶Ù…Ø§Ù† Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø² ÙÙŠ Ø§Ù„ØªØ­ÙˆÙŠÙ„.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_lineart')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary Python packages, such as torch, controlnet_aux, and diffusers.\\\\n2. Load a control image (lineart image) and resize it to the desired dimensions (e.g., 512x512).\\\\n3. Preprocess the control image using LineartDetector, a pre-trained model to detect lineart images.\\\\n4. Instantiate 'ControlNetModel', a neural network that can control diffusion models by adding extra conditions.\\\\n5. Use 'StableDiffusionControlNetPipeline' to create a pipeline for the controlled diffusion, specifying the torch data type (e.g., torch.float16), and adding the required GPU support if available.\\\\n6. Use 'UniPCMultistepScheduler' to set up a multi-step scheduler for the pipeline.\\\\n7. Generate the final image by providing the artistic prompt, the input lineart control image, and specifying the number of inference steps.\\\", '<<<code>>>': 'import torch\\\\nfrom controlnet_aux import LineartDetector\\\\nfrom diffusers import (\\\\n    ControlNetModel,\\\\n    StableDiffusionControlNetPipeline,\\\\n    UniPCMultistepScheduler,\\\\n)\\\\nfrom PIL import Image\\\\n\\\\n# Load control image (lineart image) and resize\\\\ncontrol_image = Image.open(\\\"path/to/lineart_image.png\\\")\\\\ncontrol_image = control_image.resize((512, 512))\\\\n\\\\n# Preprocess control image with LineartDetector\\\\nprocessor = LineartDetector.from_pretrained(\\\"lllyasviel/Annotators\\\")\\\\ncontrol_image = processor(control_image)\\\\n\\\\n# Instantiate ControlNetModel and pipeline\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\"lllyasviel/control_v11p_sd15_lineart\\\", torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n    \\\"runwayml/stable-diffusion-v1-5\\\", controlnet=controlnet, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\n\\\\n# Generate the final image\\\\nprompt = \\\"michael jackson concert\\\"\\\\ngenerator = torch.manual_seed(0)\\\\nfinal_image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\nfinal_image.save(\\\"output/manga_scene.png\\\")', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_lineart')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/control_v11p_sd15_lineart', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_lineart')\\\", 'api_arguments': {'checkpoint': 'ControlNet-1-1-preview/control_v11p_sd15_lineart', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['pip install diffusers transformers accelerate controlnet_aux==0.3.0'], 'example_code': \\\"import torch\\\\nimport os\\\\nfrom huggingface_hub import HfApi\\\\nfrom pathlib import Path\\\\nfrom diffusers.utils import load_image\\\\nfrom PIL import Image\\\\nimport numpy as np\\\\nfrom controlnet_aux import LineartDetector\\\\nfrom diffusers import (\\\\n ControlNetModel,\\\\n StableDiffusionControlNetPipeline,\\\\n UniPCMultistepScheduler,\\\\n)\\\\ncheckpoint = ControlNet-1-1-preview/control_v11p_sd15_lineart\\\\nimage = load_image(\\\\n https://huggingface.co/ControlNet-1-1-preview/control_v11p_sd15_lineart/resolve/main/images/input.png\\\\n)\\\\nimage = image.resize((512, 512))\\\\nprompt = michael jackson concert\\\\nprocessor = LineartDetector.from_pretrained(lllyasviel/Annotators)\\\\ncontrol_image = processor(image)\\\\ncontrol_image.save(./images/control.png)\\\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(0)\\\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\nimage.save('images/image_out.png')\\\", 'performance': {'dataset': 'ControlNet-1-1-preview', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù…ØªØ­Ø±Ùƒ Ù…Ø³ØªÙˆØ­Ù‰ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©. ÙŠØ¬Ø¨ ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø§Ù„Ø¨Ø±ÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØªØ³Ù„ÙŠØ· Ø§Ù„Ø¶ÙˆØ¡ Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ¹Ø© ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù„Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŒ Ø§Ø³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Fill-Mask. Ù‡Ù„ ØªØ³ØªØ·ÙŠØ¹ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library provided by Hugging Face. This includes the AutoModelForPreTraining class for loading the pre-trained BERT model for Portuguese, and the pipeline function for building the fill-mask pipeline.\\\\n2. Next, we use the from_pretrained() method to load the specific 'neuralmind/bert-base-portuguese-cased' BERT model for Portuguese language, which is designed for fill-mask tasks.\\\\n3. We then create a fill-mask pipeline by passing the model into the pipeline function along with the 'fill-mask' task.\\\\n4. Finally, invoke the pipeline function to complete the masked sentence provided in the input.\\\", '<<<code>>>': 'from transformers import AutoModelForPreTraining, pipeline\\\\nmodel = AutoModelForPreTraining.from_pretrained(\\\\'neuralmind/bert-base-portuguese-cased\\\\')\\\\npipe = pipeline(\\\\'fill-mask\\\\', model=model)\\\\ncompleted_sentence = pipe(\\\"Durante a nossa conversa, vocÃª mencionou estar interessado em [MASK], e queria informar que nÃ³s disponibilizamos essa soluÃ§Ã£o.\\\")', 'api_call': \\\"AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'neuralmind/bert-base-portuguese-cased', 'api_call': \\\"AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'neuralmind/bert-base-portuguese-cased'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\npipe = pipeline('fill-mask', model=model, tokenizer=tokenizer)\\\\npipe('Tinha uma [MASK] no meio do caminho.')\\\", 'performance': {'dataset': 'brWaC', 'accuracy': 'state-of-the-art'}, 'description': 'BERTimbau Base is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù†Ø¸Ù… ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ÙŠØ© Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ© Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we need to ensure that the necessary Python packages are installed. This includes torch, torchaudio, and librosa (to process audio), transformers (for the Hugging Face models) and numpy (for numerical computations).\\\\n2. Next, we import the Wav2Vec2Model class from the transformers library to load our pre-trained model 'facebook/wav2vec2-large-xlsr-53'.\\\\n3. We pass the target Russian audio file path and the sampling rate to the 'predict' function. This function is responsible for reading the audio file and processing it accordingly.\\\\n4. The model then predicts the emotions in the Russian speech.\\\", '<<<code>>>': \\\"from transformers import Wav2Vec2Model\\\\nmodel = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')\\\\naudio_file_path = '/path/to/russian_audio_speech.wav'\\\\nsampling_rate = 16000\\\\nresult = predict(audio_file_path, sampling_rate)\\\\nprint(result)\\\", 'api_call': \\\"Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'wav2vec2-xlsr-53-russian-emotion-recognition', 'api_call': \\\"Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')\\\", 'api_arguments': {'path': '/path/to/russian_audio_speech.wav', 'sampling_rate': 16000}, 'python_environment_requirements': [['torch', 'torchaudio', 'transformers', 'librosa', 'numpy']], 'example_code': \\\"result = predict('/path/to/russian_audio_speech.wav', 16000)\\\\nprint(result)\\\", 'performance': {'dataset': 'Russian Emotional Speech Dialogs', 'accuracy': '72%'}, 'description': 'A model trained to recognize emotions in Russian speech using wav2vec2. It can classify emotions such as anger, disgust, enthusiasm, fear, happiness, neutral, and sadness.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ ØªØµÙˆÙŠØ±ÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· ÙŠØªØ¶Ù…Ù† ÙˆØµÙÙ‹Ø§ Ù„ØµÙˆØ±Ø© Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ù…Ø¹ ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='albert-base-v2')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. We first import the necessary class from the transformers package. This includes the pipeline function for creating a fill-mask pipeline.\\\\n2. We then create a fill-mask pipeline using the pipeline function and the model \\\\'albert-base-v2\\\\'. This model has been trained for masked language modeling tasks, which is exactly what we need for guessing the missing word in a sentence.\\\\n3. We provide the sentence with the masked word \\\"[MASK]\\\" as an input, and the pipeline will predict the most likely word to fill in the blank.', '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'albert-base-v2\\\\')\\\\nunmasker(\\\"I want to have a [MASK] for breakfast.\\\")', 'api_call': \\\"pipeline('fill-mask', model='albert-base-v2')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'albert-base-v2', 'api_call': \\\"pipeline('fill-mask', model='albert-base-v2')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='albert-base-v2')\\\\nunmasker(Hello I'm a [MASK] model.)\\\", 'performance': {'dataset': {'SQuAD1.1': '90.2/83.2', 'SQuAD2.0': '82.1/79.3', 'MNLI': '84.6', 'SST-2': '92.9', 'RACE': '66.8'}, 'accuracy': '82.3'}, 'description': 'ALBERT Base v2 is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model, as all ALBERT models, is uncased: it does not make a difference between english and English.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ Ø®Ù„Ù‚ Ù†Ø¸Ø§Ù… ÙŠÙÙ…ÙƒÙ†Ù‡ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø¯Ù‚Ø© ÙˆÙƒÙØ§Ø¡Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨ÙƒÙØ§Ø¡Ø© Ù„ØªÙƒÙ…Ù„Ø© Ø§Ù„Ø«ØºØ±Ø§Øª Ø§Ù„Ù„ØºÙˆÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. The pipeline function is used to create a table-question-answering model, which is capable of answering questions based on the information provided in tabular data.\\\\n3. We specify the model 'google/tapas-large-finetuned-wtq' to be loaded. This model has been trained on a large corpus of text and can extract information from tables to answer questions.\\\\n4. The created table-question-answering pipeline can be used to query a table for specific information, given a natural language question, and will return the answer based on the table data.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\\nresult = qa_pipeline(table=table, query=query)\\\", 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/tapas-large-finetuned-wtq', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", 'api_arguments': {'model': 'google/tapas-large-finetuned-wtq', 'task': 'table-question-answering'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\nqa_pipeline = pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\\n\\\\nresult = qa_pipeline(table=table, query=query)\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.5097}, 'description': 'TAPAS large model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Natural Language Processing Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙØ±ÙŠØŒ Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\" ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙØ±ÙŠ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ ÙˆØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ù‚ØµØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModel.from_pretrained('typeform/squeezebert-mnli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'typeform/squeezebert-mnli'.\\\\n3. The loaded model specializes in zero-shot classification, which predicts the category of an unseen example without having matching training data.\\\\n4. Provide the input text (meeting email invitation), as well as a list of candidate categories ('operations', 'marketing', 'finance', 'technology').\\\\n5. Use the loaded model to classify the given text into the appropriate category.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'typeform/squeezebert-mnli\\\\')\\\\nemail_invitation = \\\"Dear Marketing Team, Please join us for a meeting to discuss our upcoming new marketing campaign. We will review the objectives, budget, and timeline to ensure a successful launch. Looking forward to your participation. Best, [Your Name]\\\"\\\\ncategories = [\\\\'operations\\\\', \\\\'marketing\\\\', \\\\'finance\\\\', \\\\'technology\\\\']\\\\nresult = classifier(email_invitation, categories)', 'api_call': \\\"AutoModel.from_pretrained('typeform/squeezebert-mnli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'typeform/squeezebert-mnli', 'api_call': \\\"AutoModel.from_pretrained('typeform/squeezebert-mnli')\\\", 'api_arguments': 'text, candidate_labels, multi_label', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nnlp = pipeline('zero-shot-classification', model='typeform/squeezebert-mnli')\\\\nresult = nlp('The quick brown fox jumps over the lazy dog', candidate_labels=['sports', 'language', 'animals'])\\\", 'performance': {'dataset': 'mulit_nli', 'accuracy': 'not provided'}, 'description': 'SqueezeBERT is a transformer model designed for efficient inference on edge devices. This specific model, typeform/squeezebert-mnli, is fine-tuned on the MultiNLI dataset for zero-shot classification tasks.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ¹Ù„Ù‚ Ù…Ø¬Ø§Ù„Ù†Ø§ Ø¨ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø£Ø®Ø±Ù‰ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙ†Ø§ ØªØµÙ…ÙŠÙ… Ù†Ø¸Ø§Ù… Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ø£Ø®Ø±Ù‰ Ø¨Ø£Ø³Ø§Ù„ÙŠØ¨ Ù…ØªØ¹Ø¯Ø¯Ø©ØŒ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙŠ ÙˆØ§Ù„ØªØ¹Ø¨ÙŠØ± Ø¹Ù†Ù‡ Ø¨ÙˆØ¶ÙˆØ­ ÙˆØ¯Ù‚Ø©.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-large-seq2seq')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary libraries from the transformers package: AutoTokenizer and AutoModelForSeq2SeqLM.', 'code': '', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Conversational', 'api_name': 'microsoft/GODEL-v1_1-large-seq2seq', 'api_call': '', 'api_arguments': {'instruction': 'Instruction: given a dialog context, you need to response empathically.', 'knowledge': '', 'dialog': ['Does money buy happiness?', 'It is a question. Money buys you a lot of things, but not enough to buy happiness.', 'What is the best way to buy happiness ?']}, 'python_environment_requirements': {'transformers': 'AutoTokenizer, AutoModelForSeq2SeqLM'}, 'example_code': \\\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != '':\\\\n knowledge = '[KNOWLEDGE] ' + knowledge\\\\n dialog = ' EOS '.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f'Instruction: given a dialog context, you need to response empathically.'\\\\nknowledge = ''\\\\ndialog = [\\\\n 'Does money buy happiness?',\\\\n 'It is a question. Money buys you a lot of things, but not enough to buy happiness.',\\\\n 'What is the best way to buy happiness ?'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\\\", 'performance': {'dataset': 'Reddit discussion thread, instruction and knowledge grounded dialogs', 'accuracy': 'Not provided'}, 'description': 'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† ØªØ³Ù…ÙŠØ§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"CrossEncoder('cross-encoder/nli-roberta-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We use the pipeline function from the transformers library (Hugging Face) to create a zero-shot classification model.\\\\n2. The model used in this case is the 'cross-encoder/nli-roberta-base', which can identify and categorize text into predefined categories even if it has not seen those categories during training.\\\\n3. The model is used to classify content from online news sources into different candidate_label categories such as 'autonomous vehicles' and 'urban planning'. \\\\n4. Based on the classification, recommendations for news sources with a focus on these two categories can be given.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'cross-encoder/nli-roberta-base\\\\')\\\\nnews_source_descriptions = [\\\\n    \\\"News source 1 description...\\\",\\\\n    \\\"News source 2 description...\\\",\\\\n    \\\"News source 3 description...\\\",\\\\n]\\\\ncandidate_labels = [\\\\'autonomous vehicles\\\\', \\\\'urban planning\\\\']\\\\nscores = []\\\\nfor description in news_source_descriptions:\\\\n    result = classifier(description, candidate_labels)\\\\n    scores.append(result)\\\\n\\\\nrecommended_news_sources = [source for source, score in zip(news_source_descriptions, scores) if max(score[\\\\'scores\\\\']) > 0.5]', 'api_call': \\\"CrossEncoder('cross-encoder/nli-roberta-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cross-encoder/nli-roberta-base', 'api_call': \\\"CrossEncoder('cross-encoder/nli-roberta-base')\\\", 'api_arguments': ['sentence1', 'sentence2'], 'python_environment_requirements': [['sentence_transformers', 'transformers']], 'example_code': \\\"from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model='cross-encoder/nli-roberta-base')\\\\nsent = Apple just announced the newest iPhone X\\\\ncandidate_labels = [technology, sports, politics]\\\\nres = classifier(sent, candidate_labels)\\\\nprint(res)\\\", 'performance': {'dataset': ['SNLI', 'MultiNLI'], 'accuracy': 'See SBERT.net - Pretrained Cross-Encoder'}, 'description': 'Cross-Encoder for Natural Language Inference trained on the SNLI and MultiNLI datasets. Outputs three scores corresponding to the labels: contradiction, entailment, neutral.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®ØµØ§Øª Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø³Ø±Ø¯ Ù…Ø­ØªÙˆÙŠØ§Øª Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ÙˆØ¬Ø²Ø© ÙˆÙØ¹Ø§Ù„Ø©ØŒ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù…Ø­ØªÙˆÙ‰.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the 'transformers' library provided by Hugging Face.\\\\n2. Use the pipeline function to create a 'summarization' model.\\\\n3. Specify the model 'google/pegasus-newsroom', which has been trained to perform abstractive summarization on long news articles.\\\\n4. The created model can be used to generate a summary of the input news article.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nsummarizer = pipeline('summarization', model='google/pegasus-newsroom')\\\\nsummary = summarizer(long_news_article, min_length=50, max_length=100)[0]['summary_text']\\\", 'api_call': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/pegasus-newsroom', 'api_call': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': 'newsroom', 'accuracy': '45.98/34.20/42.18'}, 'description': 'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ±Ø¬Ù…Ø© Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the AutoTokenizer and AutoModelForSeq2SeqLM classes from the transformers library.\\\\n2. Create a tokenizer using AutoTokenizer.from_pretrained() and specifying the model path 'Helsinki-NLP/opus-mt-en-de'. This tokenizer will be used to prepare the text for translation.\\\\n3. Create a translation model using AutoModelForSeq2SeqLM.from_pretrained() with the same model path.\\\\n4. With the tokenizer and model created, you can now prepare the English text, pass it through the German translation model and receive the translated text in German.\\\\n5. Users can now read English news translated to German, which helps them understand the content better.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-de\\\\')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-de\\\\')\\\\n\\\\nenglish_text = \\\"Here is some English news text\\\"\\\\ninputs = tokenizer(english_text, return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs[\\\\'input_ids\\\\'])\\\\ngerman_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-en-de', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'Helsinki-NLP/opus-mt-en-de'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'performance': {'dataset': 'newstest2018-ende.en.de', 'accuracy': {'BLEU': 45.2, 'chr-F': 0.69}}, 'description': 'The Helsinki-NLP/opus-mt-en-de model is a translation model developed by the Language Technology Research Group at the University of Helsinki. It translates English text to German using the Hugging Face Transformers library. The model is trained on the OPUS dataset and has a BLEU score of 45.2 on the newstest2018-ende.en.de dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ«ÙŠÙ‚Ø© Ù†ØµÙŠØ© ØªØªØ¶Ù…Ù† Ù†ØµÙˆØµ ÙˆØµÙˆØ± ÙˆÙ…Ù‚Ø§Ø·Ø¹ ØµÙˆØªÙŠØ© ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø®ØµÙˆØµ Ù…Ù„Ø®Øµ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©. Ø«Ù… Ø£Ø¹Ø·Ù‡Ø§ Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ù‚Ø±Ø§Ø¡ØªÙ‡Ø§ ÙˆÙ…Ø´Ø§Ù‡Ø¯Ø© Ù…Ø­ØªÙˆØ§Ù‡Ø§. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ø§Ø·Ù„Ø¨ Ù…Ù†Ù‡Ù… ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© ÙˆØ¥Ø¬Ø§Ø¨ØªÙ‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯Ø§Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'google/pegasus-newsroom\\\\')\\\\narticle = \\\"Article content goes here.\\\"\\\\nsummary = summarizer(article, max_length=150)[0][\\\\'summary_text\\\\']', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'google/pegasus-newsroom', 'api_call': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'google/pegasus-newsroom\\\\')\\\\narticle = \\\"Article content goes here.\\\"\\\\nsummary = summarizer(article, max_length=150)[0][\\\\'summary_text\\\\']', 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': 'newsroom', 'accuracy': '45.98/34.20/42.18'}, 'description': 'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· ØªÙ…Ø«Ù„ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ø¬Ù…Ù„Ø© \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler), torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary libraries, such as 'StableDiffusionPipeline' and 'EulerDiscreteScheduler'.\\\\n2. Create a stable-diffusion model instance by loading the 'stabilityai/stable-diffusion-2' model and its scheduler.\\\\n3. Move the model to the GPU (if available) for faster processing.\\\\n4. Provide text descriptions of the clothing items you want to generate images for.\\\\n5. Execute the model on the provided text descriptions to generate images.\\\\n6. Save the generated images as desired.\\\", '<<<code>>>': \\\"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nimport torch\\\\n\\\\nmodel_id = 'stabilityai/stable-diffusion-2'\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder='scheduler')\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\n\\\\nif torch.cuda.is_available():\\\\n    pipe = pipe.to('cuda')\\\\n\\\\nad_prompt = 'A photo of a stylish red dress with a matching handbag'\\\\ngenerated_image = pipe(ad_prompt).images[0]\\\\ngenerated_image.save('stylish_red_dress_handbag.png')\\\", 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler), torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler), torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors']], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ø³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ ØµÙˆØ±Ø© Ù„ØªÙ†Ø´Ø¦ ØµÙˆØ±Ø© ØªØ¸Ù‡Ø± Ø¨ÙŠØªØ§Ù‹ Ø¹ØªÙŠÙ‚Ø§Ù‹ ÙÙŠ Ù…Ù†Ø·Ù‚Ø© Ø±ÙŠÙÙŠØ© Ø®Ø¶Ø±Ø§Ø¡.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the required libraries and functions, including ControlNetModel, StableDiffusionControlNetPipeline, and UniPCMultistepScheduler from the diffusers package and pipeline function from transformers.\\\\n2. Load both the original image and the depth estimated image obtained using 'depth_estimator' pipeline.\\\\n3. Perform pre-processing on the depth image using Sobel operators, background thresholding, normalization, and other calculations.\\\\n4. Create a control net model by loading the pretrained model 'lllyasviel/sd-controlnet-normal' with appropriate specifications.\\\\n5. Set up the image pipeline, load the appropriate scheduler, and enable memory-efficient attention and model offload.\\\\n6. Use the created pipeline to generate the normal map estimation from the input image. Save the output image as 'toy_normal_out.png'.\\\", '<<<code>>>': \\\"from diffusers import ControlNetModel, StableDiffusionControlNetPipeline, UniPCMultistepScheduler\\\\nfrom transformers import pipeline\\\\nfrom PIL import Image\\\\nimport cv2\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\n# Load original_image and estimated_depth_image\\\\n# ... your_loading_and_pre-processing_code_here ...\\\\n\\\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\n\\\\nnormal_map_image = pipe(original_image, estimated_depth_image, num_inference_steps=20).images[0]\\\\nnormal_map_image.save('images/toy_normal_out.png')\\\", 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/sd-controlnet-normal', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\\\", 'api_arguments': ['image', 'num_inference_steps'], 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate']], 'example_code': \\\"from PIL import Image\\\\nfrom transformers import pipeline\\\\nimport numpy as np\\\\nimport cv2\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom diffusers.utils import load_image\\\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png).convert(RGB)\\\\ndepth_estimator = pipeline(depth-estimation, model =Intel/dpt-hybrid-midas )\\\\nimage = depth_estimator(image)['predicted_depth'][0]\\\\nimage = image.numpy()\\\\nimage_depth = image.copy()\\\\nimage_depth -= np.min(image_depth)\\\\nimage_depth /= np.max(image_depth)\\\\nbg_threhold = 0.4\\\\nx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\\\\nx[image_depth &lt; bg_threhold] = 0\\\\ny = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\\\\ny[image_depth &lt; bg_threhold] = 0\\\\nz = np.ones_like(x) * np.pi * 2.0\\\\nimage = np.stack([x, y, z], axis=2)\\\\nimage /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\\\\nimage = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\\\\nimage = Image.fromarray(image)\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\n fusing/stable-diffusion-v1-5-controlnet-normal, torch_dtype=torch.float16\\\\n)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\nimage = pipe(cute toy, image, num_inference_steps=20).images[0]\\\\nimage.save('images/toy_normal_out.png')\\\", 'performance': {'dataset': 'DIODE', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ÙƒØ§Ø¦Ù†Ø§ØªØŒ ÙˆÙ†Ø±ÙŠØ¯ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒÙ… Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙÙŠ Ø¥ÙƒØªØ´Ø§Ù ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø¨Ø¯Ù‚Ø© ÙˆÙØ¹Ø§Ù„ÙŠØ©.? [/INST]{'domain': 'Computer Vision Object Detection', 'api_call': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries, which include transformers, PIL, and requests.', 'code': \\\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://example.com/traffic_image.jpg'  # Replace with your traffic image URL\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\\n\\\\nvehicle_classes = ['car', 'bus', 'truck', 'motorcycle', 'bicycle']\\\\nvehicle_count = {}  # Initialize an empty dictionary to store vehicle counts\\\\n\\\\nfor logits_i, bbox_i in zip(logits, bboxes):\\\\n    class_id = logits_i.argmax()\\\\n    class_label = vehicle_classes[class_id]\\\\n    \\\\n    if class_label in vehicle_count:\\\\n        vehicle_count[class_label] += 1\\\\n    else:\\\\n        vehicle_count[class_label] = 1\\\\n\\\\nprint(vehicle_count)\\\", 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-tiny', 'api_call': \\\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://example.com/traffic_image.jpg'  # Replace with your traffic image URL\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\\n\\\\nvehicle_classes = ['car', 'bus', 'truck', 'motorcycle', 'bicycle']\\\\nvehicle_count = {}  # Initialize an empty dictionary to store vehicle counts\\\\n\\\\nfor logits_i, bbox_i in zip(logits, bboxes):\\\\n    class_id = logits_i.argmax()\\\\n    class_label = vehicle_classes[class_id]\\\\n    \\\\n    if class_label in vehicle_count:\\\\n        vehicle_count[class_label] += 1\\\\n    else:\\\\n        vehicle_count[class_label] = 1\\\\n\\\\nprint(vehicle_count)\\\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \\\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '28.7 AP'}, 'description': 'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†Øµ ÙŠØµÙ Ù…Ø´Ù‡Ø¯Ù‹Ø§ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù…Ù„. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the sentence-transformers library from Hugging Face Transformers.\\\\n2. Import the SentenceTransformer class and initialize it with the model name 'sentence-transformers/distilbert-base-nli-mean-tokens'.\\\\n3. Preprocess the articles to extract the most representative sentences.\\\\n4. Use the .encode() method of the model to convert those sentences into a 768-dimensional dense vector space (embeddings).\\\\n5. Apply clustering algorithms like K-means, DBSCAN, or hierarchical clustering on the obtained embeddings to group similar articles together.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.cluster import KMeans\\\\nimport numpy as np\\\\n\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/distilbert-base-nli-mean-tokens\\\\')\\\\n\\\\n# Preprocessed and extracted representative sentences from articles\\\\nrepresentative_sentences = [\\\\n    \\\"This is an example sentence from article 1.\\\",\\\\n    \\\"This sentence represents article 2.\\\",\\\\n    \\\"Another example from article 3.\\\",\\\\n    \\\"Sentence summarizing article 4.\\\",\\\\n]\\\\n\\\\n# Encode the sentences to generate embeddings\\\\nembeddings = model.encode(representative_sentences)\\\\n\\\\n# Perform clustering on the embeddings\\\\nkmeans = KMeans(n_clusters=2, random_state=0).fit(embeddings)\\\\n\\\\n# Assign articles to clusters\\\\narticle_clusters = kmeans.labels_', 'api_call': \\\"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \\\"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ¹Ù…Ù„ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ø¨ÙŠØ§Ù†ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨ØªÙƒØ±Ø© ÙˆÙØ¹Ø§Ù„Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We import the necessary libraries for the task. This includes pipeline and T5ForConditionalGeneration constructors from the transformers library provided by Hugging Face.\\\\n2. We load the pretrained 'pszemraj/long-t5-tglobal-base-16384-book-summary' model using the T5ForConditionalGeneration constructor. This model is designed specifically for summarizing long inputs like books, news articles, and academic texts.\\\\n3. We utilize the loaded model to generate a summary of the given long news article. It will extract the most important information from the article and generate a concise summarized version.\\\", '<<<code>>>': \\\"from transformers import T5ForConditionalGeneration, T5Tokenizer\\\\ntokenizer = T5Tokenizer.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\\nmodel = T5ForConditionalGeneration.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\\ninputs = tokenizer(long_news_article, return_tensors='pt', max_length=2048, truncation=True)\\\\nsummary_ids = model.generate(inputs['input_ids'])\\\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\\\", 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pszemraj/long-t5-tglobal-base-16384-book-summary', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\", 'api_arguments': ['long_text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don't want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\\\", 'performance': {'dataset': 'kmfoda/booksum', 'accuracy': {'ROUGE-1': 36.408, 'ROUGE-2': 6.065, 'ROUGE-L': 16.721, 'ROUGE-LSUM': 33.34}}, 'description': 'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.' </s>\"\n"}
{"text": "<s>[INST]ØªØ£ÙƒÙŠØ¯ Ø§Ù„ØªÙˆØ§Ø¬Ø¯ Ø¨Ø§Ù„Ù†Ø§Ø¯ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨ÙŠ Ø­ÙŠØ« ÙŠØªÙ… Ø§Ø¬ØªÙ…Ø§Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø³ÙŠØªÙ… ØªØ²ÙˆÙŠØ¯ÙƒÙ… Ø¨Ø£Ø¯Ø§Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the necessary libraries from the transformers package. These include AutoModelForCausalLM for the dialogue response generation model and AutoTokenizer for tokenizing the text.', 'code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-large')\\\\nmodel = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Transformers', 'functionality': 'Conversational', 'api_name': 'microsoft/DialoGPT-large', 'api_call': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-large')\\\\nmodel = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_arguments': ['bot_input_ids', 'max_length', 'pad_token_id'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-large)\\\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-large)', 'performance': {'dataset': 'Reddit discussion thread', 'accuracy': 'Comparable to human response quality under a single-turn conversation Turing test'}, 'description': 'DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multi-turn conversations. The model is trained on 147M multi-turn dialogues from Reddit discussion threads.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØµÙˆØ± Ø³Ù…Ø¹ÙŠ ØªØµÙ†ÙŠÙÙŠ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ù‡Ø°Ø§ØŸ? [/INST]{'domain': 'Computer Vision Image Classification', 'api_call': \\\"pipeline('image-classification', model='saltacc/anime-ai-detect')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', model='saltacc/anime-ai-detect')\\\\nresult = image_classifier(open('anime_artwork.jpg', 'rb'))\\\", 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'saltacc/anime-ai-detect', 'api_call': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', model='saltacc/anime-ai-detect')\\\\nresult = image_classifier(open('anime_artwork.jpg', 'rb'))\\\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'aibooru and imageboard sites', 'accuracy': '96%'}, 'description': 'A BEiT classifier to see if anime art was made by an AI or a human.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ­ÙˆÙŠÙ„ ØµÙˆØ±Ø© Ø¥Ù„Ù‰ ØµÙˆØ±Ø© Ø£Ø®Ø±Ù‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_inpaint')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary libraries, classes, and functions, including ControlNetModel from the diffusers package and load_image from diffusers.utils.\\\\n2. Load the damaged image and the mask indicating the damaged areas.\\\\n3. Define a function to create an inpaint condition, which allows us to guide the image inpainting process.\\\\n4. Create a control image using the damaged image and the mask.\\\\n5. Instantiate the ControlNetModel with the pre-trained model 'lllyasviel/control_v11p_sd15_inpaint'.\\\\n6. Create a StableDiffusionControlNetPipeline instance.\\\\n7. Set the scheduler for the pipeline and enable offloading the model to CPU.\\\\n8. Provide a random seed for the image generation process.\\\\n9. Run the inpainting process on the damaged image to produce a repaired image, and save the result to a file named 'output.png'.\\\", '<<<code>>>': 'import torch\\\\nfrom diffusers import ControlNetModel, StableDiffusionControlNetPipeline, UniPCMultistepScheduler\\\\nfrom diffusers.utils import load_image\\\\nfrom PIL import Image\\\\nimport numpy as np\\\\n\\\\n# Load the damaged painting image and the corresponding mask\\\\noriginal_image = load_image(\\\\'path/to/damaged_painting.png\\\\')\\\\nmask_image = load_image(\\\\'path/to/damaged_painting_mask.png\\\\')\\\\n\\\\n# Define make_inpaint_condition function\\\\n\\\\ndef make_inpaint_condition(image, image_mask):\\\\n    image = np.array(image.convert(\\\\'RGB\\\\')).astype(np.float32) / 255.0\\\\n    image_mask = np.array(image_mask.convert(\\\\'L\\\\'))\\\\n    assert image.shape[:2] == image_mask.shape[:2], \\\"image and image_mask must have the same image size\\\"\\\\n    image[image_mask < 128] = -1.0  # set as masked pixel\\\\n    image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\\\\n    image = torch.from_numpy(image)\\\\n    return image\\\\n\\\\ncontrol_image = make_inpaint_condition(original_image, mask_image)\\\\nprompt = \\\\'best quality\\\\'\\\\nnegative_prompt = \\\\'lowres, bad anatomy, bad hands, cropped, worst quality\\\\'\\\\n\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/control_v11p_sd15_inpaint\\\\')\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', controlnet=controlnet)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(2)\\\\n\\\\nrepaired_image = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\nrepaired_image.save(\\\\'output.png\\\\')', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_inpaint')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/control_v11p_sd15_inpaint', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_inpaint')\\\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_inpaint', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['pip install diffusers transformers accelerate'], 'example_code': \\\"import torch\\\\nimport os\\\\nfrom diffusers.utils import load_image\\\\nfrom PIL import Image\\\\nimport numpy as np\\\\nfrom diffusers import (\\\\n ControlNetModel,\\\\n StableDiffusionControlNetPipeline,\\\\n UniPCMultistepScheduler,\\\\n)\\\\ncheckpoint = lllyasviel/control_v11p_sd15_inpaint\\\\noriginal_image = load_image(\\\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/original.png\\\\n)\\\\nmask_image = load_image(\\\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/mask.png\\\\n)\\\\ndef make_inpaint_condition(image, image_mask):\\\\n image = np.array(image.convert(RGB)).astype(np.float32) / 255.0\\\\n image_mask = np.array(image_mask.convert(L))\\\\n assert image.shape[0:1] == image_mask.shape[0:1], image and image_mask must have the same image size\\\\n image[image_mask < 128] = -1.0 # set as masked pixel \\\\n image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\\\\n image = torch.from_numpy(image)\\\\n return image\\\\ncontrol_image = make_inpaint_condition(original_image, mask_image)\\\\nprompt = best quality\\\\nnegative_prompt=lowres, bad anatomy, bad hands, cropped, worst quality\\\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(2)\\\\nimage = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, \\\\n generator=generator, image=control_image).images[0]\\\\nimage.save('images/output.png')\\\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.' </s>\"\n"}
{"text": "<s>[INST]Ø³ØªÙ‚ÙˆÙ… Ø§Ù„Ø´Ø±ÙƒØ© Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ ØµÙˆØª ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØªØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ ØªØ­Ø³ÙŠÙ† ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª Ù„ØªØ¹Ø²ÙŠØ² ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØª.? [/INST]{'domain': 'Audio Audio-to-Audio', 'api_call': \\\"separator.from_hparams(source='speechbrain/sepformer-whamr', savedir='pretrained_models/sepformer-whamr')\\\", 'api_provider': 'SpeechBrain', 'explanation': '1. We first install the required SpeechBrain library using pip.', 'code': \\\"from speechbrain.pretrained import SepformerSeparation as separator\\\\nimport torchaudio\\\\nmodel = separator.from_hparams(source='speechbrain/sepformer-whamr', savedir='pretrained_models/sepformer-whamr')\\\\nest_sources = model.separate_file(path='input_audio.wav')\\\\ntorchaudio.save('source1hat.wav', est_sources[:, :, 0].detach().cpu(), 8000)\\\\ntorchaudio.save('source2hat.wav', est_sources[:, :, 1].detach().cpu(), 8000)\\\", 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'SpeechBrain', 'functionality': 'Audio Source Separation', 'api_name': 'speechbrain/sepformer-whamr', 'api_call': \\\"from speechbrain.pretrained import SepformerSeparation as separator\\\\nimport torchaudio\\\\nmodel = separator.from_hparams(source='speechbrain/sepformer-whamr', savedir='pretrained_models/sepformer-whamr')\\\\nest_sources = model.separate_file(path='input_audio.wav')\\\\ntorchaudio.save('source1hat.wav', est_sources[:, :, 0].detach().cpu(), 8000)\\\\ntorchaudio.save('source2hat.wav', est_sources[:, :, 1].detach().cpu(), 8000)\\\", 'api_arguments': {'source': 'speechbrain/sepformer-whamr', 'savedir': 'pretrained_models/sepformer-whamr'}, 'python_environment_requirements': 'pip install speechbrain', 'example_code': \\\"from speechbrain.pretrained import SepformerSeparation as separator\\\\nimport torchaudio\\\\nmodel = separator.from_hparams(source='speechbrain/sepformer-whamr', savedir='pretrained_models/sepformer-whamr')\\\\nest_sources = model.separate_file(path='speechbrain/sepformer-wsj02mix/test_mixture.wav')\\\\ntorchaudio.save('source1hat.wav', est_sources[:, :, 0].detach().cpu(), 8000)\\\\ntorchaudio.save('source2hat.wav', est_sources[:, :, 1].detach().cpu(), 8000)\\\", 'performance': {'dataset': 'WHAMR!', 'accuracy': '13.7 dB SI-SNRi'}, 'description': 'This repository provides all the necessary tools to perform audio source separation with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAMR! dataset, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ÙŠÙØ³ØªØ®Ø¯Ù… ÙÙŠ Ù†Ø³Ø®Ø© ØªØ­Ù„ÙŠÙ„ÙŠØ© ØºÙŠØ± Ù…Ø´Ø±ÙˆØ·Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø³ÙŠÙƒÙˆÙ† Ø­ÙŠÙˆÙŠÙ‹Ø§ ÙÙŠ Ø¥Ù†ØªØ§Ø¬ ØµÙˆØ± Ø¨Ø´ÙƒÙ„ Ø°ÙƒÙŠ ÙˆØ¯Ù‚ÙŠÙ‚ Ø¯ÙˆÙ† Ù‚ÙŠÙˆØ¯ Ù…Ø³Ø¨Ù‚Ø©.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from diffusers import DDPMPipeline\\\\nbutterfly_generator = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\\nbutterfly_image = butterfly_generator().images[0]\\\\nbutterfly_image.show()\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'sd-class-butterflies-32', 'api_call': \\\"from diffusers import DDPMPipeline\\\\nbutterfly_generator = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\\nbutterfly_image = butterfly_generator().images[0]\\\\nbutterfly_image.show()\\\", 'api_arguments': {'model_id': 'clp/sd-class-butterflies-32'}, 'python_environment_requirements': ['diffusers'], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': None, 'accuracy': None}, 'description': 'This model is a diffusion model for unconditional image generation of cute butterflies.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙƒØªÙˆØ¨Ø© Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ø¬Ù…Ù‡ÙˆØ± Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and functions such as torch, torchaudio, transformers, librosa, and numpy.\\\\n2. Load the model 'facebook/wav2vec2-large-xlsr-53' using the Wav2Vec2Model.from_pretrained method. This model is trained to recognize emotions in Russian speech.\\\\n3. Preprocess the input audio in the correct format as required by the model.\\\\n4. Use the model to classify emotions in users' speech, such as anger, disgust, enthusiasm, fear, happiness, neutral, or sadness.\\\\n5. The emotional recognition can then be utilized in the smart home system to adapt its responses accordingly.\\\", '<<<code>>>': 'import torch\\\\nimport torchaudio\\\\nfrom transformers import Wav2Vec2Model, Wav2Vec2Processor\\\\n\\\\n# Load the trained model and its processor\\\\nmodel = Wav2Vec2Model.from_pretrained(\\\\'facebook/wav2vec2-large-xlsr-53\\\\')\\\\nprocessor = Wav2Vec2Processor.from_pretrained(\\\\'facebook/wav2vec2-large-xlsr-53\\\\')\\\\n\\\\n# Load the audio file, resample, and preprocess the audio\\\\nsample_rate = 16000\\\\naudio_file = \\\\'/path/to/russian_audio_speech.wav\\\\'\\\\nspeech, _ = torchaudio.load(audio_file)\\\\nspeech = torchaudio.transforms.Resample()(speech, sample_rate)\\\\n\\\\n# Process and classify the emotion in speech\\\\ninput_data = processor(speech.numpy(), sampling_rate=sample_rate, return_tensors=\\\"pt\\\")\\\\noutput = model(input_data[\\\\'input_values\\\\'])\\\\n\\\\n# Find the most probable emotion\\\\nemotion_idx = torch.argmax(output[\\\"logits\\\"]).item()\\\\nemotion_categories = [\\\\'anger\\\\', \\\\'disgust\\\\', \\\\'enthusiasm\\\\', \\\\'fear\\\\', \\\\'happiness\\\\', \\\\'neutral\\\\', \\\\'sadness\\\\']\\\\nprediction = emotion_categories[emotion_idx]', 'api_call': \\\"Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'wav2vec2-xlsr-53-russian-emotion-recognition', 'api_call': \\\"Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')\\\", 'api_arguments': {'path': '/path/to/russian_audio_speech.wav', 'sampling_rate': 16000}, 'python_environment_requirements': [['torch', 'torchaudio', 'transformers', 'librosa', 'numpy']], 'example_code': \\\"result = predict('/path/to/russian_audio_speech.wav', 16000)\\\\nprint(result)\\\", 'performance': {'dataset': 'Russian Emotional Speech Dialogs', 'accuracy': '72%'}, 'description': 'A model trained to recognize emotions in Russian speech using wav2vec2. It can classify emotions such as anger, disgust, enthusiasm, fear, happiness, neutral, and sadness.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø«Ù„Ø§Ø«ÙŠØ© Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ù„ØªØ®Ù…ÙŠÙ† Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a depth estimation model.\\\\n3. Specify the model 'sayakpaul/glpn-nyu-finetuned-diode-221121-113853' to be loaded. This model is trained specifically for depth estimation tasks and can provide distance information for objects within an input image.\\\\n4. The created depth estimation model can be used to analyze the images captured by the robot's cameras in order to provide distance estimates for objects in the environment.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ndepth_estimator = pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\\\\ndepth_map = depth_estimator(image_path)\\\", 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221121-113853', 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers==4.24.0', 'torch==1.12.1', 'tokenizers==0.13.2']], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3384, 'Mae': 0.2739, 'Rmse': 0.3959, 'Abs Rel': 0.323, 'Log Mae': 0.1148, 'Log Rmse': 0.1651, 'Delta1': 0.5576, 'Delta2': 0.8345, 'Delta3': 0.9398}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…ÙÙ‡ÙˆÙ… ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='distilgpt2')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'pipeline' from transformers and 'set_seed' to control randomness.\\\\n2. Use the 'pipeline' function from transformers to create a text generation model using the 'distilgpt2' model.\\\\n3. The loaded model will be used for text generation, specifically creating a short topic summary for a news article.\\\\n4. Provide the necessary input, such as the beginning of the article or a prompt, and set the 'max_length' parameter to control the length of the generated summary.\\\\n5. The model will generate a summary based on the input provided to it.\\\", '<<<code>>>': 'from transformers import pipeline, set_seed\\\\nset_seed(42)\\\\ntext_generator = pipeline(\\\\'text-generation\\\\', model=\\\\'distilgpt2\\\\')\\\\nnews_article_prompt = \\\"Article snippet or prompt...\\\"\\\\ngenerated_summary = text_generator(news_article_prompt, max_length=50, num_return_sequences=1)\\\\nsummary = generated_summary[0][\\\\'generated_text\\\\']', 'api_call': \\\"pipeline('text-generation', model='distilgpt2')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilgpt2', 'api_call': \\\"pipeline('text-generation', model='distilgpt2')\\\", 'api_arguments': ['model'], 'python_environment_requirements': [['from transformers import pipeline, set_seed']], 'example_code': 'set_seed(42)\\\\ngenerator(Hello, Iâ€™m a language model, max_length=20, num_return_sequences=5)', 'performance': {'dataset': 'WikiText-103', 'accuracy': '21.100'}, 'description': 'DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. With 82 million parameters, it was developed using knowledge distillation and designed to be a faster, lighter version of GPT-2. It can be used for text generation, writing assistance, creative writing, entertainment, and more.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªÙˆÙ„ÙŠÙ Ù†Ù…ÙˆØ°Ø¬ ØªÙ†Ø¨Ø¤ÙŠ Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø¯Ø®Ù„ÙŠØ© Ø¥Ù„Ù‰ Ø¬Ù…Ù„Ø© Ù†Ø§ØªØ¬Ø© Ø¨ÙˆØ§Ø³Ø·Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© ÙƒØ¨ÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…ØªØ²Ø§Ù†Ø©.? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We start by importing SentenceTransformer from the sentence_transformers library.', 'code': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\n\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/paraphrase-multilingual-mpnet-base-v2\\\\')\\\\nreview1 = \\\"Example movie review 1.\\\"\\\\nreview2 = \\\"Example movie review 2.\\\"\\\\n\\\\n# Get embeddings\\\\nembedding1 = model.encode([review1])\\\\nembedding2 = model.encode([review2])\\\\n\\\\n# Calculate similarity\\\\nsimilarity = cosine_similarity(embedding1, embedding2)[0][0]', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Sentence Embeddings', 'api_name': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2', 'api_call': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\n\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/paraphrase-multilingual-mpnet-base-v2\\\\')\\\\nreview1 = \\\"Example movie review 1.\\\"\\\\nreview2 = \\\"Example movie review 2.\\\"\\\\n\\\\n# Get embeddings\\\\nembedding1 = model.encode([review1])\\\\nembedding2 = model.encode([review2])\\\\n\\\\n# Calculate similarity\\\\nsimilarity = cosine_similarity(embedding1, embedding2)[0][0]', 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¶ÙˆØ¦ÙŠØ© Ù„Ù…Ø´Ø§Ù‡Ø¯ ØªØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø§Ø³ØªÙ†Ø§Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙƒØªÙˆØ¨Ø© Ø¨ØªÙ‚Ù†ÙŠØ© Ù…Ø±Ù†Ø© ØªÙ‚ÙˆÙ… Ø¹Ù„Ù‰ Ø¯Ù…Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØµÙˆØ±. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙ†ÙÙŠØ° Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ØŸ? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion', torch_dtype=torch.float32)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. First, we import the necessary libraries, which include torch, autocast, and StableDiffusionPipeline.\\\\n2. We then use the from_pretrained method to load the pre-trained model 'hakurei/waifu-diffusion'. This model has been trained on high-quality anime images and is capable of generating images from text descriptions.\\\\n3. We provide a text prompt describing the desired painting, which includes 'Sunset', 'Ocean', 'Seagulls', and 'Palm Trees'.\\\\n4. We then use the generated image to inspire a painting.\\\", '<<<code>>>': \\\"import torch\\\\nfrom torch import autocast\\\\nfrom diffusers import StableDiffusionPipeline\\\\npipe = StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion', torch_dtype=torch.float32).to('cuda')\\\\nprompt = 'Sunset, Ocean, Seagulls, Palm Trees'\\\\nwith autocast(cuda):\\\\n    image = pipe(prompt, guidance_scale=6)[sample][0]\\\\nimage.save('inspired_painting.png')\\\", 'api_call': \\\"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion', torch_dtype=torch.float32)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion', torch_dtype=torch.float32)\\\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': [{'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}], 'example_code': \\\"import torch\\\\nfrom torch import autocast\\\\nfrom diffusers import StableDiffusionPipeline\\\\npipe = StableDiffusionPipeline.from_pretrained(\\\\n 'hakurei/waifu-diffusion',\\\\n torch_dtype=torch.float32\\\\n).to('cuda')\\\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\\\nwith autocast(cuda):\\\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\\\nimage.save(test.png)\\\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¹Ù„Ù‰ Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ÙØ¹Ø§Ù„ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŸ? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. First, we import the required modules from the transformers library provided by Hugging Face: AutoModelForQuestionAnswering, AutoTokenizer, and pipeline.', 'code': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel_name = \\\\'deepset/roberta-base-squad2\\\\'\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=model_name, tokenizer=model_name)\\\\nQA_input = {\\\\n    \\\\'question\\\\': \\\"What is the capital of France?\\\",\\\\n    \\\\'context\\\\': \\\"France is a country in Western Europe, and its capital is Paris. It\\\\'s known for its cuisine, art, architecture, and cultural heritage.\\\"\\\\n}\\\\nres = nlp(QA_input)\\\\nanswer = res[\\\\'answer\\\\']', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'deepset/roberta-base-squad2', 'api_call': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel_name = \\\\'deepset/roberta-base-squad2\\\\'\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=model_name, tokenizer=model_name)\\\\nQA_input = {\\\\n    \\\\'question\\\\': \\\"What is the capital of France?\\\",\\\\n    \\\\'context\\\\': \\\"France is a country in Western Europe, and its capital is Paris. It\\\\'s known for its cuisine, art, architecture, and cultural heritage.\\\"\\\\n}\\\\nres = nlp(QA_input)\\\\nanswer = res[\\\\'answer\\\\']', 'api_arguments': {'model_name': 'deepset/roberta-base-squad2', 'tokenizer': 'deepset/roberta-base-squad2'}, 'python_environment_requirements': ['transformers'], 'example_code': {'code': \\\"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel_name = deepset/roberta-base-squad2\\\\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\\\\nQA_input = {\\\\n 'question': 'Why is model conversion important?',\\\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\\\n}\\\\nres = nlp(QA_input)\\\"}, 'performance': {'dataset': 'squad_v2', 'accuracy': {'exact': 79.87029394424324, 'f1': 82.91251169582613}}, 'description': \\\"This is the roberta-base model, fine-tuned using the SQuAD2.0 dataset for the task of Question Answering. It's been trained on question-answer pairs, including unanswerable questions.\\\"}} </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required pipeline class from the transformers package.\\\\n2. Create an instance of the text-generation pipeline using the 'sshleifer/tiny-gpt2' model. This is a small GPT-2 model specifically designed for faster inference and low-resource environments.\\\\n3. Use the generated pipeline object to create a birthday congratulation message based on the given prompt. You can customize the prompt or keep it simple like 'Happy Birthday!'.\\\\n4. The pipeline will generate a text sample with birthday congratulation text.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\\\\nresult = nlp('Happy Birthday!')\\\\nbirthday_message = result[0]['generated_text']\\\\nprint(birthday_message)\\\", 'api_call': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sshleifer/tiny-gpt2', 'api_call': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", 'api_arguments': {'model': 'sshleifer/tiny-gpt2'}, 'python_environment_requirements': [{'huggingface_transformers': '>=4.0.0'}], 'example_code': \\\"from transformers import pipeline\\\\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\\\\nresult = nlp('Once upon a time')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A tiny GPT-2 model for text generation, suitable for low-resource environments and faster inference. This model is part of the Hugging Face Transformers library and can be used for generating text given a prompt.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries: PIL for image manipulation, transformers for using the ViTForImageClassification model, and requests for getting the image from a URL.\\\\n2. Load the pre-trained model 'nateraw/vit-age-classifier' using the from_pretrained method of the ViTForImageClassification class.\\\\n3. Load the ViTFeatureExtractor from the same 'nateraw/vit-age-classifier' model to preprocess and transform the input image.\\\\n4. Open the image to be classified and pass it through the feature extractor and the model.\\\\n5. The model will return an array of probabilities for each age group, which you can then use to predict the age of the visitor(s) in the image.\\\", '<<<code>>>': 'from PIL import Image\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\nimport requests\\\\nimage_url = \\\"https://path_to_image.jpg\\\" # Replace with the URL of the image containing visitor\\\\'s face\\\\nresponse = requests.get(image_url)\\\\nimage = Image.open(BytesIO(response.content))\\\\nmodel = ViTForImageClassification.from_pretrained(\\\\'nateraw/vit-age-classifier\\\\')\\\\ntransforms = ViTFeatureExtractor.from_pretrained(\\\\'nateraw/vit-age-classifier\\\\')\\\\ninputs = transforms(image, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**inputs)\\\\nproba = output.logits.softmax(1)\\\\npreds = proba.argmax(1)', 'api_call': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nateraw/vit-age-classifier', 'api_call': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/vit-age-classifier'}, 'python_environment_requirements': [['requests', 'PIL', 'transformers']], 'example_code': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom io import BytesIO\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\n\\\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\\\nim = Image.open(BytesIO(r.content))\\\\n\\\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\\\n\\\\ninputs = transforms(im, return_tensors='pt')\\\\noutput = model(**inputs)\\\\n\\\\nproba = output.logits.softmax(1)\\\\npreds = proba.argmax(1)\\\", 'performance': {'dataset': 'fairface', 'accuracy': None}, 'description': \\\"A vision transformer finetuned to classify the age of a given person's face.\\\" </s>\"\n"}
{"text": "<s>[INST]ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙŠÙ‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø¯ÙˆÙ† Ø´Ø±Ø· Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨ÙˆØ§Ø³Ø·Ø© Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨. ÙŠØªØ·Ù„Ø¨ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªÙ‚ÙˆÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± ØºÙŠØ± Ù…Ø´Ø±ÙˆØ·Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø©. ÙŠØ¬Ø¨ Ø£Ù† ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ù† ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± ÙˆØ§Ù‚Ø¹ÙŠØ© ÙˆØ¨Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install diffusers library required to load and use DDPMPipeline.\\\\n2. Import DDPMPipeline from the diffusers library provided by Hugging Face.\\\\n3. Use DDPMPipeline.from_pretrained method to create an instance of DDPMPipeline by loading the pretrained model 'ntrant7/sd-class-butterflies-32'.\\\\n4. Use the pipeline to generate an unconditional image of a butterfly. The resulting image can be accessed by referencing the images attribute of the generated object.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\\ngenerated_butterfly = pipeline().images[0]\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ntrant7/sd-class-butterflies-32', 'api_call': \\\"DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\", 'api_arguments': [], 'python_environment_requirements': [['diffusers']], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This model is a diffusion model for unconditional image generation of cute butterflies.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù†Ø¸Ø§Ù… Ù„ØªØ¹Ù„Ù… ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù…Ù† Ù†ÙˆØ¹ ÙˆØ§Ø­Ø¯ Ø¥Ù„Ù‰ Ø¢Ø®Ø±.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import necessary libraries and classes including Image from PIL, StableDiffusionControlNetPipeline, ControlNetModel, and UniPCMultistepScheduler from diffusers, torch from PyTorch, and MLSDdetector from controlnet_aux.\\\\n2. Load the M-LSD detector with the pretrained 'lllyasviel/ControlNet' checkpoint.\\\\n3. Load the original image using the provided URL or a local file, then apply the M-LSD detector to detect straight lines.\\\\n4. Load the ControlNetModel using the 'lllyasviel/sd-controlnet-mlsd' checkpoint, specifying torch.float16 as the data type.\\\\n5. Create a StableDiffusionControlNetPipeline using the 'runwayml/stable-diffusion-v1-5' checkpoint, and set the scheduler as UniPCMultistepScheduler.\\\\n6. Enable memory-efficient attention and CPU offloading for the pipeline.\\\\n7. Use the pipeline to straighten lines in the input image and save the output image.\\\", '<<<code>>>': \\\"from PIL import Image\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom controlnet_aux import MLSDdetector\\\\nfrom diffusers.utils import load_image\\\\n\\\\nmlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png')\\\\nimage = mlsd(image)\\\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\nimage = pipe('room', image, num_inference_steps=20).images[0]\\\\nimage.save('images/room_mlsd_out.png')\\\", 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/sd-controlnet-mlsd', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\\\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': [{'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}], 'example_code': {'import': ['from PIL import Image', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'import torch', 'from controlnet_aux import MLSDdetector', 'from diffusers.utils import load_image'], 'setup': [\\\"mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)', 'image = mlsd(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)'], 'execution': ['pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', 'image = pipe(room, image, num_inference_steps=20).images[0]', \\\"image.save('images/room_mlsd_out.png')\\\"]}, 'performance': {'dataset': '600k edge-image, caption pairs generated from Places2', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.' </s>\"\n"}
{"text": "<s>[INST]Ø¹ÙŠÙÙ‘Ù† Ø³Ø¤Ø§Ù„Ù‹Ø§ Ø­ÙˆÙ„ Ø¬Ø¯ÙˆÙ„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import necessary classes from transformers and torch packages. We will need AutoModelForCausalLM and AutoTokenizer for the model and tokenization.', 'code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-large')\\\\nmodel = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\\n\\\\ndef respond(user_input):\\\\n    input_ids = tokenizer.encode(user_input, return_tensors='pt')\\\\n    output = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.eos_token_id)\\\\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\\\\n    return response\\\", 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Transformers', 'functionality': 'Conversational', 'api_name': 'microsoft/DialoGPT-large', 'api_call': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-large')\\\\nmodel = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\\n\\\\ndef respond(user_input):\\\\n    input_ids = tokenizer.encode(user_input, return_tensors='pt')\\\\n    output = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.eos_token_id)\\\\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\\\\n    return response\\\", 'api_arguments': ['bot_input_ids', 'max_length', 'pad_token_id'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-large)\\\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-large)', 'performance': {'dataset': 'Reddit discussion thread', 'accuracy': 'Comparable to human response quality under a single-turn conversation Turing test'}, 'description': 'DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multi-turn conversations. The model is trained on 147M multi-turn dialogues from Reddit discussion threads.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„Ø¢Ù„ÙŠ ÙˆØ§Ø³ØªØ¯Ø¹Ø§Ø¡ ÙˆØ¸ÙŠÙØ© Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ Ù†Øµ. Ø­Ø§ÙˆÙ„ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ø·Ù‚ ÙˆØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ù„Ø²ÙŠØ§Ø¯Ø© Ø¯Ù‚Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù….? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary classes and functions from the transformers library provided by Hugging Face, including the WhisperProcessor and WhisperForConditionalGeneration classes.\\\\n2. We load the pre-trained model 'openai/whisper-large-v2' using the from_pretrained method of the WhisperForConditionalGeneration class.\\\\n3. We use the WhisperProcessor to extract features from the audio sample, which is then provided as input to the pre-trained model for inference.\\\\n4. The model generates transcription predictions which are then decoded back into human-readable text using the WhisperProcessor.\\\\n5. The transcriptions can then be used for various applications such as subtitles, transcriptions, or even translations.\\\", '<<<code>>>': \\\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\\naudio_sample = YOUR_AUDIO_SAMPLE_HERE\\\\n# Replace YOUR_AUDIO_SAMPLE_HERE with the actual audio sample\\\\ninput_features = processor(audio_sample, return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.decode(predicted_ids, skip_special_tokens=True)\\\", 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai/whisper-large-v2', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", 'api_arguments': {'forced_decoder_ids': \\\"WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')\\\"}, 'python_environment_requirements': [['transformers', 'datasets']], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', \\\"processor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\\\", \\\"model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", 'model.config.forced_decoder_ids = None', \\\"ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')\\\", \\\"sample = ds[0]['audio']\\\", \\\"input_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\\\", 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)'], 'performance': {'dataset': 'LibriSpeech test-clean', 'accuracy': 3.0003583080317573}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…ØªØ®ØµØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ. ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ Ù…Ø³Ù…ÙˆØ¹. Ù‡Ù„ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ø±Ø´Ø§Ø¯Ùƒ Ø¨Ø®Ø·ÙˆØ§Øª ØªÙ†ÙÙŠØ° Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù…Ù„ØŸ? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"pipeline('text-generation', model='facebook/opt-125m')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. Import the necessary libraries, which are 'pipeline' and 'set_seed' from transformers. \\\", 'code': 'from transformers import pipeline, set_seed\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'facebook/opt-125m\\\\')\\\\nprompt = \\\"Once upon a time in a magical forest,\\\"\\\\nset_seed(42)\\\\nstory = generator(prompt, do_sample=True, max_length=100)', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Transformers', 'functionality': 'Text Generation', 'api_name': 'facebook/opt-125m', 'api_call': 'from transformers import pipeline, set_seed\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'facebook/opt-125m\\\\')\\\\nprompt = \\\"Once upon a time in a magical forest,\\\"\\\\nset_seed(42)\\\\nstory = generator(prompt, do_sample=True, max_length=100)', 'api_arguments': {'do_sample': 'True'}, 'python_environment_requirements': 'from transformers import pipeline, set_seed', 'example_code': \\\"generator(Hello, I'm am conscious and)\\\", 'performance': {'dataset': 'Various', 'accuracy': 'Roughly matches GPT-3 performance'}, 'description': 'OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. It was predominantly pretrained with English text, but a small amount of non-English data is present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective. OPT can be used for prompting for evaluation of downstream tasks as well as text generation.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ ØªÙ†Ø¨Ø¤ ØªØµÙ†ÙŠÙ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù†ØµÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. This includes the AutoModelForDocumentQuestionAnswering and AutoTokenizer for the pre-processing of text input.\\\\n2. Load the pre-trained model 'impira/layoutlm-invoices' using the from_pretrained method of the AutoModelForDocumentQuestionAnswering class. This model is specifically designed for question answering tasks on invoices and other documents.\\\\n3. Now, define a function for the chatbot that takes the question and the context from the invoice as input parameters.\\\\n4. Inside the function, use the tokenizer to preprocess the input text and then feed it into the model, which will generate an answer.\\\\n5. Return the answer text and incorporate it into your chatbot interface.\\\", '<<<code>>>': 'from transformers import AutoModelForDocumentQuestionAnswering, AutoTokenizer\\\\nmodel = AutoModelForDocumentQuestionAnswering.from_pretrained(\\\\'impira/layoutlm-invoices\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'impira/layoutlm-invoices\\\\')\\\\n\\\\ndef answer_invoice_question(question, context):\\\\n    inputs = tokenizer(question, context, return_tensors=\\\"pt\\\")\\\\n    outputs = model(**inputs)\\\\n    answer_start = outputs.start_logits.argmax(-1).item()\\\\n    answer_end = outputs.end_logits.argmax(-1).item() + 1\\\\n    answer = context[answer_start:answer_end]\\\\n    return answer', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'layoutlm-invoices', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\\\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers'], 'example_code': \\\"nlp(question='What is the total amount?', context='your_invoice_text')\\\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ±ØºØ¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¹Ø±Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¥Ù„Ù‰ Ù†Øµ. Ù‡Ù„ ÙŠÙ…ÙƒÙ† ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø´Ø£Ù† ÙƒÙŠÙÙŠØ© Ø¨Ø¯Ø¡ Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØŸ? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"Wav2Vec2ForCTC.from_pretrained('vitouphy/wav2vec2-xls-r-300m-phoneme')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the Wav2Vec2ForCTC class from the transformers library provided by Hugging Face.\\\\n2. Use the from_pretrained method of the Wav2Vec2ForCTC class to load the pre-trained model 'vitouphy/wav2vec2-xls-r-300m-phoneme'. This model is designed for Automatic Speech Recognition (ASR) tasks.\\\\n3. Process the audio file to obtain the input values for the model.\\\\n4. Feed the input values into the model and obtain the output logits.\\\\n5. Decode the logits into text to obtain the transcription of the audio.\\\", '<<<code>>>': 'from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\\\nimport soundfile as sf\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\\\'vitouphy/wav2vec2-xls-r-300m-phoneme\\\\')\\\\nprocessor = Wav2Vec2Processor.from_pretrained(\\\\'vitouphy/wav2vec2-xls-r-300m-phoneme\\\\')\\\\naudio_input, _ = sf.read(\\\\'path_to_your_audio_file.wav\\\\') # Replace with your audio file path\\\\ninput_values = processor(audio_input, return_tensors=\\\"pt\\\", sampling_rate=16000).input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.decode(predicted_ids[0])', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('vitouphy/wav2vec2-xls-r-300m-phoneme')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'vitouphy/wav2vec2-xls-r-300m-phoneme', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('vitouphy/wav2vec2-xls-r-300m-phoneme')\\\", 'api_arguments': ['input_values'], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'None', 'accuracy': {'Loss': 0.3327, 'Cer': 0.1332}}, 'description': 'This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the None dataset. It is designed for Automatic Speech Recognition tasks.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ Ù„Ø¬Ù…Ù„Ø© Ù‚ØµÙŠØ±Ø© ØªØªØ­Ø¯Ø« Ø¹Ù† ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§ØªØŒ Ø«Ù… Ù‚Ù… Ø¨ØªØ·Ø¨ÙŠÙ‚ Ù…Ø­Ø±Ùƒ Ø§Ù„ØµÙˆØª Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØ­ÙˆÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø¥Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ Ø¢Ø®Ø± ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ÙØ³ Ø§Ù„Ø¬Ù…Ù„Ø© Ø¨ØµÙˆØª Ù…Ø®ØªÙ„Ù ÙˆÙ…Ø´Ø§Ø¨Ù‡ Ù„Ù„ØµÙˆØª Ø§Ù„Ø£ØµÙ„ÙŠ. Ù‚Ø¯Ù… Ø§Ù„ØµÙˆØªÙŠÙ† Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class ViltForQuestionAnswering from the transformers package.\\\\n2. We create an instance of the VILT model for Visual Question Answering by using the from_pretrained method and loading the pre-trained model 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering'.\\\\n3. The pre-trained model is designed to receive both image data and a textual question about that image as its input. It then generates an output based on the analysis of both modalities.\\\\n4. We can integrate the model into an app to describe images and answer questions related to them, helping visually impaired individuals better understand the visual content.\\\", '<<<code>>>': \\\"from transformers import ViltForQuestionAnswering\\\\nmodel = ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\\\\n# Prepare image and question inputs (image_path and question_text)\\\\nresult = model(image=image_path, question=question_text)\\\", 'api_call': \\\"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering', 'api_call': \\\"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\\\", 'api_arguments': {'image': 'path/to/image/file', 'question': 'your_question'}, 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random model for Visual Question Answering using the VILT framework.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ù„ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ÙŠØ© Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø¨Ø¯Ù‚Ø© ÙˆÙØ¹Ø§Ù„ÙŠØ©. ÙŠØªØ·Ù„Ø¨ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØªÙŠØ§Øª ÙˆØªØµÙ†ÙŠÙÙ‡Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø®ØµØ§Ø¦ØµÙ‡Ø§ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©. ÙŠØ¬Ø¨ Ø£Ù† ÙŠØªØ¶Ù…Ù† Ø§Ù„Ù†Ø¸Ø§Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ø³Ø¹Ø© Ù„ØªØ¯Ø±ÙŠØ¨Ù‡ Ø¹Ù„Ù‰ ØªØ¹Ø±Ù Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø¨Ø¯Ù‚Ø©.? [/INST]{'domain': 'Audio Classification', 'api_call': \\\"EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the required libraries, which are 'EncoderClassifier' from speechbrain.pretrained and 'torchaudio'.\\\", 'code': \\\"import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\n\\\\nlanguage_id = EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\\nsignal = language_id.load_audio('https://omniglot.com/soundfiles/udhr/udhr_th.mp3')\\\\nprediction = language_id.classify_batch(signal)\\\\nprint(prediction)\\\", 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Spoken Language Identification', 'api_name': 'TalTechNLP/voxlingua107-epaca-tdnn', 'api_call': \\\"import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\n\\\\nlanguage_id = EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\\nsignal = language_id.load_audio('https://omniglot.com/soundfiles/udhr/udhr_th.mp3')\\\\nprediction = language_id.classify_batch(signal)\\\\nprint(prediction)\\\", 'api_arguments': ['signal'], 'python_environment_requirements': ['speechbrain', 'torchaudio'], 'example_code': 'import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\nlanguage_id = EncoderClassifier.from_hparams(source=TalTechNLP/voxlingua107-epaca-tdnn, savedir=tmp)\\\\nsignal = language_id.load_audio(https://omniglot.com/soundfiles/udhr/udhr_th.mp3)\\\\nprediction = language_id.classify_batch(signal)\\\\nprint(prediction)', 'performance': {'dataset': 'VoxLingua107', 'accuracy': '93%'}, 'description': 'This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain. The model uses the ECAPA-TDNN architecture that has previously been used for speaker recognition. The model can classify a speech utterance according to the language spoken. It covers 107 different languages.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ ØªÙˆØ¬ÙŠÙ‡ÙŠ ÙŠØµÙ ÙƒÙŠÙÙŠØ© ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"pipeline('text-classification', model='finiteautomata/bertweet-base-sentiment-analysis')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' function from the transformers package.\\\\n2. Use the 'pipeline' function to load the pre-trained sentiment analysis model 'finiteautomata/bertweet-base-sentiment-analysis'.\\\\n3. Apply the sentiment analysis model to analyze the given review text.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nnlp = pipeline(\\\\'text-classification\\\\', model=\\\\'finiteautomata/bertweet-base-sentiment-analysis\\\\')\\\\nreview_text = \\\"The room was great, hosts were friendly :) but the shared bathrooms were dirty and the Wi-Fi was unreliable.\\\"\\\\nresult = nlp(review_text)', 'api_call': \\\"pipeline('text-classification', model='finiteautomata/bertweet-base-sentiment-analysis')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'finiteautomata/bertweet-base-sentiment-analysis', 'api_call': \\\"pipeline('text-classification', model='finiteautomata/bertweet-base-sentiment-analysis')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['Transformers'], 'example_code': \\\"from transformers import pipeline\\\\nnlp = pipeline('text-classification', model='finiteautomata/bertweet-base-sentiment-analysis')\\\\nresult = nlp('I love this movie!')\\\", 'performance': {'dataset': 'SemEval 2017', 'accuracy': None}, 'description': 'Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is BERTweet, a RoBERTa model trained on English tweets. Uses POS, NEG, NEU labels.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ© ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the required libraries including MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation from the transformers package, Image from the PIL package, and requests for retrieving the image from a URL.\\\\n2. Load the feature_extractor and model using the 'facebook/maskformer-swin-base-coco' pre-trained model. This model is trained for panoptic segmentation tasks on the COCO dataset.\\\\n3. Open the image from the provided URL using the Image.open method from the PIL package, and requests.get to download the image.\\\\n4. Extract the features from the image using the feature_extractor and provide those as inputs to the model.\\\\n5. Obtain the segmentation results from the model by post-processing the outputs using the feature_extractor.post_process_panoptic_segmentation function.\\\\n6. The panoptic segmentation map can be used to identify the boundaries and categories of objects present in the input image.\\\", '<<<code>>>': \\\"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\\nurl = 'https://example.com/image.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result['segmentation']\\\", 'api_call': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/maskformer-swin-base-coco', 'api_call': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\", 'api_arguments': ['image'], 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': \\\"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\\n\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\n\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\n\\\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result['segmentation']\\\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion', torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We start by importing the necessary packages, including StableDiffusionPipeline for the text-to-image model and torch for handling tensors.\\\\n2. We instantiate the model using the from_pretrained method with the 'nitrosocke/nitro-diffusion' model.\\\\n3. We then create a prompt describing the main character as a space pirate with a pet dragon. This prompt will be used by the model to generate an appropriate image.\\\\n4. We pass the prompt to the model and obtain an image as the output.\\\\n5. The generated image can then be saved as a file for further use in your children's book.\\\", '<<<code>>>': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = \\\\'nitrosocke/nitro-diffusion\\\\'\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\nprompt = \\\"Space pirate with a pet dragon\\\"\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(\\\"space_pirate_with_pet_dragon.png\\\")', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion', torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion', torch_dtype=torch.float16)\\\", 'api_arguments': ['prompt'], 'python_environment_requirements': [['torch', 'diffusers']], 'example_code': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = nitrosocke/nitro-diffusion\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = archer arcane style magical princess with golden hair\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include AutoFeatureExtractor and RegNetForImageClassification from transformers, torch, and load_dataset from datasets.\\\\n2. Load a dataset containing a collection of images, which can be obtained from sources like Hugging Face's Dataset Hub.\\\\n3. Preprocess the input image data using AutoFeatureExtractor, which helps in preparing the image data to be fed to the RegNet model.\\\\n4. Load the pre-trained image classification model 'zuppif/regnet-y-040' using RegNetForImageClassification.from_pretrained(). This model is trained to categorize images based on their content.\\\\n5. Use the model to classify the images and retrieve the predicted label for each image.\\\", '<<<code>>>': \\\"from transformers import AutoFeatureExtractor, RegNetForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset('<your-dataset>')  # replace with your dataset\\\\nimage = dataset['test']['image'][0]  # sample image from the dataset\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('zuppif/regnet-y-040')\\\\nmodel = RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\\ninputs = feature_extractor(image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/regnet-y-008', 'api_call': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'zuppif/regnet-y-040'}, 'python_environment_requirements': [{'transformers': 'AutoFeatureExtractor, RegNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}], 'example_code': 'from transformers import AutoFeatureExtractor, RegNetForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset(huggingface/cats-image)\\\\nimage = dataset[test][image][0]\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\\\ninputs = feature_extractor(image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n... logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ¨Ø¹ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ: Ù¡. Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ ÙˆØªØ«Ø¨ÙŠØª Ù…ÙƒØªØ¨Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù¢. Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ø³Ø¹Ø©. Ù£. Ø­Ø§ÙˆÙ„ ØªÙˆÙ„ÙŠØ¯ Ø¬Ù…Ù„ Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨. Ù¤. Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù…. Ù¥. Ù‚Ù… Ø¨ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ¬ÙˆØ¯Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('bigscience/bloomz-560m')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library provided by Hugging Face, including AutoModelForCausalLM and AutoTokenizer.\\\\n2. Specify the model checkpoint 'bigscience/bloomz-560m' and use it to load the tokenizer and the model.\\\\n3. Use the tokenizer to convert the input French text into tensors.\\\\n4. Use the model to generate a response given the input tensors.\\\\n5. Decode the output tensors to obtain the translated text in English.\\\\n6. This process can be performed for each message in the chat in real-time.\\\", '<<<code>>>': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ncheckpoint = \\\\'bigscience/bloomz-560m\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\\\n\\\\ndef translate_french_to_english(text):\\\\n    inputs = tokenizer.encode(\\\"Translate to English: \\\" + text, return_tensors=\\\\'pt\\\\')\\\\n    outputs = model.generate(inputs)\\\\n    return tokenizer.decode(outputs[0])\\\\n\\\\nenglish_text = translate_french_to_english(\\\"Je t\\\\'aime.\\\")', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('bigscience/bloomz-560m')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bigscience/bloomz-560m', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('bigscience/bloomz-560m')\\\", 'api_arguments': {'checkpoint': 'bigscience/bloomz-560m', 'inputs': 'Translate to English: Je tâ€™aime.'}, 'python_environment_requirements': [['transformers', 'accelerate', 'bitsandbytes']], 'example_code': {'CPU': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ncheckpoint = bigscience/bloomz-560m\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\\\ninputs = tokenizer.encode(Translate to English: Je tâ€™aime., return_tensors=pt)\\\\noutputs = model.generate(inputs)\\\\nprint(tokenizer.decode(outputs[0]))'}, 'performance': {'dataset': 'bigscience/xP3', 'accuracy': {'Winogrande XL (xl) validation set': 52.41, 'XWinograd (en) test set': 51.01, 'XWinograd (fr) test set': 51.81, 'XWinograd (jp) test set': 52.03, 'XWinograd (pt) test set': 53.99, 'XWinograd (ru) test set': 53.97, 'XWinograd (zh) test set': 54.76, 'ANLI (r1) validation set': 33.4, 'ANLI (r2) validation set': 33.4, 'ANLI (r3) validation set': 33.5}}, 'description': 'BLOOMZ & mT0 are a family of models capable of following human instructions in dozens of languages zero-shot. Finetuned on the crosslingual task mixture (xP3), these models can generalize to unseen tasks & languages. Useful for tasks expressed in natural language, such as translation, summarization, and question answering.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„ØªÙˆÙ„ÙŠÙ ØµÙˆØ± Ø±Ù‚Ù…ÙŠØ© Ø¨Ø¯ÙˆÙ† Ø´Ø±ÙˆØ· ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ÙÙ†ØªØ¬Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªÙˆÙ„ÙŠÙ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the DDPMPipeline class from the diffusers library.\\\\n2. Use the from_pretrained method of the DDPMPipeline class to load the pre-trained model 'ocariz/universe_1400'. This model is specifically designed for generating images of the universe unconditionally.\\\\n3. Invoke the loaded model to generate an image of the universe.\\\\n4. The generated image can be used as a background for different scenes in the space-themed VR game.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\\nimage = pipeline().images[0]\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ocariz/universe_1400', 'api_call': \\\"DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\", 'api_arguments': '', 'python_environment_requirements': ['diffusers'], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is a diffusion model for unconditional image generation of the universe trained for 1400 epochs.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø§Ø¯Ù Ù„Ø·Ù„Ø§Ø¨ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†ØµÙˆØµ ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ. ÙŠØ¬ØªÙ…Ø¹ Ø§Ù„Ø£Ø¹Ø¶Ø§Ø¡ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù…Ù…Ø§Ø±Ø³Ø© ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠÙ…ÙƒÙ† Ù„Ø£Ø¹Ø¶Ø§Ø¡ Ø§Ù„Ù†Ø§Ø¯ÙŠ Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª ÙˆÙ…Ù‡Ø§Ù… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ù…Ø¹ ØªÙˆØ§ÙØ± Ø£Ø¯ÙˆØ§Øª Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙ‡Ù… Ø¹Ù„Ù‰ Ø§Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To start, import the pipeline function from the transformers library provided by Hugging Face. \\\\n2. Then, use the pipeline function to create a text-to-text generation model with the specific model 'pszemraj/flan-t5-large-grammar-synthesis'. This is a pre-trained model for grammar correction.\\\\n3. Pass any raw text with grammar mistakes to the created model, which will detect and correct the grammatical errors in the text.\\\\n4. The corrected version of the text will be generated, which can then be used for various purposes like email templates, articles, or any other text-based content that requires correct grammar.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntext = 'I are goint to the store to by milks.'\\\\ngrammar_corrector = pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\\ncorrected_text = grammar_corrector(text)\\\", 'api_call': \\\"pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pszemraj/flan-t5-large-grammar-synthesis', 'api_call': \\\"pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\", 'api_arguments': ['raw_text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\n\\\\ncorrector = pipeline(\\\\n 'text2text-generation',\\\\n 'pszemraj/flan-t5-large-grammar-synthesis',\\\\n )\\\\n\\\\nraw_text = 'i can has cheezburger'\\\\nresults = corrector(raw_text)\\\\nprint(results)\\\", 'performance': {'dataset': 'jfleg', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of google/flan-t5-large for grammar correction on an expanded version of the JFLEG dataset.' </s>\"\n"}
{"text": "<s>[INST] ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. This includes DPRQuestionEncoder and DPRQuestionEncoderTokenizer.\\\\n2. We then use the from_pretrained method to load the pre-trained model 'facebook/dpr-question_encoder-single-nq-base'. This model is designed to encode questions effectively so that it can be used for high-quality open-domain question answering tasks.\\\\n3. With this model, you can now use the tokenizer to tokenize any dog-related question into input_ids and feed those ids to the model to get embeddings for that question.\\\\n4. These embeddings can then be used to find and rank relevant documents, passages or answers for the given question.\\\", '<<<code>>>': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\\\\'facebook/dpr-question_encoder-single-nq-base\\\\')\\\\nmodel = DPRQuestionEncoder.from_pretrained(\\\\'facebook/dpr-question_encoder-single-nq-base\\\\')\\\\ninput_ids = tokenizer(\\\"What is the average lifespan of a dog?\\\", return_tensors=\\\\'pt\\\\')[\\\\'input_ids\\\\']\\\\nembeddings = model(input_ids).pooler_output', 'api_call': \\\"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \\\"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\\\", 'api_arguments': ['input_ids'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ ÙŠØµÙ Ù…Ø´Ù‡Ø¯ Ù„Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required libraries: AutoModelForCausalLM and AutoTokenizer from the transformers library provided by Hugging Face.\\\\n2. Load the DialoGPT-large model using the AutoModelForCausalLM.from_pretrained() function with the specified model name 'microsoft/DialoGPT-large'. This is a pretrained dialogue response generation model trained on multi-turn conversations, suitable for creating a chatbot.\\\\n3. Create a tokenizer using the AutoTokenizer.from_pretrained() function, which will convert the text into tokens that the model can understand.\\\\n4. With the loaded model and tokenizer, you can now process user messages and generate appropriate responses for your customer support chatbot.\\\", '<<<code>>>': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-large')\\\\nmodel = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/DialoGPT-large', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_arguments': ['bot_input_ids', 'max_length', 'pad_token_id'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-large)\\\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-large)', 'performance': {'dataset': 'Reddit discussion thread', 'accuracy': 'Comparable to human response quality under a single-turn conversation Turing test'}, 'description': 'DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multi-turn conversations. The model is trained on 147M multi-turn dialogues from Reddit discussion threads.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø´ÙˆØ§Ø¦Ø¨.? [/INST]{'domain': 'Tabular Tabular Regression Anomaly Detection', 'api_call': \\\"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\", 'api_provider': 'Keras', 'explanation': '1. Import the necessary libraries and components, such as TensorFlow, Keras, and the TFAutoModelForSequenceClassification class.', 'code': \\\"import tensorflow as tf\\\\nfrom tensorflow.keras import layers\\\\nfrom transformers import TFAutoModelForSequenceClassification\\\\n\\\\nmodel = TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\\n\\\\n# Prepare your data here and preprocess it as needed.\\\\nx_train, x_test = None, None  # Replace with your actual data\\\\n\\\\n# Train the model on your data\\\\nhistory = model.fit(x_train, x_train, epochs=100, batch_size=128, validation_data=(x_test, x_test))\\\\n\\\\n# Detect anomalies by computing the difference between reconstructed data and original data\\\\nreconstructed_data = model.predict(x_test)\\\\ndiff = tf.abs(x_test - reconstructed_data)\\\\n\\\\n# Set a threshold to identify anomalies\\\\nthreshold = 0.1\\\\nanomalies = diff > threshold\\\", 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Keras', 'functionality': 'anomaly-detection', 'api_name': 'keras-io/timeseries-anomaly-detection', 'api_call': \\\"import tensorflow as tf\\\\nfrom tensorflow.keras import layers\\\\nfrom transformers import TFAutoModelForSequenceClassification\\\\n\\\\nmodel = TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\\n\\\\n# Prepare your data here and preprocess it as needed.\\\\nx_train, x_test = None, None  # Replace with your actual data\\\\n\\\\n# Train the model on your data\\\\nhistory = model.fit(x_train, x_train, epochs=100, batch_size=128, validation_data=(x_test, x_test))\\\\n\\\\n# Detect anomalies by computing the difference between reconstructed data and original data\\\\nreconstructed_data = model.predict(x_test)\\\\ndiff = tf.abs(x_test - reconstructed_data)\\\\n\\\\n# Set a threshold to identify anomalies\\\\nthreshold = 0.1\\\\nanomalies = diff > threshold\\\", 'api_arguments': {'optimizer': {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'training_precision': 'float32'}, 'python_environment_requirements': ['tensorflow', 'keras'], 'example_code': '', 'performance': {'dataset': 'Numenta Anomaly Benchmark(NAB)', 'accuracy': {'Train Loss': 0.006, 'Validation Loss': 0.008}}, 'description': 'This script demonstrates how you can use a reconstruction convolutional autoencoder model to detect anomalies in timeseries data. We will use the Numenta Anomaly Benchmark(NAB) dataset. It provides artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered, timestamped, single-valued metrics.'}} </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚ ØµÙˆØªÙŠ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†Ø§ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø´ÙƒÙ„ Ø³Ø±ÙŠØ¹ ÙˆÙØ¹Ø§Ù„.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': '1. Install Hugging Face and ESPnet packages, as they are required for using the API.\\\\n2. In the terminal, run the provided shell script (run.sh) with the specified command-line arguments. Set skip_data_prep to false as we need to prepare the input text for TTS, and set skip_train to true because we are not training the model.\\\\n3. Download and use the \\\\'mio/amadeus\\\\' model for text-to-speech conversion. This model is trained using the amadeus recipe in ESPnet and capable of generating high-quality speech from text.\\\\n4. Provide the given text to the TTS system as the input.\\\\n<<<example_input>>>: \\\"Welcome to the grand opening of our new store! Enjoy unbeatable prices and amazing discounts.\\\"', '<<<code>>>': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus --input_text \\\"Welcome to the grand opening of our new store! Enjoy unbeatable prices and amazing discounts.\\\"', 'api_call': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kazusam/kt', 'api_call': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'api_arguments': ['input_text'], 'python_environment_requirements': [['Hugging Face', 'ESPnet']], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': 'amadeus', 'accuracy': 'Not provided'}, 'description': 'An ESPnet2 TTS model trained by mio using amadeus recipe in espnet.' </s>\"\n"}
{"text": "<s>[INST]Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ù…Ù† ÙˆØµÙ Ù†ØµÙŠØŒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ ØªÙ†ÙÙŠØ° Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We begin by importing the necessary components from the transformers library, which includes the MarianMTModel for translation and MarianTokenizer for tokenization.\\\\n2. Next, we instantiate the model with the OPUS pre-trained Romance languages to English model 'Helsinki-NLP/opus-mt-ROMANCE-en'. This model can handle multiple source languages such as French, Spanish, Italian, etc.\\\\n3. We tokenize the input document into subwords using the tokenizer with correct source language.\\\\n4. Finally, we use the model to translate the document into English, and then decode the output into an understandable format.\\\", '<<<code>>>': \\\"from transformers import MarianMTModel, MarianTokenizer\\\\nmodel_name = 'Helsinki-NLP/opus-mt-ROMANCE-en'\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\n\\\\n# replace 'source_text' with the text in Italian, French, or Spanish\\\\nsource_text = 'Text to be translated'\\\\ntranslated = model.generate(**tokenizer(source_text, return_tensors='pt', padding=True))\\\\ndecoded_translation = tokenizer.decode(translated[0], skip_special_tokens=True)\\\", 'api_call': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-ROMANCE-en', 'api_call': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", 'api_arguments': ['source languages', 'target languages'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': '', 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': 62.2, 'chr-F': 0.75}}, 'description': 'A model for translating Romance languages to English, trained on the OPUS dataset. It supports multiple source languages such as French, Spanish, Portuguese, Italian, and Romanian, among others. The model is based on the transformer architecture and uses normalization and SentencePiece for pre-processing.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ±ØŒ Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©ØŸ? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, such as decord, torch, numpy, and transformers.\\\\n2. Initialize the feature extractor and the VideoMAEForVideoClassification model with a pretrained model, 'nateraw/videomae-base-finetuned-ucf101'.\\\\n3. Load the video file using the VideoReader from the decord library.\\\\n4. Define and call a function to sample frame indices from the video.\\\\n5. Use the feature extractor to process the video frames and convert them into a suitable format for the model.\\\\n6. Feed the processed frames into the pretrained VideoMAEForVideoClassification model.\\\\n7. Extract the output logits and identify the action with the highest prediction score.\\\", '<<<code>>>': \\\"from decord import VideoReader, cpu\\\\nimport torch\\\\nimport numpy as np\\\\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\\\nfrom huggingface_hub import hf_hub_download\\\\n\\\\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\\\\n    converted_len = int(clip_len * frame_sample_rate)\\\\n    end_idx = np.random.randint(converted_len, seg_len)\\\\n    start_idx = end_idx - converted_len\\\\n    indices = np.linspace(start_idx, end_idx, num=clip_len)\\\\n    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\\\\n    return indices\\\\n\\\\nfile_path = 'path/to/video_file.mp4'\\\\n\\\\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\\\\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\\\\nvideo = videoreader.get_batch(indices).asnumpy()\\\\n\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\\n\\\\ninputs = feature_extractor(list(video), return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'videomae-base-finetuned-ucf101', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/videomae-base-finetuned-ucf101'}, 'python_environment_requirements': [['transformers', 'decord', 'huggingface_hub']], 'example_code': 'from decord import VideoReader, cpu\\\\nimport torch\\\\nimport numpy as np\\\\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\\\nfrom huggingface_hub import hf_hub_download\\\\nnp.random.seed(0)\\\\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\\\\n converted_len = int(clip_len * frame_sample_rate)\\\\n end_idx = np.random.randint(converted_len, seg_len)\\\\n start_idx = end_idx - converted_len\\\\n indices = np.linspace(start_idx, end_idx, num=clip_len)\\\\n indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\\\\n return indices\\\\nfile_path = hf_hub_download(\\\\n repo_id=nateraw/dino-clips, filename=archery.mp4, repo_type=space\\\\n)\\\\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\\\\nvideoreader.seek(0)\\\\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\\\\nvideo = videoreader.get_batch(indices).asnumpy()\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\\\\ninputs = feature_extractor(list(video), return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'UCF101', 'accuracy': 0.758209764957428}, 'description': 'VideoMAE Base model fine tuned on UCF101 for Video Action Recognition' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­Ø¯ÙŠØ¯ Ø¹Ù…Ù‚ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„ÙˆØ§Ø±Ø¯Ø© Ø¥Ù„Ù‰ Ø£Ø¯Ù‚ ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¹Ù…Ù‚.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"etermine the depth of objects in an image for an autonomous robot to identify and navigate better in its environment.\\\\n###Output: <<<domain>>>: Computer Vision Depth Estimation\\\\n<<<api_call>>>: DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\\\\n<<<api_provider>>>: Hugging Face Transformers\\\\n<<<explanation>>>(1)We need to import 'DPTForDepthEstimation' class from transformers package.\\\\n(2)Next, we will load the pre-trained 'hf-tiny-model-private/tiny-random-DPTForDepthEstimation' model.\\\\n(3)After loading the model, we input the image file to the model. The model then outputs a depth map of the image that can help an autonomous robot to better understand the depth of the objects in its environment.\\\", '<<<code>>>': \\\"from transformers import DPTForDepthEstimation\\\\nimport cv2\\\\nmodel = DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\\\\nimage = cv2.imread('path/to/image.jpg')\\\\ndepth_map = model(image)\\\", 'api_call': \\\"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tiny-random-DPTForDepthEstimation', 'api_call': \\\"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random DPT model for depth estimation using Hugging Face Transformers library.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ Ù„Ø¹Ù„ÙˆÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ø­ÙŠØ« ÙŠØ¬ØªÙ…Ø¹ Ø§Ù„Ø£Ø¹Ø¶Ø§Ø¡ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ ØªÙˆÙ„ÙŠØ¯ÙŠ. ÙŠØ¬Ø¨ ØªØ²ÙˆÙŠØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-xl')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. We import the necessary classes from the transformers package. This includes T5Tokenizer for tokenizing the text input and T5ForConditionalGeneration for the text generation model.\\\\n2. We load the pre-trained model \\\\'google/flan-t5-xl\\\\', which has been fine-tuned on more than 1000 tasks covering multiple languages, making it suitable for a multilingual chatbot.\\\\n3. We then use this tokenizer and model to process and generate responses to questions and tasks. The model can handle translation, arithmetic problems, and generative storytelling tasks, among others.\\\\n4. The input_text variable will contain your chatbot\\\\'s input like \\\"translate English to German: How old are you?\\\" or \\\"calculate 4 * 4\\\". The model will decode the output and return the appropriate response.', '<<<code>>>': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'google/flan-t5-xl\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'google/flan-t5-xl\\\\')\\\\ninput_text = \\\"your_multilingual_chatbot_input\\\"\\\\ninput_ids = tokenizer(input_text, return_tensors=\\\\'pt\\\\').input_ids\\\\noutputs = model.generate(input_ids)\\\\nresponse = tokenizer.decode(outputs[0])', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-xl')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/flan-t5-xl', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-xl')\\\", 'api_arguments': ['input_text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained('google/flan-t5-xl')\\\\nmodel = T5ForConditionalGeneration.from_pretrained('google/flan-t5-xl')\\\\ninput_text = 'translate English to German: How old are you?'\\\\ninput_ids = tokenizer(input_text, return_tensors='pt').input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))\\\", 'performance': {'dataset': [{'name': 'MMLU', 'accuracy': '75.2%'}]}, 'description': 'FLAN-T5 XL is a large-scale language model fine-tuned on more than 1000 tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot and few-shot NLP tasks, such as reasoning, question answering, and understanding the limitations of current large language models.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ ØªØµÙ…ÙŠÙ… Ù†Ù…Ø§Ø°Ø¬ Ø§Ø®ØªØ¨Ø§Ø± ØµÙˆØªÙŠØ© Ù…ØªÙ†Ø¹Ù…Ø© Ø°Ø§Øª Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙÙŠ Ø§Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙŠØ±Ø¬Ù‰ ØªÙˆÙÙŠØ± ØªÙ‚Ø¯ÙŠØ±Ø§Øª Ø¹Ù…Ù‚ Ø³ÙˆØ¨Ø³Ø§Øª Ø§Ù„ÙƒÙ„Ø§Ù… Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries â€“ torch and transformers. In particular, we need the AutoModel class from transformers.\\\\n2. Load the pre-trained model using the from_pretrained() method with the provided model name 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'. This model has been fine-tuned for depth estimation tasks, which is useful for a robot navigating through a room.\\\\n3. Acquire the image data captured by the robot's camera, preprocess it, and convert it into a suitable format or tensor for the model.\\\\n4. Use the loaded model to predict depth information by passing the preprocessed image data as input.\\\\n5. Based on the obtained depth information, the robot can process the data and take appropriate actions, like avoiding obstacles and successfully navigating through the room.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nfrom PIL import Image\\\\nimport torch\\\\n\\\\nimage = Image.open('room_image.jpg')\\\\n# replace 'room_image.jpg' with the path to your image\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\\\\ninput_tensor = preprocess_image(image)  # preprocess_image is a custom function to preprocess image\\\\ndepth_map = model(input_tensor)\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221116-054332', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\\\", 'api_arguments': {'model_name': 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'}, 'python_environment_requirements': [{'transformers': '4.24.0', 'pytorch': '1.13.0+cu117', 'tokenizers': '0.13.2'}], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.6028, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ù„Ø·Ù„Ø§Ø¨ ØªØ¹Ù„Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø¥ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø¢Ù„ÙŠ.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('ToddGoldfarb/Cadet-Tiny', low_cpu_mem_usage=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries from the transformers library: AutoTokenizer and AutoModelForSeq2SeqLM. Also, import the torch library for utilizing GPU acceleration if available.\\\\n2. Define a CadetTinyAgent class to handle the conversational agent's functions like observing user input, generating model responses, and managing conversation history.\\\\n3. Load the Cadet-Tiny model trained on the SODA dataset using the from_pretrained method with low_cpu_mem_usage=True. This model is based on the t5-small pretrained model and is suitable for devices with limited resources like Raspberry Pi.\\\\n4. Implement methods like observe, set_input, generate, reset_history, run, and chat in the CadetTinyAgent class to create an interactive conversational loop between the user and the AI agent.\\\\n5. Instantiate the CadetTinyAgent object and call the run method to start the conversation.\\\", '<<<code>>>': 'import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nclass CadetTinyAgent:\\\\n    def __init__(self):\\\\n        self.device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\\\n        self.tokenizer = AutoTokenizer.from_pretrained(\\\\'t5-small\\\\', model_max_length=512)\\\\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(\\\\'ToddGoldfarb/Cadet-Tiny\\\\', low_cpu_mem_usage=True).to(self.device)\\\\n        self.conversation_history = \\\"\\\"\\\\n    # Implement other necessary methods like observe, set_input, generate, reset_history, run, and chat\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('ToddGoldfarb/Cadet-Tiny', low_cpu_mem_usage=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ToddGoldfarb/Cadet-Tiny', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('ToddGoldfarb/Cadet-Tiny', low_cpu_mem_usage=True)\\\", 'api_arguments': {'pretrained_model': 't5-small', 'model_max_length': 512}, 'python_environment_requirements': [{'torch': '', 'transformers': '', 'colorful': ''}], 'example_code': \\\"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style('monokai')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == '<strong>main</strong>':\\\\n main()\\\", 'performance': {'dataset': 'allenai/soda', 'accuracy': ''}, 'description': 'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ÙˆÙ… Ø¨ØªØµÙ…ÙŠÙ… Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„ØªØµÙ†ÙŠÙ ØµÙØ± Ø§Ù„Ø£Ø­ÙƒØ§Ù…ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø­Ø³Ø§Ø¨ Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ø¦Ø¯ ÙÙŠ ÙƒÙ„ Ø­Ø§Ù„Ø© Ù„Ø¯Ø¹Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬.? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"DebertaModel.from_pretrained('Narsil/deberta-large-mnli-zero-cls')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We import the DebertaModel class and pipeline function from the transformers library provided by Hugging Face.', 'code': \\\"from transformers import DebertaModel, pipeline\\\\nmodel = DebertaModel.from_pretrained('Narsil/deberta-large-mnli-zero-cls')\\\\nclassifier = pipeline('zero-shot-classification', model=model)\\\\nresult = classifier(user_question, candidate_labels=possible_responses)\\\", 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'Narsil/deberta-large-mnli-zero-cls', 'api_call': \\\"from transformers import DebertaModel, pipeline\\\\nmodel = DebertaModel.from_pretrained('Narsil/deberta-large-mnli-zero-cls')\\\\nclassifier = pipeline('zero-shot-classification', model=model)\\\\nresult = classifier(user_question, candidate_labels=possible_responses)\\\", 'api_arguments': 'text, candidate_labels', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': {'SQuAD 1.1': {'F1': 95.5, 'EM': 90.1}, 'SQuAD 2.0': {'F1': 90.7, 'EM': 88.0}, 'MNLI-m/mm': {'Accuracy': 91.3}, 'SST-2': {'Accuracy': 96.5}, 'QNLI': {'Accuracy': 95.3}, 'CoLA': {'MCC': 69.5}, 'RTE': {'Accuracy': 91.0}, 'MRPC': {'Accuracy': 92.6}, 'QQP': {}, 'STS-B': {'P/S': 92.8}}}, 'description': 'DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on the majority of NLU tasks with 80GB training data. This is the DeBERTa large model fine-tuned with MNLI task.'}} </s>\"\n"}
{"text": "<s>[INST]Ø³ÙˆÙ ØªØ¹Ù…Ù„ Ø§Ù„Ø´Ø±ÙƒØ© Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„ØµÙˆØªÙŠØ©ØŒ ÙˆØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØµÙ…ÙŠÙ… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªØ³ØªØ·ÙŠØ¹ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠ Ù„Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"pipeline('audio-classification', model=Wav2Vec2ForCTC.from_pretrained('anton-l/wav2vec2-random-tiny-classifier'))\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required packages and classes such as 'pipeline' from transformers and 'Wav2Vec2ForCTC' from the transformers.models.wav2vec2 module.\\\\n2. Instantiate the audio classification pipeline by providing the 'audio-classification' task and the pre-trained model 'anton-l/wav2vec2-random-tiny-classifier', which is based on the Wav2Vec2 architecture.\\\\n3. You can now pass an audio file into the pipeline, which will classify the input as one of the predefined categories: speech, music, or background noise.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nfrom transformers.models.wav2vec2 import Wav2Vec2ForCTC\\\\naudio_classifier = pipeline('audio-classification', model=Wav2Vec2ForCTC.from_pretrained('anton-l/wav2vec2-random-tiny-classifier'))\\\\nclassification_result = audio_classifier(audio_file)\\\", 'api_call': \\\"pipeline('audio-classification', model=Wav2Vec2ForCTC.from_pretrained('anton-l/wav2vec2-random-tiny-classifier'))\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'wav2vec2-random-tiny-classifier', 'api_call': \\\"pipeline('audio-classification', model=Wav2Vec2ForCTC.from_pretrained('anton-l/wav2vec2-random-tiny-classifier'))\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'An audio classification model based on wav2vec2.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù…Ù†ØªØ¬ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙŠØªÙŠØ­ ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµØŒ ÙˆÙ†Ø±ÙŠØ¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø¢Ù„ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Using the Hugging Face Transformers library, we will import the pipeline function.', 'code': \\\"from transformers import pipeline\\\\n\\\\nmultilingual_qa = pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\\n\\\\nresult = multilingual_qa({\\\\n    'context': 'Manuel Romero lavora duramente nel repository hugginface/transformers di recente.',\\\\n    'question': 'Chi ha lavorato duramente per hugginface/transformers di recente?'})\\\", 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Multilingual Question Answering', 'api_name': 'mrm8488/bert-multi-cased-finetuned-xquadv1', 'api_call': \\\"from transformers import pipeline\\\\n\\\\nmultilingual_qa = pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\\n\\\\nresult = multilingual_qa({\\\\n    'context': 'Manuel Romero lavora duramente nel repository hugginface/transformers di recente.',\\\\n    'question': 'Chi ha lavorato duramente per hugginface/transformers di recente?'})\\\", 'api_arguments': {'context': 'string', 'question': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"qa_pipeline({\\\\n 'context': Manuel Romero has been working hardly in the repository hugginface/transformers lately,\\\\n 'question': Who has been working hard for hugginface/transformers lately?\\\\n})\\\", 'performance': {'dataset': 'XQuAD', 'accuracy': 'Not provided'}, 'description': 'This model is a BERT (base-multilingual-cased) fine-tuned for multilingual Question Answering on 11 different languages using the XQuAD dataset and additional data augmentation techniques.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ† Ù„ÙØ±ÙŠÙ‚ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ ØªÙ„Ù‚Ø§Ø¦ÙŠØ© ØªÙ‚Ø§ÙˆÙ…ÙŠØ©. Ø¬Ù‡Ø² Ø§Ù„ÙØ±ÙŠÙ‚ Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ØµÙ†Ø§Ø¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries and classes from the transformers and torch packages.\\\\n2. Load the pre-trained transformer model 'facebook/opt-66b' using the AutoModelForCausalLM class. This is a powerful text generation model aimed at matching GPT-3's performance.\\\\n3. Set-up a tokenizer for the model, using the AutoTokenizer class from transformers.\\\\n4. You can now feed the model with a prompt or seed text that will be used as a starting point for generating creative headlines.\\\\n5. Use the generate() function to produce multiple possible headlines by adjusting the sampling parameters such as 'do_sample', 'num_return_sequences', and 'max_length'.\\\\n6. Finally, use the tokenizer's batch_decode() method to convert the generated tokens back into text form.\\\", '<<<code>>>': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\n\\\\nmodel = AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained('facebook/opt-66b', use_fast=False)\\\\n\\\\nprompt = 'Introducing our new product: '\\\\ninput_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\\\\nset_seed(32)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\\\\nheadlines = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\\\", 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/opt-66b', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", 'api_arguments': ['input_ids', 'do_sample', 'num_return_sequences', 'max_length'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-66b, torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-66b, use_fast=False)\\\\nprompt = Hello, I am conscious and\\\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\\\nset_seed(32)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\\\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)', 'performance': {'dataset': 'GPT-3', 'accuracy': 'roughly matched'}, 'description': 'OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. OPT models are trained to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ ØµÙˆØ± ØºÙŠØ± Ù…Ø´Ø±ÙˆØ· Ø¨ÙˆØ§Ø³Ø·Ø© ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the DDPMPipeline class from the diffusers package.', 'code': \\\"from diffusers import DDPMPipeline\\\\nbutterfly_pipeline = DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\\nbutterfly_image = butterfly_pipeline().images[0]\\\\nbutterfly_image.save('butterfly_background.png')\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'ntrant7/sd-class-butterflies-32', 'api_call': \\\"from diffusers import DDPMPipeline\\\\nbutterfly_pipeline = DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\\nbutterfly_image = butterfly_pipeline().images[0]\\\\nbutterfly_image.save('butterfly_background.png')\\\", 'api_arguments': [], 'python_environment_requirements': ['diffusers'], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This model is a diffusion model for unconditional image generation of cute butterflies.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¹Ù…Ù„ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ \\\"Natural Language Processing Text Classification\\\" Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¯ÙˆÙ† ØªÙˆØ¬ÙŠÙ‡ Ù…Ø³Ø¨Ù‚. Ø§Ù„Ø­Ø§Ø¬Ø© Ù…Ø§Ø³Ù‘Ø© Ø¥Ù„Ù‰ ØªØ·Ø¨ÙŠÙ‚ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØªØµÙ†ÙŠÙÙ‡Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ù…Ø³Ø¨Ù‚Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Then, we use the pipeline function to create a sentiment analysis model, and we specify the model 'ProsusAI/finbert' to be loaded. This model has been fine-tuned for financial sentiment classification tasks, making it suitable for analyzing the sentiment of a cryptocurrency news article.\\\\n3. We can now use the loaded model to analyze the sentiment of the given news article. The model will classify the sentiment as 'positive', 'negative', or 'neutral'.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'sentiment-analysis\\\\', model=\\\\'ProsusAI/finbert\\\\')\\\\nresult = classifier(\\\"Bitcoin has recently seen an impressive surge in value, leading to a renewed interest in cryptocurrencies. This has created a lot of excitement among investors, with many predicting a profitable future. However, the market remains volatile, and it would be wise for investors to approach with caution, as sudden fluctuations can result in significant losses.\\\")', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ProsusAI/finbert', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline; classifier = pipeline('sentiment-analysis', model='ProsusAI/finbert'); classifier('your_text_here')\\\", 'performance': {'dataset': 'Financial PhraseBank', 'accuracy': 'Not provided'}, 'description': 'FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø§Ø­Ø«ÙŠÙ† ØªØ´Ø¬ÙŠØ¹ ØªØ·ÙˆÙŠØ± ÙˆØªÙ‚ÙŠÙŠÙ… Ù†Ù…Ø§Ø°Ø¬ Ø·Ø¨Ù‚ Ø§Ù„ØµÙ†Ø§Ø¹Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ²ÙŠ Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø°Ø§ØªÙŠ Ù„Ù„ØµÙˆØ± Ø¨Ù…Ø§ ÙÙŠÙ‡Ø§ Ù…Ù† ØªØ¯Ø±ÙŠØ¨ Ù…Ø³Ø¨Ù‚ ÙÙŠ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù…. ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø®Ù„Ø§Ù„ Ø¬Ù„Ø³Ø© Ø§Ù„Ù†Ø§Ø¯ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨ÙŠ Ù„ØªØ¹Ø²ÙŠØ² Ø§Ù„ØªÙØ§Ø¹Ù„ ÙˆØ§Ù„Ø§Ø¨ØªÙƒØ§Ø±.? [/INST]{'domain': 'Reinforcement Learning', 'api_call': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\\\", 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'edbeeching/decision-transformer-gym-walker2d-expert', 'api_call': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\\\", 'api_arguments': {'mean': [1.2384834, 0.19578537, -0.10475016, -0.18579608, 0.23003316, 0.022800924, -0.37383768, 0.337791, 3.925096, -0.0047428459, 0.025267061, -0.0039287535, -0.01736751, -0.48212224, 0.00035432147, -0.0037124525, 0.0026285544], 'std': [0.06664903, 0.16980624, 0.17309439, 0.21843709, 0.74599105, 0.02410989, 0.3729872, 0.6226182, 0.9708009, 0.72936815, 1.504065, 2.495893, 3.511518, 5.3656907, 0.79503316, 4.317483, 6.1784487]}, 'python_environment_requirements': ['transformers'], 'example_code': 'See our Blog Post, Colab notebook or Example Script for usage.', 'performance': {'dataset': 'Gym Walker2d environment', 'accuracy': 'Not provided'}, 'description': 'Decision Transformer model trained on expert trajectories sampled from the Gym Walker2d environment.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ù ØµÙˆØªÙŠ Ù…Ù† Ù†Øµ Ù…ÙƒØªÙˆØ¨. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ§Ø­Ø© Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ Ù„Ù‡Ø°Ø§ Ø§Ù„ØºØ±Ø¶. Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('google/byt5-small')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library, which include T5ForConditionalGeneration for the text-to-text generation model and AutoTokenizer for tokenizing the input and output texts.\\\\n2. Load the pre-trained model 'google/byt5-small' using T5ForConditionalGeneration.\\\\n3. Load the tokenizer from the same model name.\\\\n4. Tokenize the input French text using the tokenizer and create input tensors for the model.\\\\n5. Use the translation model to generate the translated English text.\\\\n6. Decode the model output to get the translation as a readable string.\\\", '<<<code>>>': 'from transformers import T5ForConditionalGeneration, AutoTokenizer\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'google/byt5-small\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'google/byt5-small\\\\')\\\\ninput_text = \\\"Bonjour, comment Ã§a va?\\\"\\\\ninput_ids = tokenizer(input_text, return_tensors=\\\\'pt\\\\').input_ids\\\\noutput_ids = model.generate(input_ids)\\\\ntranslation = tokenizer.decode(output_ids[0], skip_special_tokens=True)', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/byt5-small')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/byt5-small', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/byt5-small')\\\", 'api_arguments': ['input_ids', 'labels'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import T5ForConditionalGeneration, AutoTokenizer\\\\nmodel = T5ForConditionalGeneration.from_pretrained('google/byt5-small')\\\\ntokenizer = AutoTokenizer.from_pretrained('google/byt5-small')\\\\nmodel_inputs = tokenizer([Life is like a box of chocolates., Today is Monday.], padding=longest, return_tensors=pt)\\\\nlabels = tokenizer([La vie est comme une boÃ®te de chocolat., Aujourd'hui c'est lundi.], padding=longest, return_tensors=pt).input_ids\\\\nloss = model(**model_inputs, labels=labels).loss\\\", 'performance': {'dataset': 'mc4', 'accuracy': 'Not provided'}, 'description': \\\"ByT5 is a tokenizer-free version of Google's T5 and generally follows the architecture of MT5. ByT5 was only pre-trained on mC4 excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is usable on a downstream task. ByT5 works especially well on noisy text data, e.g., google/byt5-small significantly outperforms mt5-small on TweetQA.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ù„ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„ØµÙˆØ± ØºÙŠØ± Ù…Ø´Ø±ÙˆØ·Ø© ÙˆØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªÙØ§ØµÙŠÙ„ Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙˆØ§Ù‚Ø¹ÙŠØ©. ØªØ¬Ù†Ø¨ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© ÙƒÙˆØ³ÙŠÙ„Ø© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the necessary class from the diffusers package, which includes the DDPMPipeline for unconditional image generation.\\\\n2. Next, use the from_pretrained method of the DDPMPipeline class to load the pre-trained model 'WiNE-iNEFF/Minecraft-Skin-Diffusion-V2'. This model is specifically trained to generate Minecraft skin images, perfect for our client's promotional event.\\\\n3. The model can then be used to generate a new Minecraft skin image in RGBA format.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\\\\nimage = pipeline().images[0].convert('RGBA')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'WiNE-iNEFF/Minecraft-Skin-Diffusion-V2', 'api_call': \\\"DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\\\", 'api_arguments': [], 'python_environment_requirements': [['diffusers']], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\\\\nimage = pipeline().images[0].convert('RGBA')\\\\nimage\\\", 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unconditional image generation model for generating Minecraft skin images using the diffusion model.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ©. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø±Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ ØµÙØ§Øª Ø§Ù„ØµÙˆØª ÙˆØªØµÙ†ÙŠÙÙ‡Ø§ Ø¨Ø¯Ù‚Ø© ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries: 'pipeline' from transformers, 'Audio' from the IPython package, and librosa for processing audio data.\\\\n2. Load the 'superb/hubert-large-superb-er' model for audio classification using the pipeline function.\\\\n3. Analyze the audio input using the preloaded model to recognize the emotion (angry, happy, sad, neutral) in the customer's speech.\\\\n4. Based on the detected emotion, respond with the appropriate predefined response according to the emotion detected.\\\", '<<<code>>>': 'import librosa\\\\nfrom transformers import pipeline\\\\nfrom IPython.display import Audio\\\\n\\\\naudio_classifier = pipeline(\\\\'audio-classification\\\\', model=\\\\'superb/hubert-large-superb-er\\\\')\\\\n\\\\n# Load audio file\\\\naudio_file = \\\\'path_to_audio_file.wav\\\\'\\\\naudio_data, sample_rate = librosa.load(audio_file, sr=16000)\\\\n\\\\n# Classify emotion\\\\nresult = audio_classifier(audio_data)\\\\n\\\\n# Extract the detected emotion\\\\nemotion = result[0][\\\\'label\\\\']\\\\n\\\\n# Predefined responses\\\\nresponses = {\\\\n    \\\\'angry\\\\': \\\\'I understand that you are upset. Let us find a solution together.\\\\',\\\\n    \\\\'happy\\\\': \\\\'Glad to hear you are happy! Is there anything else we can help you with?\\\\',\\\\n    \\\\'sad\\\\': \\\\'I am sorry to hear that. Let us know how we can support you.\\\\',\\\\n    \\\\'neutral\\\\': \\\\'Thank you for your input. How may I assist you further?\\\\'\\\\n}\\\\n\\\\n# Select the appropriate response\\\\nresponse = responses.get(emotion, \\\\'I am unable to detect your emotion. Can you please clarify your issue?\\\\')\\\\n\\\\nprint(f\\\"User emotion detected: {emotion}\\\")\\\\nprint(f\\\"Response: {response}\\\")', 'api_call': \\\"pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'superb/hubert-large-superb-er', 'api_call': \\\"pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\", 'api_arguments': ['model'], 'python_environment_requirements': [['transformers', 'datasets', 'librosa']], 'example_code': \\\"from datasets import load_dataset\\\\nfrom transformers import pipeline\\\\ndataset = load_dataset('anton-l/superb_demo', 'er', split='session1')\\\\nclassifier = pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\\nlabels = classifier(dataset[0]['file'], top_k=5)\\\", 'performance': {'dataset': 'IEMOCAP', 'accuracy': 0.6762}, 'description': \\\"This is a ported version of S3PRL's Hubert for the SUPERB Emotion Recognition task. The base model is hubert-large-ll60k, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.\\\" </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ† Ù„Ù…Ø·ÙˆØ±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ ÙÙŠ Ø§Ù„Ù†Ø§Ø¯ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨ÙŠ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„ÙŠØ­Ø³Ù†ÙˆØ§ Ù…Ù‡Ø§Ø±Ø§ØªÙ‡Ù… ÙÙŠ ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ. ÙŠÙÙ†ØµØ­ Ø¨ØªÙˆÙÙŠØ± Ø£Ø¯Ø§Ø© Ù„Ø¹Ø¨ Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù„Ù„Ù…Ø´Ø§Ø±ÙƒÙŠÙ† Ù…Ù† Ø£Ø¬Ù„ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„ØªØ­Ø¯ÙŠ ÙˆØ§Ù„ØªØ¹Ù„Ù….? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"pipeline('sentiment-analysis', model='lvwerra/distilbert-imdb')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary components from the transformers package, specifically the 'pipeline' function.\\\\n2. Use the 'pipeline' function to load the pre-trained model, 'lvwerra/distilbert-imdb', which has been fine-tuned on the IMDb dataset for sentiment analysis of movie reviews.\\\\n3. With this model, we can classify movie reviews into positive or negative sentiment categories.\\\\n4. The model will determine the sentiment behind the movie review, giving insight into audience perception of the movie.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'sentiment-analysis\\\\', model=\\\\'lvwerra/distilbert-imdb\\\\')\\\\nreview = \\\"I absolutely loved this movie, the acting was fantastic\\\"\\\\nresult = classifier(review)', 'api_call': \\\"pipeline('sentiment-analysis', model='lvwerra/distilbert-imdb')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lvwerra/distilbert-imdb', 'api_call': \\\"pipeline('sentiment-analysis', model='lvwerra/distilbert-imdb')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers', 'pytorch']], 'example_code': \\\"classifier('I love this movie!')\\\", 'performance': {'dataset': 'imdb', 'accuracy': 0.928}, 'description': 'This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙŠØ³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© Ù…Ù„Ø£ Ø§Ù„ÙØ¬ÙˆØ©. Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='roberta-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the 'pipeline' function to create an unmasker object that takes a masked text as input and predicts the missing word(s).\\\\n3. Specify the 'roberta-base' model for the 'fill-mask' task in the pipeline.\\\\n4. You can now use the unmasker object to process user input and generate predictions for the missing word in the given sentence.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'roberta-base\\\\')\\\\nmasked_sentence = \\\"The adventurer entered the <mask> room.\\\"\\\\npredicted_words = unmasker(masked_sentence)', 'api_call': \\\"pipeline('fill-mask', model='roberta-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'roberta-base', 'api_call': \\\"pipeline('fill-mask', model='roberta-base')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='roberta-base')\\\\nunmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': [{'name': 'MNLI', 'accuracy': 87.6}, {'name': 'QQP', 'accuracy': 91.9}, {'name': 'QNLI', 'accuracy': 92.8}, {'name': 'SST-2', 'accuracy': 94.8}, {'name': 'CoLA', 'accuracy': 63.6}, {'name': 'STS-B', 'accuracy': 91.2}, {'name': 'MRPC', 'accuracy': 90.2}, {'name': 'RTE', 'accuracy': 78.7}]}, 'description': 'RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using the Masked language modeling (MLM) objective. This model is case-sensitive and can be fine-tuned on a downstream task.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ‚Ø¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ù…Ø¹ÙŠÙ†Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary classes from the transformers package, which includes DPRQuestionEncoder for the question encoding model and DPRQuestionEncoderTokenizer for tokenizing the input text.\\\\n2. We use the from_pretrained method of the DPRQuestionEncoder and DPRQuestionEncoderTokenizer classes to load the pre-trained model 'facebook/dpr-question_encoder-single-nq-base'.\\\\n3. This model has been trained for question answering tasks, and it can convert a given question into a dense vector representation.\\\\n4. The input question is tokenized, and the tokens are passed to the model, returning the dense vector representation, which can be used for further processing in the question-answering pipeline.\\\", '<<<code>>>': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\\\\'facebook/dpr-question_encoder-single-nq-base\\\\')\\\\nmodel = DPRQuestionEncoder.from_pretrained(\\\\'facebook/dpr-question_encoder-single-nq-base\\\\')\\\\nquestion = \\\"Hello, is my dog cute?\\\"\\\\ninput_ids = tokenizer(question, return_tensors=\\\\'pt\\\\')[\\\\'input_ids\\\\']\\\\nembeddings = model(input_ids).pooler_output', 'api_call': \\\"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \\\"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\\\", 'api_arguments': ['input_ids'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… ÙŠÙ…ÙƒÙ† ÙÙ‡Ù…Ù‡ Ù…Ù† Ù‚Ø¨Ù„ Ù…Ø³ØªØ®Ø¯Ù…ÙŠ ØªØ·Ø¨ÙŠÙ‚Ù†Ø§ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a translation model for English to Arabic translation using the pre-trained model 'Helsinki-NLP/opus-mt-en-ar'.\\\\n3. In order to provide an automated financial report in Arabic, the English text data from the financial report should be passed through the created translation pipeline. This will result in a translated version of the report in Arabic.\\\\n4. Share the translated report with the Middle East clients.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntranslator = pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\\narabic_financial_report = translator(english_financial_report_text)\\\", 'api_call': \\\"pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-en-ar', 'api_call': \\\"pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\\ntranslated_text = translation('Hello World')\\\", 'performance': {'dataset': 'Tatoeba-test.eng.ara', 'accuracy': {'BLEU': 14.0, 'chr-F': 0.437}}, 'description': \\\"A Hugging Face Transformers model for English to Arabic translation, trained on the Tatoeba dataset. It uses a transformer architecture and requires a sentence initial language token in the form of '>>id<<' (id = valid target language ID).\\\" </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"pipeline('text-generation', model='distilgpt2')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. Import the required libraries 'pipeline' and 'set_seed' from the transformers library.\\\", 'code': \\\"from transformers import pipeline, set_seed\\\\ngenerator = pipeline('text-generation', model='distilgpt2')\\\\nset_seed(42)\\\\nprompt = 'In a world of advanced technology,'\\\\nideas = generator(prompt, max_length=50, num_return_sequences=5)\\\", 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Transformers', 'functionality': 'Text Generation', 'api_name': 'distilgpt2', 'api_call': \\\"from transformers import pipeline, set_seed\\\\ngenerator = pipeline('text-generation', model='distilgpt2')\\\\nset_seed(42)\\\\nprompt = 'In a world of advanced technology,'\\\\nideas = generator(prompt, max_length=50, num_return_sequences=5)\\\", 'api_arguments': ['model'], 'python_environment_requirements': ['from transformers import pipeline, set_seed'], 'example_code': 'set_seed(42)\\\\ngenerator(Hello, Iâ€™m a language model, max_length=20, num_return_sequences=5)', 'performance': {'dataset': 'WikiText-103', 'accuracy': '21.100'}, 'description': 'DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. With 82 million parameters, it was developed using knowledge distillation and designed to be a faster, lighter version of GPT-2. It can be used for text generation, writing assistance, creative writing, entertainment, and more.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ ØªØ¹Ù„ÙŠÙ…Ø© ØµÙˆØªÙŠØ© Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ØªØ¨Ø¯Ø£ Ø¨Ø¬Ù…Ù„Ø© \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8m-nlf-head-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries which are 'YOLO' and 'render_result' from ultralyticsplus.\\\\n2. Use the YOLO class to load the pre-trained model 'keremberke/yolov8m-nlf-head-detection'. This model has been trained for object detection tasks, particularly for detecting helmets in American football footage.\\\\n3. Set the appropriate parameter overrides, such as confidence, Intersection over Union (IOU), and maximum number of detections.\\\\n4. Load the image or video frame from the football game.\\\\n5. Use the model's 'predict' method to detect helmets in the footage.\\\\n6. Optionally, use the 'render_result' function to visualize the detected helmets by overlaying bounding boxes on the image or video frame.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO('keremberke/yolov8m-nlf-head-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage_url = 'http://url_to_image_or_video_frame'\\\\n\\\\nresults = model.predict(image_url)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image_url, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8m-nlf-head-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8m-nlf-head-detection', 'api_call': \\\"YOLO('keremberke/yolov8m-nlf-head-detection')\\\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.24 ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8m-nlf-head-detection')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'nfl-object-detection', 'accuracy': 0.287}, 'description': 'A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ­ØªØ§Ø¬ Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø¬Ø¯ÙˆÙ„ÙŠØ©. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚Ø§Ø¯Ø±Ù‹Ø§ Ø¹Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. First, import the necessary libraries, which include 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'google/tapas-medium-finetuned-wtq'.\\\\n3. The loaded model will be used for table question answering, which is designed to answer questions based on a table with data in it.\\\\n4. Convert your school grade report into tabular data format and use the model to ask questions and analyze your academic performance.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntable_qa = pipeline(\\\\'table-question-answering\\\\', model=\\\\'google/tapas-medium-finetuned-wtq\\\\')\\\\n# Define table data\\\\ntable = [{\\\\n            \\\"Subject\\\": \\\"Math\\\",\\\\n            \\\"Score\\\": 85,\\\\n            \\\"Grade\\\": \\\"B\\\",\\\\n        },{\\\\n            \\\"Subject\\\": \\\"History\\\",\\\\n            \\\"Score\\\": 70,\\\\n            \\\"Grade\\\": \\\"C\\\",\\\\n        },{\\\\n            \\\"Subject\\\": \\\"Physics\\\",\\\\n            \\\"Score\\\": 91,\\\\n            \\\"Grade\\\": \\\"A\\\",\\\\n    }]\\\\nquery = \\\"What is my average score?\\\"\\\\nanswer = table_qa(table=table, query=query)', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/tapas-medium-finetuned-wtq', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", 'api_arguments': 'table, query', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntable_qa = pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\\n\\\\n# Define the table and the query\\\\ntable = {...}\\\\nquery = '...'\\\\n\\\\n# Get the answer\\\\nanswer = table_qa(table=table, query=query)\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.4324}, 'description': 'TAPAS medium model fine-tuned on WikiTable Questions (WTQ). This model is pretrained on a large corpus of English data from Wikipedia and is used for answering questions related to a table.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­Ù† Ù†Ø¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ© ÙˆØ¥Ù†Ø´Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ ØªØµÙ†ÙŠÙ ØªÙ„Ùƒ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ ØªØ¹Ù„ÙŠÙ…Ø§Øª ØªÙØ§Ø¹Ù„ÙŠØ© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£ÙØ¶Ù„ Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. Import the required libraries: joblib, pandas, and json.\\\\n2. Load the saved model using joblib.load function and the provided model path ('model.joblib').\\\\n3. Load the configuration file containing the selected features using json.load and open functions.\\\\n4. Read the input tabular data as a pandas DataFrame.\\\\n5. Filter the DataFrame to include only the features specified in the configuration file.\\\\n6. Rename the DataFrame columns to match the format expected by the model.\\\\n7. Use the trained model to make predictions on the input data and obtain results as a numpy array.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nimport json\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\n\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = [feat_ + str(col) for col in data.columns]\\\\n\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pachi107/autotrain-in-class-test-1780161764', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data.csv'], 'python_environment_requirements': [['json', 'joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv(data.csv)\\\\ndata = data[features]\\\\ndata.columns = [feat_ + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'Validation Metrics', 'accuracy': 0.974}, 'description': 'A binary classification model for predicting CO2 emissions based on tabular data. Trained using AutoTrain with a model ID of 1780161764.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ØµÙˆØ± Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„ØªØµÙˆÙŠØ±ÙŠØ© Ø§Ù„ØºÙŠØ± Ù…ØµÙ†ÙØ© Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø§Ù„Ø¨Ø±ÙŠØ© Ø¥Ù„Ù‰ Ù…Ù†Ø§Ø·Ù‚ Ù…Ø®ØªÙ„ÙØ©. Ù‡Ù„ ØªØ³ØªØ·ÙŠØ¹ Ù…Ø³Ø§Ø¹Ø¯ØªÙ†Ø§ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers and PIL packages. This includes MaskFormerFeatureExtractor and MaskFormerForInstanceSegmentation for the image segmentation model and Image for processing image data.\\\\n2. We load an image from a URL or a file (replace with the path to a satellite image of a city).\\\\n3. Import MaskFormerFeatureExtractor.from_pretrained method and create the feature_extractor, using the pre-trained model 'facebook/maskformer-swin-base-ade'. This model has been trained for image segmentation tasks.\\\\n4. This model can then be used to analyze a satellite image and segment it into distinct land-use categories.\\\", '<<<code>>>': \\\"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\\\nfrom PIL import Image\\\\n\\\\nimage = Image.open('satellite_image_path.jpg')\\\\n# replace 'satellite_image_path.jpg' with path to your satellite image \\\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-ade')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\\\\noutputs = model(**inputs)\\\\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\", 'api_call': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/maskformer-swin-base-ade', 'api_call': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\\\", 'api_arguments': {'from_pretrained': 'facebook/maskformer-swin-base-ade'}, 'python_environment_requirements': [{'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}], 'example_code': 'from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\\\\noutputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.' </s>\"\n"}
{"text": "<s>[INST]Ø¹Ù†Ø¯Ù…Ø§ ØªØ¨Ø¯Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†ØµÙˆØµØŒ Ø§Ø­Ø±Øµ Ø¹Ù„Ù‰ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ù„Ø¶Ù…Ø§Ù† ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù‚Ø¹ÙŠØ©. Ù‚Ù… Ø¨ØªÙˆÙ„ÙŠØ¯ Ø¬Ù…Ù„ ÙˆÙÙ‚Ø±Ø§Øª ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø­ØµØ§Ø¡ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ø²ÙŠØ§Ø¯Ø© Ø¯Ù‚Ø© Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"pipeline('text-generation', model='Pi3141/DialoGPT-medium-elon-3')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We will begin by importing the pipeline function from the transformers library.', 'code': \\\"from transformers import pipeline\\\\nelon_chatbot = pipeline('text-generation', model='Pi3141/DialoGPT-medium-elon-3')\\\\n# Replace 'your_question_here' with your actual question or conversation starter\\\\ninput_text = 'your_question_here'\\\\ngenerated_text = elon_chatbot(input_text)[0]['generated_text']\\\", 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Conversational', 'api_name': 'Pi3141/DialoGPT-medium-elon-3', 'api_call': \\\"from transformers import pipeline\\\\nelon_chatbot = pipeline('text-generation', model='Pi3141/DialoGPT-medium-elon-3')\\\\n# Replace 'your_question_here' with your actual question or conversation starter\\\\ninput_text = 'your_question_here'\\\\ngenerated_text = elon_chatbot(input_text)[0]['generated_text']\\\", 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.', 'performance': {'dataset': 'Twitter tweets by Elon Musk', 'accuracy': 'N/A'}, 'description': 'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ© Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ø±ØºØ¨ ÙÙŠ ØªØ­Ø±ÙŠ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ù„Ø°ÙŠ ØªÙ‚Ø¯Ù… Ø¹Ù„ÙŠÙ‡ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fi-en')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. To translate the Finnish paragraph to English, we need to import the relevant classes from the transformers library. Import AutoTokenizer and AutoModelForSeq2SeqLM.\\\\n2. Create a tokenizer to tokenize the Finnish input text using the AutoTokenizer.from_pretrained() method and specifying the pre-trained model 'Helsinki-NLP/opus-mt-fi-en'.\\\\n3. Load the pre-trained translation model from 'Helsinki-NLP/opus-mt-fi-en' using the AutoModelForSeq2SeqLM.from_pretrained() method.\\\\n4. Tokenize the Finnish input text and pass it to the loaded translation model.\\\\n5. Decode the output tensor from the model and obtain the translated English text.\\\", '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-fi-en')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fi-en')\\\\ninput_text = 'TÃ¤mÃ¤ on esimerkki tekstistÃ¤.'\\\\ninputs = tokenizer.encode(input_text, return_tensors='pt')\\\\noutputs = model.generate(inputs)\\\\ntranslation = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\", 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fi-en')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-fi-en', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fi-en')\\\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-fi-en')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fi-en')\\\\ninput_text = 'TÃ¤mÃ¤ on esimerkki tekstistÃ¤.'\\\\ninputs = tokenizer.encode(input_text, return_tensors='pt')\\\\noutputs = model.generate(inputs)\\\\ntranslation = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\", 'performance': {'dataset': [{'name': 'newsdev2015-enfi-fineng.fin.eng', 'accuracy': 'BLEU: 25.3, chr-F: 0.536'}, {'name': 'newstest2015-enfi-fineng.fin.eng', 'accuracy': 'BLEU: 26.9, chr-F: 0.547'}, {'name': 'newstest2016-enfi-fineng.fin.eng', 'accuracy': 'BLEU: 29.0, chr-F: 0.571'}, {'name': 'newstest2017-enfi-fineng.fin.eng', 'accuracy': 'BLEU: 32.3, chr-F: 0.594'}, {'name': 'newstest2018-enfi-fineng.fin.eng', 'accuracy': 'BLEU: 23.8, chr-F: 0.517'}, {'name': 'newstest2019-fien-fineng.fin.eng', 'accuracy': 'BLEU: 29.0, chr-F: 0.565'}, {'name': 'newstestB2016-enfi-fineng.fin.eng', 'accuracy': 'BLEU: 24.5, chr-F: 0.527'}, {'name': 'newstestB2017-enfi-fineng.fin.eng', 'accuracy': 'BLEU: 27.4, chr-F: 0.557'}, {'name': 'newstestB2017-fien-fineng.fin.eng', 'accuracy': 'BLEU: 27.4, chr-F: 0.557'}, {'name': 'Tatoeba-test.fin.eng', 'accuracy': 'BLEU: 53.4, chr-F: 0.697'}]}, 'description': 'Helsinki-NLP/opus-mt-fi-en is a machine translation model for translating Finnish text to English text. It is trained on the OPUS dataset and can be used with the Hugging Face Transformers library.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø³ØªØ·ÙŠØ¹ ØªÙ†Ø¸ÙŠÙ… Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªØ¯Ø±ÙŠØ¨ÙŠ Ø¯ÙˆØ±ÙŠ Ù„Ù…Ù„Ø®Øµ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§Ø±ÙƒÙŠÙ† ØªØ·Ø¨ÙŠÙ‚ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ù„Ø®Øµ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ØªÙ†ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡. ÙƒÙ…ÙƒÙˆÙ† Ø¥Ø¶Ø§ÙÙŠØŒ ÙŠØ¬Ø¨ Ø§Ù‚ØªØ±Ø§Ø­ Ù…Ø³Ø§Ø¨Ù‚Ø© Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ØªØªÙ… Ø§Ø®ØªÙŠØ§Ø±Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù„ØªØ¹Ø²ÙŠØ² Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. First, import the appropriate model (BarthezModel) from the Transformers library provided by Hugging Face.', 'code': 'from transformers import BarthezModel, BarthezTokenizer\\\\ntokenizer = BarthezTokenizer.from_pretrained(\\\\'moussaKam/barthez-orangesum-abstract\\\\')\\\\nmodel = BarthezModel.from_pretrained(\\\\'moussaKam/barthez-orangesum-abstract\\\\')\\\\n\\\\narticle = \\\"Les ventes de voitures Ã©lectriques ont connu une hausse significative tout au long de l\\\\'annÃ©e derniÃ¨re en France. Selon les donnÃ©es rÃ©centes, le nombre de voitures Ã©lectriques vendues a doublÃ© par rapport Ã  l\\\\'annÃ©e prÃ©cÃ©dente.\\\"\\\\n\\\\ninputs = tokenizer(article, return_tensors=\\\\'pt\\\\', max_length=512, truncation=True)\\\\nsummary_ids = model.generate(inputs[\\\\'input_ids\\\\'], num_beams=4, max_length=100, early_stopping=True)\\\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'moussaKam/barthez-orangesum-abstract', 'api_call': 'from transformers import BarthezModel, BarthezTokenizer\\\\ntokenizer = BarthezTokenizer.from_pretrained(\\\\'moussaKam/barthez-orangesum-abstract\\\\')\\\\nmodel = BarthezModel.from_pretrained(\\\\'moussaKam/barthez-orangesum-abstract\\\\')\\\\n\\\\narticle = \\\"Les ventes de voitures Ã©lectriques ont connu une hausse significative tout au long de l\\\\'annÃ©e derniÃ¨re en France. Selon les donnÃ©es rÃ©centes, le nombre de voitures Ã©lectriques vendues a doublÃ© par rapport Ã  l\\\\'annÃ©e prÃ©cÃ©dente.\\\"\\\\n\\\\ninputs = tokenizer(article, return_tensors=\\\\'pt\\\\', max_length=512, truncation=True)\\\\nsummary_ids = model.generate(inputs[\\\\'input_ids\\\\'], num_beams=4, max_length=100, early_stopping=True)\\\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)', 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'orangeSum', 'accuracy': ''}, 'description': 'Barthez model finetuned on orangeSum for abstract generation in French language'}} </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ÙˆØ§Ø±Ø¯Ø© Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙØ§Øª Ù…Ø­Ø¯Ø¯Ø© Ù…Ø«Ù„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±ØŒ Ø§Ù„Ø±ÙŠØ§Ø¶Ø©ØŒ ÙˆØ§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"pipeline('text-classification', model='roberta-base-openai-detector')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a text classification model, which will detect whether the input text was likely generated by an AI model.\\\\n3. Specify the model 'roberta-base-openai-detector' to be loaded. This model is specifically designed for detecting GPT-2 generated text.\\\\n4. Pass the text in question to the classifier to receive the likelihood score that the text is AI-generated or not.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ndetector = pipeline(\\\"text-classification\\\", model=\\\\'roberta-base-openai-detector\\\\')\\\\ntext = \\\"The importance of maintaining a balanced lifestyle cannot be overstated. Without balance in our lives, we can quickly become overwhelmed by life\\\\'s challenges and setbacks. A healthy balance enables us to better manage stress, maintain a positive mindset, and achieve our goals. Prioritizing self-care and personal growth can lead to more satisfying and fulfilling experiences, both in our personal and professional lives.\\\"\\\\nresult = detector(text)', 'api_call': \\\"pipeline('text-classification', model='roberta-base-openai-detector')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'roberta-base-openai-detector', 'api_call': \\\"pipeline('text-classification', model='roberta-base-openai-detector')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\npipe = pipeline(text-classification, model=roberta-base-openai-detector)\\\\nprint(pipe(Hello world! Is this content AI-generated?))', 'performance': {'dataset': 'WebText', 'accuracy': '95%'}, 'description': 'RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙˆØªÙŠ Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ù…ÙˆØ¬Ø²Ø©ØŒ ÙˆÙ†Ø­Ù† Ù†Ø±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"pipeline('summarization', model='Samuel-Fipps/t5-efficient-large-nl36_fine_tune_sum_V2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"pipeline('summarization', model='Samuel-Fipps/t5-efficient-large-nl36_fine_tune_sum_V2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 't5-efficient-large-nl36_fine_tune_sum_V2', 'api_call': \\\"pipeline('summarization', model='Samuel-Fipps/t5-efficient-large-nl36_fine_tune_sum_V2')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': [{'name': 'samsum', 'accuracy': {'ROUGE-1': 54.933, 'ROUGE-2': 31.797, 'ROUGE-L': 47.006, 'ROUGE-LSUM': 51.203, 'loss': 1.131, 'gen_len': 23.799}}, {'name': 'cnn_dailymail', 'accuracy': {'ROUGE-1': 34.406, 'ROUGE-2': 14.127, 'ROUGE-L': 24.335, 'ROUGE-LSUM': 31.658, 'loss': 2.446, 'gen_len': 45.928}}]}, 'description': \\\"A T5-based summarization model trained on the Samsum dataset. This model can be used for text-to-text generation tasks such as summarization without adding 'summarize' to the start of the input string. It has been fine-tuned for 10K steps with a batch size of 10.\\\" </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù…Ù„. Ù‡Ù„ ØªØ³ØªØ·ÙŠØ¹ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/gtr-t5-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the SentenceTransformer class from the sentence-transformers library provided by Hugging Face.\\\\n2. Initialize a SentenceTransformer instance by specifying the model 'sentence-transformers/gtr-t5-base' to be loaded. This is a model that has been trained on a large corpus of text, specifically for the task of semantic search.\\\\n3. Use the model's encode method to convert your list of sentences into a high-dimensional vector space.\\\\n4. Once the sentences are encoded into their corresponding vectors in the high-dimensional space, similarity between sentences can be calculated using metrics such as cosine similarity or euclidean distance. High similarity indicates similar content in the sentences.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nsentences = [\\\"This product is amazing\\\", \\\"I love the quality of this item\\\", \\\"The delivery service was fast\\\"]\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/gtr-t5-base\\\\')\\\\nembeddings = model.encode(sentences)', 'api_call': \\\"SentenceTransformer('sentence-transformers/gtr-t5-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/gtr-t5-base', 'api_call': \\\"SentenceTransformer('sentence-transformers/gtr-t5-base')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': [['sentence-transformers>=2.2.0']], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/gtr-t5-base')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'N/A'}, 'description': 'This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of semantic search.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ù„ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙŠØ¨Ø¯Ø£ Ø§Ù„Ù†Ø§ØªØ¬ Ø¨Ø¬Ù…Ù„Ø© \\\"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries for text generation, which are 'AutoTokenizer' and 'AutoModelForCausalLM' from the transformers library.\\\\n2. Load the 'Salesforce' code generation model, 'codegen-2B-multi', pretrained to generate programming code.\\\\n3. Provide a text prompt describing the desired Python program.\\\\n4. Use the tokenizer to convert the text prompt into tokens suitable for the model. Generate a Python program based on these tokens.\\\\n5. Run the generated Python code for a demonstration purpose.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Salesforce/codegen-2B-multi\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'Salesforce/codegen-2B-multi\\\\')\\\\ntext_prompt = \\\"Write a Python program to check if a number is even or odd.\\\"\\\\ninput_ids = tokenizer(text_prompt, return_tensors=\\\"pt\\\").input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\ngenerated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\\\\nprint(generated_code)', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Salesforce/codegen-2B-multi', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\\\", 'api_arguments': {'input_ids': 'input_ids', 'max_length': 128}, 'python_environment_requirements': [{'transformers': 'AutoTokenizer, AutoModelForCausalLM'}], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-2B-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-2B-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval, MTPB'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B). The checkpoint included in this repository is denoted as CodeGen-Multi 2B, where Multi means the model is initialized with CodeGen-NL 2B and further pre-trained on a dataset of multiple programming languages, and 2B refers to the number of trainable parameters.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªØŒ Ø§Ø¬Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ØªÙ†ÙˆØ¹Ø© Ù…Ù† Ø¹ÙŠÙ†Ø§Øª Ø§Ù„ØµÙˆØª Ø§Ù„Ù…ØµÙ†ÙØ©. Ø«Ù… Ù‚Ù… Ø¨ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ø¥Ù„Ù‰ ÙØ¦Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯ÙˆØ§Øª ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØª Ø§Ù„Ù…ØªØ§Ø­Ø©. Ù‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ù„Ø§Ù…Ø© ÙˆÙ‚ÙŠÙ… Ø£Ø¯Ø§Ø¤Ù‡ Ø¨Ø¯Ù‚Ø© ÙˆØ§Ø³ØªØ¬Ø§Ø¨ØªÙ‡ Ù„Ù…Ø®ØªÙ„Ù Ø§Ù„Ø£ØµÙˆØ§Øª. ÙˆØ£Ø®ÙŠØ±Ø§Ù‹ØŒ Ù‚Ù… Ø¨ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£ÙØ¶Ù„ Ù†ØªØ§Ø¦Ø¬.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\", '<<<api_provider>>>': 'SpeechBrain', '<<<explanation>>>': \\\"1. Start by importing the foreign_class from the speechbrain.pretrained.interfaces package.\\\\n2. Use the foreign_class function to load the pretrained model 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP'. This will create a CustomEncoderWav2vec2Classifier instance, which can detect emotions in speech audio.\\\\n3. Use the classify_file method of this classifier to analyze a user's spoken audio. This function will return the emotion probabilities, scores, indexes, and text labels for the detected emotions.\\\\n4. You can use the obtained text labels and other relevant information to react appropriately to the user's emotional state while delivering counseling services.\\\", '<<<code>>>': \\\"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\\nout_prob, score, index, text_lab = classifier.classify_file(user_audio_file_path)\\\\nprint(text_lab)\\\", 'api_call': \\\"foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\", 'provider': 'SpeechBrain', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP', 'api_call': \\\"foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\", 'api_arguments': ['file_path'], 'python_environment_requirements': ['pip install speechbrain'], 'example_code': \\\"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\\nout_prob, score, index, text_lab = classifier.classify_file('speechbrain/emotion-recognition-wav2vec2-IEMOCAP/anger.wav')\\\\nprint(text_lab)\\\", 'performance': {'dataset': 'IEMOCAP', 'accuracy': '78.7%'}, 'description': 'This repository provides all the necessary tools to perform emotion recognition with a fine-tuned wav2vec2 (base) model using SpeechBrain. It is trained on IEMOCAP training data.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± ØºÙŠØ± Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline class from transformers which is provided by Hugging Face.\\\\n2. Create a zero-shot-image-classification pipeline with the model 'Bingsu/clip-vit-base-patch32-ko'.\\\\n3. The provided model is trained for Korean CLIP and can be used for zero-shot image classification tasks without prior training data.\\\\n4. The user provides a URL of the image, and the model will return the plant genus based on the provided candidate_labels.\\\\n5. Add the plant genera you want to classify as candidate_labels.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nplant_classifier = pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\\\\nimage_url = 'https://example.com/user_plant_image.jpg'\\\\ncandidate_labels = ['acer', 'quercus', 'pinus', 'betula', 'fagus']\\\\nplant_genus = plant_classifier(images=image_url, candidate_labels=candidate_labels, hypothesis_template='{}')\\\", 'api_call': \\\"pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'clip-vit-base-patch32-ko', 'api_call': \\\"pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\\\", 'api_arguments': {'images': 'url', 'candidate_labels': 'Array of strings', 'hypothesis_template': 'String'}, 'python_environment_requirements': [['transformers', 'torch', 'PIL', 'requests']], 'example_code': \\\"from transformers import pipeline\\\\nrepo = 'Bingsu/clip-vit-base-patch32-ko'\\\\npipe = pipeline('zero-shot-image-classification', model=repo)\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nresult = pipe(images=url, candidate_labels=[], hypothesis_template='{}')\\\\nresult\\\", 'performance': {'dataset': 'AIHUB', 'accuracy': 'Not provided'}, 'description': 'Korean CLIP model trained by Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. It is a zero-shot image classification model that can be used to classify images without any training data.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ ØµÙˆØ± Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ Ø­ÙŠØ« Ù†Ù‚ÙˆÙ… Ø¨ØªØ­Ø¯ÙŠØ¯ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ØµÙˆØ± Ù„ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒØ´Ù.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which in this case include the YOLOv8 class from ultralyticsplus.\\\\n2. Initialize the YOLOv8 object detection model by specifying the model name 'keremberke/yolov8s-csgo-player-detection'.\\\\n3. Modify appropriate configurations like confidence threshold, IOU threshold, agnostic NMS, and max_det to fine-tune the object detection process.\\\\n4. Once the model is loaded, use the 'predict' method to analyze the input image and detect players in the image.\\\\n5. The detected players' coordinates can be extracted from the 'boxes' attribute in the returned results.\\\\n6. Optionally, render the result by overlaying the detected boxes onto the input image using the 'render_result' function.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage_url = 'https://example.com/csgo_gameplay_image.jpg'\\\\nresults = model.predict(image_url)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image_url, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \\\"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\\\" </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ù„ÙƒØ´Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª.? [/INST]{'domain': 'Computer Vision Object Detection', 'api_call': \\\"YOLO('keremberke/yolov8m-table-extraction')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. First, we import the YOLO class and the render_result function from the ultralyticsplus package.', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-table-extraction')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path_to_scanned_document.jpg'\\\\n# replace 'path_to_scanned_document.jpg' with the path to your scanned document\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\", 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8m-table-extraction', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-table-extraction')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path_to_scanned_document.jpg'\\\\n# replace 'path_to_scanned_document.jpg' with the path to your scanned document\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-table-extraction')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'table-extraction', 'accuracy': 0.952}, 'description': 'A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset.'}} </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªÙˆÙÙŠØ± Ù…Ù„ÙØ§Øª ØµÙˆØªÙŠØ© Ù„Ù„Ø¥Ø¯Ø®Ø§Ù„ ÙˆØ§Ù„Ø¥Ø®Ø±Ø§Ø¬ØŒ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØµÙˆØª Ø§Ù„Ø°ÙŠ ØªÙˆØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬Ù‡ ÙˆÙ†Ø³Ø®ØªÙ‡ Ø§Ù„ØµÙˆØªÙŠØ©.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. The pipeline function is used to create an audio-source-separation model, which is capable of separating individual voices from a mixture of audio signals. This model, 'Awais/Audio_Source_Separation', has been trained on the Libri2Mix dataset.\\\\n3. We then provide the podcast audio file as input to the model, which will perform the source separation.\\\\n4. The resulting separated audio sources can then be individually saved and analyzed as required.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nseparator = pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\\\\nseparated_audio_sources = separator(create_audio_patch(file_path))\\\\n# Replace 'file_path' with the path to your audio file\\\", 'api_call': \\\"pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Awais/Audio_Source_Separation', 'api_call': \\\"pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\\\", 'api_arguments': 'audio', 'python_environment_requirements': ['Asteroid'], 'example_code': '', 'performance': {'dataset': 'Libri2Mix', 'accuracy': {'si_sdr': 14.764543634468069, 'si_sdr_imp': 14.764029375607246, 'sdr': 15.29337970745095, 'sdr_imp': 15.114146605113111, 'sir': 24.092904661115366, 'sir_imp': 23.913669683141528, 'sar': 16.06055906916849, 'sar_imp': -51.980784441287454, 'stoi': 0.9311142440593033, 'stoi_imp': 0.21817376142710482}}, 'description': 'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚Ø¯Ù… Ø´Ø±ÙƒØªÙ†Ø§ Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù…Ø¹ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ©ØŒ ÙˆÙ†ÙˆØ¯ ØªÙ‚ÙŠÙŠÙ… Ù‚Ø¯Ø±Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆÙ…ÙÙ‡ÙˆÙ…Ø© Ø¨Ø¯Ù‚Ø© ÙˆØ³Ù„Ø§Ø³Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='roberta-large')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We will first import the necessary library from the transformers package, which includes the pipeline function for creating a model pipeline.\\\\n2. Then, we use the pipeline function to create a fill-mask model using the 'roberta-large' pre-trained model. This model is designed to predict suitable words for masked positions in a given sentence.\\\\n3. The created unmasker can be used to suggest appropriate words for the missing positions in the input sentence.\\\\n4. Users can provide their sentences with '<mask>' symbol representing the missing word, and the trained model will generate candidates for the masked positions.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'roberta-large\\\\')\\\\nsentence = \\\"The weather today is <mask> and beautiful.\\\"\\\\nsuggestions = unmasker(sentence)', 'api_call': \\\"pipeline('fill-mask', model='roberta-large')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'roberta-large', 'api_call': \\\"pipeline('fill-mask', model='roberta-large')\\\", 'api_arguments': {'model': 'roberta-large'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='roberta-large')\\\\nunmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': 'GLUE', 'accuracy': {'MNLI': 90.2, 'QQP': 92.2, 'QNLI': 94.7, 'SST-2': 96.4, 'CoLA': 68.0, 'STS-B': 96.4, 'MRPC': 90.9, 'RTE': 86.6}}, 'description': 'RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using the Masked language modeling (MLM) objective. It can be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ø·Ø§Ù‚Ù†Ø§ ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù†ØµÙˆØµ ÙˆØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ù„Ø¥Ù†ØªØ§Ø¬ Ù…Ù„Ø®ØµØ§Øª Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø­ÙˆØ³Ø¨Ø©ØŒ ÙˆØ³ÙŠØªÙ… Ø¥ØªÙ‚Ø§Ù† ØªØµÙ…ÙŠÙ… Ù†Ù…Ø§Ø°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ ÙˆÙŠØ¬Ø¨ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„ ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙÙŠ ØªÙ„Ùƒ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To perform automatic summarization of articles, you will need to import the 'pipeline' function from the transformers package.\\\\n2. Create an instance of the summarization pipeline using the pre-trained 'google/pegasus-newsroom' model, which has been specifically trained for summarization tasks on news-related datasets.\\\\n3. To use the pipeline for summarization, simply call it with the text of an article, and it will return a summarized version.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'google/pegasus-newsroom\\\\')\\\\narticle_text = \\\"Insert the full article text here...\\\"\\\\nsummary = summarizer(article_text, max_length=100, min_length=10, do_sample=False)[0][\\\\'summary_text\\\\']', 'api_call': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/pegasus-newsroom', 'api_call': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': 'newsroom', 'accuracy': '45.98/34.20/42.18'}, 'description': 'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks.' </s>\"\n"}
{"text": "<s>[INST]ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø¹Ø·Ø§Ø©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. We import the necessary function (pipeline) from the transformers library provided by Hugging Face.\\\\n2. We use the pipeline function, specifying the task as \\\\'text2text-generation\\\\' and the model \\\\'pszemraj/flan-t5-large-grammar-synthesis\\\\'. This model has been fine-tuned for grammar correction tasks, providing improved language quality.\\\\n3. We pass the raw input text with incorrect grammar/spelling to the pipeline(\\\"text2text-generation\\\", \\\"pszemraj/flan-t5-large-grammar-synthesis\\\") function to generate corrected, more professional text.', '<<<code>>>': \\\"from transformers import pipeline\\\\ncorrector = pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\\nraw_text = 'I haz some truble with mi code. I waz workin on a projct n I kep gitin errorz. Can u pls luk at it and hlp mi fix it? Thanx in advanc.'\\\\nresult = corrector(raw_text)\\\\ncorrected_text = result[0]['generated_text']\\\", 'api_call': \\\"pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pszemraj/flan-t5-large-grammar-synthesis', 'api_call': \\\"pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\", 'api_arguments': ['raw_text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\n\\\\ncorrector = pipeline(\\\\n 'text2text-generation',\\\\n 'pszemraj/flan-t5-large-grammar-synthesis',\\\\n )\\\\n\\\\nraw_text = 'i can has cheezburger'\\\\nresults = corrector(raw_text)\\\\nprint(results)\\\", 'performance': {'dataset': 'jfleg', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of google/flan-t5-large for grammar correction on an expanded version of the JFLEG dataset.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØ¬Ø¨ ØªØµÙ…ÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©ØŒ ÙˆÙ‡Ùˆ Ø£Ù…Ø± Ø­Ø§Ø³Ù… Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ¯Ù‚Ø© Ø§Ù„Ù†Ø¸Ø§Ù….? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary libraries, including 'AutoModelForVideoClassification' from transformers and 'torch' library for handling video tensors.\\\", 'code': 'import torch\\\\nfrom transformers import AutoModelForVideoClassification\\\\nfrom preprocessing import preprocess_video\\\\n\\\\nmodel = AutoModelForVideoClassification.from_pretrained(\\\\'lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2\\\\')\\\\nvideo_path = \\\"path/to/your/video.mp4\\\"\\\\nprocessed_video = preprocess_video(video_path)\\\\npredictions = model(processed_video)\\\\n\\\\n# Analyze the output to determine the action being performed in the video.', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2', 'api_call': 'import torch\\\\nfrom transformers import AutoModelForVideoClassification\\\\nfrom preprocessing import preprocess_video\\\\n\\\\nmodel = AutoModelForVideoClassification.from_pretrained(\\\\'lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2\\\\')\\\\nvideo_path = \\\"path/to/your/video.mp4\\\"\\\\nprocessed_video = preprocess_video(video_path)\\\\npredictions = model(processed_video)\\\\n\\\\n# Analyze the output to determine the action being performed in the video.', 'api_arguments': 'video_path', 'python_environment_requirements': 'transformers==4.27.4, torch==2.0.0+cu117, datasets==2.11.0, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': 0.7212}, 'description': 'This model is a fine-tuned version of MCG-NJU/videomae-large-finetuned-kinetics on an unknown dataset.'}} </s>\"\n"}
{"text": "<s>[INST]ØªØ­ØªØ§Ø¬ ØªÙ‚Ù†ÙŠØ© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„ØªÙŠ Ù†Ø³ØªØ®Ø¯Ù…Ù‡Ø§ ÙÙŠ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª Ø¥Ù„Ù‰ Ù…Ø¹Ø±ÙØ© ÙƒÙŠÙÙŠØ© ÙÙ‡Ù… ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆØªØ·ÙˆÙŠØ± Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø·Ù‚ÙŠ ÙˆÙ…ÙÙ‡ÙˆÙ….? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the AutoModelForCausalLM and AutoTokenizer classes from the transformers library.\\\\n2. Load the tokenizer and the DialoGPT-large model using the from_pretrained method of the respective classes.\\\\n3. Encode user-input questions using the tokenizer.\\\\n4. Use the model's generate method to generate chatbot responses based on the user inputs by providing the bot_input_ids.\\\\n5. Decode the chatbot responses and print them out.\\\", '<<<code>>>': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'microsoft/DialoGPT-large\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'microsoft/DialoGPT-large\\\\')\\\\n\\\\nuser_input = \\\"Can you tell me about your company\\\\'s services?\\\"\\\\ninput_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\\\\'pt\\\\')\\\\ngenerated_response_ids = model.generate(input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\nresponse = tokenizer.decode(generated_response_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\\\\nprint(response)', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DialoGPT-large', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_arguments': ['bot_input_ids', 'max_length', 'pad_token_id'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-large)\\\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-large)\\\\n\\\\nfor step in range(5):\\\\n new_user_input_ids = tokenizer.encode(input(&gt;&gt; User:) + tokenizer.eos_token, return_tensors='pt')\\\\n bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step &gt; 0 else new_user_input_ids\\\\n chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n print(DialoGPT: {}.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\\\", 'performance': {'dataset': 'Reddit discussion thread', 'accuracy': 'Comparable to human response quality under a single-turn conversation Turing test'}, 'description': 'DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.' </s>\"\n"}
{"text": "<s>[INST]ÙŠØªØ·Ù„Ø¨ Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø¨ØµØ±ÙŠØ© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØªØ­Ù„ÙŠÙ„ ÙˆØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ù…Ø¹Ù†Ù‰ ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø¯Ø®Ù„Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ø¯ÙˆÙ„ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Multimodal Visual Question Answering', 'api_call': \\\"pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')\\\", 'api_provider': 'Hugging Face', 'explanation': \\\"1. We first import the required libraries, which mainly include the 'pipeline' function from the 'transformers' package provided by Hugging Face.\\\", 'code': 'from transformers import pipeline\\\\nvqa = pipeline(\\\\'visual-question-answering\\\\', model=\\\\'JosephusCheung/GuanacoVQAOnConsumerHardware\\\\')\\\\nimage_path = \\\"path/to/street_scene_image.jpg\\\"\\\\nquestion = \\\"Is there a zebra crossing in the image?\\\"\\\\nanswer = vqa(image_path, question)', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'api_call': 'from transformers import pipeline\\\\nvqa = pipeline(\\\\'visual-question-answering\\\\', model=\\\\'JosephusCheung/GuanacoVQAOnConsumerHardware\\\\')\\\\nimage_path = \\\"path/to/street_scene_image.jpg\\\"\\\\nquestion = \\\"Is there a zebra crossing in the image?\\\"\\\\nanswer = vqa(image_path, question)', 'api_arguments': {'model': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'tokenizer': 'JosephusCheung/GuanacoVQAOnConsumerHardware'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': 'vqa(image_path, question)', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'unknown'}, 'description': 'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…ØªØ®ØµØµ ÙÙŠ ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ø¨Ø± Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©ØŒ ÙˆØ£ÙˆØ¯ Ø£Ù† Ø£Ø¹Ù„Ù… ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªÙˆÙ„ÙŠØ¯ ÙˆØµÙ Ù†ØµÙŠ Ø¯Ù‚ÙŠÙ‚ Ù„ØµÙˆØ±Ø© Ø«Ù„Ø§Ø«ÙŠØ© Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù…Ù† ØµÙˆØ±Ø© Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ ØªØµÙˆØ± Ù…Ø´Ù‡Ø¯ Ù„Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ù‡Ø°Ø§ØŸ? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://example.com/image.jpg'\\\\n# replace with the URL of the image you want to analyze\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\\\\nprocessor = Pix2StructProcessor.from_pretrained('google/pix2struct-textcaps-base')\\\\ninputs = processor(images=image, return_tensors='pt')\\\\npredictions = model.generate(**inputs)\\\\n\\\\nsummary = processor.decode(predictions[0], skip_special_tokens=True)\\\", 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \\\"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://example.com/image.jpg'\\\\n# replace with the URL of the image you want to analyze\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\\\\nprocessor = Pix2StructProcessor.from_pretrained('google/pix2struct-textcaps-base')\\\\ninputs = processor(images=image, return_tensors='pt')\\\\npredictions = model.generate(**inputs)\\\\n\\\\nsummary = processor.decode(predictions[0], skip_special_tokens=True)\\\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª Ù„Ø§ÙƒØªØ´Ø§Ù Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØª ÙÙŠ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ. Ù‡Ù„ Ø¨Ø¥Ù…ÙƒØ§Ù†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø£ØµÙˆØ§Øª ÙÙŠ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©ØŸ? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"VoiceActivityDetection(segmentation='pyannote/segmentation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the pyannote.audio.pipelines package. Specifically, VoiceActivityDetection is required for detecting voice activity.\\\\n2. Create a VoiceActivityDetection pipeline and use the 'pyannote/segmentation' model as the segmentation parameter.\\\\n3. Set the desired hyperparameters for onset, offset, minimum duration on, and minimum duration off.\\\\n4. Instantiate the pipeline with the hyperparameters using the instantiate() method.\\\\n5. Process the audio file using the pipeline to get voice activity detection results.\\\", '<<<code>>>': \\\"from pyannote.audio.pipelines import VoiceActivityDetection\\\\npipeline = VoiceActivityDetection(segmentation='pyannote/segmentation')\\\\nHYPER_PARAMETERS = {\\\\n  'onset': 0.5,\\\\n  'offset': 0.5,\\\\n  'min_duration_on': 0.0,\\\\n  'min_duration_off': 0.0\\\\n}\\\\npipeline.instantiate(HYPER_PARAMETERS)\\\\nvad_results = pipeline('audio_path.wav')\\\\n# Replace 'audio_path.wav' with the path to your audio file\\\", 'api_call': \\\"VoiceActivityDetection(segmentation='pyannote/segmentation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pyannote/segmentation', 'api_call': \\\"VoiceActivityDetection(segmentation='pyannote/segmentation')\\\", 'api_arguments': {'onset': 0.5, 'offset': 0.5, 'min_duration_on': 0.0, 'min_duration_off': 0.0}, 'python_environment_requirements': ['pyannote.audio 2.0'], 'example_code': 'from pyannote.audio.pipelines import VoiceActivityDetection\\\\npipeline = VoiceActivityDetection(segmentation=pyannote/segmentation)\\\\nHYPER_PARAMETERS = {\\\\n  onset: 0.5, offset: 0.5,\\\\n  min_duration_on: 0.0,\\\\n  min_duration_off: 0.0\\\\n}\\\\npipeline.instantiate(HYPER_PARAMETERS)\\\\nvad = pipeline(audio.wav)', 'performance': {'dataset': {'ami': {'accuracy': {'onset': 0.684, 'offset': 0.577, 'min_duration_on': 0.181, 'min_duration_off': 0.037}}, 'dihard': {'accuracy': {'onset': 0.767, 'offset': 0.377, 'min_duration_on': 0.136, 'min_duration_off': 0.067}}, 'voxconverse': {'accuracy': {'onset': 0.767, 'offset': 0.713, 'min_duration_on': 0.182, 'min_duration_off': 0.501}}}}, 'description': 'Model from End-to-end speaker segmentation for overlap-aware resegmentation, by HervÃ© Bredin and Antoine Laurent. It provides voice activity detection, overlapped speech detection, and resegmentation functionalities.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·Ø¨ÙŠÙ‚ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ²ÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ¦Ø© Ù…Ø¹ÙŠÙ†Ø© ÙˆÙ‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡Ù‡ Ø¨Ù…Ø±ÙˆØ± Ø§Ù„ÙˆÙ‚Øª.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"load_from_hub(repo_id='sb3/ppo-BreakoutNoFrameskip-v4',filename='{MODEL FILENAME}.zip',)\\\", '<<<api_provider>>>': 'Stable-Baselines3', '<<<explanation>>>': \\\"1. Install the 'RL Zoo', 'stable-baselines3', and 'stable-baselines3-contrib' packages.\\\\n2. Use the 'load_from_hub' function from 'rl_zoo3' package to load the pre-trained model 'sb3/ppo-BreakoutNoFrameskip-v4', which is a Proximal Policy Optimization (PPO) model trained to play the 'BreakoutNoFrameskip-v4' game environment.\\\\n3. Save the downloaded model in a zip file named '{MODEL FILENAME}.zip'.\\\\n4. Use the model to create an AI player for the Breakout game in the children's museum by integrating it into the game code.\\\", '<<<code>>>': \\\"from rl_zoo3 import load_from_hub\\\\nmodel = load_from_hub(repo_id='sb3/ppo-BreakoutNoFrameskip-v4', filename='{MODEL FILENAME}.zip',)\\\", 'api_call': \\\"load_from_hub(repo_id='sb3/ppo-BreakoutNoFrameskip-v4',filename='{MODEL FILENAME}.zip',)\\\", 'provider': 'Stable-Baselines3', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ppo-BreakoutNoFrameskip-v4', 'api_call': \\\"load_from_hub(repo_id='sb3/ppo-BreakoutNoFrameskip-v4',filename='{MODEL FILENAME}.zip',)\\\", 'api_arguments': ['algo', 'env', 'orga', 'f'], 'python_environment_requirements': [['rl_zoo3', 'stable-baselines3', 'stable-baselines3-contrib']], 'example_code': ['python -m rl_zoo3.load_from_hub --algo ppo --env BreakoutNoFrameskip-v4 -orga sb3 -f logs/', 'python enjoy.py --algo ppo --env BreakoutNoFrameskip-v4 -f logs/', 'python train.py --algo ppo --env BreakoutNoFrameskip-v4 -f logs/', 'python -m rl_zoo3.push_to_hub --algo ppo --env BreakoutNoFrameskip-v4 -f logs/ -orga sb3'], 'performance': {'dataset': 'BreakoutNoFrameskip-v4', 'accuracy': '398.00 +/- 16.30'}, 'description': 'This is a trained model of a PPO agent playing BreakoutNoFrameskip-v4 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ø¨Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø¥Ù„Ù‰ Ù†Øµ Ù…ÙƒØªÙˆØ¨ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙˆØ¯Ù‚ÙŠÙ‚ØŒ ÙˆÙ‚Ù… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© ÙƒØ¨ÙŠØ±Ø© ÙˆÙ…ØªÙ†ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ù„Ø¶Ù…Ø§Ù† Ø£Ø¯Ø§Ø¡Ù‡ Ø§Ù„Ù…ØªÙ…ÙŠØ².? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes, such as AutoTokenizer and AutoModelForTokenClassification, from the transformers library provided by Hugging Face.\\\\n2. Load the pre-trained model 'd4data/biomedical-ner-all', which is capable of recognizing 107 bio-medical entities from text, using AutoModelForTokenClassification.\\\\n3. Load the corresponding tokenizer for the model using AutoTokenizer.\\\\n4. Create a pipeline using the loaded model, tokenizer, and the pipeline function from the transformers library.\\\\n5. Use the created pipeline to analyze the biomedical paper and extract bio-medical entities from the text.\\\", '<<<code>>>': \\\"from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\\\\nmodel = AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\\ntokenizer = AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\\\\nbiomedical_ner = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy='simple')\\\\nentities = biomedical_ner(biomedical_paper_text)\\\", 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'd4data/biomedical-ner-all', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\", 'api_arguments': {'model': 'AutoModelForTokenClassification.from_pretrained(d4data/biomedical-ner-all)', 'tokenizer': 'AutoTokenizer.from_pretrained(d4data/biomedical-ner-all)', 'aggregation_strategy': 'simple'}, 'python_environment_requirements': [{'transformers': 'from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification'}], 'example_code': 'pipe(The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.)', 'performance': {'dataset': 'Maccrobat', 'accuracy': 'Not provided'}, 'description': 'An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ ØªÙ†ÙÙŠØ° Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ØŸ? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, which include 'AutoModel' from the transformers package, provided by Hugging Face.\\\\n2. Use 'AutoModel.from_pretrained()' function to load the pre-trained depth estimation model 'sayakpaul/glpn-nyu-finetuned-diode-221215-095508'.\\\\n3. The loaded model is based on the GLPN architecture and fine-tuned on the DIODE dataset, which is a large dataset for depth estimation.\\\\n4. Use this model to perform depth estimation on input images. By processing these depth maps, the augmented reality app can render the environment accurately.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\ndepth_model = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\\\\ndepth_estimation = depth_model(image_input)\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221215-095508', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\\\", 'api_arguments': None, 'python_environment_requirements': [['transformers']], 'example_code': None, 'performance': {'dataset': 'DIODE', 'accuracy': None}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.' </s>\"\n"}
{"text": "<s>[INST]Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ© ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ·ÙŠØ¹ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù…Ù‚ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„ÙØ¹Ø§Ù„ÙŠØ© ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§ØªÙ†Ø§.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first need to import the necessary class from the transformers package. This includes AutoModel for loading the pre-trained model of interest.\\\\n2. We load the pre-trained depth estimation model 'sayakpaul/glpn-nyu-finetuned-diode-221116-062619'.\\\\n3. We then need to preprocess the input image using the appropriate transforms, such as resizing and normalization.\\\\n4. Pass the preprocessed image as input to the model to estimate the depth map.\\\\n5. The depth map generated can then be used to measure the depth of various objects in the image.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nimport torch\\\\nimport torchvision.transforms as transforms\\\\nfrom PIL import Image\\\\n\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\\\\nimage = Image.open('input_image.jpg')\\\\n\\\\ntransform = transforms.Compose([\\\\n    transforms.Resize((640, 480)), \\\\n    transforms.ToTensor(), \\\\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\\\\n\\\\npreprocessed_image = transform(image).unsqueeze(0)\\\\noutput = model(preprocessed_image)\\\\ndepth_map = output.cpu().squeeze().numpy()\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221116-062619', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\\\", 'api_arguments': 'None', 'python_environment_requirements': ['Transformers 4.24.0, Pytorch 1.13.0+cu117, Tokenizers 0.13.2'], 'example_code': 'None', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.548, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ Ù…Ø¬Ø§Ù„ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. To create this tool, we will use the pre-trained model `martinezomg/vit-base-patch16-224-diabetic-retinopathy`, which is specifically fine-tuned for diabetic retinopathy detection in retinal images.\\\\n2. First, import the `pipeline` function from the transformers library.\\\\n3. Use the `pipeline()` function to create an image classification model, specifying the `martinezomg/vit-base-patch16-224-diabetic-retinopathy` model to be loaded.\\\\n4. Once the model is loaded, it can be used to classify retinal images, returning the probability of diabetic retinopathy.\\\\n5. The created classifier can be used to evaluate any retinal images for the presence of diabetic retinopathy.', '<<<code>>>': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\\nresult = image_classifier('path/to/retinal_image.jpg')\\\", 'api_call': \\\"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy', 'api_call': \\\"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\", 'api_arguments': {'model_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy'}, 'python_environment_requirements': [{'transformers': '4.28.1', 'pytorch': '2.0.0+cu118', 'datasets': '2.11.0', 'tokenizers': '0.13.3'}], 'example_code': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\\nresult = image_classifier('path/to/image.jpg')\\\", 'performance': {'dataset': 'None', 'accuracy': 0.7744}, 'description': 'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.' </s>\"\n"}
{"text": "<s>[INST]Ù†Ù‚ØªØ±Ø­ Ø¹Ù„ÙŠÙƒ Ø§Ø®ØªÙŠØ§Ø± Ù…ÙƒØªØ¨Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ Ù‚ØµÙŠØ± ÙˆÙ…ÙÙŠØ¯ Ù„Ù„Ù†ØµÙˆØµ. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø«Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ø¨ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙˆØªØ¬Ø²Ø¦Ø© Ø§Ù„Ø¬Ù…Ù„ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ù…ØªÙƒØ§Ù…Ù„ ÙŠØ¹ÙƒØ³ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù†Øµ. Ø¬Ø±Ø¨ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ ÙˆÙ‚Ù… Ø¨Ø¶Ø¨Ø· Ø§Ù„Ø¨Ø§Ø±Ø§Ù…ØªØ±Ø§Øª ÙˆØ§Ù„Ø®ØµØ§Ø¦Øµ ÙˆÙÙ‚Ø§Ù‹ Ù„Ø§Ø­ØªÙŠØ§Ø¬Ø§ØªÙƒ Ø§Ù„Ø®Ø§ØµØ©.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", 'api_provider': 'Transformers', 'explanation': '1. We use the PegasusForConditionalGeneration and the PegasusTokenizer classes from the transformers library.', 'code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration\\\\nmodel_name = \\\\'human-centered-summarization/financial-summarization-pegasus\\\\'\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\n\\\\ntext_to_summarize = \\\"Your financial news article text here\\\"\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=\\\\'pt\\\\').input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nsummary = tokenizer.decode(output[0], skip_special_tokens=True)', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Transformers', 'functionality': 'text2text-generation', 'api_name': 'financial-summarization-pegasus', 'api_call': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration\\\\nmodel_name = \\\\'human-centered-summarization/financial-summarization-pegasus\\\\'\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\n\\\\ntext_to_summarize = \\\"Your financial news article text here\\\"\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=\\\\'pt\\\\').input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nsummary = tokenizer.decode(output[0], skip_special_tokens=True)', 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabiaâ€™s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Sambaâ€™s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf regionâ€™s third-largest lender. The entityâ€™s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle Eastâ€™s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))', 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 35.206, 'ROUGE-2': 16.569, 'ROUGE-L': 30.128, 'ROUGE-LSUM': 30.171}}, 'description': 'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ ØªØ·Ø¨ÙŠÙ‚ Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø£Ø´Ø®Ø§Øµ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('af1tang/personaGPT')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library, such as AutoTokenizer and AutoModelForCausalLM.\\\\n2. Use the from_pretrained method to load the 'af1tang/personaGPT' model. This model is designed for generating personalized responses based on input personality facts and incorporating turn-level goals in its responses.\\\\n3. If a GPU is available, move the model to the GPU.\\\\n4. Prepare the input context by providing personality facts and previous dialog history.\\\\n5. Use the model to generate a response for a given user message. The generated response can then be used to find common preferences and discuss common interests with the user.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\nimport torch\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'af1tang/personaGPT\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'af1tang/personaGPT\\\\')\\\\n\\\\nif torch.cuda.is_available():\\\\n    model = model.cuda()\\\\n\\\\n# Provide the input context, e.g., personality facts and previous dialog history, and generate a response\\\\ninput_context = \\\"your input here\\\"\\\\ndialog_hx = [input_context]\\\\n\\\\nbot_input_ids = tokenizer.encode(dialog_hx, return_tensors=\\\\'pt\\\\')\\\\nresponse_ids = model.generate(bot_input_ids, do_sample=True, top_k=10, top_p=0.92, max_length=1000)\\\\nresponse_text = tokenizer.decode(response_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('af1tang/personaGPT')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'af1tang/personaGPT', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('af1tang/personaGPT')\\\", 'api_arguments': {'do_sample': 'True', 'top_k': '10', 'top_p': '.92', 'max_length': '1000', 'pad_token': 'tokenizer.eos_token_id'}, 'python_environment_requirements': [{'transformers': 'GPT2Tokenizer, GPT2LMHeadModel', 'torch': 'torch'}], 'example_code': {'load_model': 'tokenizer = AutoTokenizer.from_pretrained(af1tang/personaGPT)\\\\nmodel = AutoModelForCausalLM.from_pretrained(af1tang/personaGPT)\\\\nif torch.cuda.is_available():\\\\n model = model.cuda()', 'generate_response': 'bot_input_ids = to_var([personas + flatten(dialog_hx)]).long()\\\\nmsg = generate_next(bot_input_ids)\\\\ndialog_hx.append(msg)\\\\nprint(Bot: {}.format(tokenizer.decode(msg, skip_special_tokens=True)))'}, 'performance': {'dataset': 'Persona-Chat', 'accuracy': 'Not provided'}, 'description': 'PersonaGPT is an open-domain conversational agent designed to do 2 tasks: decoding personalized responses based on input personality facts (the persona profile of the bot) and incorporating turn-level goals into its responses through action codes (e.g., talk about work, ask about favorite music). It builds on the DialoGPT-medium pretrained model based on the GPT-2 architecture.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨ØªØ¬Ù…ÙŠØ¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„ØµÙˆØ± Ù…Ø¹ ØªÙˆØµÙŠÙ„Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù†ØµÙŠ ÙˆØµÙˆØªÙŠ. Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù‡Ø§ØŒ Ø«Ù… Ù‚Ù… Ø¨ØªÙˆÙ„ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ù…Ø¹ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ ØªØ¬Ø±Ø¨Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary pipeline function from the transformers library.\\\\n2. Use the pipeline function to instantiate the 'kha-white/manga-ocr-base' model, a pre-trained OCR model specialized in Japanese manga text recognition.\\\\n3. This model uses the Vision Encoder Decoder framework and is able to recognize both vertical and horizontal text, text with furigana, text overlaid on images, and various fonts and font styles typically used in manga.\\\\n4. Pass the manga image (either as a file path or an image object) as an argument to run the OCR model and get the extracted text.\\\\n5. The extracted text can now be used as subtitles for foreign viewers.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nmanga_ocr = pipeline('ocr', model='kha-white/manga-ocr-base')\\\\nimage_path = 'path/to/your/manga/image.jpg'\\\\nextracted_text = manga_ocr(image_path)\\\", 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'api_arguments': 'image', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ‚ÙˆÙ… ÙØ±ÙŠÙ‚Ù†Ø§ Ø¨ØªØ·ÙˆÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ Ø¨ÙØ¹Ø§Ù„ÙŠØ©ØŒ ÙˆÙ†Ù‡Ø¯Ù Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ÙŠØ®ØªØµØ± Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù„ØºÙˆÙŠ Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ ÙˆÙ…Ù†Ø§Ø³Ø¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary class (PegasusForConditionalGeneration) and tokenizer (PegasusTokenizer) from transformers.', 'code': \\\"from transformers import PegasusForConditionalGeneration, PegasusTokenizer\\\\nmodel = PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntokenizer = PegasusTokenizer.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntext = 'é•¿ç¯‡æ–°é—»æ–‡ç« ...'\\\\ninputs = tokenizer(text, max_length=1024, return_tensors='pt')\\\\nsummary_ids = model.generate(inputs['input_ids'])\\\\nsummary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\\\", 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Summarization', 'api_name': 'Randeng-Pegasus-238M-Summary-Chinese', 'api_call': \\\"from transformers import PegasusForConditionalGeneration, PegasusTokenizer\\\\nmodel = PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntokenizer = PegasusTokenizer.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntext = 'é•¿ç¯‡æ–°é—»æ–‡ç« ...'\\\\ninputs = tokenizer(text, max_length=1024, return_tensors='pt')\\\\nsummary_ids = model.generate(inputs['input_ids'])\\\\nsummary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\\\", 'api_arguments': {'text': 'string', 'max_length': 'integer'}, 'python_environment_requirements': ['transformers', 'tokenizers_pegasus.py', 'data_utils.py'], 'example_code': \\\"from transformers import PegasusForConditionalGeneration\\\\nfrom tokenizers_pegasus import PegasusTokenizer\\\\nmodel = PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntokenizer = PegasusTokenizer.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntext = '1'\\\\ninputs = tokenizer(text, max_length=1024, return_tensors='pt')\\\\nsummary_ids = model.generate(inputs['input_ids'])\\\\ntokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\\\", 'performance': {'dataset': 'LCSTS', 'accuracy': {'rouge-1': 43.46, 'rouge-2': 29.59, 'rouge-L': 39.76}}, 'description': 'Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus. It is fine-tuned on 7 Chinese text summarization datasets including education, new2016zh, nlpcc, shence, sohu, thucnews, and weibo. The model can be used to generate summaries for Chinese text inputs.'}} </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©. ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø¯Ø®Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠÙ‡Ø§. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù„Ø¯ÙŠÙƒ Ù…Ø¬Ù…ÙˆØ¹Ø© ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ù…Ù† Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØµÙ†ÙØ© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. Ø§Ø³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø«Ù„ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© ÙˆØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ±.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries such as requests, PIL (Python Imaging Library), and transformers.\\\\n2. Utilize the Vision Transformer (ViT) for image classification by loading the 'nateraw/vit-age-classifier' pretrained model.\\\\n3. To preprocess the input image, obtain the ViTFeatureExtractor object from the same pretrained model.\\\\n4. Apply the feature extractor (transforms) to the input image to create tensors that will be passed into the ViT model.\\\\n5. Obtain the model output, which includes logits representing age categories.\\\\n6. Apply softmax to the logits for obtaining probabilities and find the age class with the highest probability as the final prediction.\\\", '<<<code>>>': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom io import BytesIO\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\n\\\\nurl = 'https://example.com/image.jpg'\\\\nresponse = requests.get(url)\\\\nimage = Image.open(BytesIO(response.content))\\\\n\\\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\\\n\\\\ninputs = transforms(image, return_tensors='pt')\\\\noutput = model(**inputs)\\\\n\\\\nproba = output.logits.softmax(1)\\\\npredicted_age_class = proba.argmax(1)\\\", 'api_call': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nateraw/vit-age-classifier', 'api_call': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/vit-age-classifier'}, 'python_environment_requirements': [['requests', 'PIL', 'transformers']], 'example_code': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom io import BytesIO\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\n\\\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\\\nim = Image.open(BytesIO(r.content))\\\\n\\\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\\\n\\\\ninputs = transforms(im, return_tensors='pt')\\\\noutput = model(**inputs)\\\\n\\\\nproba = output.logits.softmax(1)\\\\npreds = proba.argmax(1)\\\", 'performance': {'dataset': 'fairface', 'accuracy': None}, 'description': \\\"A vision transformer finetuned to classify the age of a given person's face.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙƒØªÙˆØ¨ Ø¥Ù„Ù‰ Ù†Øµ Ù…ÙƒØªÙˆØ¨ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© (NLP) Ù„Ù„Ø¬ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ (Text2Text Generation).? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-question-generation-ap')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. First, import the AutoTokenizer and AutoModelWithLMHead classes from the Hugging Face transformers package.\\\\n2. Load the pre-trained T5 model \\\"mrm8488/t5-base-finetuned-question-generation-ap\\\" using the AutoTokenizer and AutoModelWithLMHead methods.\\\\n3. To generate a question from the provided context and answer, combine the answer and context into a single input string.\\\\n4. Pass the input string into the tokenizer, and then feed the tokenized input into the pre-trained T5 model.\\\\n5. The model returns a question generated from the provided context and answer.', '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'mrm8488/t5-base-finetuned-question-generation-ap\\\\')\\\\nmodel = AutoModelWithLMHead.from_pretrained(\\\\'mrm8488/t5-base-finetuned-question-generation-ap\\\\')\\\\n\\\\ndef get_question(answer, context):\\\\n    input_string = f\\\"answer: {answer} context: {context}\\\"\\\\n    tokenized_input = tokenizer(input_string, return_tensors=\\\"pt\\\")\\\\n    outputs = model.generate(**tokenized_input)\\\\n    question = tokenizer.decode(outputs[0])\\\\n    return question\\\\n\\\\ncontext = \\\"In 1705, the mathematician and physicist Isaac Newton published his theories on the laws of motion and universal gravitation in a book called \\\\'PhilosophiÃ¦ Naturalis Principia Mathematica\\\\'.\\\"\\\\nanswer = \\\"Newton\\\"\\\\ngenerated_question = get_question(answer, context)', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-question-generation-ap')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mrm8488/t5-base-finetuned-question-generation-ap', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-question-generation-ap')\\\", 'api_arguments': {'answer': 'string', 'context': 'string', 'max_length': 'integer (optional, default=64)'}, 'python_environment_requirements': [{'transformers': 'from source'}], 'example_code': {'tokenizer': 'AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-question-generation-ap)', 'model': 'AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-question-generation-ap)', 'get_question': 'get_question(answer, context)'}, 'performance': {'dataset': 'SQuAD', 'accuracy': 'Not provided'}, 'description': \\\"Google's T5 model fine-tuned on SQuAD v1.1 for Question Generation by prepending the answer to the context.\\\" </s>\"\n"}
{"text": "<s>[INST]ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±Ø© Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ù…Ø¬Ø§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ© Ù„ØµÙˆØ± ØªØµÙˆÙŠØ±ÙŠØ© ØºÙŠØ± Ù…ØµÙ†ÙØ© ÙˆØªÙ‚Ø¯ÙŠÙ… Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù…Ù„.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"SequenceTagger.load('flair/ner-german')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the flair package. This includes Sentence for processing textual data and SequenceTagger for the named entity recognition model.\\\\n2. We then use the load method of the SequenceTagger class to load the pre-trained model 'flair/ner-german'. This model has been trained for four-class named entity recognition in the German language.\\\\n3. We create a Sentence object containing the input text that we want to analyze.\\\\n4. The tagger.predict function is called on the sentence object. This function uses the pre-trained model to identify named entities in the text.\\\\n5. Lastly, we iterate over the recognized entities and display them.\\\", '<<<code>>>': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\ntagger = SequenceTagger.load('flair/ner-german')\\\\nsentence = Sentence('Berlins BÃ¼rgermeister Michael MÃ¼ller und Frankfurts OberbÃ¼rgermeister Peter Feldmann besuchten das ReichstagsgebÃ¤ude.')\\\\ntagger.predict(sentence)\\\\n\\\\nfor entity in sentence.get_spans('ner'):\\\\n    print(entity)\\\", 'api_call': \\\"SequenceTagger.load('flair/ner-german')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'flair/ner-german', 'api_call': \\\"SequenceTagger.load('flair/ner-german')\\\", 'api_arguments': ['Sentence'], 'python_environment_requirements': [['flair']], 'example_code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\n# load tagger\\\\ntagger = SequenceTagger.load('flair/ner-german')\\\\n\\\\n# make example sentence\\\\nsentence = Sentence('George Washington ging nach Washington')\\\\n\\\\n# predict NER tags\\\\ntagger.predict(sentence)\\\\n\\\\n# print sentence\\\\nprint(sentence)\\\\n\\\\n# print predicted NER spans\\\\nprint('The following NER tags are found:')\\\\n\\\\n# iterate over entities and print\\\\nfor entity in sentence.get_spans('ner'):\\\\n    print(entity)\\\", 'performance': {'dataset': 'conll2003', 'accuracy': '87.94'}, 'description': 'This is the standard 4-class NER model for German that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF.' </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø´Ø¦ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù„Ø¹Ù„ÙˆÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ø­ÙŠØ« ÙŠØ¬ØªÙ…Ø¹ Ø§Ù„Ø£Ø¹Ø¶Ø§Ø¡ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ§Øª. ÙŠØ¬Ø¨ ØªØ²ÙˆÙŠØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù…Ù„Ø¡ Ø§Ù„ÙØ¬ÙˆØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. To assist with automatically completing sentences, we will import the pipeline function from the Hugging Face transformers library.\\\\n2. We'll instantiate the fill-mask pipeline with the model 'distilroberta-base'. This model is a distilled version of RoBERTa specifically designed for the task of filling in the blanks (mask tokens) in sentences.\\\\n3. By passing the partially completed sentence with a mask token (mask) in place of the missing word(s), the model can predict the most likely words to fill in the blank to make the sentence more fluent.\\\", '<<<code>>>': 'from transformers import pipeline\\\\n\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'distilroberta-base\\\\')\\\\ncompleted_sentence = unmasker(\\\"The software allows users to <mask> files between devices.\\\")', 'api_call': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilroberta-base', 'api_call': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"unmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': 'openwebtext', 'accuracy': 'Not provided'}, 'description': \\\"DistilRoBERTa is a distilled version of the RoBERTa-base model, designed to be smaller, faster, and lighter. It is a Transformer-based language model trained on the OpenWebTextCorpus, which is a reproduction of OpenAI's WebText dataset. The model has 6 layers, 768 dimensions, and 12 heads, totaling 82M parameters. It is primarily intended for fine-tuning on downstream tasks such as sequence classification, token classification, or question answering.\\\" </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø±Ø¦ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a visual question answering model by specifying the 'visual-question-answering' task and the model to be loaded ('ivelin/donut-refexp-combined-v1').\\\\n3. Provide the image file and the visitor's question as inputs to the created model.\\\\n4. The model will analyze the image and provide an answer to the visitor's question based on the visual content and the context of the question.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nvqa = pipeline(\\\\'visual-question-answering\\\\', model=\\\\'ivelin/donut-refexp-combined-v1\\\\')\\\\nimage_path = \\\\'path/to/image.jpg\\\\'\\\\nquestion = \\\"What is the color of the object?\\\"\\\\nanswer = vqa(image=image_path, question=question)', 'api_call': \\\"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ivelin/donut-refexp-combined-v1', 'api_call': \\\"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\\\", 'api_arguments': 'image, question', 'python_environment_requirements': ['transformers'], 'example_code': \\\"vqa(image='path/to/image.jpg', question='What is the color of the object?')\\\", 'performance': {'dataset': 'ivelin/donut-refexp-combined-v1', 'accuracy': 'N/A'}, 'description': 'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.' </s>\"\n"}
{"text": "<s>[INST]Ø£ÙƒØªØ¨ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­ÙˆØ§Ø³Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ù…Ø¹ÙŠÙ†Ø©.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries AutoModel from transformers package.\\\\n2. Load the pre-trained model 'sayakpaul/glpn-nyu-finetuned-diode-221228-072509'. This model specializes in depth estimation for images, which will help the robot's navigation in the 3D printing facility.\\\\n3. The robot's camera can be used to capture images of the environment in real-time, which will then be processed by the model to estimate depth information.\\\\n4. This estimated depth information can be integrated into the robot's navigation system to enhance its ability to avoid obstacles and move safely through the 3D printing facility.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\\\\n# Load image from the robot's camera\\\\n# Process the image to get depth estimation\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221228-072509', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\\\", 'api_arguments': '', 'python_environment_requirements': ['Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4012, 'Mae': 0.403, 'Rmse': 0.6173, 'Abs Rel': 0.3487, 'Log Mae': 0.1574, 'Log Rmse': 0.211, 'Delta1': 0.4308, 'Delta2': 0.6997, 'Delta3': 0.8249}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]Ø¥Ù„Ù‰ ÙƒÙ„ Ø®Ø¨ÙŠØ± ÙÙŠ ØªØ±Ø¬Ù…Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©: Ù„ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµØŒ ÙŠÙÙ‚ØªØ±Ø­ Ø¥Ø¶Ø§ÙØ© Ø£Ø¯Ø§Ø© Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ Ù†ÙˆØ§Ø¯ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. ÙŠØªÙ… Ø§Ø¬ØªÙ…Ø§Ø¹ Ø§Ù„Ù†Ø§Ø¯ÙŠ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠØŒ Ù…Ù‡Ù…ØªÙ‡ ØªØ­Ø³ÙŠÙ† ØªÙ‚Ù†ÙŠØ§Øª ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries and classes, including pipeline and AutoModelForSeq2SeqLM from the transformers package.', 'code': 'from transformers import pipeline, AutoModelForSeq2SeqLM\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-ru\\\\')\\\\ntranslation_pipeline = pipeline(\\\\'translation_en_to_ru\\\\', model=model)\\\\ndocument = \\\"<your document in English>\\\"\\\\ntranslated_text = translation_pipeline(document)\\\\n# Save the translated text as desired', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Translation', 'api_name': 'opus-mt-en-ru', 'api_call': 'from transformers import pipeline, AutoModelForSeq2SeqLM\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-ru\\\\')\\\\ntranslation_pipeline = pipeline(\\\\'translation_en_to_ru\\\\', model=model)\\\\ndocument = \\\"<your document in English>\\\"\\\\ntranslated_text = translation_pipeline(document)\\\\n# Save the translated text as desired', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers', 'torch', 'tensorflow'], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ru', model='Helsinki-NLP/opus-mt-en-ru')\\\\ntranslated_text = translation('Hello, how are you?')\\\", 'performance': {'dataset': 'newstest2019-enru', 'accuracy': '27.1'}, 'description': 'Helsinki-NLP/opus-mt-en-ru is a translation model trained on the OPUS dataset, which translates English text to Russian. It is based on the Marian NMT framework and can be used with Hugging Face Transformers.'}} </s>\"\n"}
{"text": "<s>[INST]Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø´Ø¨Ø§Ø¨ÙŠÙƒ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import required classes from the transformers library to load the speech recognition model and processor.\\\\n2. We load the WhisperProcessor and WhisperForConditionalGeneration from the pre-trained model 'openai/whisper-tiny.en'. This model is trained to transcribe human speech to text.\\\\n3. For recording transcription, we first prepare the audio file containing the meeting recordings.\\\\n4. The processor is used to preprocess the audio file and generate input features that can be fed to the model for performing automatic speech recognition (ASR).\\\\n5. We then feed the input features to the ASR model, which generates a sequence of predicted token IDs.\\\\n6. The predicted token IDs are then decoded using the WhisperProcessor to obtain the final transcription of the recorded meeting.\\\", '<<<code>>>': \\\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\n\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-tiny.en')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\\\\n\\\\n# replace 'audio_file.wav' with path to your audio file\\\\nsample = {'array': 'audio_file.wav', 'sampling_rate': 16000}\\\\ninput_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\\", 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai/whisper-tiny.en', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\\\", 'api_arguments': {'model_name': 'openai/whisper-tiny.en'}, 'python_environment_requirements': [['transformers', 'datasets', 'torch']], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', 'processor = WhisperProcessor.from_pretrained(openai/whisper-tiny.en)', 'model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-tiny.en)', 'ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)', 'sample = ds[0][audio]', 'input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features', 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)'], 'performance': {'dataset': 'LibriSpeech (clean)', 'accuracy': 8.437}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.' </s>\"\n"}
{"text": "<s>[INST]Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ø­ÙŠØ« ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ù…Ù† Ù„ØºØ© Ù…ØµØ¯Ø±ÙŠØ© Ø¥Ù„Ù‰ Ù„ØºØ© Ù‡Ø¯ÙÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ±Ø¬Ù…Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¦Ù‡.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import necessary classes from the transformers library provided by Hugging Face, which include: AutoTokenizer and AutoModelForSeq2SeqLM.\\\\n2. Instantiate the tokenizer and the model using the pre-trained Swedish-to-English translation model 'Helsinki-NLP/opus-mt-sv-en'.\\\\n3. Tokenize the input Swedish text and pass the tokenized input to the model for translation.\\\\n4. Decode the model's output to obtain the translated English text.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Helsinki-NLP/opus-mt-sv-en\\\\')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'Helsinki-NLP/opus-mt-sv-en\\\\')\\\\n\\\\nswedish_text = \\\"Ditt exempel pÃ¥ svenska hÃ¤r\\\"\\\\ninputs = tokenizer.encode(swedish_text, return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs)\\\\nenglish_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)', 'api_call': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-sv-en', 'api_call': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", 'api_arguments': ['inputs'], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'Tatoeba.sv.en', 'accuracy': 'BLEU: 64.5, chr-F: 0.763'}, 'description': 'A Swedish to English translation model trained on the OPUS dataset using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece.' </s>\"\n"}

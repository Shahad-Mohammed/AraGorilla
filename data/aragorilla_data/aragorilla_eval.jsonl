{"text": "<s>[INST]أنا مهتم بتوليد تسميات للرموز المنطقية من النص الطبيعي لتصنيف الرموز. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. This includes AutoTokenizer and AutoModelForTokenClassification.\\\\n2. Use the from_pretrained method of the AutoTokenizer class to load the tokenizer with the model 'Davlan/distilbert-base-multilingual-cased-ner-hrl'.\\\\n3. Also, use the from_pretrained method of the AutoModelForTokenClassification class to load the named entity recognition model for the same multilingual model.\\\\n4. Create a pipeline with 'ner' as the task, and pass the model and tokenizer as arguments.\\\\n5. Pass an example text to the pipeline, which will recognize and classify named entities in the text.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Davlan/distilbert-base-multilingual-cased-ner-hrl\\\\')\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'Davlan/distilbert-base-multilingual-cased-ner-hrl\\\\')\\\\nnlp = pipeline(\\\\'ner\\\\', model=model, tokenizer=tokenizer)\\\\nexample = \\\"Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\\\"\\\\nner_results = nlp(example)\\\\nprint(ner_results)', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilbert-base-multilingual-cased-ner-hrl', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl')\\\", 'api_arguments': {'model': 'Davlan/distilbert-base-multilingual-cased-ner-hrl', 'tokenizer': 'Davlan/distilbert-base-multilingual-cased-ner-hrl'}, 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoTokenizer, AutoModelForTokenClassification\\\\nfrom transformers import pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\\\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\\\nexample = Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\\\\nner_results = nlp(example)\\\\nprint(ner_results)', 'performance': {'dataset': [{'name': 'ANERcorp', 'language': 'Arabic'}, {'name': 'conll 2003', 'language': 'German'}, {'name': 'conll 2003', 'language': 'English'}, {'name': 'conll 2002', 'language': 'Spanish'}, {'name': 'Europeana Newspapers', 'language': 'French'}, {'name': 'Italian I-CAB', 'language': 'Italian'}, {'name': 'Latvian NER', 'language': 'Latvian'}, {'name': 'conll 2002', 'language': 'Dutch'}, {'name': 'Paramopama + Second Harem', 'language': 'Portuguese'}, {'name': 'MSRA', 'language': 'Chinese'}], 'accuracy': 'Not specified'}, 'description': 'distilbert-base-multilingual-cased-ner-hrl is a Named Entity Recognition model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned Distiled BERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER).' </s>\"\n"}
{"text": "<s>[INST]قم بتطبيق تقنية السؤال والجواب المتعددة الوسائط على الوثائق للإجابة على الأسئلة المتعلقة بالنصوص والصور ومقاطع الفيديو.? [/INST]{'domain': 'Multimodal Document Question Answer', 'api_call': \\\"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the pipeline function from the transformers library provided by Hugging Face.', 'code': \\\"from transformers import pipeline\\\\ndoc_vqa = pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\\nresult = doc_vqa(image_path='path/to/image.jpg', question='What is the total cost?')\\\", 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'vision-encoder-decoder', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': \\\"from transformers import pipeline\\\\ndoc_vqa = pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\\nresult = doc_vqa(image_path='path/to/image.jpg', question='What is the total cost?')\\\", 'api_arguments': 'image_path, question', 'python_environment_requirements': 'transformers', 'example_code': \\\"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\\\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتصميم نموذج تحليل جدولي للبيانات لتقدير عمق الأشياء في النصوص التي تم إنشاؤها باستخدام معالجة اللغة الطبيعية.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"pipeline('text-generation', model='distilgpt2')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': 'from transformers import pipeline, set_seed\\\\nset_seed(42)\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'distilgpt2\\\\')\\\\nprompts = [\\\"Revolutionize your daily routine with\\\", \\\"Elevate your lifestyle by\\\"]\\\\nresults = []\\\\nfor prompt in prompts:\\\\n    generated_text = generator(prompt, max_length=30, num_return_sequences=1)\\\\n    results.append(generated_text[0][\\\\'generated_text\\\\'])\\\\nresults', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Transformers', 'functionality': 'Text Generation', 'api_name': 'distilgpt2', 'api_call': 'from transformers import pipeline, set_seed\\\\nset_seed(42)\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'distilgpt2\\\\')\\\\nprompts = [\\\"Revolutionize your daily routine with\\\", \\\"Elevate your lifestyle by\\\"]\\\\nresults = []\\\\nfor prompt in prompts:\\\\n    generated_text = generator(prompt, max_length=30, num_return_sequences=1)\\\\n    results.append(generated_text[0][\\\\'generated_text\\\\'])\\\\nresults', 'api_arguments': ['model'], 'python_environment_requirements': ['from transformers import pipeline, set_seed'], 'example_code': 'set_seed(42)\\\\ngenerator(Hello, I’m a language model, max_length=20, num_return_sequences=5)', 'performance': {'dataset': 'WikiText-103', 'accuracy': '21.100'}, 'description': 'DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. With 82 million parameters, it was developed using knowledge distillation and designed to be a faster, lighter version of GPT-2. It can be used for text generation, writing assistance, creative writing, entertainment, and more.'}} </s>\"\n"}
{"text": "<s>[INST]يشير الدليل حول مجال معالجة اللغة الطبيعية إلى صورة المتعدد الوسائط إلى أنه يمكن تنظيم نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الإفتراضية. نوصي بإضافة أداة تمكنهم من لعب ضد وكيل التعلم الآلي لزيادة التفاعل والإستفادة.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary 'pipeline' function from the transformers library provided by Hugging Face.\\\\n2. Use the 'pipeline' function to load the 'Helsinki-NLP/opus-mt-sv-en' model, which is a Swedish to English translation model trained on the OPUS dataset.\\\\n3. The loaded model will be used for translating your Swedish text to English.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntranslator = pipeline('translation_sv_to_en', model='Helsinki-NLP/opus-mt-sv-en')\\\\nswedish_text = 'Svensk text här...'\\\\nenglish_translation = translator(swedish_text, max_length=250)[0]['translation_text']\\\", 'api_call': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-sv-en', 'api_call': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", 'api_arguments': ['inputs'], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'Tatoeba.sv.en', 'accuracy': 'BLEU: 64.5, chr-F: 0.763'}, 'description': 'A Swedish to English translation model trained on the OPUS dataset using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece.' </s>\"\n"}
{"text": "<s>[INST]انطق النص التالي باستخدام نطق آلي للكلام: \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. In order to transcribe the podcast audio, begin by importing the necessary classes from the transformers package. This includes Wav2Vec2Processor for audio preprocessing and Wav2Vec2ForCTC for the automatic speech recognition model.\\\\n2. Load the pre-trained model 'facebook/wav2vec2-base-960h' using the Wav2Vec2ForCTC class's from_pretrained method. This model has been specifically trained for transcription tasks.\\\\n3. Convert the audio file into a format that can be processed by the model, making sure it is sampled at 16kHz. This can be done using the Wav2Vec2Processor.\\\\n4. Use the model to generate logits, then select the most probable sequence of characters for each chunk of audio.\\\\n5. Lastly, decode the transcription to produce human-readable text.\\\", '<<<code>>>': \\\"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nimport torch\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\\\\n\\\\n# Load the audio file here\\\\ninput_values = processor(audio_array, return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/wav2vec2-base-960h', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\\\", 'api_arguments': ['input_values'], 'python_environment_requirements': [['transformers', 'datasets', 'torch', 'jiwer']], 'example_code': \\\"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\\\\nds = load_dataset('patrickvonplaten/librispeech_asr_dummy', 'clean', split='validation')\\\\ninput_values = processor(ds[0]['audio']['array'], return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'performance': {'dataset': 'LibriSpeech', 'accuracy': {'clean': 3.4, 'other': 8.6}}, 'description': \\\"Facebook's Wav2Vec2 base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It is designed for automatic speech recognition and can transcribe audio files.\\\" </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي في مجال معالجة اللغة الطبيعية والتلخيص.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'financial-summarization-pegasus', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabia’s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba’s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region’s third-largest lender. The entity’s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East’s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))', 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 35.206, 'ROUGE-2': 16.569, 'ROUGE-L': 30.128, 'ROUGE-LSUM': 30.171}}, 'description': 'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.' </s>\"\n"}
{"text": "<s>[INST]ستقوم الشركة بتطوير برنامج لمعالجة اللغة الطبيعية وتحويل النصوص إلى نصوص. يرغب العميل في إنشاء برنامج يمكنه تحويل النصوص من نوع إلى نوع آخر بشكل آلي ودقيق.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"pipeline('text2text-generation', model='google/t5-v1_1-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a text2text generation model, specifying the model 'google/t5-v1_1-base'.\\\\n3. This model is a state-of-the-art text-to-text transformer trained on various NLP tasks, including summarization.\\\\n4. By providing a text in the target language of the user, the model can produce a summary, aiding the user in learning the language more efficiently.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nt5 = pipeline('text2text-generation', model='google/t5-v1_1-base')\\\\nsummary = t5('summarize: <Text in the target language here>')\\\", 'api_call': \\\"pipeline('text2text-generation', model='google/t5-v1_1-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/t5-v1_1-base', 'api_call': \\\"pipeline('text2text-generation', model='google/t5-v1_1-base')\\\", 'api_arguments': {'model': 'google/t5-v1_1-base'}, 'python_environment_requirements': [{'transformers': '>=4.0.0'}], 'example_code': \\\"from transformers import pipeline\\\\n\\\\nt5 = pipeline('text2text-generation', model='google/t5-v1_1-base')\\\\nt5('translate English to French: Hugging Face is a great company')\\\", 'performance': {'dataset': 'c4', 'accuracy': 'Not provided'}, 'description': \\\"Google's T5 Version 1.1 is a state-of-the-art text-to-text transformer model that achieves high performance on various NLP tasks such as summarization, question answering, and text classification. It is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on downstream tasks.\\\" </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نص توليدي يستخدم معالجة اللغة الطبيعية لتحويل النصوص إلى نصوص، وابدأ بـ \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"pipeline('translation_en_to_de', model='sshleifer/tiny-marian-en-de')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library function, which is 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'sshleifer/tiny-marian-en-de'.\\\\n3. The loaded model will be used for English to German translation.\\\\n4. The model will translate the given sentence from English to German.\\\\n5. The translated output will be returned.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_en_to_de\\\\', model=\\\\'sshleifer/tiny-marian-en-de\\\\')\\\\nsentence = \\\"A beautiful sunset faded into the dark night.\\\"\\\\ntranslated_sentence = translator(sentence)[0][\\\\'translation_text\\\\']', 'api_call': \\\"pipeline('translation_en_to_de', model='sshleifer/tiny-marian-en-de')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sshleifer/tiny-marian-en-de', 'api_call': \\\"pipeline('translation_en_to_de', model='sshleifer/tiny-marian-en-de')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny English to German translation model using the Marian framework in Hugging Face Transformers.' </s>\"\n"}
{"text": "<s>[INST]نحن بحاجة لتطوير برنامج في مجال معالجة اللغة الطبيعية لتصنيف الرموز الدالة. يهدف هذا التطبيق إلى تحويل النصوص إلى صور وبالعكس. علينا توفير وسيلة للمستخدمين لتحقيق هذه المهمة بكل سهولة ويسر.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library: AutoTokenizer, AutoModelForTokenClassification, and pipeline.\\\\n2. Use the from_pretrained method of the AutoTokenizer to create a tokenizer for the 'Babelscape/wikineural-multilingual-ner' model.\\\\n3. Similarly, use the from_pretrained method of the AutoModelForTokenClassification to create a token classification model appropriate for named entity recognition.\\\\n4. Create an NER pipeline using the pipeline function, passing in the NER model and tokenizer created earlier.\\\\n5. Finally, pass the Spanish text as input to the NER pipeline.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Babelscape/wikineural-multilingual-ner\\\\')\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'Babelscape/wikineural-multilingual-ner\\\\')\\\\nnlp = pipeline(\\\\'ner\\\\', model=model, tokenizer=tokenizer)\\\\ntext_in_spanish = \\\"Me llamo Pablo y vivo en Madrid.\\\"\\\\nner_results = nlp(text_in_spanish)\\\\nprint(ner_results)', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Babelscape/wikineural-multilingual-ner', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\\\", 'api_arguments': {'model': 'AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)', 'tokenizer': 'AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)'}, 'python_environment_requirements': [{'transformers': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline'}], 'example_code': 'tokenizer = AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\\\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\\\nexample = My name is Wolfgang and I live in Berlin\\\\nner_results = nlp(example)\\\\nprint(ner_results)', 'performance': {'dataset': 'Babelscape/wikineural-multilingual-ner', 'accuracy': 'span-based F1-score up to 6 points over previous state-of-the-art systems for data creation'}, 'description': 'A multilingual Named Entity Recognition (NER) model fine-tuned on the WikiNEuRal dataset, supporting 9 languages (de, en, es, fr, it, nl, pl, pt, ru). It is based on the mBERT architecture and trained on all 9 languages jointly. The model can be used with the Hugging Face Transformers pipeline for NER tasks.' </s>\"\n"}
{"text": "<s>[INST]قم بتوليد قصة قصيرة باللغة العربية تبدأ بالعبارة \\\"كان ياما كان في أرض بعيدة\\\" واستخدم تقنية معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\\\", '<<<api_provider>>>': 'PyTorch Transformers', '<<<explanation>>>': 'e are an international business company, and one of our Spanish customers shares their document in Spanish. Translate it into English.\\\\n###Input: \\\"Hola, necesitamos una respuesta rápidamente sobre nuestra consulta en relación con el último negocio que realizamos juntos. Por favor, avísenos lo antes posible. Gracias.\\\"\\\\n###Output:\\\\n<<<domain>>>: Natural Language Processing Translation\\\\n<<<api_call>>>: pipeline(\\\\'translation_xx_to_yy\\\\', model=\\\\'facebook/nllb-200-distilled-600M\\\\')\\\\n<<<api_provider>>>: PyTorch Transformers\\\\n\\\\n<<<translation>>>: \\\"Hello, we need a quick response to our inquiry regarding the last business we did together. Please let us know as soon as possible. Thank you.\\\"', '<<<code>>>': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_es_to_en\\\\', model=\\\\'facebook/nllb-200-distilled-600M\\\\')\\\\ntranslated_text = translator(\\\"Hola, necesitamos una respuesta rápidamente sobre nuestra consulta en relación con el último negocio que realizamos juntos. Por favor, avísenos lo antes posible. Gracias.\\\")', 'api_call': \\\"pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\\\", 'provider': 'PyTorch Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/nllb-200-distilled-600M', 'api_call': \\\"pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\\\", 'api_arguments': ['model', 'text'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline; translator = pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M'); translator('Hello World')\\\", 'performance': {'dataset': 'Flores-200', 'accuracy': 'BLEU, spBLEU, chrF++'}, 'description': 'NLLB-200 is a machine translation model primarily intended for research in machine translation, especially for low-resource languages. It allows for single sentence translation among 200 languages. The model was trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation.' </s>\"\n"}
{"text": "<s>[INST]لإنشاء نموذج لإنتاج النصوص من النصوص في مجال معالجة اللغة الطبيعية، يرجى توفير بيانات تدريبية تتضمن أزواج من الجمل المدخلة والجملة المخرجة المقابلة. يمكن استخدام تقنيات التوليف اللوائي أو الشبكات العصبية التركيبية لتحقيق الهدف. ثم يمكن تقييم أداء النموذج باستخدام مقاييس مثل BLEU وROUGE.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"pipeline('text2text-generation', model='google/t5-v1_1-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and functions, namely, 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function to create a text2text generation model, in this case 'google/t5-v1_1-base', which is capable of translating text automatically.\\\\n3. To use this model for translating English text to French, provide the input text in English as the input with the prefix 'translate English to French:'.\\\\n4. The model will automatically translate the given text to French.\\\", '<<<code>>>': 'from transformers import pipeline\\\\n\\\\nt5 = pipeline(\\\\'text2text-generation\\\\', model=\\\\'google/t5-v1_1-base\\\\')\\\\n\\\\nbreaking_news = \\\"Hugging Face releases the most advanced language model to date.\\\"\\\\ntranslated_news = t5(f\\\"translate English to French: {breaking_news}\\\")[0][\\\\'generated_text\\\\']', 'api_call': \\\"pipeline('text2text-generation', model='google/t5-v1_1-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/t5-v1_1-base', 'api_call': \\\"pipeline('text2text-generation', model='google/t5-v1_1-base')\\\", 'api_arguments': {'model': 'google/t5-v1_1-base'}, 'python_environment_requirements': [{'transformers': '>=4.0.0'}], 'example_code': \\\"from transformers import pipeline\\\\n\\\\nt5 = pipeline('text2text-generation', model='google/t5-v1_1-base')\\\\nt5('translate English to French: Hugging Face is a great company')\\\", 'performance': {'dataset': 'c4', 'accuracy': 'Not provided'}, 'description': \\\"Google's T5 Version 1.1 is a state-of-the-art text-to-text transformer model that achieves high performance on various NLP tasks such as summarization, question answering, and text classification. It is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on downstream tasks.\\\" </s>\"\n"}
{"text": "<s>[INST]يجب على الكود توليد نص تصنيفي لملخص تصميم معالجة اللغة الطبيعية بخصوص الفئات والتصنيفات المُحتملة للنصوص. هل تستطيع تنفيذ هذا الأمر؟? [/INST]{'domain': 'Natural Language Processing Text Classification', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\\\", 'api_provider': 'Transformers', 'explanation': '1. We first import the tokenizer and model from the transformers library. The tokenizer will be used to tokenize the text input, and the model will be used for classification.', 'code': 'from transformers import AutoModelForSequenceClassification, AutoTokenizer\\\\nimport numpy as np\\\\nfrom scipy.special import softmax\\\\n\\\\ndef preprocess(text):\\\\n    new_text = []\\\\n    for t in text.split(\\\\' \\\\'):\\\\n        t = \\\\'@user\\\\' if t.startswith(\\\\'@\\\\') and len(t) > 1 else t\\\\n        t = \\\\'http\\\\' if t.startswith(\\\\'http\\\\') else t\\\\n        new_text.append(t)\\\\n    return \\\\' \\\\'.join(new_text)\\\\n\\\\nmodel_name = \\\\'cardiffnlp/twitter-roberta-base-sentiment\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\\\\n\\\\nreview_text = \\\"The hotel was clean, staff were friendly, and the location was perfect. Definitely coming back!\\\"\\\\npreprocessed_text = preprocess(review_text)\\\\nencoded_input = tokenizer(preprocessed_text, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**encoded_input)\\\\nscores = output[0][0].detach().numpy()\\\\nscores = softmax(scores)\\\\n\\\\nlabels = [\\\\'negative\\\\', \\\\'neutral\\\\', \\\\'positive\\\\']\\\\nranking = np.argsort(scores)[::-1]\\\\n\\\\nresult = [(labels[index], round(score, 4)) for index, score in enumerate(scores[ranking])]', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Transformers', 'functionality': 'Sentiment Analysis', 'api_name': 'cardiffnlp/twitter-roberta-base-sentiment', 'api_call': 'from transformers import AutoModelForSequenceClassification, AutoTokenizer\\\\nimport numpy as np\\\\nfrom scipy.special import softmax\\\\n\\\\ndef preprocess(text):\\\\n    new_text = []\\\\n    for t in text.split(\\\\' \\\\'):\\\\n        t = \\\\'@user\\\\' if t.startswith(\\\\'@\\\\') and len(t) > 1 else t\\\\n        t = \\\\'http\\\\' if t.startswith(\\\\'http\\\\') else t\\\\n        new_text.append(t)\\\\n    return \\\\' \\\\'.join(new_text)\\\\n\\\\nmodel_name = \\\\'cardiffnlp/twitter-roberta-base-sentiment\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\\\\n\\\\nreview_text = \\\"The hotel was clean, staff were friendly, and the location was perfect. Definitely coming back!\\\"\\\\npreprocessed_text = preprocess(review_text)\\\\nencoded_input = tokenizer(preprocessed_text, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**encoded_input)\\\\nscores = output[0][0].detach().numpy()\\\\nscores = softmax(scores)\\\\n\\\\nlabels = [\\\\'negative\\\\', \\\\'neutral\\\\', \\\\'positive\\\\']\\\\nranking = np.argsort(scores)[::-1]\\\\n\\\\nresult = [(labels[index], round(score, 4)) for index, score in enumerate(scores[ranking])]', 'api_arguments': ['MODEL'], 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import AutoModelForSequenceClassification\\\\nfrom transformers import TFAutoModelForSequenceClassification\\\\nfrom transformers import AutoTokenizer\\\\nimport numpy as np\\\\nfrom scipy.special import softmax\\\\nimport csv\\\\nimport urllib.request\\\\n\\\\ndef preprocess(text):\\\\n new_text = []\\\\nfor t in text.split( ):\\\\n t = '@user' if t.startswith('@') and len(t) > 1 else t\\\\n t = 'http' if t.startswith('http') else t\\\\n new_text.append(t)\\\\nreturn  .join(new_text)\\\\n\\\\ntask='sentiment'\\\\nMODEL = fcardiffnlp/twitter-roberta-base-{task}\\\\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\\\\n\\\\nlabels=[]\\\\nmapping_link = fhttps://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\\\\nwith urllib.request.urlopen(mapping_link) as f:\\\\n html = f.read().decode('utf-8').split(\\\\\\\\n)\\\\n csvreader = csv.reader(html, delimiter='\\\\\\\\t')\\\\nlabels = [row[1] for row in csvreader if len(row) > 1]\\\\n\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\\\\nmodel.save_pretrained(MODEL)\\\\ntext = Good night 😊\\\\ntext = preprocess(text)\\\\nencoded_input = tokenizer(text, return_tensors='pt')\\\\noutput = model(**encoded_input)\\\\nscores = output[0][0].detach().numpy()\\\\nscores = softmax(scores)\\\\n\\\\nranking = np.argsort(scores)\\\\nranking = ranking[::-1]\\\\nfor i in range(scores.shape[0]):\\\\n l = labels[ranking[i]]\\\\n s = scores[ranking[i]]\\\\n print(f{i+1}) {l} {np.round(float(s), 4)})\\\", 'performance': {'dataset': 'tweet_eval', 'accuracy': 'Not provided'}, 'description': 'Twitter-roBERTa-base for Sentiment Analysis. This is a roBERTa-base model trained on ~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark. This model is suitable for English.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد نصوص فيديو من وصف نصي لمشاهد تصور الحيوانات في البرية. هل يمكنك مساعدتي في تطوير تكنولوجيا معالجة اللغة الطبيعية لزيادة التشابه بين الجمل؟? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"BertModel.from_pretrained('setu4993/LaBSE')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To perform the sentence similarity comparison, we first import the BertModel and BertTokenizerFast from the transformers library provided by Hugging Face.\\\\n2. We load the pre-trained Language-agnostic BERT Sentence Encoder (LaBSE) with BertModel.from_pretrained('setu4993/LaBSE') and the corresponding tokenizer with BertTokenizerFast.from_pretrained('setu4993/LaBSE').\\\\n3. LaBSE is a BERT-based model trained on embedding sentences in 109 languages, making it suitable for multilingual sentence similarity analysis.\\\\n4. Tokenize your sentences using the LaBSE tokenizer and obtain their embeddings from the LaBSE model.\\\\n5. The embeddings can be used to compute the distance or similarity between the sentences in multiple languages.\\\", '<<<code>>>': 'import torch\\\\nfrom transformers import BertModel, BertTokenizerFast\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\\\'setu4993/LaBSE\\\\')\\\\nmodel = BertModel.from_pretrained(\\\\'setu4993/LaBSE\\\\')\\\\nmodel = model.eval()\\\\nenglish_sentences = [\\\\'dog\\\\', \\\\'Puppies are nice.\\\\', \\\\'I enjoy taking long walks along the beach with my dog.\\\\']\\\\nitalian_sentences = [\\\"cane\\\", \\\"I cuccioli sono carini.\\\", \\\"Mi piace fare lunghe passeggiate sulla spiaggia con il mio cane.\\\"]\\\\njapanese_sentences = [\\\"犬\\\", \\\"子犬はいいです。\\\", \\\"私は犬と一緒にビーチで長い散歩を楽しむ。\\\"]\\\\nenglish_inputs = tokenizer(english_sentences, return_tensors=\\\\'pt\\\\', padding=True)\\\\nitalian_inputs = tokenizer(italian_sentences, return_tensors=\\\\'pt\\\\', padding=True)\\\\njapanese_inputs = tokenizer(japanese_sentences, return_tensors=\\\\'pt\\\\', padding=True)\\\\nwith torch.no_grad():\\\\n    english_outputs = model(**english_inputs)\\\\n    italian_outputs = model(**italian_inputs)\\\\n    japanese_outputs = model(**japanese_inputs)\\\\nenglish_embeddings = english_outputs.pooler_output\\\\nitalian_embeddings = italian_outputs.pooler_output\\\\njapanese_embeddings = japanese_outputs.pooler_output', 'api_call': \\\"BertModel.from_pretrained('setu4993/LaBSE')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \\\"BertModel.from_pretrained('setu4993/LaBSE')\\\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': [['torch', 'transformers']], 'example_code': \\\"import torch\\\\nfrom transformers import BertModel, BertTokenizerFast\\\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\\\nmodel = model.eval()\\\\nenglish_sentences = [\\\\n 'dog',\\\\n 'Puppies are nice.',\\\\n 'I enjoy taking long walks along the beach with my dog.',\\\\n]\\\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\\\nwith torch.no_grad():\\\\n english_outputs = model(**english_inputs)\\\\nenglish_embeddings = english_outputs.pooler_output\\\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية باستخدام تقنية تقدير العمق. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a depth estimation model.\\\\n3. Specify the model 'sayakpaul/glpn-nyu-finetuned-diode' to be loaded, which was trained on the diode-subset dataset for depth estimation.\\\\n4. Provide an image with desired dimensions as an input, and the loaded model will estimate the depth information for each pixel in the image.\\\\n5. This depth information can be further used to adapt the properties of window blinds by estimating the distance of objects from the windows.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimport cv2\\\\ndepth_estimator = pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\\\\n\\\\n# Read the image using OpenCV\\\\nimage = cv2.imread('path/to/image.jpg')\\\\n\\\\n# Get the depth estimation for each pixel in the image\\\\ndepth_map = depth_estimator(image)\\\", 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode', 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4359, 'Rmse': 0.4276}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]يتعين عليك تشغيل التطبيق لتحويل النص إلى كلام مسموع. يمكنك تحديد لغة النص واختيار النبرة والسرعة المناسبة للنطق.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the SpeechRecognitionModel from the huggingsound library provided by Hugging Face. \\\\n2. Create an instance of the model by specifying 'jonatasgrosman/wav2vec2-large-xlsr-53-japanese' as the model to be loaded. \\\\n3. This model has been trained on a large number of Japanese speech data, allowing it to recognize spoken words and transcribe audio files to text.\\\\n4. Pass the audio file paths to the model's 'transcribe' method to obtain the text summary of the spoken content.\\\", '<<<code>>>': \\\"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')\\\\n\\\\naudio_paths = ['/path/to/audio1.wav', '/path/to/audio2.mp3']\\\\ntranscriptions = model.transcribe(audio_paths)\\\", 'api_call': \\\"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'jonatasgrosman/wav2vec2-large-xlsr-53-japanese', 'api_call': \\\"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')\\\", 'api_arguments': ['audio_paths'], 'python_environment_requirements': [['huggingsound', 'torch', 'librosa', 'datasets', 'transformers']], 'example_code': 'from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel(jonatasgrosman/wav2vec2-large-xlsr-53-japanese)\\\\naudio_paths = [/path/to/file.mp3, /path/to/another_file.wav]\\\\ntranscriptions = model.transcribe(audio_paths)', 'performance': {'dataset': 'common_voice', 'accuracy': {'WER': 81.8, 'CER': 20.16}}, 'description': 'Fine-tuned XLSR-53 large model for speech recognition in Japanese. Trained on Common Voice 6.1, CSS10, and JSUT datasets. Make sure your speech input is sampled at 16kHz.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج لغوي لمعالجة اللغات الطبيعية باستخدام تقنيات التحدث والتفاعل.? [/INST]{'domain': 'Natural Language Processing Conversational', 'api_call': \\\"pipeline('conversational', model='mywateriswet/ShuanBot')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\nchatbot = pipeline(\\\\'conversational\\\\', model=\\\\'mywateriswet/ShuanBot\\\\')\\\\nmessage = \\\"What do you think of artificial intelligence?\\\"\\\\nresponse = chatbot(message)\\\\nprint(response)', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Generation', 'api_name': 'mywateriswet/ShuanBot', 'api_call': 'from transformers import pipeline\\\\nchatbot = pipeline(\\\\'conversational\\\\', model=\\\\'mywateriswet/ShuanBot\\\\')\\\\nmessage = \\\"What do you think of artificial intelligence?\\\"\\\\nresponse = chatbot(message)\\\\nprint(response)', 'api_arguments': 'message', 'python_environment_requirements': 'transformers', 'example_code': \\\"response = chatbot('What is your name?')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.'}} </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى تطوير نظام قادر على تحليل البيانات المعقدة التي تشمل النصوص والصور والرسوم البيانية بجودة عالية لتحقيق أداء متميز في تعلم الآلة لتطبيقات متعددة.? [/INST]'<<<domain>>>': 'Multimodal Graph Machine Learning', '<<<api_call>>>': \\\"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class from the transformers package, in this case, the AutoModel class.\\\\n2. We then use the from_pretrained method of the AutoModel class to load the pre-trained Graphormer model 'graphormer-base-pcqm4mv1'. This model is specifically designed for graph classification tasks and has been pretrained on the PCQM4M-LSC dataset, which is relevant for molecule modeling.\\\\n3. This Graphormer-based model can be applied to drug discovery projects by providing input data related to molecular graphs. The model can be used for graph classification, representation, or for fine-tuning on specific drug discovery tasks.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\ngraphormer_model = AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\\\\n# Apply the Graphormer model to the drug discovery task using the input data and following the Graph Classification with Transformers tutorial instructions.\\\", 'api_call': \\\"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': \\\"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers']], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتصنيف مقاطع الفيديو باستخدام تقنيات رؤية الحاسوب، ونريد تحديد نوع الفيديو بناءً على المحتوى والمعلومات المتاحة في الفيديو.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers and decord packages. This includes VideoMAEFeatureExtractor for processing input videos, VideoMAEForVideoClassification for the video classification model, and VideoReader for decoding video files.\\\\n2. Load the VideoMAE model from the Hugging Face model hub using the from_pretrained method, specifying 'nateraw/videomae-base-finetuned-ucf101' as the model name. This model is designed for video action recognition and is fine-tuned on the UCF101 dataset.\\\\n3. Define a function `sample_frame_indices` to randomly select a desired number of equally spaced frames from the input video.\\\\n4. Use the VideoReader class from the decord package to load the video file and retrieve the selected frames.\\\\n5. Create a feature extractor to process the video frames and transform them into the appropriate format for the model.\\\\n6. Use the VideoMAEForVideoClassification model to classify the actions in the video.\\\\n7. Obtain the predicted action label and print the corresponding action name.\\\", '<<<code>>>': \\\"from decord import VideoReader, cpu\\\\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\\\nfile_path = 'video_file_path.mp4'\\\\n# replace 'video_file_path.mp4' with path to your video\\\\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\\\\nvideoreader.seek(0)\\\\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\\\\nvideo = videoreader.get_batch(indices).asnumpy()\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\\ninputs = feature_extractor(list(video), return_tensors='pt')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\\noutputs = model(**inputs)\\\\npredicted_label = outputs.logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'videomae-base-finetuned-ucf101', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/videomae-base-finetuned-ucf101'}, 'python_environment_requirements': [['transformers', 'decord', 'huggingface_hub']], 'example_code': 'from decord import VideoReader, cpu\\\\nimport torch\\\\nimport numpy as np\\\\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\\\nfrom huggingface_hub import hf_hub_download\\\\nnp.random.seed(0)\\\\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\\\\n converted_len = int(clip_len * frame_sample_rate)\\\\n end_idx = np.random.randint(converted_len, seg_len)\\\\n start_idx = end_idx - converted_len\\\\n indices = np.linspace(start_idx, end_idx, num=clip_len)\\\\n indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\\\\n return indices\\\\nfile_path = hf_hub_download(\\\\n repo_id=nateraw/dino-clips, filename=archery.mp4, repo_type=space\\\\n)\\\\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\\\\nvideoreader.seek(0)\\\\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\\\\nvideo = videoreader.get_batch(indices).asnumpy()\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\\\\ninputs = feature_extractor(list(video), return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'UCF101', 'accuracy': 0.758209764957428}, 'description': 'VideoMAE Base model fine tuned on UCF101 for Video Action Recognition' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتحويل التسجيل الصوتي إلى تسجيل صوتي آخر. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, including the SpeechT5Processor and SpeechT5ForSpeechToSpeech classes as well as the required dataset-processing modules.\\\\n2. Load the speech dataset and select the example speech audio.\\\\n3. Create a processor by loading the pretrained 'microsoft/speecht5_vc' model.\\\\n4. Use the loaded model, processor, and vocoder to convert the original speech audio into the desired voice output.\\\\n5. Save the new audio file in wav format.\\\", '<<<code>>>': \\\"from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\nimport numpy as np\\\\nimport soundfile as sf\\\\n\\\\ndataset = load_dataset('hf-internal-testing/librispeech_asr_demo', 'clean', split='validation')\\\\nsampling_rate = dataset.features['audio'].sampling_rate\\\\nexample_speech = dataset[0]['audio']['array']\\\\n\\\\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_vc')\\\\nmodel = SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\\\\n\\\\ninputs = processor(audio=example_speech, sampling_rate=sampling_rate, return_tensors='pt')\\\\nspeaker_embeddings = np.load('xvector_speaker_embedding.npy')\\\\nspeaker_embeddings = torch.tensor(speaker_embeddings).unsqueeze(0)\\\\n\\\\nspeech = model.generate_speech(inputs['input_values'], speaker_embeddings, vocoder=vocoder)\\\\n\\\\nsf.write('speech.wav', speech.numpy(), samplerate=16000)\\\", 'api_call': \\\"SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/speecht5_vc', 'api_call': \\\"SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\", 'api_arguments': {'audio': 'example_speech', 'sampling_rate': 'sampling_rate', 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'datasets', 'numpy', 'torch', 'soundfile']], 'example_code': \\\"from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset('hf-internal-testing/librispeech_asr_demo', 'clean', split='validation')\\\\ndataset = dataset.sort('id')\\\\nsampling_rate = dataset.features['audio'].sampling_rate\\\\nexample_speech = dataset[0]['audio']['array']\\\\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_vc')\\\\nmodel = SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\\\\ninputs = processor(audio=example_speech, sampling_rate=sampling_rate, return_tensors='pt')\\\\nimport numpy as np\\\\nimport torch\\\\nspeaker_embeddings = np.load('xvector_speaker_embedding.npy')\\\\nspeaker_embeddings = torch.tensor(speaker_embeddings).unsqueeze(0)\\\\nspeech = model.generate_speech(inputs['input_values'], speaker_embeddings, vocoder=vocoder)\\\\nimport soundfile as sf\\\\nsf.write('speech.wav', speech.numpy(), samplerate=16000)\\\", 'performance': {'dataset': 'CMU ARCTIC', 'accuracy': 'Not specified'}, 'description': 'SpeechT5 model fine-tuned for voice conversion (speech-to-speech) on CMU ARCTIC. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. It is designed to improve the modeling capability for both speech and text. This model can be used for speech conversion tasks.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى توليد تعليمات برمجية لتصنيف الرموز في معالجة اللغة الطبيعية. هل يمكنك مساعدتي في ذلك؟? [/INST]{'domain': 'Natural Language Processing Token Classification', 'api_call': \\\"SequenceTagger.load('flair/ner-english-ontonotes-fast')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load('flair/ner-english-ontonotes-fast')\\\\nsentence = Sentence('On September 1st George Washington won 1 dollar.')\\\\ntagger.predict(sentence)\\\\nentities = sentence.get_spans('ner')\\\\nfor entity in entities:\\\\n    print(entity)\\\", 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Named Entity Recognition', 'api_name': 'flair/ner-english-ontonotes-fast', 'api_call': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load('flair/ner-english-ontonotes-fast')\\\\nsentence = Sentence('On September 1st George Washington won 1 dollar.')\\\\ntagger.predict(sentence)\\\\nentities = sentence.get_spans('ner')\\\\nfor entity in entities:\\\\n    print(entity)\\\", 'api_arguments': ['sentence'], 'python_environment_requirements': ['flair'], 'example_code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load(flair/ner-english-ontonotes-fast)\\\\nsentence = Sentence(On September 1st George Washington won 1 dollar.)\\\\ntagger.predict(sentence)\\\\nprint(sentence)\\\\nfor entity in sentence.get_spans('ner'):\\\\n  print(entity)\\\", 'performance': {'dataset': 'Ontonotes', 'accuracy': 'F1-Score: 89.3'}, 'description': 'This is the fast version of the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on Flair embeddings and LSTM-CRF.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام للرؤية الحاسوبية وكشف الأجسام. نحتاج للقدرة على تمييز الأشكال والأنماط في الصور لتحديد الأجسام وتعريفها.? [/INST]{'domain': 'Computer Vision Object Detection', 'api_call': \\\"yolov5.load('keremberke/yolov5s-license-plate')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import and install the required packages such as yolov5 from Hugging Face.', 'code': \\\"import yolov5\\\\nmodel = yolov5.load('keremberke/yolov5s-license-plate')\\\\nmodel.conf = 0.25\\\\nmodel.iou = 0.45\\\\nmodel.agnostic = False\\\\nmodel.multi_label = False\\\\nmodel.max_det = 1000\\\\nimg = 'https://example.com/car_image.jpg'\\\\nresults = model(img, size=640, augment=True)\\\\npredictions = results.pred[0]\\\\nboxes = predictions[:, :4]\\\\nscores = predictions[:, 4]\\\\ncategories = predictions[:, 5]\\\\nresults.show()\\\\nrecognized_plates = len(predictions)\\\\ntotal_plates = len(car_images)\\\\npercentage_recognized_plates = (recognized_plates / total_plates) * 100\\\", 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5s-license-plate', 'api_call': \\\"import yolov5\\\\nmodel = yolov5.load('keremberke/yolov5s-license-plate')\\\\nmodel.conf = 0.25\\\\nmodel.iou = 0.45\\\\nmodel.agnostic = False\\\\nmodel.multi_label = False\\\\nmodel.max_det = 1000\\\\nimg = 'https://example.com/car_image.jpg'\\\\nresults = model(img, size=640, augment=True)\\\\npredictions = results.pred[0]\\\\nboxes = predictions[:, :4]\\\\nscores = predictions[:, 4]\\\\ncategories = predictions[:, 5]\\\\nresults.show()\\\\nrecognized_plates = len(predictions)\\\\ntotal_plates = len(car_images)\\\\npercentage_recognized_plates = (recognized_plates / total_plates) * 100\\\", 'api_arguments': {'img': 'image url or path', 'size': 'image resize dimensions', 'augment': 'optional, test time augmentation'}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \\\"model = yolov5.load('keremberke/yolov5s-license-plate')\\\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \\\"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \\\"results.save(save_dir='results/')\\\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.985}, 'description': 'A YOLOv5 based license plate detection model trained on a custom dataset.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نص مقروء في مجال معالجة اللغات الطبيعية لتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"pipeline('summarization', model='it5/it5-base-news-summarization')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a summarization model.\\\\n3. Specify the model 'it5/it5-base-news-summarization' to be loaded. This model is trained to automatically summarize news articles in Italian.\\\\n4. Use the created summarizer to generate a brief summary of the given news article content.\\\", '<<<code>>>': 'from transformers import pipeline\\\\narticle_summarizer = pipeline(\\\\'summarization\\\\', model=\\\\'it5/it5-base-news-summarization\\\\')\\\\nnews_article = \\\"Dal 31 maggio è infine partita la piattaforma ITsART...\\\"\\\\nsummary = article_summarizer(news_article)', 'api_call': \\\"pipeline('summarization', model='it5/it5-base-news-summarization')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'it5-base-news-summarization', 'api_call': \\\"pipeline('summarization', model='it5/it5-base-news-summarization')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': 'newsum(Dal 31 maggio è infine partita la piattaforma ITsART, a più di un anno da quando – durante il primo lockdown – il ministro della Cultura Dario Franceschini ne aveva parlato come di «una sorta di Netflix della cultura», pensata per «offrire a tutto il mondo la cultura italiana a pagamento». È presto per dare giudizi definitivi sulla piattaforma, e di certo sarà difficile farlo anche più avanti senza numeri precisi. Al momento, l’unica cosa che si può fare è guardare com’è fatto il sito, contare quanti contenuti ci sono (circa 700 “titoli”, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro varietà. Intanto, una cosa notata da più parti è che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.)', 'performance': {'dataset': 'NewsSum-IT', 'accuracy': {'Rouge1': 0.339, 'Rouge2': 0.16, 'RougeL': 0.263}}, 'description': 'IT5 Base model fine-tuned on news summarization on the Fanpage and Il Post corpora for Italian Language Understanding and Generation.' </s>\"\n"}
{"text": "<s>[INST]بناء نماذج للمعالجة اللغوية الطبيعية في مجال الترجمة، يتطلب تطوير نظام قادر على فهم النصوص بشكل دقيق وتوليد ترجمات فعالة ودقيقة.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"ORTModelForSeq2SeqLM.from_pretrained('optimum/t5-small')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the required libraries: the pipeline and AutoTokenizer from the transformers package, and ORTModelForSeq2SeqLM from the optimum.onnxruntime package.', 'code': 'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'optimum/t5-small\\\\')\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(\\\\'optimum/t5-small\\\\')\\\\ntranslator = pipeline(\\\\'translation_en_to_fr\\\\', model=model, tokenizer=tokenizer)\\\\nresults = translator(\\\"My name is Eustache and I have a pet raccoon.\\\")\\\\nprint(results)', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Transformers', 'functionality': 'Text-to-Text Generation', 'api_name': 'optimum/t5-small', 'api_call': 'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'optimum/t5-small\\\\')\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(\\\\'optimum/t5-small\\\\')\\\\ntranslator = pipeline(\\\\'translation_en_to_fr\\\\', model=model, tokenizer=tokenizer)\\\\nresults = translator(\\\"My name is Eustache and I have a pet raccoon.\\\")\\\\nprint(results)', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers', 'optimum.onnxruntime'], 'example_code': 'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\\\nresults = translator(My name is Eustache and I have a pet raccoon)\\\\nprint(results)', 'performance': {'dataset': 'c4', 'accuracy': 'N/A'}, 'description': 'T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتحويل البيانات الجدولية إلى نموذج تنبؤي يستطيع التنبؤ بالقيم المستقبلية باستخدام تقنيات التحليل الاحصائي. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. To use the model for predicting carbon emissions, first import necessary libraries: joblib, json, and pandas.\\\\n2. Load the trained model from the file 'model.joblib' using joblib.load() function.\\\\n3. Load the configuration file, which contains the feature names for the dataset, using json.load() function.\\\\n4. Read the input data using pandas' read_csv() function, and filter the data based on the features listed in the configuration file.\\\\n5. Rename the columns of filtered data with the prefix 'feat_'.\\\\n6. Use the model's predict() function to obtain predictions for carbon emissions based on the input data.\\\", '<<<code>>>': \\\"import joblib\\\\nimport json\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kochetkovIT/autotrain-ironhack-49741119788', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['json', 'joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'kochetkovIT/autotrain-data-ironhack', 'accuracy': {'Loss': 2.603, 'R2': 0.013, 'MSE': 6.776, 'MAE': 1.666, 'RMSLE': 0.502}}, 'description': 'A tabular regression model trained using AutoTrain to predict carbon emissions (in grams) with an R2 score of 0.013.' </s>\"\n"}
{"text": "<s>[INST]يحتاج عميلنا المتخصص في مجال معالجة اللغات الطبيعية والإجابة على الأسئلة إلى تطوير برنامج يمكنه تحويل النصوص إلى كلام قابل للفهم لمستخدمي تطبيقه.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model='deepset/roberta-large-squad2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the necessary class 'pipeline' from the transformers package. This will allow us to load the model and set it up to answer questions.\\\\n2. Use the 'pipeline' function to load the pre-trained RoBERTa question answering model 'deepset/roberta-large-squad2'. This model is specifically trained to answer questions based on a given context and is suitable for answering readers' questions based on your medical articles.\\\\n3. With the loaded model, you can pass the reader's question and the article's text to the model to produce an answer for the reader.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=\\\\'deepset/roberta-large-squad2\\\\')\\\\nquestion = \\\"What is the best treatment for diabetes?\\\"\\\\narticle_context = \\\"The best treatment for diabetes includes a combination of healthy eating, regular physical activity, and medications as needed.\\\"\\\\nanswer = nlp({\\\"question\\\": question, \\\"context\\\": article_context})', 'api_call': \\\"pipeline('question-answering', model='deepset/roberta-large-squad2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'deepset/roberta-large-squad2', 'api_call': \\\"pipeline('question-answering', model='deepset/roberta-large-squad2')\\\", 'api_arguments': ['question', 'context'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; nlp = pipeline('question-answering', model='deepset/roberta-large-squad2'); nlp({'question': 'What is the capital of Germany?', 'context': 'Berlin is the capital of Germany.'})\\\", 'performance': {'dataset': 'squad_v2', 'accuracy': 'Not provided'}, 'description': 'A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.' </s>\"\n"}
{"text": "<s>[INST]أنشئ سؤالًا قصيرًا حول التشابه بين الجمل لمعالجة اللغة الطبيعية واستخدم اللغة العربية.? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Install the required Python package using 'pip install -U sentence-transformers'.\\\", 'code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = ['This is an example sentence.', 'Each sentence is converted.']\\\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Sentence Transformers', 'api_name': 'sentence-transformers/all-MiniLM-L6-v2', 'api_call': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = ['This is an example sentence.', 'Each sentence is converted.']\\\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': '1B sentence pairs dataset', 'accuracy': 'https://seb.sbert.net'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}} </s>\"\n"}
{"text": "<s>[INST]أوجد لي توليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية في مجال معالجة اللغة الطبيعية. هل يمكنك المساعدة في تحقيق ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. We first import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. We then create a text generation pipeline by specifying the model \\\\'sshleifer/tiny-gpt2\\\\'. This is a tiny GPT-2 model that is suitable for generating text efficiently.\\\\n3. With this pipeline, we can pass in the prompt \\\"It was a dark and stormy night.\\\" as input and generate a continuation to the given text.', '<<<code>>>': \\\"from transformers import pipeline\\\\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\\\\nresult = nlp('It was a dark and stormy night.')\\\", 'api_call': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sshleifer/tiny-gpt2', 'api_call': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", 'api_arguments': {'model': 'sshleifer/tiny-gpt2'}, 'python_environment_requirements': [{'huggingface_transformers': '>=4.0.0'}], 'example_code': \\\"from transformers import pipeline\\\\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\\\\nresult = nlp('Once upon a time')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A tiny GPT-2 model for text generation, suitable for low-resource environments and faster inference. This model is part of the Hugging Face Transformers library and can be used for generating text given a prompt.' </s>\"\n"}
{"text": "<s>[INST]عميلنا يحتاج إلى تطوير نموذج لتصنيف الرموز اللغوية في مجال معالجة اللغة الطبيعية. نحن بحاجة إلى برنامج يستطيع تحديد الأنواع المختلفة من الرموز في النص، مثل الأسماء الخاصة والتواريخ والمكان، وذلك لتحسين أداء تطبيقاتنا.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers library, which includes BertTokenizerFast for tokenizing the text and AutoModel to build the Part-of-speech (POS) tagging model.\\\\n2. Initialize the tokenizer with the pre-trained 'bert-base-chinese' tokenizer using the from_pretrained method of the BertTokenizerFast class.\\\\n3. Initialize the POS tagging model with the pre-trained 'ckiplab/bert-base-chinese-pos' using the from_pretrained method of the AutoModel class.\\\\n4. Tokenize the Chinese email text using the tokenizer.\\\\n5. Input the tokenized text to the model to get the POS tags for each word in the text.\\\", '<<<code>>>': \\\"from transformers import BertTokenizerFast, AutoModel\\\\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\\\\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\\ninputs = tokenizer(email_text, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\npredicted_pos_tags = outputs.logits.argmax(-1).tolist()\\\", 'api_call': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ckiplab/bert-base-chinese-pos', 'api_call': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'api_arguments': {'tokenizer': \\\"BertTokenizerFast.from_pretrained('bert-base-chinese')\\\"}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import (\\\\n  BertTokenizerFast,\\\\n  AutoModel,\\\\n)\\\\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\\\\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).' </s>\"\n"}
{"text": "<s>[INST]قم بتصنيف الرموز الخاصة بمعالجة اللغة الطبيعية باستخدام تقنية تصنيف الرموز الخاصة بالنص. يرجى ضمان دقة التصنيف وتحديد الرموز بشكل صحيح.? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"XLMRobertaForSequenceClassification.from_pretrained('vicgalle/xlm-roberta-large-xnli-anli')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import XLMRobertaForSequenceClassification\\\\nmodel = XLMRobertaForSequenceClassification.from_pretrained('vicgalle/xlm-roberta-large-xnli-anli')\\\\ndocument_classifier = pipeline('zero-shot-classification', model=model)\\\\ncategories = ['travel', 'cooking', 'dancing']\\\\nclassification_results = document_classifier(document_text, candidate_labels=categories)\\\", 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Classification', 'api_name': 'vicgalle/xlm-roberta-large-xnli-anli', 'api_call': \\\"from transformers import XLMRobertaForSequenceClassification\\\\nmodel = XLMRobertaForSequenceClassification.from_pretrained('vicgalle/xlm-roberta-large-xnli-anli')\\\\ndocument_classifier = pipeline('zero-shot-classification', model=model)\\\\ncategories = ['travel', 'cooking', 'dancing']\\\\nclassification_results = document_classifier(document_text, candidate_labels=categories)\\\", 'api_arguments': ['sequence_to_classify', 'candidate_labels'], 'python_environment_requirements': ['transformers'], 'example_code': \\\"sequence_to_classify = Algún día iré a ver el mundo\\\\ncandidate_labels = ['viaje', 'cocina', 'danza']\\\\nclassifier(sequence_to_classify, candidate_labels)\\\", 'performance': {'dataset': [{'name': 'XNLI-es', 'accuracy': '93.7%'}, {'name': 'XNLI-fr', 'accuracy': '93.2%'}, {'name': 'ANLI-R1', 'accuracy': '68.5%'}, {'name': 'ANLI-R2', 'accuracy': '53.6%'}, {'name': 'ANLI-R3', 'accuracy': '49.0%'}]}, 'description': 'XLM-RoBERTa-large model finetunned over several NLI datasets, ready to use for zero-shot classification.'}} </s>\"\n"}
{"text": "<s>[INST]لطاز ندعوك لحضور ورشة عمل حول موازنة الجمل في معالجة اللغة الطبيعية. ستتضمن الورشة تدريبات على تحسين مهارات التناسق والتشابه بين الجمل النصية المختلفة.? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"INSTRUCTOR('hkunlp/instructor-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from InstructorEmbedding import INSTRUCTOR\\\\nmodel = INSTRUCTOR(\\\\'hkunlp/instructor-base\\\\')\\\\ninstruction = \\\"Find books with a similar description.\\\"\\\\n\\\\nbook_descriptions = [\\\"Book 1 description\\\", \\\"Book 2 description\\\", \\\"Book 3 description\\\"]\\\\nembeddings = [model.encode([[instruction, description]]) for description in book_descriptions]\\\\n\\\\n# Compute similarities between each pair of embeddings and find the most similar pairs.', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Sentence Transformers', 'api_name': 'hkunlp/instructor-base', 'api_call': 'from InstructorEmbedding import INSTRUCTOR\\\\nmodel = INSTRUCTOR(\\\\'hkunlp/instructor-base\\\\')\\\\ninstruction = \\\"Find books with a similar description.\\\"\\\\n\\\\nbook_descriptions = [\\\"Book 1 description\\\", \\\"Book 2 description\\\", \\\"Book 3 description\\\"]\\\\nembeddings = [model.encode([[instruction, description]]) for description in book_descriptions]\\\\n\\\\n# Compute similarities between each pair of embeddings and find the most similar pairs.', 'api_arguments': {'instruction': 'string', 'sentence': 'string'}, 'python_environment_requirements': 'pip install InstructorEmbedding', 'example_code': \\\"from InstructorEmbedding import INSTRUCTOR\\\\nmodel = INSTRUCTOR('hkunlp/instructor-base')\\\\nsentence = 3D ActionSLAM: wearable person tracking in multi-floor environments\\\\ninstruction = Represent the Science title:\\\\nembeddings = model.encode([[instruction,sentence]])\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'MTEB AmazonCounterfactualClassification (en)', 'accuracy': 86.209}, 'description': 'Instructor is an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor achieves state-of-the-art performance on 70 diverse embedding tasks.'}} </s>\"\n"}
{"text": "<s>[INST]عميلنا يرغب في تطوير تطبيق لتحويل ملفات الصوت إلى تسجيلات صوتية أخرى. يجب تصميم برنامج يمكنه تحويل صيغ الصوت المختلفة بدقة وسرعة عالية لتلبية احتياجات المستخدمين.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"hf_hub_download(repo_id='JorisCos/ConvTasNet_Libri2Mix_sepclean_8k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is 'hf_hub_download' from huggingface_hub.\\\\n2. Download the ConvTasNet_Libri2Mix_sepclean_8k model from the Hugging Face Hub using the hf_hub_download function with the repo_id 'JorisCos/ConvTasNet_Libri2Mix_sepclean_8k'. This model has been trained on the Libri2Mix dataset to separate speakers in an audio recording.\\\\n3. Load the downloaded model using the appropriate library (e.g., Asteroid) to perform source separation on the noisy audio input.\\\\n4. Pass the noisy audio input to the model for processing. The output will be a version of the input audio with the speaker's voices separated.\\\", '<<<code>>>': \\\"from huggingface_hub import hf_hub_download\\\\nrepo_id = 'JorisCos/ConvTasNet_Libri2Mix_sepclean_8k'\\\\nmodel_file = hf_hub_download(repo_id=repo_id)\\\\n# Load the model using the appropriate library (e.g., Asteroid) and perform source separation on the input audio.\\\", 'api_call': \\\"hf_hub_download(repo_id='JorisCos/ConvTasNet_Libri2Mix_sepclean_8k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ConvTasNet_Libri2Mix_sepclean_8k', 'api_call': \\\"hf_hub_download(repo_id='JorisCos/ConvTasNet_Libri2Mix_sepclean_8k')\\\", 'api_arguments': ['repo_id', 'filename'], 'python_environment_requirements': [['huggingface_hub']], 'example_code': '', 'performance': {'dataset': 'Libri2Mix', 'accuracy': {'si_sdr': 14.764543634468069, 'si_sdr_imp': 14.764029375607246, 'sdr': 15.29337970745095, 'sdr_imp': 15.114146605113111, 'sir': 24.092904661115366, 'sir_imp': 23.913669683141528, 'sar': 16.06055906916849, 'sar_imp': -51.980784441287454, 'stoi': 0.9311142440593033, 'stoi_imp': 0.21817376142710482}}, 'description': 'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي في مجال معالجة اللغة الطبيعية والإجابة على الأسئلة.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"pipeline('table-question-answering', model='Meena/table-question-answering-tapas')\\\", '<<<api_provider>>>': 'PyTorch Transformers', '<<<explanation>>>': '1. Import the pipeline function from the transformers library provided by PyTorch Transformers\\\\n2. The pipeline function is used to create a table question answering (TQA) model, which is capable of understanding and answering questions about structured tabular data.\\\\n3. We specify the model \\\\'Meena/table-question-answering-tapas\\\\' to be loaded. TAPAS is a BERT-based model that has been pre-trained on millions of tables from English Wikipedia and can answer questions about tabular data.\\\\n4. The created TQA model can be used to process the tabular information and answer questions about the data contained in the table, such as \\\"What is the price of the DVD with the highest resolution?\\\"', '<<<code>>>': 'from transformers import pipeline\\\\ntqa = pipeline(\\\\'table-question-answering\\\\', model=\\\\'Meena/table-question-answering-tapas\\\\')\\\\nquestion = \\\"What is the price of the DVD with the highest resolution?\\\"\\\\n# Replace \\\\'table_data\\\\' with the actual table containing DVD features and prices\\\\nresult = tqa(question=question, table=table_data)', 'api_call': \\\"pipeline('table-question-answering', model='Meena/table-question-answering-tapas')\\\", 'provider': 'PyTorch Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'table-question-answering-tapas', 'api_call': \\\"pipeline('table-question-answering', model='Meena/table-question-answering-tapas')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': [{'name': 'SQA (Sequential Question Answering by Microsoft)', 'accuracy': None}, {'name': 'WTQ (Wiki Table Questions by Stanford University)', 'accuracy': None}, {'name': 'WikiSQL (by Salesforce)', 'accuracy': None}]}, 'description': 'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء موديل يستطيع الإجابة على أسئلة متعددة الوسائط مثل الصور والفيديو باستخدام الذكاء الاصطناعي. ضع خطة لتدريب النموذج باستخدام مجموعة البيانات المخصصة وضبطه لأداء مثالي في الإجابة على الأسئلة.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"AutoModel.from_pretrained('microsoft/git-base-textvqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and functions, specifically AutoModel and the tokenizer from transformers.\\\\n2. Load the 'microsoft/git-base-textvqa' model using the AutoModel.from_pretrained() method. This model is designed to generate text conditioned on both image and text tokens.\\\\n3. Write a function that takes an image file path and the text question as input. This function should tokenize the text and preprocess the image.\\\\n4. Utilize the model to generate text tokens conditioned on the image tokens and text tokens of the input image and question. Then, convert the generated text tokens back to text.\\\\n5. The resulting text will provide a description of the actions happening within the image and answer any questions regarding the contents of the image.\\\", '<<<code>>>': \\\"from transformers import AutoModel, AutoTokenizer\\\\ngit_model = AutoModel.from_pretrained('microsoft/git-base-textvqa')\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/git-base-textvqa')\\\\n\\\\ndef generate_description_and_answer(image_path, question):\\\\n    with open(image_path, 'rb') as image_file:\\\\n        encoded_image = tokenizer(image_file.read(), return_tensors='pt')\\\\n\\\\n    input_text = tokenizer(question, return_tensors='pt')\\\\n    input_tokens = {key: val + input_text[key] for key, val in encoded_image.items()}\\\\n    generated = git_model.generate(**input_tokens)\\\\n    description_and_answer = tokenizer.decode(generated[0])\\\\n    return description_and_answer\\\\n\\\\nresult = generate_description_and_answer('path/to/image.jpg', 'What is happening in the image?')\\\", 'api_call': \\\"AutoModel.from_pretrained('microsoft/git-base-textvqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/git-base-textvqa', 'api_call': \\\"AutoModel.from_pretrained('microsoft/git-base-textvqa')\\\", 'api_arguments': 'image, question', 'python_environment_requirements': ['transformers'], 'example_code': \\\"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\\\", 'performance': {'dataset': 'TextVQA', 'accuracy': 'Refer to the paper'}, 'description': \\\"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\\\" </s>\"\n"}
{"text": "<s>[INST]يتعين على النموذج إنشاء مقطع فيديو من النص الذي يصف مهمة استجابة الأسئلة بتقنيات معالجة اللغة الطبيعية بحيث يمكن تصنيف الأسئلة المختلفة من دون دعم سابق.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"AutoModel.from_pretrained('deepset/bert-medium-squad2-distilled')\\\", 'api_provider': 'PyTorch Transformers', 'explanation': '1. We start by importing the necessary modules from the transformers library—AutoModel and AutoTokenizer—for working with the pre-trained model.', 'code': \\\"from transformers import AutoModel, AutoTokenizer\\\\nmodel = AutoModel.from_pretrained('deepset/bert-medium-squad2-distilled')\\\\ntokenizer = AutoTokenizer.from_pretrained('deepset/bert-medium-squad2-distilled')\\\\ninputs = tokenizer.encode_plus(question, context, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nanswer_start = outputs[0].argmax().item()\\\\nanswer_end = outputs[1].argmax().item()\\\\nanswer = tokenizer.decode(inputs['input_ids'][0][answer_start:answer_end + 1])\\\", 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'PyTorch Transformers', 'functionality': 'Question Answering', 'api_name': 'deepset/bert-medium-squad2-distilled', 'api_call': \\\"from transformers import AutoModel, AutoTokenizer\\\\nmodel = AutoModel.from_pretrained('deepset/bert-medium-squad2-distilled')\\\\ntokenizer = AutoTokenizer.from_pretrained('deepset/bert-medium-squad2-distilled')\\\\ninputs = tokenizer.encode_plus(question, context, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nanswer_start = outputs[0].argmax().item()\\\\nanswer_end = outputs[1].argmax().item()\\\\nanswer = tokenizer.decode(inputs['input_ids'][0][answer_start:answer_end + 1])\\\", 'api_arguments': {'model': 'deepset/bert-medium-squad2-distilled'}, 'python_environment_requirements': {'transformers': '>=4.0.0'}, 'example_code': \\\"qa({'context': 'This is an example context.', 'question': 'What is this example about?'})\\\", 'performance': {'dataset': 'squad_v2', 'exact': 68.6431398972458, 'f1': 72.7637083790805}, 'description': 'This model is a distilled version of deepset/bert-large-uncased-whole-word-masking-squad2, trained on the SQuAD 2.0 dataset for question answering tasks. It is based on the BERT-medium architecture and uses the Hugging Face Transformers library.'}} </s>\"\n"}
{"text": "<s>[INST]أنا متخصص في مجال الرؤية الحاسوبية لتحويل الصور من صورة إلى أخرى. هل تحتاج مساعدة في تطوير نظام لتحويل الصور إلى صور أخرى بشكل تلقائي؟? [/INST]{'domain': 'Computer Vision Image-to-Image', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': \\\"from diffusers import ControlNetModel, StableDiffusionControlNetPipeline, UniPCMultistepScheduler\\\\nfrom PIL import Image\\\\nimport torch\\\\nfrom diffusers.utils import load_image\\\\n\\\\nimage = load_image('input_image.jpg').convert('RGB')\\\\n\\\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained('fusing/stable-diffusion-v1-5-controlnet-normal', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\n\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\n\\\\nnormal_map_image = pipe('image_description', image, num_inference_steps=20).images[0]\\\\nnormal_map_image.save('normal_map_output.png')\\\", 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Normal Map Estimation', 'api_name': 'lllyasviel/sd-controlnet-normal', 'api_call': \\\"from diffusers import ControlNetModel, StableDiffusionControlNetPipeline, UniPCMultistepScheduler\\\\nfrom PIL import Image\\\\nimport torch\\\\nfrom diffusers.utils import load_image\\\\n\\\\nimage = load_image('input_image.jpg').convert('RGB')\\\\n\\\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained('fusing/stable-diffusion-v1-5-controlnet-normal', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\n\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\n\\\\nnormal_map_image = pipe('image_description', image, num_inference_steps=20).images[0]\\\\nnormal_map_image.save('normal_map_output.png')\\\", 'api_arguments': ['image', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \\\"from PIL import Image\\\\nfrom transformers import pipeline\\\\nimport numpy as np\\\\nimport cv2\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom diffusers.utils import load_image\\\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png).convert(RGB)\\\\ndepth_estimator = pipeline(depth-estimation, model =Intel/dpt-hybrid-midas )\\\\nimage = depth_estimator(image)['predicted_depth'][0]\\\\nimage = image.numpy()\\\\nimage_depth = image.copy()\\\\nimage_depth -= np.min(image_depth)\\\\nimage_depth /= np.max(image_depth)\\\\nbg_threhold = 0.4\\\\nx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\\\\nx[image_depth &lt; bg_threhold] = 0\\\\ny = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\\\\ny[image_depth &lt; bg_threhold] = 0\\\\nz = np.ones_like(x) * np.pi * 2.0\\\\nimage = np.stack([x, y, z], axis=2)\\\\nimage /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\\\\nimage = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\\\\nimage = Image.fromarray(image)\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\n fusing/stable-diffusion-v1-5-controlnet-normal, torch_dtype=torch.float16\\\\n)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\nimage = pipe(cute toy, image, num_inference_steps=20).images[0]\\\\nimage.save('images/toy_normal_out.png')\\\", 'performance': {'dataset': 'DIODE', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم الشركة بتطوير نظام لتقدير العمق في مجال رؤية الحاسوب، ونحتاج إلى بناء نموذج يستند إلى البيانات المتوفرة لتحقيق ذلك.? [/INST]{'domain': 'Computer Vision Depth Estimation', 'api_call': 'AutoModel.from_pretrained(\\\"sayakpaul/glpn-nyu-finetuned-diode-221215-095508\\\")', 'api_provider': 'Hugging Face Transformers', 'explanation': '1. First, import the AutoModel class from the transformers library, which is provided by Hugging Face.', 'code': 'from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained(\\\"sayakpaul/glpn-nyu-finetuned-diode-221215-095508\\\")', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-095508', 'api_call': 'from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained(\\\"sayakpaul/glpn-nyu-finetuned-diode-221215-095508\\\")', 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': 'DIODE', 'accuracy': None}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. The pipeline function is used to create a text-to-image model capable of generating images based on text descriptions, specifically the 'gsdf/Counterfeit-V2.5' model.\\\\n3. This model is designed for generating anime-style images, which is suitable for creating custom trading card game characters, such as a mystical creature.\\\\n4. We can provide a text description of the mystical creature, and the model will generate an image based on it. The generated image can then be used as an illustration for the trading card game.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nimage_generator = pipeline(\\\\'text-to-image\\\\', model=\\\\'gsdf/Counterfeit-V2.5\\\\')\\\\nmystical_creature_description = \\\"A mystical creature with dragon wings, a unicorn horn, and a lion\\\\'s body, surrounded by enchanted forest\\\"\\\\nresult = image_generator(mystical_creature_description)', 'api_call': \\\"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \\\"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>\"\n"}
{"text": "<s>[INST]يقوم فريق البحث بتطوير نموذج لقياس تشابه الجمل بين النصوص باستخدام تقنيات معالجة اللغة الطبيعية، ونحن بحاجة لزيادة الدقة والكفاءة في تقدير تشابه الجمل.? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the SentenceTransformer from the sentence_transformers library.', 'code': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/paraphrase-MiniLM-L3-v2\\\\')\\\\nquestion = \\\"How do I reset my password?\\\"\\\\narticles = [\\\"How to change your password\\\", \\\"Updating your email address\\\", \\\"Resetting your account password\\\"]\\\\nembeddings = model.encode([question] + articles)\\\\nquestion_embedding, *article_embeddings = embeddings\\\\nsimilarity_scores = cosine_similarity([question_embedding], article_embeddings)\\\\nrecommended_articles = sorted(articles, key=lambda x: -similarity_scores[0][articles.index(x)])\\\\nprint(\\\"Most relevant articles are: \\\", recommended_articles[:3])', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Sentence Embeddings', 'api_name': 'sentence-transformers/paraphrase-MiniLM-L3-v2', 'api_call': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/paraphrase-MiniLM-L3-v2\\\\')\\\\nquestion = \\\"How do I reset my password?\\\"\\\\narticles = [\\\"How to change your password\\\", \\\"Updating your email address\\\", \\\"Resetting your account password\\\"]\\\\nembeddings = model.encode([question] + articles)\\\\nquestion_embedding, *article_embeddings = embeddings\\\\nsimilarity_scores = cosine_similarity([question_embedding], article_embeddings)\\\\nrecommended_articles = sorted(articles, key=lambda x: -similarity_scores[0][articles.index(x)])\\\\nprint(\\\"Most relevant articles are: \\\", recommended_articles[:3])', 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'snli, multi_nli, ms_marco', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج تجريبي لمقارنة الجمل في مجال معالجة اللغة الطبيعية، ابدأ بتحميل مجموعة البيانات وتصفيتها ثم تدريب النموذج باستخدام خوارزمية مقارنة جمل. قم بإجراء الاختبارات الضرورية على النموذج لضمان دقته. احتفظ بتحديثات النموذج وقم بمشاركتها مع فريقك للحصول على ملاحظات وتعديلات.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the SentenceTransformer class from the sentence_transformers module.\\\\n2. We load the pre-trained model 'sentence-transformers/nli-mpnet-base-v2', which has been trained to produce meaningful embeddings for natural language texts.\\\\n3. We encode the input sentences using the model.encode() method to compute embeddings for each sentence.\\\\n4. We calculate the pairwise cosine similarity between the embeddings of the input sentences to measure their similarity.\\\\n5. We display the pairs with highest similarity, representing the most similar variations to the original sentence.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nimport numpy as np\\\\n\\\\nsentences = [\\\"The perfect sale for the holiday season\\\", \\\"Amazing holiday sale awaits you\\\", \\\"Holiday season special: incredible sales\\\", \\\"Huge discounts are close, don\\\\'t miss them\\\", \\\"Celebrate the holidays with our exclusive sale\\\"]\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/nli-mpnet-base-v2\\\\')\\\\nembeddings = model.encode(sentences)\\\\nsimilarity_matrix = cosine_similarity(embeddings)\\\\nsimilar_pairs = np.argwhere(similarity_matrix > 0.8)\\\\nresult = [tuple(sentences[i] for i in pair) for pair in similar_pairs if pair[0] != pair[1]]\\\\nprint(result)', 'api_call': \\\"SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/nli-mpnet-base-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا ببناء نموذج لتصنيف الصور بدون تصنيف بناءً على مجموعة من الصور التي لم تتم تصنيفها سابقًا، ونريد التنبؤ بتصنيف الصور بدقة باستخدام تقنيات الرؤية الحاسوبية.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"CLIPModel.from_pretrained('patrickjohncyh/fashion-clip')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. This includes CLIPProcessor for pre-processing the image and text data and CLIPModel for the zero-shot image classification model.\\\\n2. Load the pre-trained model 'patrickjohncyh/fashion-clip' using the from_pretrained method of the CLIPModel class. This model is specifically trained for fashion industry applications.\\\\n3. Load the processor that matches the model using the from_pretrained method of the CLIPProcessor class.\\\\n4. Prepare the inputs to the model, which includes the image and text data. The text data should include names of the clothing categories you wish to classify the image.\\\\n5. Use the model to get logits_per_image, which represent the probability distribution of the image belonging to each of the categories mentioned earlier. The softmax function is used to convert logits into probabilities.\\\", '<<<code>>>': 'from transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(\\\\'patrickjohncyh/fashion-clip\\\\')\\\\nprocessor = CLIPProcessor.from_pretrained(\\\\'patrickjohncyh/fashion-clip\\\\')\\\\nimage_path = \\\"path/to/image.jpg\\\" # replace with the path to the customer\\\\'s image\\\\nimage = Image.open(image_path)\\\\nclass_names = \\\"t-shirt, dress, shoes, pants, hat\\\"\\\\ninputs = processor(text=class_names, images=image, return_tensors=\\\\'pt\\\\', padding=True)\\\\nlogits_per_image = model(**inputs).logits_per_image\\\\nprobs = logits_per_image.softmax(dim=-1).tolist()[0]', 'api_call': \\\"CLIPModel.from_pretrained('patrickjohncyh/fashion-clip')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'patrickjohncyh/fashion-clip', 'api_call': \\\"CLIPModel.from_pretrained('patrickjohncyh/fashion-clip')\\\", 'api_arguments': {'image': 'File', 'class_names': 'String (comma-separated)'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained('patrickjohncyh/fashion-clip'); processor = CLIPProcessor.from_pretrained('patrickjohncyh/fashion-clip'); inputs = processor(text='blue shoes', images=image, return_tensors='pt', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\\\", 'performance': {'dataset': [{'name': 'FMNIST', 'accuracy': 0.83}, {'name': 'KAGL', 'accuracy': 0.73}, {'name': 'DEEP', 'accuracy': 0.62}]}, 'description': 'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.' </s>\"\n"}
{"text": "<s>[INST]قم بتدريب نموذج شبكة عصبونية لتقسيم الصور باستخدام بيانات تدريب ملصقة لتنفيذ مهمة تقسيم الصور في مجال رؤية الحاسوب.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries such as PIL, transformers, and requests.\\\\n2. Set up the API processor using AutoImageProcessor.from_pretrained() with the model 'facebook/mask2former-swin-large-coco-panoptic'.\\\\n3. Load the image from your shipping yard ['url'] and preprocess it using the processor.\\\\n4. Predict the universal segmentation using the trained Mask2Former model.\\\\n5. Post-process the output prediction to generate a panoptic segmentation map, which will have different sections and types of objects segmented and labeled in the image.\\\", '<<<code>>>': \\\"from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\\n\\\\nurl = 'your_shipping_yard_image_url_here'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n\\\\npredicted_panoptic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]['segmentation']\\\", 'api_call': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/mask2former-swin-large-coco-panoptic', 'api_call': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\", 'api_arguments': ['image'], 'python_environment_requirements': [['requests', 'torch', 'PIL', 'transformers']], 'example_code': 'processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result[segmentation]', 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نماذج لتحويل النصوص إلى صور متعددة الوسائط، ونحتاج إلى تقدير عمق الأشياء المصورة في الصور، خاصة في تقنيات تقسيم الصور في رؤية الحواسيب.? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the necessary classes for image processing and the Mask2FormerForUniversalSegmentation class from the provided libraries.', 'code': \\\"from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nurl = 'http://your_surveillance_camera_image_url'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-cityscapes-semantic', 'api_call': \\\"from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nurl = 'http://your_surveillance_camera_image_url'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-large-cityscapes-semantic'}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \\\"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نماذج لتصنيف الصور الصفرية باستخدام رؤية الحاسوب، ونحتاج إلى تقدير عمق الأشياء في الصور التي سيتم تصنيفها.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Initialize the image classification model by specifying the 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k' model.\\\\n3. This model is a part of the CLIP architecture, which is designed for zero-shot image classification. So, even if the model is not trained on a specific animal, it can recognize the animal from a given image.\\\\n4. Use the created classifier to determine whether the image contains a cat or a dog by providing the image file and possible class names (in this case, 'cat' and 'dog').\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\\\\nresult = image_classifier(image_file, possible_class_names=['cat', 'dog'])\\\", 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k', 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\\\", 'api_arguments': ['image', 'possible_class_names'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'); classifier(image, possible_class_names=['cat', 'dog'])\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '80.1'}, 'description': 'A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. The model is intended for research purposes and enables researchers to better understand and explore zero-shot, arbitrary image classification. It can be used for interdisciplinary studies of the potential impact of such models. The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام للتعرف التلقائي على الكلام الصوتي، ونحتاج إلى تحسين دقة التعرف على الكلام من خلال استخدام تقنيات متقدمة في مجال تحليل الموجات الصوتية والذكاء الاصطناعي.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the necessary classes from the transformers library, which are WhisperProcessor and WhisperForConditionalGeneration.\\\\n2. Load the whisper-large pretrained model using the from_pretrained method of the WhisperForConditionalGeneration class.\\\\n3. Load the processor (openai/whisper-large) using the from_pretrained method of the WhisperProcessor class.\\\\n4. Pass the audio file to the processor and obtain the input features.\\\\n5. Pass the input features to the whisper model using the generate method to get the predicted transcription.\\\\n6. Decode the predicted transcription to obtain the final text.', '<<<code>>>': \\\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-large')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large')\\\\n\\\\naudio_sample = '<audio_sample_representation>'\\\\nsampling_rate = '<sample_rate>'\\\\ninput_features = processor(audio_sample, sampling_rate=sampling_rate, return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\\\", 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai/whisper-large', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large')\\\", 'api_arguments': ['audio', 'sampling_rate'], 'python_environment_requirements': [['transformers', 'datasets']], 'example_code': 'from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\n\\\\nprocessor = WhisperProcessor.from_pretrained(openai/whisper-large)\\\\nmodel = WhisperForConditionalGeneration.from_pretrained(openai/whisper-large)\\\\n\\\\nmodel.config.forced_decoder_ids = None\\\\n\\\\nds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\\\\nsample = ds[0][audio]\\\\ninput_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\\\\n\\\\npredicted_ids = model.generate(input_features)\\\\n\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=False)', 'performance': {'dataset': [{'name': 'LibriSpeech (clean)', 'accuracy': 3.0}, {'name': 'LibriSpeech (other)', 'accuracy': 5.4}, {'name': 'Common Voice 11.0', 'accuracy': 54.8}]}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.' </s>\"\n"}
{"text": "<s>[INST]قم بتنفيذ الخطوات التالية للعب ضد وكيل التعلم الآلي في نادي طلابي لكرة القدم الافتراضية: 1. افتح التطبيق أو المنصة الخاصة بلعبة الكرة القدم الافتراضية. 2. اختر خيار اللعب ضد الوكيل التفاعلي. 3. قم بتحديد اعدادات اللعبة وابدأ المباراة. 4. استخدم الاشارات الصوتية أو البصرية للتفاعل مع الوكيل التعلم الآلي. 5. اتبع التعليمات المعروضة على الشاشة للعب بكفاءة والوصول إلى الأداء العالي. حظًا سعيدًا واستمتع باللعب!? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries and classes from the transformers package. This includes AutoModelForDocumentQuestionAnswering for the question answering model.\\\\n2. Load the pre-trained 'tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa' document question answering model using the from_pretrained method.\\\\n3. Prepare the documents you wish to analyze and convert them into plain text format. This can be achieved by reading the document or converting it using appropriate libraries (e.g. for PDFs, you can use the PyPDF2 library).\\\\n4. Use the loaded model to analyze the plain text documents and answer questions based on the information found within the document.\\\", '<<<code>>>': 'from transformers import AutoModelForDocumentQuestionAnswering, AutoTokenizer\\\\n\\\\nmodel_name = \\\"tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa\\\"\\\\ndocument_model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_name)\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\n\\\\n# Replace \\\\'your_question\\\\' and \\\\'your_text_document\\\\' with the actual question and document text\\\\nquestion = \\\"your_question\\\"\\\\ncontext = \\\"your_text_document\\\"\\\\n\\\\ninputs = tokenizer(question, context, return_tensors=\\\"pt\\\")\\\\noutputs = document_model(**inputs)\\\\n# Extract answer\\\\nstart_location = torch.argmax(outputs[\\\\'start_logits\\\\'])\\\\nend_location = torch.argmax(outputs[\\\\'end_logits\\\\']) + 1\\\\nanswer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\\\\'input_ids\\\\'][0][start_location:end_location]))', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')\\\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers==4.12.3, Pytorch==1.8.0+cu101, Datasets==1.15.1, Tokenizers==0.10.3'], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.2207}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى برنامج في مجال معالجة اللغة الطبيعية يقوم بتحويل النصوص المكتوبة في ترجمة إلى كلام بصوت واضح ودقيق لدعم مستخدمي تطبيق عميلنا الذين يعانون من إعاقة بصرية، خاصة عند الإجابة على أسئلة في جداول. يجب أن يكون الصوت واضحًا وسهل الفهم ليساعد الأشخاص ذوي الإعاقة البصرية في فهم المحتوى بشكل أفضل.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the Hugging Face Transformers library.\\\\n2. Use the pipeline function to create an automatic translation model for Dutch to English translation tasks.\\\\n3. Specify the model 'Helsinki-NLP/opus-mt-nl-en' to be loaded. This model is a transformer-align architecture trained on the OPUS dataset with normalization and SentencePiece pre-processing.\\\\n4. With the created translation pipeline, you can translate Dutch texts into English automatically.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntranslator = pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\\ntranslated_text = translator('Hallo, hoe gaat het met je?')[0]['translation_text']\\\", 'api_call': \\\"pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-nl-en', 'api_call': \\\"pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\\ntranslated_text = translation('Hallo, hoe gaat het met je?')[0]['translation_text']\\\", 'performance': {'dataset': 'Tatoeba.nl.en', 'accuracy': {'BLEU': 60.9, 'chr-F': 0.749}}, 'description': 'A Dutch to English translation model based on the OPUS dataset, using a transformer-align architecture with normalization and SentencePiece pre-processing.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية باستخدام تقنية تحليل الصور للكشف عن الكائنات. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries YOLO and render_result from the ultralyticsplus module.\\\\n2. Create a YOLO model instance by specifying the model name 'keremberke/yolov8s-csgo-player-detection'.\\\\n3. Set the model override parameters like confidence threshold, intersection over union, and maximum number of detections.\\\\n4. Use the model.predict() method with the image input to produce detection results.\\\\n5. To visualize the detection results, use the render_result() function.\\\\n6. Display the rendered result by calling the show() method.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://path.to/csgo_image.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \\\"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\\\" </s>\"\n"}
{"text": "<s>[INST]يتطلب عميلنا تطوير تطبيق في مجال معالجة اللغة الطبيعية التحدثية. نحن بحاجة إلى تطبيق قادر على تحويل النص إلى كلام لمساعدة مستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the 'pipeline' function from the transformers package.\\\\n2. We use the 'pipeline' function, specifying the task 'text-generation' and the model 'PygmalionAI/pygmalion-1.3b', to create a conversation generator.\\\\n3. We provide an input message that follows the format required by the model. The input should include the character persona, dialogue history, and user input message.\\\\n4. Using the generator, we generate a dialogue response appropriate for a wise old wizard character.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ngenerator = pipeline(\\\\'text-generation\\\\', \\\\'PygmalionAI/pygmalion-1.3b\\\\')\\\\ncharacter_persona = \\\"Wise Old Wizard Persona: A wise old wizard who has spent centuries studying ancient texts and possesses vast knowledge of the magical realm. He is patient, kind, and always willing to impart wisdom to those who seek it.\\\"\\\\nuser_input = \\\"You: Tell me, wise wizard, what is the secret to mastering the arcane arts?\\\"\\\\ninput_prompt = f\\\"{character_persona}\\\\\\\\n{user_input}\\\\\\\\nWizard:\\\"\\\\ngenerated_text = generator(input_prompt)\\\\nwizard_response = generated_text[0][\\\\'generated_text\\\\']', 'api_call': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pygmalion-1.3b', 'api_call': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", 'api_arguments': 'input_prompt', 'python_environment_requirements': [['transformers']], 'example_code': \\\"[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\\\", 'performance': {'dataset': '56MB of dialogue data', 'accuracy': 'Not provided'}, 'description': \\\"Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\\\" </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"pipeline('image-to-text', model='microsoft/git-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required library, which is the 'pipeline' function from the transformers library provided by Hugging Face.\\\\n2. Create a pipeline object that uses the microsoft/git-base model for the image-to-text task.\\\\n3. Provide the input image to the pipeline, and it will use the model to generate a caption for the image.\\\\n4. GIT (GenerativeImage2Text) model is used for generating captions for images, and it is based on the concept of predicting the next text token, given previous tokens and image tokens.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ncaption_pipeline = pipeline('image-to-text', model='microsoft/git-base')\\\\ncaption = caption_pipeline(image)\\\", 'api_call': \\\"pipeline('image-to-text', model='microsoft/git-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/git-base', 'api_call': \\\"pipeline('image-to-text', model='microsoft/git-base')\\\", 'api_arguments': 'image', 'python_environment_requirements': ['transformers'], 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \\\"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\\\" </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير خوارزميات استخراج الميزات المتعددة في مجال النصوص والصور للتفاعل الثنائي مع الذكاء الاصطناعي ونحن بحاجة إلى تصميم نماذج لاستخراج السمات من البيانات المتعددة كالصوت والصور والنصوص وغيرها لتحسين الأداء وتحقيق أهداف البحث المحددة.? [/INST]{'domain': 'Multimodal Text-to-Image', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion', torch_dtype=torch.float32)\\\", 'api_provider': 'Hugging Face', 'explanation': '1. To generate an image, we first import the required libraries, including torch and autocast from torch, and StableDiffusionPipeline from diffusers.', 'code': 'import torch\\\\nfrom torch import autocast\\\\nfrom diffusers import StableDiffusionPipeline\\\\npipe = StableDiffusionPipeline.from_pretrained(\\\\'hakurei/waifu-diffusion\\\\', torch_dtype=torch.float32).to(\\\\'cuda\\\\')\\\\nprompt = \\\"a garden full of mythical creatures\\\"\\\\nwith autocast(cuda=True):\\\\n    image = pipe(prompt, guidance_scale=6)[0]\\\\nimage.save(\\\\'garden_mythical_creatures.png\\\\')', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': 'import torch\\\\nfrom torch import autocast\\\\nfrom diffusers import StableDiffusionPipeline\\\\npipe = StableDiffusionPipeline.from_pretrained(\\\\'hakurei/waifu-diffusion\\\\', torch_dtype=torch.float32).to(\\\\'cuda\\\\')\\\\nprompt = \\\"a garden full of mythical creatures\\\"\\\\nwith autocast(cuda=True):\\\\n    image = pipe(prompt, guidance_scale=6)[0]\\\\nimage.save(\\\\'garden_mythical_creatures.png\\\\')', 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \\\"import torch\\\\nfrom torch import autocast\\\\nfrom diffusers import StableDiffusionPipeline\\\\npipe = StableDiffusionPipeline.from_pretrained(\\\\n 'hakurei/waifu-diffusion',\\\\n torch_dtype=torch.float32\\\\n).to('cuda')\\\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\\\nwith autocast(cuda):\\\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\\\nimage.save(test.png)\\\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتصميم نموذج لترجمة النصوص الطبيعية باستخدام تقنيات المعالجة اللغوية الطبيعية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import MarianMTModel from the transformers library by Hugging Face to facilitate translation tasks.\\\\n2. We use the from_pretrained method of MarianMTModel to load the pre-trained model 'Helsinki-NLP/opus-mt-ROMANCE-en'. This model is designed for translating Romance languages, such as French, Spanish, Portuguese, Italian, and Romanian to English.\\\\n3. With the model in place, we can process user comments and translate them from various Romance languages to English so that website moderators or administrators can understand and evaluate the comments.\\\", '<<<code>>>': 'from transformers import MarianMTModel, MarianTokenizer\\\\nmodel = MarianMTModel.from_pretrained(\\\\'Helsinki-NLP/opus-mt-ROMANCE-en\\\\')\\\\ntokenizer = MarianTokenizer.from_pretrained(\\\\'Helsinki-NLP/opus-mt-ROMANCE-en\\\\')\\\\ntext = \\\"User comment in Romance language, e.g., French, Spanish, Portuguese, Italian, or Romanian.\\\"\\\\ninput_tokens = tokenizer.encode(text, return_tensors=\\\"pt\\\")\\\\ntranslated_tokens = model.generate(input_tokens)\\\\ntranslated_text = tokenizer.decode(translated_tokens[0])', 'api_call': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-ROMANCE-en', 'api_call': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", 'api_arguments': ['source languages', 'target languages'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': '', 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': 62.2, 'chr-F': 0.75}}, 'description': 'A model for translating Romance languages to English, trained on the OPUS dataset. It supports multiple source languages such as French, Spanish, Portuguese, Italian, and Romanian, among others. The model is based on the transformer architecture and uses normalization and SentencePiece for pre-processing.' </s>\"\n"}
{"text": "<s>[INST]مجال تقدير العمق في رؤية الحاسوب في تصنيف الصوت يحتاج إلى تحليل وفهم البيانات الصوتية، وقدرةعلى التعرف وتصنيف الصوتيات بدقة وفعالية.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the necessary dependencies from the transformers library. This will include the AutoModelForAudioClassification class, which is needed for loading the pre-trained model.\\\\n2. Use the from_pretrained method of the AutoModelForAudioClassification class to load the 'MIT/ast-finetuned-speech-commands-v2' model. This model is specifically fine-tuned for classifying speech commands in audio files.\\\\n3. Import the audio file's data and decode it accordingly.\\\\n4. Use the created audio classification model to classify different types of speech commands in the provided audio file.\\\", '<<<code>>>': \\\"from transformers import AutoModelForAudioClassification\\\\naudio_classifier = AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\\nresult = audio_classifier('path/to/audio/file.wav')\\\\n# Replace 'path/to/audio/file.wav' with the path to your audio file\\\", 'api_call': \\\"AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ast-finetuned-speech-commands-v2', 'api_call': \\\"AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\", 'api_arguments': 'audio file', 'python_environment_requirements': ['transformers library'], 'example_code': \\\"result = audio_classifier('path/to/audio/file.wav')\\\", 'performance': {'dataset': 'Speech Commands v2', 'accuracy': '98.120'}, 'description': 'Audio Spectrogram Transformer (AST) model fine-tuned on Speech Commands v2. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.' </s>\"\n"}
{"text": "<s>[INST]قم بتوفير صورة من نهر واطلب من وكيل التعلم الآلي توليد وصف نصي دقيق للصورة مع التركيز على التفاصيل المتعلقة بالمياه والبيئة المحيطة.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import necessary libraries, including MgpstrProcessor and MgpstrForSceneTextRecognition classes from the transformers package.\\\\n2. Instantiate the MgpstrProcessor and MgpstrForSceneTextRecognition classes using the from_pretrained() method with the model name 'alibaba-damo/mgp-str-base'. This pre-trained MGP-STR model is designed for OCR tasks and can read text in images.\\\\n3. Load your image file into a PIL.Image object and convert it to the RGB format.\\\\n4. Preprocess the image using the MgpstrProcessor instance, and pass the pixel values of the image to the MgpstrForSceneTextRecognition model.\\\\n5. Decode the output logits using the MgpstrProcessor's batch_decode() method to obtain the recognized text.\\\", '<<<code>>>': \\\"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\\\nfrom PIL import Image\\\\nimport requests\\\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\\\nurl = 'https://i.postimg.cc/ZKwLg2Gw/367-14.png'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\noutputs = model(pixel_values)\\\", 'api_call': \\\"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mgp-str', 'api_call': \\\"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\\", 'api_arguments': {'model_name': 'alibaba-damo/mgp-str-base'}, 'python_environment_requirements': [{'packages': ['transformers']}], 'example_code': \\\"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\\\nimport requests\\\\nfrom PIL import Image\\\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\\\noutputs = model(pixel_values)\\\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\\\", 'performance': {'dataset': 'MJSynth and SynthText', 'accuracy': None}, 'description': 'MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد صورة باستخدام تقنيات الرؤية الحاسوبية لتقسيم الصورة إلى أجزاء مستقلة باستخدام وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the YOLO class from the 'ultralyticsplus' package.\\\\n2. Create a YOLO object using the 'keremberke/yolov8n-pcb-defect-segmentation' model.\\\\n3. Set the model overrides for 'conf', 'iou', 'agnostic_nms', and 'max_det'.\\\\n4. Specify the path or URL of the target PCB image for the detection.\\\\n5. Use the 'predict' method to get the results of the detection and segmentation.\\\\n6. Extract the box coordinates and mask information from the results.\\\\n7. Visualize the rendered result on the PCB image.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\npcb_image = 'path/to/pcb_image.jpg'\\\\nresults = model.predict(pcb_image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=pcb_image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8n-pcb-defect-segmentation', 'api_call': \\\"YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.512, 'mAP@0.5(mask)': 0.517}}, 'description': 'A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>\"\n"}
{"text": "<s>[INST]قم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية بواسطة تقنيات تصنيف الصور في رؤية الحاسوب. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline class from the transformers package for easy API usage.\\\\n2. Create an image classification pipeline using the provided model 'martinezomg/vit-base-patch16-224-diabetic-retinopathy'. This model is specifically designed for diabetic retinopathy detection and classifying it into severity categories: none, mild, moderate, severe, and proliferative.\\\\n3. Given an image of a patient's retina, the created classifier can be used to predict the severity of diabetic retinopathy. For example, you can provide the path to an image and the classifier will return the severity class.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\\nresult = image_classifier('path/to/image.jpg')\\\\n# replace 'path/to/image.jpg' with the image path\\\", 'api_call': \\\"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy', 'api_call': \\\"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\", 'api_arguments': {'model_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy'}, 'python_environment_requirements': [{'transformers': '4.28.1', 'pytorch': '2.0.0+cu118', 'datasets': '2.11.0', 'tokenizers': '0.13.3'}], 'example_code': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\\nresult = image_classifier('path/to/image.jpg')\\\", 'performance': {'dataset': 'None', 'accuracy': 0.7744}, 'description': 'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.' </s>\"\n"}
{"text": "<s>[INST]يجب على العميل تطوير نموذج لإنشاء صور بدون شروط باستخدام تقنيات رؤية الحاسوب. يتطلب ذلك تطوير نموذج قادر على إنتاج صور جديدة تلقائياً وبدون الحاجة لتوجيهات محددة.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. To generate a piece of art inspired by WikiArt, first, import the DDPMPipeline class from the 'diffusers' package.\\\", 'code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\\\ngenerated_image = pipeline().images[0]\\\\ngenerated_image.save('generated_wikiart_inspired_image.png')\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'johnowhitaker/sd-class-wikiart-from-bedrooms', 'api_call': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\\\ngenerated_image = pipeline().images[0]\\\\ngenerated_image.save('generated_wikiart_inspired_image.png')\\\", 'api_arguments': '', 'python_environment_requirements': 'diffusers', 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': 'https://huggingface.co/datasets/huggan/wikiart', 'accuracy': 'Not provided'}, 'description': 'This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart.'}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لتحديد تشابه الجمل في مجال معالجة اللغة الطبيعية باستخدام تقنيات الذكاء الصناعي.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"BertModel.from_pretrained('setu4993/LaBSE')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers and torch packages. We will need the BertModel class to create the language-agnostic sentence embeddings and the BertTokenizerFast class to tokenize the sentences in all three languages.\\\\n2. We then use the from_pretrained method of the BertModel to load the pre-trained model 'setu4993/LaBSE'. This model has been trained to generate multilingual sentence embeddings.\\\\n3. Tokenize and encode the sentences using the tokenizer's method called tokenizer.\\\\n4. Finally, compute the sentence embeddings by feeding the tokenized and encoded sentences to the model.\\\\n5. The output embeddings can be compared to determine the similarity among the sentences using an appropriate similarity metric, such as cosine similarity or L2 distance.\\\", '<<<code>>>': \\\"import torch\\\\nfrom transformers import BertModel, BertTokenizerFast\\\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\\\nenglish_sentences = ['english_query1', 'english_query2']\\\\nitalian_sentences = ['italian_query1', 'italian_query2']\\\\njapanese_sentences = ['japanese_query1', 'japanese_query2']\\\\nall_sentences = english_sentences + italian_sentences + japanese_sentences\\\\ninputs = tokenizer(all_sentences, return_tensors='pt', padding=True)\\\\nmodel = model.eval()\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nsentence_embeddings = outputs.pooler_output\\\", 'api_call': \\\"BertModel.from_pretrained('setu4993/LaBSE')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \\\"BertModel.from_pretrained('setu4993/LaBSE')\\\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': [['torch', 'transformers']], 'example_code': \\\"import torch\\\\nfrom transformers import BertModel, BertTokenizerFast\\\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\\\nmodel = model.eval()\\\\nenglish_sentences = [\\\\n 'dog',\\\\n 'Puppies are nice.',\\\\n 'I enjoy taking long walks along the beach with my dog.',\\\\n]\\\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\\\nwith torch.no_grad():\\\\n english_outputs = model(**english_inputs)\\\\nenglish_embeddings = english_outputs.pooler_output\\\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>\"\n"}
{"text": "<s>[INST]لتصنيف الصوت، استخدم برنامج التعلم الآلي لتمييز أصوات الطبيعة عن الأصوات البشرية. قم بتزويد البرنامج بتسميات معبرة للصفقات الصوتية المتنوعة الموجودة في مجموعة البيانات.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-ar-cv7')\\\", '<<<api_provider>>>': 'Fairseq', '<<<explanation>>>': \\\"1. We import the necessary functions from the fairseq and IPython libraries.\\\\n2. We load the pre-trained model 'facebook/tts_transformer-ar-cv7' using the load_model_ensemble_and_task_from_hf_hub function. This Transformer text-to-speech model is trained for Arabic language with a single-speaker male voice.\\\\n3. To generate speech from text, we first update the model configuration based on the available data.\\\\n4. Then, we build a generator object, which is used to produce the speech output.\\\\n5. Finally, we provide the input text, generate the audio waveform, and play the audio using the IPython.Audio function.\\\", '<<<code>>>': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\'facebook/tts_transformer-ar-cv7\\\\', arg_overrides={\\\\'vocoder\\\\': \\\\'hifigan\\\\', \\\\'fp16\\\\': False})\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\ntext = \\\"مرحبا، هذا اختبار تشغيل.\\\"\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-ar-cv7')\\\", 'provider': 'Fairseq', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tts_transformer-ar-cv7', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-ar-cv7')\\\", 'api_arguments': {'text': 'input text'}, 'python_environment_requirements': [['fairseq', 'IPython']], 'example_code': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n facebook/tts_transformer-ar-cv7,\\\\n arg_overrides={vocoder: hifigan, fp16: False}\\\\n)\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\ntext = مرحبًا ، هذا اختبار تشغيل.\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)', 'performance': {'dataset': 'common_voice', 'accuracy': 'Not specified'}, 'description': 'Transformer text-to-speech model for Arabic language with a single-speaker male voice, trained on Common Voice v7 dataset.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج لتصنيف النصوص بدون تصنيف (Zero-Shot Classification) في مجال معالجة اللغات الطبيعية، باستخدام شبكة عصبونية متعددة الطبقات.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"mDeBERTaForSequenceClassification.from_pretrained('MoritzLaurer/mDeBERTa-v3-base-mnli-xnli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the mDeBERTaForSequenceClassification model from the transformers library provided by Hugging Face.\\\\n2. Use the 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli' model, which has been trained on the XNLI dataset and is suitable for multilingual zero-shot classification tasks.\\\\n3. Load the model by using the from_pretrained method.\\\\n4. This model can be used to classify text into different categories in multiple languages. Simply pass in the text to be analyzed and the candidate category labels in the desired language.\\\", '<<<code>>>': \\\"from transformers import mDeBERTaForSequenceClassification\\\\nmultilingual_classifier = mDeBERTaForSequenceClassification.from_pretrained('MoritzLaurer/mDeBERTa-v3-base-mnli-xnli')\\\", 'api_call': \\\"mDeBERTaForSequenceClassification.from_pretrained('MoritzLaurer/mDeBERTa-v3-base-mnli-xnli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli', 'api_call': \\\"mDeBERTaForSequenceClassification.from_pretrained('MoritzLaurer/mDeBERTa-v3-base-mnli-xnli')\\\", 'api_arguments': {'model': 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli'}, 'python_environment_requirements': [{'transformers': '>=4.13'}], 'example_code': {'simple_zero_shot_classification_pipeline': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=MoritzLaurer/mDeBERTa-v3-base-mnli-xnli)\\\\nsequence_to_classify = Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\\\\ncandidate_labels = [politics, economy, entertainment, environment]\\\\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\\\\nprint(output)'}, 'performance': {'dataset': {'average': 0.808, 'ar': 0.802, 'bg': 0.829, 'de': 0.825, 'el': 0.826, 'en': 0.883, 'es': 0.845, 'fr': 0.834, 'hi': 0.771, 'ru': 0.813, 'sw': 0.748, 'th': 0.793, 'tr': 0.807, 'ur': 0.74, 'vi': 0.795, 'zh': 0.8116}, 'accuracy': '0.808'}, 'description': 'This multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual zero-shot classification. The underlying model was pre-trained by Microsoft on the CC100 multilingual dataset. It was then fine-tuned on the XNLI dataset, which contains hypothesis-premise pairs from 15 languages, as well as the English MNLI dataset.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نماذج توليد الصور بدون شروط في الرؤية الحاسوبية، ونحتاج إلى دقة عالية في إنشاء صور جديدة بشكل تلقائي.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('Apocalypse-19/shoe-generator')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. In order to create a generated shoe design, we start by importing the DDPMPipeline from the diffusers package.\\\\n2. The DDPMPipeline is used to create a model that is capable of generating high-quality images of shoes. The pretrained model 'Apocalypse-19/shoe-generator' is a diffusion model trained to generate shoes at 128x128 resolution.\\\\n3. This model can then be used to generate new, unique shoe designs. It can be utilized to provide creative inspiration, or combined with other designs to create original shoe models.\\\\n4. Finally, the generated image can be saved for further refinement and design purposes.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('Apocalypse-19/shoe-generator')\\\\nimage = pipeline().images[0]\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('Apocalypse-19/shoe-generator')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Apocalypse-19/shoe-generator', 'api_call': \\\"DDPMPipeline.from_pretrained('Apocalypse-19/shoe-generator')\\\", 'api_arguments': [], 'python_environment_requirements': [['diffusers']], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('Apocalypse-19/shoe-generator')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': 'custom dataset', 'accuracy': '128x128 resolution'}, 'description': 'This model is a diffusion model for unconditional image generation of shoes trained on a custom dataset at 128x128 resolution.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج في مجال تقنيات رؤية الحاسوب لإنشاء صور بشكل غير مشروط، ونحتاج إلى توليد صور جديدة بناءً على التصنيفات المتاحة للصور الحالية، في مجال تحويل الصور إلى نصوص متعددة الوسائط.? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\nmanga_ocr = pipeline('ocr', model='kha-white/manga-ocr-base')\\\\nrecognized_text = manga_ocr(image_file_path)\\\", 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \\\"from transformers import pipeline\\\\nmanga_ocr = pipeline('ocr', model='kha-white/manga-ocr-base')\\\\nrecognized_text = manga_ocr(image_file_path)\\\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.'}} </s>\"\n"}
{"text": "<s>[INST]لتطوير نماذج تحليل جدولي للاستخدام في تنبؤات تحليل البيانات والتحليل الإحصائي، نحتاج إلى فهم متعمق للبيانات والعلاقات بين المتغيرات والنماذج الإحصائية.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"load('path_to_folder/example.pkl')\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': \\\"1. The first step is to download and store the pre-trained model's weights in a folder using the download function from the skops.hub_utils.\\\\n2. Next, import the load function from the skops.io package to access the pre-trained GradientBoostingRegressor model.\\\\n3. Use the load() function to load the model saved in the specified folder.\\\\n4. This model can be used to make predictions for the fish data provided by the fishery company. By providing the features of the fish (length, height, etc.), the model can estimate the weight of the fish.\\\", '<<<code>>>': \\\"from skops.hub_utils import download\\\\nfrom skops.io import load\\\\ndownload('brendenc/Fish-Weight', 'path_to_folder')\\\\nmodel = load('path_to_folder/example.pkl')\\\\nfish_weight_prediction = model.predict(fish_data_features)\\\", 'api_call': \\\"load('path_to_folder/example.pkl')\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Fish-Weight', 'api_call': \\\"load('path_to_folder/example.pkl')\\\", 'api_arguments': {'model_path': 'path_to_folder/example.pkl'}, 'python_environment_requirements': [{'skops.hub_utils': 'download', 'skops.io': 'load'}], 'example_code': \\\"from skops.hub_utils import download\\\\nfrom skops.io import load\\\\ndownload('brendenc/Fish-Weight', 'path_to_folder')\\\\nmodel = load('path_to_folder/example.pkl')\\\", 'performance': {'dataset': 'Fish dataset', 'accuracy': 'Not provided'}, 'description': 'This is a GradientBoostingRegressor on a fish dataset. This model is intended for educational purposes.' </s>\"\n"}
{"text": "<s>[INST]يقوم النظام بتصنيف النصوص المتعلقة بمجال معالجة اللغة الطبيعية، ونحتاج إلى تحليل النصوص وتصنيفها وفقًا للموضوعات المحددة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('shahrukhx01/question-vs-statement-classifier')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, which include AutoTokenizer and AutoModelForSequenceClassification from the transformers package.\\\\n2. Load the tokenizer and model using the from_pretrained method using the provided model name: 'shahrukhx01/question-vs-statement-classifier'.\\\\n3. Use the tokenizer to convert user input (queries) into tokens that can be processed by the model.\\\\n4. Pass these tokens to the model, and it will output the probabilities of the input being a question or a statement. The model is trained on the Haystack dataset specifically for question/statement classification tasks.\\\\n5. According to the highest probability, you can classify the input as either a question or a statement.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'shahrukhx01/question-vs-statement-classifier\\\\')\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\\\'shahrukhx01/question-vs-statement-classifier\\\\')\\\\nuser_query = \\\"Input query here...\\\"\\\\ntokens = tokenizer(user_query, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**tokens)\\\\nprediction = output.logits.argmax(dim=-1).item()\\\\nclassification = \\\\'question\\\\' if prediction == 0 else \\\\'statement\\\\'', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('shahrukhx01/question-vs-statement-classifier')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'shahrukhx01/question-vs-statement-classifier', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('shahrukhx01/question-vs-statement-classifier')\\\", 'api_arguments': {'tokenizer': 'AutoTokenizer.from_pretrained(shahrukhx01/question-vs-statement-classifier)'}, 'python_environment_requirements': [{'transformers': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification'}], 'example_code': 'tokenizer = AutoTokenizer.from_pretrained(shahrukhx01/question-vs-statement-classifier)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(shahrukhx01/question-vs-statement-classifier)', 'performance': {'dataset': 'Haystack', 'accuracy': 'Not provided'}, 'description': 'Trained to add the feature for classifying queries between Question Query vs Statement Query using classification in Haystack' </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج لاكتشاف الكائنات باستخدام الرؤية الحاسوبية. يجب أن يكون النموذج قادرًا على تحديد وتعيين الكائنات في الصور بدقة. يجب أن يتضمن النموذج خطوات لتحديد مكان ونوع الكائنات المختلفة في الصور.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8n-csgo-player-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first install ultralyticsplus and ultralytics to access the YOLO model. Then, we import YOLO and render_result from ultralyticsplus library.\\\\n2. Next, we load the pre-trained YOLOv8 model 'keremberke/yolov8n-csgo-player-detection' specifically trained for detecting CS:GO players. This model can recognize the following labels: ['ct', 'cthead', 't', 'thead'].\\\\n3. Configure the required confidences, inter-cell overlap, and maximum detected objects allowed by the model.\\\\n4. Provide the image URL or local path containing CS:GO players and make the predictions using the predict method.\\\\n5. To render and visualize results, use the render_result function with the model, image, and detected results.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path_to_your_csgo_image.jpg'\\\\n# replace 'path_to_your_csgo_image.jpg' with your image URL or local path\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8n-csgo-player-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8n-csgo-player-detection', 'api_call': \\\"YOLO('keremberke/yolov8n-csgo-player-detection')\\\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\\\n\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\n\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\n\\\\nresults = model.predict(image)\\\\n\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.844}, 'description': \\\"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\\\" </s>\"\n"}
{"text": "<s>[INST]المطور يقوم بتصميم نموذج لتحويل النص إلى صورة. يجب أن يدعم البرنامج العديد من اللغات والأساليب لضمان استيعاب جميع المستخدمين ذوي الإعاقة البصرية.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We import the necessary function which is the pipeline' from the 'transformers' package provided by Hugging Face.\\\\n2. We initialize a 'text-to-image' pipeline with the pre-trained model 'gsdf/Counterfeit-V2.5'. This model generates anime-style images based on text prompts, which is suitable for creating animal characters.\\\\n3. We can provide text prompts describing the animal character we want the model to generate. The model will generate an image representation of the character based on the given text prompt.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntext_to_image = pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\\\\ntext_prompt = 'cute fox, standing, looking happy, wearing a hat'\\\\ngenerated_image = text_to_image(text_prompt)\\\", 'api_call': \\\"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \\\"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>\"\n"}
{"text": "<s>[INST]قم ببناء نموذج لتنبؤ قيم رقمية مستند إلى الجداول الموجودة، واستخدم أساليب التعلم الآلي لتحسين أداء النموذج وضبط المعلمات للحصول على أفضل أداء ممكن.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We start by importing the necessary libraries, which include joblib for loading the trained model, and pandas for working with tabular data.\\\\n2. We load the pre-trained model using `joblib.load('model.joblib')`.\\\\n3. Next, we load our dataset (data.csv) into a pandas DataFrame and extract only the columns specified in the configuration file (config.json) as features for the prediction.\\\\n4. We then use the loaded model to predict CO2 emissions for each instance in the data.\\\\n5. The output can be used by your application to provide insights on the eco-friendliness of the cars.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\n\\\\ndata = pd.read_csv('data.csv')  # replace 'data.csv' with path to your dataset\\\\n# process data according to config\\\\npredictions = model.predict(data)\\\\n\\\\n# predictions now contain CO2 emissions for each instance in the data\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': '45473113800', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'samvelkoch/autotrain-data-prknsn-2', 'accuracy': {'Loss': 5.079, 'R2': 0.109, 'MSE': 25.795, 'MAE': 3.78, 'RMSLE': 0.849}}, 'description': 'A tabular regression model trained with AutoTrain for predicting carbon emissions.' </s>\"\n"}
{"text": "<s>[INST]يجب بناء نموذج تحليل لغوي يمكنه ملء الفجوات في النصوص بشكل تلقائي ودقيق. ينبغي للنموذج أن يكون قادراً على التعرف على النص الموجود وملء الفجوات فيه بكلمات مناسبة بناءً على السياق المحيط.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We first import the necessary library, which is 'pipeline' from transformers.\\\\n2. We use the 'pipeline' function from transformers to load the model, which in this case is 'distilroberta-base'. \\\\n3. This model is designed to complete sentences where certain words or phrases have been omitted, represented by the special token <mask>.\\\\n4. The model will generate an appropriate word or phrase to fill the <mask> token.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'distilroberta-base\\\\')\\\\nsentence = \\\"Science is constantly <mask> new discoveries and challenging our understanding of the universe.\\\"\\\\nresult = unmasker(sentence)\\\\nbest_completion = result[0][\\\\'token_str\\\\']', 'api_call': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilroberta-base', 'api_call': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"unmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': 'openwebtext', 'accuracy': 'Not provided'}, 'description': \\\"DistilRoBERTa is a distilled version of the RoBERTa-base model, designed to be smaller, faster, and lighter. It is a Transformer-based language model trained on the OpenWebTextCorpus, which is a reproduction of OpenAI's WebText dataset. The model has 6 layers, 768 dimensions, and 12 heads, totaling 82M parameters. It is primarily intended for fine-tuning on downstream tasks such as sequence classification, token classification, or question answering.\\\" </s>\"\n"}
{"text": "<s>[INST]نقوم بإنشاء نموذج لتوليد نصوص باستخدام معالجة اللغة الطبيعية، لتحويل النصوص من شكل إلى آخر، باستخدام البيانات المتاحة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-small')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the required libraries from transformers, specifically T5Tokenizer and T5ForConditionalGeneration.\\\\n2. Load the pre-trained model \\\\'google/flan-t5-small\\\\' using the T5ForConditionalGeneration.from_pretrained() method.\\\\n3. Load the corresponding tokenizer using T5Tokenizer.from_pretrained(\\\\'google/flan-t5-small\\\\'\\\\').\\\\n4. Prepare the input text with the translation instruction, e.g., \\\"translate German to English: <German text>\\\".\\\\n5. Tokenize the input text using the loaded tokenizer and convert it to input_ids.\\\\n6. Generate the translated text by calling model.generate() with the input_ids.\\\\n7. Decode the generated output using the tokenizer\\\\'s decode() method to obtain the translated text in English.', '<<<code>>>': \\\"from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained('google/flan-t5-small')\\\\nmodel = T5ForConditionalGeneration.from_pretrained('google/flan-t5-small')\\\\ninput_text = 'translate German to English: Wie alt bist du?'\\\\ninput_ids = tokenizer(input_text, return_tensors='pt').input_ids\\\\noutputs = model.generate(input_ids)\\\\ntranslated_text = tokenizer.decode(outputs[0])\\\", 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-small')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/flan-t5-small', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-small')\\\", 'api_arguments': ['input_text'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-small)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-small)\\\\ninput_text = translate English to German: How old are you?\\\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))', 'performance': {'dataset': [{'name': 'MMLU', 'accuracy': '75.2%'}]}, 'description': 'FLAN-T5 small is a fine-tuned version of T5 language model on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. The model is designed for research on language models, including zero-shot and few-shot NLP tasks, reasoning, question answering, fairness, and safety research. It has not been tested in real-world applications and should not be used directly in any application without prior assessment of safety and fairness concerns specific to the application.' </s>\"\n"}
{"text": "<s>[INST]يتطلب على المستخدم تطوير نموذج لتصنيف الصور بشكل تلقائي في مجال رؤية الحاسوب. النموذج يجب أن يكون قادراً على تصنيف الصور بدون حاجة لتدريب مسبق، لتحقيق حلاً فعالاً في هذا المجال.? [/INST]{'domain': 'Computer Vision Image Classification', 'api_call': \\\"ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import ViTForImageClassification\\\\nmodel = ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\\\\n# Now, fine-tune the model further on specific dataset of city life and nature landscapes\\\", 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_base_patch16_224.augreg2_in21k_ft_in1k', 'api_call': \\\"from transformers import ViTForImageClassification\\\\nmodel = ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\\\\n# Now, fine-tune the model further on specific dataset of city life and nature landscapes\\\", 'api_arguments': {'pretrained_model_name_or_path': 'timm/vit_base_patch16_224.augreg2_in21k_ft_in1k'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد نصوص باللغة الطبيعية من خلال خوارزميات لمعالجة اللغة الطبيعية لإنتاج وصف نصي لمشاهد تصور الحيوانات في البرية. هل يمكنك مساعدتي بذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('bigcode/santacoder', trust_remote_code=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We will first import the necessary classes from the transformers library. This includes AutoModelForCausalLM for text generation and AutoTokenizer for tokenizing the input code.\\\\n2. Then, we load the 'bigcode/santacoder' model using the from_pretrained method. This model is designed for generating code based on an existing code snippet, which makes it suitable for generating your area_coverage method.\\\\n3. We provide the input code snippet - the Python class and a brief description of the area_coverage method you are looking for.\\\\n4. The model generates the code for the area_coverage method which can be incorporated into your existing Python class.\\\", '<<<code>>>': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained('bigcode/santacoder')\\\\nmodel = AutoModelForCausalLM.from_pretrained('bigcode/santacoder', trust_remote_code=True)\\\\ninput_code = 'class MyClass:\\\\\\\\n  # Add method for calculating area_coverage'\\\\ninputs = tokenizer.encode(input_code, return_tensors='pt')\\\\noutputs = model.generate(inputs)\\\\ngenerated_method = tokenizer.decode(outputs[0])\\\", 'api_call': \\\"AutoModelForCausalLM.from_pretrained('bigcode/santacoder', trust_remote_code=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bigcode/santacoder', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('bigcode/santacoder', trust_remote_code=True)\\\", 'api_arguments': ['inputs'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ncheckpoint = bigcode/santacoder\\\\ndevice = cuda # for GPU usage or cpu for CPU usage\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\\\\ninputs = tokenizer.encode(def print_hello_world():, return_tensors=pt).to(device)\\\\noutputs = model.generate(inputs)\\\\nprint(tokenizer.decode(outputs[0]))', 'performance': {'dataset': 'bigcode/the-stack', 'accuracy': {'pass@1 on MultiPL HumanEval (Python)': 0.18, 'pass@10 on MultiPL HumanEval (Python)': 0.29, 'pass@100 on MultiPL HumanEval (Python)': 0.49, 'pass@1 on MultiPL MBPP (Python)': 0.35, 'pass@10 on MultiPL MBPP (Python)': 0.58, 'pass@100 on MultiPL MBPP (Python)': 0.77, 'pass@1 on MultiPL HumanEval (JavaScript)': 0.16, 'pass@10 on MultiPL HumanEval (JavaScript)': 0.27, 'pass@100 on MultiPL HumanEval (JavaScript)': 0.47, 'pass@1 on MultiPL MBPP (Javascript)': 0.28, 'pass@10 on MultiPL MBPP (Javascript)': 0.51, 'pass@100 on MultiPL MBPP (Javascript)': 0.7, 'pass@1 on MultiPL HumanEval (Java)': 0.15, 'pass@10 on MultiPL HumanEval (Java)': 0.26, 'pass@100 on MultiPL HumanEval (Java)': 0.41, 'pass@1 on MultiPL MBPP (Java)': 0.28, 'pass@10 on MultiPL MBPP (Java)': 0.44, 'pass@100 on MultiPL MBPP (Java)': 0.59, 'single_line on HumanEval FIM (Python)': 0.44, 'single_line on MultiPL HumanEval FIM (Java)': 0.62, 'single_line on MultiPL HumanEval FIM (JavaScript)': 0.6, 'BLEU on CodeXGLUE code-to-text (Python)': 18.13}}, 'description': 'The SantaCoder models are a series of 1.1B parameter models trained on the Python, Java, and JavaScript subset of The Stack (v1.1) (which excluded opt-out requests). The main model uses Multi Query Attention, was trained using near-deduplication and comment-to-code ratio as filtering criteria and using the Fill-in-the-Middle objective. In addition there are several models that were trained on datasets with different filter parameters and with architecture and objective variations.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام للكشف عن الكائنات باستخدام رؤية الحاسوب، ونريد التنبؤ بوجود الكائنات وتحديد مواقعها في الصور بناء على بعض مجموعات البيانات المتوفرة في مجال الرؤية الحاسوبية للكشف عن الكائنات.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes YOLO and render_result from the ultralyticsplus package.\\\\n2. Use the YOLO class to load the pre-trained model 'keremberke/yolov8n-table-extraction' which is designed for table extraction in documents.\\\\n3. Set the model's overrides for 'conf', 'iou', 'agnostic_nms', and 'max_det' to achieve the right level of confidence threshold, intersection over union threshold, and maximum detections.\\\\n4. Provide the URL of the image containing the scanned document to the model.\\\\n5. Pass the image to the model's predict method to detect tables in the image.\\\\n6. Render and display the detected tables in the image.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-table-extraction')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8n-table-extraction', 'api_call': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8n-table-extraction')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'table-extraction', 'accuracy': 0.967}, 'description': \\\"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\\\" </s>\"\n"}
{"text": "<s>[INST]قام العميل بتطوير تطبيق لمساعدة ذوي الإعاقة البصرية. نحن بحاجة إلى بشرياً يقوم بتحويل النص إلى كلام لمستخدمي هذا التطبيق في مجال الترجمة الآلية للغات الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"MarianMTModel.from_pretrained('pytorch-models/opus-mt-tc-big-en-pt')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package, in this case, MarianMTModel and MarianTokenizer.\\\\n2. Define the model name 'pytorch-models/opus-mt-tc-big-en-pt', which specializes in translating from English to Portuguese.\\\\n3. Create the tokenizer and model objects using the from_pretrained method with the specified model name.\\\\n4. For each participant's speech input in the conference call, tokenize the input text and use the model to generate real-time translations.\\\\n5. Decode the translated text and display or send it to the other participants in the conference call.\\\", '<<<code>>>': \\\"from transformers import MarianMTModel, MarianTokenizer\\\\n\\\\nsrc_text = ['>por< INPUT_SPEECH']\\\\nmodel_name = 'pytorch-models/opus-mt-tc-big-en-pt'\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\n\\\\ntranslated = model.generate(**tokenizer(src_text, return_tensors='pt', padding=True))\\\\ntranslation_output = tokenizer.decode(translated[0], skip_special_tokens=True)\\\", 'api_call': \\\"MarianMTModel.from_pretrained('pytorch-models/opus-mt-tc-big-en-pt')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-tc-big-en-pt', 'api_call': \\\"MarianMTModel.from_pretrained('pytorch-models/opus-mt-tc-big-en-pt')\\\", 'api_arguments': {'src_text': 'list of text strings with language tokens'}, 'python_environment_requirements': [{'transformers': '4.16.2'}], 'example_code': 'from transformers import MarianMTModel, MarianTokenizer\\\\nsrc_text = [\\\\n &gt;&gt;por&lt;&lt; Tom tried to stab me.,\\\\n &gt;&gt;por&lt;&lt; He has been to Hawaii several times.\\\\n]\\\\nmodel_name = pytorch-models/opus-mt-tc-big-en-pt\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\ntranslated = model.generate(**tokenizer(src_text, return_tensors=pt, padding=True))\\\\nfor t in translated:\\\\n print( tokenizer.decode(t, skip_special_tokens=True) )', 'performance': {'dataset': [{'name': 'flores101-devtest', 'accuracy': 50.4}, {'name': 'tatoeba-test-v2021-08-07', 'accuracy': 49.6}]}, 'description': 'Neural machine translation model for translating from English (en) to Portuguese (pt). This model is part of the OPUS-MT project, an effort to make neural machine translation models widely available and accessible for many languages in the world.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى برنامج يستخدم تقنية تحويل الصور إلى نص لتطبيقنا متعدد الوسائط. يجب أن يكون البرنامج قادراً على التعرف على المحتوى الموجود في الصور وتحويله إلى نص قابل للقراءة لمساعدة المستخدمين ذوي الإعاقة البصرية.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary packages, including TrOCRProcessor from transformers and Image from PIL.\\\\n2. Instantiate the TrOCRProcessor with the pre-trained 'microsoft/trocr-large-handwritten' model.\\\\n3. Load the VisionEncoderDecoderModel using the from_pretrained method and the 'microsoft/trocr-large-handwritten' model.\\\\n4. Load the image of the handwritten text using the Image class from the PIL library. Convert the image to RGB format if needed.\\\\n5. Process the image with the TrOCRProcessor to obtain pixel values.\\\\n6. Use the VisionEncoderDecoderModel to generate an ID representation of the transcribed text.\\\\n7. Use the TrOCRProcessor to decode the generated IDs into text.\\\", '<<<code>>>': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\n\\\\nimage = Image.open('handwritten_image.jpg').convert('RGB')\\\\n# replace 'handwritten_image.jpg' with the path to your image\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\\n\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ntranscribed_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': [{'packages': ['transformers', 'PIL', 'requests']}], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>\"\n"}
{"text": "<s>[INST]تشمل أعمال شركتنا تطوير نظام استجابة الأسئلة بمعالجة اللغة الطبيعية لتوليد النصوص، حيث نرغب في تصنيف النصوص المولدة بناءً على البيانات المتاحة.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"AutoModel.from_pretrained('deepset/roberta-base-squad2-distilled')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import 'pipeline' from the transformers library provided by Hugging Face.\\\\n2. Create the question answering pipeline using the 'pipeline' function and the specified model 'deepset/roberta-base-squad2-distilled'. This model is pre-trained on the SQuAD 2.0 dataset and is designed for question answering tasks.\\\\n3. To find the answer to your question from the paragraph, call the pipeline with the context and question as input.\\\\n4. The output 'result' will contain the answer to your question and the associated confidence score for the answer.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='deepset/roberta-base-squad2-distilled')\\\\ncontext = 'This is a paragraph with the information necessary to answer your question.'\\\\nquestion = 'What is the answer to my question?'\\\\nresult = qa_pipeline({'context': context, 'question': question})\\\\nprint(result)\\\", 'api_call': \\\"AutoModel.from_pretrained('deepset/roberta-base-squad2-distilled')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'deepset/roberta-base-squad2-distilled', 'api_call': \\\"AutoModel.from_pretrained('deepset/roberta-base-squad2-distilled')\\\", 'api_arguments': {'context': 'string', 'question': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='deepset/roberta-base-squad2-distilled')\\\\nresult = qa_pipeline({'context': 'This is a context.', 'question': 'What is this?'})\\\\nprint(result)\\\", 'performance': {'dataset': 'squad_v2', 'exact': 79.8366040596311, 'f1': 83.916407079888}, 'description': \\\"This model is a distilled version of deepset/roberta-large-squad2, trained on SQuAD 2.0 dataset for question answering tasks. It is based on the Roberta architecture and has been fine-tuned using Haystack's distillation feature.\\\" </s>\"\n"}
{"text": "<s>[INST]تعمل شركة تطوير عميلنا على تطوير تطبيق يستهدف ذوي الإعاقة البصرية بمجال معالجة اللغة الطبيعية. نحتاج إلى برنامج لتعزيز التعلم يمكنه ملء الفجوات في النص تلقائيًا لتحسين تجربة المستخدمين في هذا التطبيق.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-hopper-medium')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, which are 'AutoModel' from transformers.\\\\n2. Use 'AutoModel.from_pretrained()' with argument 'edbeeching/decision-transformer-gym-hopper-medium' to load the trained decision transformer model for the Gym Hopper environment.\\\\n3. Once the model is loaded, enable evaluation mode by using 'model.eval()' to disable dropout and other noise that are only used during training.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('edbeeching/decision-transformer-gym-hopper-medium')\\\\nmodel.eval()\\\", 'api_call': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-hopper-medium')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'decision-transformer-gym-hopper-medium', 'api_call': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-hopper-medium')\\\", 'api_arguments': {'mean': [1.311279, -0.08469521, -0.5382719, -0.07201576, 0.04932366, 2.1066856, -0.15017354, 0.00878345, -0.2848186, -0.18540096, -0.28461286], 'std': [0.17790751, 0.05444621, 0.21297139, 0.14530419, 0.6124444, 0.85174465, 1.4515252, 0.6751696, 1.536239, 1.6160746, 5.6072536]}, 'python_environment_requirements': [{'transformers': 'latest'}], 'example_code': 'See our Blog Post, Colab notebook or Example Script for usage.', 'performance': {'dataset': 'Gym Hopper environment', 'accuracy': 'Not provided'}, 'description': 'Decision Transformer model trained on medium trajectories sampled from the Gym Hopper environment.' </s>\"\n"}
{"text": "<s>[INST]تهدف شركتنا إلى تطوير نماذج توليد الصور غير المشروطة في مجال رؤية الكمبيوتر التي تتيح إنتاج صور تشبه الصور الواقعية دون الحاجة إلى إدخال معلومات قيد التصميم، ونحن بحاجة لبناء نماذج تنبؤ تعتمد على تحليل الصور وإنشاء صور جديدة.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the 'diffusers' package and import the DDPMPipeline class provided by the diffusers library.\\\\n2. Use the DDPMPipeline.from_pretrained() method to load the 'myunus1/diffmodels_galaxies_scratchbook' model.\\\\n3. Call the pipeline() method to generate an image according to the model's specifications.\\\\n4. Access the image from pipeline's `images` attribute and use your preferred method to display the generated image.\\\", '<<<code>>>': 'from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\\"myunus1/diffmodels_galaxies_scratchbook\\\")\\\\ngenerated_image_data = pipeline().images[0]\\\\n# Use any image display library to show the generated image, e.g. PIL, matplotlib, or OpenCV', 'api_call': \\\"DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'myunus1/diffmodels_galaxies_scratchbook', 'api_call': \\\"DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", 'api_arguments': {'from_pretrained': 'myunus1/diffmodels_galaxies_scratchbook'}, 'python_environment_requirements': [{'package': 'diffusers', 'import': 'from diffusers import DDPMPipeline'}], 'example_code': {'initialize_pipeline': \\\"pipeline = DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", 'generate_image': 'image = pipeline().images[0]', 'display_image': 'image'}, 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'This model is a diffusion model for unconditional image generation of cute 🦋.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لمعالجة اللغة الطبيعية والحوار الذكي، حيث نحتاج لتصنيف البيانات الجدولية وتحديد فئتها بناءً على مجموعات البيانات المتوفرة.? [/INST]{'domain': 'Natural Language Processing Conversational', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('ToddGoldfarb/Cadet-Tiny', low_cpu_mem_usage=True)\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import necessary libraries such as torch, transformers, and the CadetTinyAgent class, which is a conversational model trained on the SODA dataset.', 'code': 'import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nfrom CadetTinyAgent_class import CadetTinyAgent\\\\n\\\\ncadet_tiny_agent = CadetTinyAgent()\\\\nuser_input = \\\"What is your favorite color?\\\"\\\\ncadet_response = cadet_tiny_agent.generate(\\\"\\\", \\\"\\\", user_input)\\\\nprint(\\\"Cadet-Tiny response:\\\", cadet_response)', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'ToddGoldfarb/Cadet-Tiny', 'api_call': 'import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nfrom CadetTinyAgent_class import CadetTinyAgent\\\\n\\\\ncadet_tiny_agent = CadetTinyAgent()\\\\nuser_input = \\\"What is your favorite color?\\\"\\\\ncadet_response = cadet_tiny_agent.generate(\\\"\\\", \\\"\\\", user_input)\\\\nprint(\\\"Cadet-Tiny response:\\\", cadet_response)', 'api_arguments': {'pretrained_model': 't5-small', 'model_max_length': 512}, 'python_environment_requirements': {'torch': '', 'transformers': '', 'colorful': ''}, 'example_code': \\\"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style('monokai')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == '<strong>main</strong>':\\\\n main()\\\", 'performance': {'dataset': 'allenai/soda', 'accuracy': ''}, 'description': 'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام للمعالجة اللغوية الطبيعية لتوليد النصوص، حيث نحتاج إلى بولة التوقع بناءً على البيانات المتاحة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='openai-gpt')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function and set_seed function from the transformers library provided by Hugging Face.\\\\n2. Set a random seed for reproducible results.\\\\n3. Use the pipeline function to create a text generation model.\\\\n4. Specify the model 'openai-gpt' to be loaded. This model is a transformer-based language model trained on a large corpus of text data, making it suitable for generating creative responses to user inquiries.\\\\n5. The created generator can be used to generate a variety of responses to user inputs.\\\", '<<<code>>>': 'from transformers import pipeline, set_seed\\\\nset_seed(42)\\\\nchatbot_generator = pipeline(\\\\'text-generation\\\\', model=\\\\'openai-gpt\\\\')\\\\nuser_input = \\\"How can I cook the perfect steak?\\\"\\\\nresponses = chatbot_generator(user_input, max_length=30, num_return_sequences=5)', 'api_call': \\\"pipeline('text-generation', model='openai-gpt')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai-gpt', 'api_call': \\\"pipeline('text-generation', model='openai-gpt')\\\", 'api_arguments': {'model': 'openai-gpt', 'max_length': 30, 'num_return_sequences': 5}, 'python_environment_requirements': [{'transformers': 'from transformers import pipeline, set_seed'}], 'example_code': \\\"from transformers import pipeline, set_seed\\\\n\\\\nset_seed(42)\\\\ngenerator = pipeline('text-generation', model='openai-gpt')\\\\n\\\\ngenerator(Hello, I'm a language model,, max_length=30, num_return_sequences=5)\\\", 'performance': {'dataset': [{'name': 'SNLI', 'accuracy': 89.9}, {'name': 'MNLI Matched', 'accuracy': 82.1}, {'name': 'MNLI Mismatched', 'accuracy': 81.4}, {'name': 'SciTail', 'accuracy': 88.3}, {'name': 'QNLI', 'accuracy': 88.1}, {'name': 'RTE', 'accuracy': 56.0}, {'name': 'STS-B', 'accuracy': 82.0}, {'name': 'QQP', 'accuracy': 70.3}, {'name': 'MPRC', 'accuracy': 82.3}, {'name': 'RACE', 'accuracy': 59.0}, {'name': 'ROCStories', 'accuracy': 86.5}, {'name': 'COPA', 'accuracy': 78.6}, {'name': 'SST-2', 'accuracy': 91.3}, {'name': 'CoLA', 'accuracy': 45.4}, {'name': 'GLUE', 'accuracy': 72.8}]}, 'description': 'openai-gpt is a transformer-based language model created and released by OpenAI. The model is a causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long-range dependencies.' </s>\"\n"}
{"text": "<s>[INST]أنشئ جدول تصنيفي يتضمن الأقسام التالية: الفئة، السمة الأولى، السمة الثانية والتصنيف، واملأ الجدول ببيانات تعرض تصنيف معين لعناصر مختلفة.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': '1. Import required libraries: joblib, json, and pandas.\\\\n2. Load the pre-trained carbon emissions classification model using joblib.load().\\\\n3. Open the configuration file for the model (config.json) and extract required features.\\\\n4. Preprocess the input dataset by applying the necessary transformations and selecting only required features.\\\\n5. Use the model.predict() method to get the category predictions for the given dataset entries.\\\\n6. As a result, you will get the category predictions for the carbon emissions of the given dataset.', '<<<code>>>': \\\"import joblib\\\\nimport json\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tejas23/autotrain-amx2-1702259729', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'json', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'tejas23/autotrain-data-amx2', 'accuracy': 0.824}, 'description': 'A multi-class classification model for predicting carbon emissions.' </s>\"\n"}
{"text": "<s>[INST]لتوليد تسجيل مسموع من النص المكتوب، يمكنك استخدام خدمة التعرف التلقائي على الكلام. يرجى استخدام API المتاحة لتحويل النص إلى تسجيل صوتي. ويمكنك الاطلاع على الوثائق الفنية والتوجيهات للمزيد من المعلومات.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"pipeline('automatic-speech-recognition', model='facebook/wav2vec2-base-960h')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. Start by importing the necessary libraries. In this case, you need to import the 'pipeline' function from the 'transformers' library.\\\", 'code': \\\"from transformers import pipeline\\\\nasr = pipeline('automatic-speech-recognition', model='facebook/wav2vec2-base-960h')\\\\ntranscription = asr(audio_file_path)\\\", 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Transformers', 'functionality': 'Transcription', 'api_name': 'facebook/wav2vec2-base-960h', 'api_call': \\\"from transformers import pipeline\\\\nasr = pipeline('automatic-speech-recognition', model='facebook/wav2vec2-base-960h')\\\\ntranscription = asr(audio_file_path)\\\", 'api_arguments': ['input_values'], 'python_environment_requirements': ['transformers', 'datasets', 'torch', 'jiwer'], 'example_code': \\\"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\\\\nds = load_dataset('patrickvonplaten/librispeech_asr_dummy', 'clean', split='validation')\\\\ninput_values = processor(ds[0]['audio']['array'], return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'performance': {'dataset': 'LibriSpeech', 'accuracy': {'clean': 3.4, 'other': 8.6}}, 'description': \\\"Facebook's Wav2Vec2 base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It is designed for automatic speech recognition and can transcribe audio files.\\\"}} </s>\"\n"}
{"text": "<s>[INST]قم ببناء نموذج لمعالجة اللغة الطبيعية لتلخيص النصوص.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries. In this case, we use the 'pipeline' function from the transformers package.\\\\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'moussaKam/barthez-orangesum-abstract'.\\\\n3. The loaded model will be used for the summarization task, which generates a concise and informative summary of the given lengthy article.\\\\n4. The model will process the input text (the long article in French) and produce a summary as output.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'moussaKam/barthez-orangesum-abstract\\\\')\\\\nfrench_article = \\\"Long French article goes here...\\\"\\\\nsummary = summarizer(french_article)', 'api_call': \\\"BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'moussaKam/barthez-orangesum-abstract', 'api_call': \\\"BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'orangeSum', 'accuracy': ''}, 'description': 'Barthez model finetuned on orangeSum for abstract generation in French language' </s>\"\n"}
{"text": "<s>[INST]هذا الدليل متعلق بتقنية معالجة اللغة الطبيعية NLP في مجال ملء الفجوة Fill-Mask. يجب بناء نموذج قادر على استخدام البيانات المتاحة لتوقع وتحليل النصوص.? [/INST]{'domain': 'Natural Language Processing Fill-Mask', 'api_call': \\\"pipeline('fill-mask', model='microsoft/deberta-v3-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\nfill_mask = pipeline('fill-mask', model='microsoft/deberta-v3-base')\\\\nsentence_with_mask = 'The quick [MASK] fox jumps over the lazy dog.'\\\\nresult = fill_mask(sentence_with_mask)\\\", 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Fill-Mask', 'api_name': 'microsoft/deberta-v3-base', 'api_call': \\\"from transformers import pipeline\\\\nfill_mask = pipeline('fill-mask', model='microsoft/deberta-v3-base')\\\\nsentence_with_mask = 'The quick [MASK] fox jumps over the lazy dog.'\\\\nresult = fill_mask(sentence_with_mask)\\\", 'api_arguments': ['model_name_or_path'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \\\"from transformers import pipeline\\\\nfill_mask = pipeline('fill-mask', model='microsoft/deberta-v3-base')\\\\nresult = fill_mask('Hugging Face is a [MASK] company.')\\\", 'performance': {'dataset': {'SQuAD 2.0': {'F1': 88.4, 'EM': 85.4}, 'MNLI-m/mm': {'ACC': '90.6/90.7'}}}, 'description': 'DeBERTa V3 improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It further improves the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. The DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer. This model was trained using the 160GB data as DeBERTa V2.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا ببناء نموذج لمعالجة اللغة الطبيعية لتصنيف البجهات من غير المنضمة باستخدام تقنية الصفر الشاملة، ونريد التنبؤ بتصنيف نص معين إلى فئات مختلفة دون الحاجة إلى تدريب على البيانات المحددة.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Use the pipeline function from the transformers library provided by Hugging Face. \\\\n2. Specify the task 'zero-shot-classification' and the model 'cross-encoder/nli-deberta-v3-xsmall' to create a zero-shot classification model.\\\\n3. This model is trained for natural language inference tasks, and it can be used for zero-shot classification tasks as well.\\\\n4. The created classifier can be used to analyze the text of news articles and categorize them into topics like technology, sports, or politics without the need for labeled training data.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'cross-encoder/nli-deberta-v3-xsmall\\\\')\\\\nsent = \\\"Apple just announced the newest iPhone X\\\"\\\\ncandidate_labels = [\\\"technology\\\", \\\"sports\\\", \\\"politics\\\"]\\\\nres = classifier(sent, candidate_labels)\\\\nprint(res)', 'api_call': \\\"pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cross-encoder/nli-deberta-v3-xsmall', 'api_call': \\\"pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\\\", 'api_arguments': ['sent', 'candidate_labels'], 'python_environment_requirements': [['sentence_transformers', 'transformers']], 'example_code': \\\"from transformers import pipeline\\\\nclassifier = pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\\\\nsent = 'Apple just announced the newest iPhone X'\\\\ncandidate_labels = ['technology', 'sports', 'politics']\\\\nres = classifier(sent, candidate_labels)\\\\nprint(res)\\\", 'performance': {'dataset': {'SNLI-test': '91.64', 'MNLI_mismatched': '87.77'}}, 'description': 'This model is a Cross-Encoder for Natural Language Inference, trained on the SNLI and MultiNLI datasets. It can be used for zero-shot classification tasks.' </s>\"\n"}
{"text": "<s>[INST]القسم العلوي مُخصص لعرض العناصر الرئيسية وغيرها من الأشكال المختصة بالتعرف على الكائنات، الرجاء تجاهل أي رموز أو ملصقات لا تبدو واضحة في الصورة أو لا تسهم في تحديد الكائنات، بعد تحويل الصورة إلى نص، يرجى كتابة قصة قصيرة تبدأ بجملة \\\"كان ياما كان في أرض بعيدة\\\" بمعاونة تعبير الملء.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To accomplish this task, we first import the necessary classes from the ultralyticsplus package. This includes YOLO, which is an object detection model.\\\\n2. We then use the YOLO function to load the pre-trained model 'keremberke/yolov8n-table-extraction'. This model has been trained for table extraction tasks and is capable of identifying both bordered and borderless tables.\\\\n3. We can then use this model to analyze an image of a form and extract the tables present in it. To do that, we provide several parameters: confidence threshold, IoU threshold, Non-Maximum Suppression setting, and maximum number of detections.\\\\n4. We use the predict function to obtain the table's bounding boxes and display the image with the detected tables using the render_result function.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-table-extraction')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path_to_your_image.jpg'\\\\n# replace 'path_to_your_image.jpg' with the path to your form image\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8n-table-extraction', 'api_call': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8n-table-extraction')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'table-extraction', 'accuracy': 0.967}, 'description': \\\"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\\\" </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لاستخلاص الميزات متعددة الوسائط، ونسعى لتحليل البيانات الصوتية والبصرية المتاحة لتحقيق دقة أفضل في التصنيف.? [/INST]{'domain': 'Multimodal Feature Extraction', 'api_call': \\\"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary libraries, which are 'HubertModel' from transformers.\\\", 'code': \\\"from transformers import HubertModel\\\\nhubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\\n# Process the input speech and obtain the speech embeddings\\\\nspeech_embeddings = hubert(speech_input)\\\", 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \\\"from transformers import HubertModel\\\\nhubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\\n# Process the input speech and obtain the speech embeddings\\\\nspeech_embeddings = hubert(speech_input)\\\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \\\"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير تطبيق لمعالجة اللغة الطبيعية الذي يمكنه التفاعل المحادثاتية، ونحن بحاجة لإنشاء نماذج تنبؤ بالنصوص التي تأخذ في الاعتبار كيفية التفاعل مع العبارات والأساليب المستخدمة.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"pipeline('conversational', model='mywateriswet/ShuanBot')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' method from transformers.\\\\n2. Use the 'pipeline' function to load the 'mywateriswet/ShuanBot' model, which is a conversational chatbot based on GPT-2 architecture.\\\\n3. The model can be used to generate human-like responses during a chat. For this task, we can prompt the model with a question or request related to movie script ideas.\\\\n4. The model is expected to produce a creative response containing a storyline.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nchatbot = pipeline(\\\\'conversational\\\\', model=\\\\'mywateriswet/ShuanBot\\\\')\\\\nprompt = \\\"Provide me with a movie script idea about a thrilling adventure.\\\"\\\\nresponse = chatbot(prompt)\\\\nprint(response[0][\\\\'generated_text\\\\'])', 'api_call': \\\"pipeline('conversational', model='mywateriswet/ShuanBot')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mywateriswet/ShuanBot', 'api_call': \\\"pipeline('conversational', model='mywateriswet/ShuanBot')\\\", 'api_arguments': 'message', 'python_environment_requirements': ['transformers'], 'example_code': \\\"response = chatbot('What is your name?')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.' </s>\"\n"}
{"text": "<s>[INST]في مجال التصنيف الصوتي، يُطلب من الباحثين تطوير نموذج قادر على تمييز وتصنيف الملفات الصوتية بدقة عالية. يُرجى تقديم المساعدة في هذا المجال.? [/INST]{'domain': 'Audio Audio Classification', 'api_call': \\\"pipeline('audio-classification', model=Wav2Vec2ForCTC.from_pretrained('anton-l/wav2vec2-random-tiny-classifier'))\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import Wav2Vec2ForCTC, pipeline\\\\ncommercial_classifier = pipeline('audio-classification', model=Wav2Vec2ForCTC.from_pretrained('anton-l/wav2vec2-random-tiny-classifier'))\\\\nclassification_result = commercial_classifier(podcast_audio_path)\\\", 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'wav2vec2-random-tiny-classifier', 'api_call': \\\"from transformers import Wav2Vec2ForCTC, pipeline\\\\ncommercial_classifier = pipeline('audio-classification', model=Wav2Vec2ForCTC.from_pretrained('anton-l/wav2vec2-random-tiny-classifier'))\\\\nclassification_result = commercial_classifier(podcast_audio_path)\\\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'An audio classification model based on wav2vec2.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتدريب نموذج الرؤية المتعددة الجوانب على تحويل الصور إلى نص. استخدم مجموعة بيانات تحتوي على تنوع في الصور والنصوص لضمان كفاءة النموذج في التحويل.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes and functions from the torch, requests, PIL, and transformers packages.\\\\n2. We are using the pre-trained 'ydshieh/vit-gpt2-coco-en' model, which is a VisionEncoderDecoderModel - this combines an image processing model (ViT) and a language modeling model (GPT-2) to generate captions.\\\\n3. Load the model and set it to evaluation mode.\\\\n4. Define a function (predict) that takes an image as input, extracts features from the image using the ViTFeatureExtractor, generates text based on the image features using the language model, and returns a list of generated captions.\\\\n5. To generate a caption for an image, call the predict function with the image as the argument.\\\", '<<<code>>>': \\\"from torch import no_grad\\\\nfrom PIL import Image\\\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\\\n\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\\\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('ydshieh/vit-gpt2-coco-en')\\\\ntokenizer = AutoTokenizer.from_pretrained('ydshieh/vit-gpt2-coco-en')\\\\nmodel.eval()\\\\n\\\\ndef generate_caption(image):\\\\n    pixel_values = feature_extractor(images=image, return_tensors='pt').pixel_values\\\\n    with no_grad():\\\\n        output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\\\n    captions = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\\\n    return captions\\\\n\\\\nimage = Image.open('path/to/image') # replace 'path/to/image' with the path to your image\\\\ncaptions = generate_caption(image)\\\\nprint(captions)\\\", 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\\\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': [['torch', 'requests', 'PIL', 'transformers']], 'example_code': 'import torch\\\\nimport requests\\\\nfrom PIL import Image\\\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\\\nloc = ydshieh/vit-gpt2-coco-en\\\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\\\nmodel.eval()\\\\ndef predict(image):\\\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\\\n with torch.no_grad():\\\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\\\n preds = [pred.strip() for pred in preds]\\\\n return preds\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\\\n preds = predict(image)\\\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج تصويري يعكس نص قصير من مجال ترجمة معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary packages, including the MarianMTModel and MarianTokenizer from the transformers library.\\\\n2. Instantiate the pretrained model and tokenizer for translating from Romance languages to English using 'Helsinki-NLP/opus-mt-ROMANCE-en'.\\\\n3. Convert the input phrase from a Romance language to English by first tokenizing it with the tokenizer and then feeding it to the model for translation. Finally, use the decode() method to convert the tokens back to a human-readable English string.\\\", '<<<code>>>': 'from transformers import MarianMTModel, MarianTokenizer\\\\nmodel = MarianMTModel.from_pretrained(\\\\'Helsinki-NLP/opus-mt-ROMANCE-en\\\\')\\\\ntokenizer = MarianTokenizer.from_pretrained(\\\\'Helsinki-NLP/opus-mt-ROMANCE-en\\\\')\\\\nsrc_text = \\\\'Phrase in a romance language here...\\\\'\\\\ninput_tokens = tokenizer(src_text, return_tensors=\\\"pt\\\")\\\\ntranslated_tokens = model.generate(**input_tokens)\\\\nresult = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)', 'api_call': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-ROMANCE-en', 'api_call': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", 'api_arguments': ['source languages', 'target languages'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': '', 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': 62.2, 'chr-F': 0.75}}, 'description': 'A model for translating Romance languages to English, trained on the OPUS dataset. It supports multiple source languages such as French, Spanish, Portuguese, Italian, and Romanian, among others. The model is based on the transformer architecture and uses normalization and SentencePiece for pre-processing.' </s>\"\n"}
{"text": "<s>[INST]نحتاج لإنشاء نظام يستطيع توليد النصوص بشكل تلقائي وطبيعي. البرنامج المطلوب يقوم بتحليل اللغة الطبيعية وإنشاء نصوص بشكل مبتكر وذكي.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='facebook/opt-125m')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a text generation model.\\\\n3. Specify the model \\\\'facebook/opt-125m\\\\' to be loaded. This model is a decoder-only pre-trained transformer that can generate text.\\\\n4. Use the created text generation model to generate a piece of writing starting with the prompt \\\"It was a dark and stormy night...\\\".', '<<<code>>>': 'from transformers import pipeline\\\\ntext_generator = pipeline(\\\\'text-generation\\\\', model=\\\\'facebook/opt-125m\\\\')\\\\ninitial_sentence = \\\"It was a dark and stormy night...\\\"\\\\ngenerated_text = text_generator(initial_sentence, do_sample=True, max_length=100)\\\\nstory = generated_text[0][\\\\'generated_text\\\\']', 'api_call': \\\"pipeline('text-generation', model='facebook/opt-125m')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/opt-125m', 'api_call': \\\"pipeline('text-generation', model='facebook/opt-125m')\\\", 'api_arguments': {'do_sample': 'True'}, 'python_environment_requirements': ['from transformers import pipeline, set_seed'], 'example_code': \\\"generator(Hello, I'm am conscious and)\\\", 'performance': {'dataset': 'Various', 'accuracy': 'Roughly matches GPT-3 performance'}, 'description': 'OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. It was predominantly pretrained with English text, but a small amount of non-English data is present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective. OPT can be used for prompting for evaluation of downstream tasks as well as text generation.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي في استخدام النصوص الطبيعية لتصنيف الرموز.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"SequenceTagger.load('flair/ner-english-large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the flair library, such as Sentence and SequenceTagger.\\\\n2. We load the pre-trained SequenceTagger model 'flair/ner-english-large', which can recognize named entities, such as person names, organization names, and location names.\\\\n3. We create a Sentence object from the given text using the Sentence class.\\\\n4. The tagger.predict method is used to predict the named entities in the sentence.\\\\n5. We print the detected named entities by iterating over the sentence's spans and filtering for organizations specifically.\\\", '<<<code>>>': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\ntagger = SequenceTagger.load('flair/ner-english-large')\\\\n\\\\nsentence = Sentence(text)\\\\ntagger.predict(sentence)\\\\n\\\\norganization_entities = [entity for entity in sentence.get_spans('ner') if entity.tag == 'ORG']\\\\n\\\\nfor entity in organization_entities:\\\\n    print(entity.text)\\\", 'api_call': \\\"SequenceTagger.load('flair/ner-english-large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'flair/ner-english-large', 'api_call': \\\"SequenceTagger.load('flair/ner-english-large')\\\", 'api_arguments': 'sentence', 'python_environment_requirements': ['Flair (pip install flair)'], 'example_code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\n# load tagger\\\\ntagger = SequenceTagger.load(flair/ner-english-large)\\\\n\\\\n# make example sentence\\\\nsentence = Sentence(George Washington went to Washington)\\\\n\\\\n# predict NER tags\\\\ntagger.predict(sentence)\\\\n\\\\n# print sentence\\\\nprint(sentence)\\\\n\\\\n# print predicted NER spans\\\\nprint('The following NER tags are found:')\\\\n\\\\n# iterate over entities and print\\\\nfor entity in sentence.get_spans('ner'):\\\\n print(entity)\\\", 'performance': {'dataset': 'conll2003', 'accuracy': '94.36'}, 'description': 'This is the large 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on document-level XLM-R embeddings and FLERT.' </s>\"\n"}
{"text": "<s>[INST]سيقوم النظام بتطبيق تعلم التعزيز على البيانات التي يتم إدخالها لتحسين الأداء واتخاذ القرارات الذكية في مجال التطبيقات المختلفة، مثل الألعاب والروبوتات المتحركة.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-halfcheetah-expert')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary AutoModel class from the transformers package.\\\\n2. Use the from_pretrained method of the AutoModel class to load the pre-trained model 'edbeeching/decision-transformer-gym-halfcheetah-expert'. This model has been trained on expert trajectories sampled from the Gym HalfCheetah environment, which can be useful to study the movement of a robotic cheetah.\\\\n3. Utilize the model's predictions to analyze the movements of the robotic cheetah and apply them to your own robot's legs to improve its flexibility and movement.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('edbeeching/decision-transformer-gym-halfcheetah-expert')\\\", 'api_call': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-halfcheetah-expert')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'edbeeching/decision-transformer-gym-halfcheetah-expert', 'api_call': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-halfcheetah-expert')\\\", 'api_arguments': {'mean': [-0.04489148, 0.03232588, 0.06034835, -0.17081226, -0.19480659, -0.05751596, 0.09701628, 0.03239211, 11.047426, -0.07997331, -0.32363534, 0.36297753, 0.42322603, 0.40836546, 1.1085187, -0.4874403, -0.0737481], 'std': [0.04002118, 0.4107858, 0.54217845, 0.41522816, 0.23796624, 0.62036866, 0.30100912, 0.21737163, 2.2105937, 0.572586, 1.7255033, 11.844218, 12.06324, 7.0495934, 13.499867, 7.195647, 5.0264325]}, 'python_environment_requirements': [['transformers']], 'example_code': 'See our Blog Post, Colab notebook or Example Script for usage.', 'performance': {'dataset': 'Gym HalfCheetah environment', 'accuracy': 'Not specified'}, 'description': 'Decision Transformer model trained on expert trajectories sampled from the Gym HalfCheetah environment' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لغة طبيعية لتوليد نص يبدأ بالعبارة \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='bigscience/bloom-560m')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We can use a text generation model to come up with an introduction paragraph.\\\\n2. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n3. Use the pipeline function to create a text generation model.\\\\n4. Specify the model 'bigscience/bloom-560m' to be loaded. This is a large-scale text generation model trained on multiple languages, making it suitable for generating content for an article in your language of choice.\\\\n5. Now, use the created model to generate an introductory paragraph based on the given input.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntext_generator = pipeline('text-generation', model='bigscience/bloom-560m')\\\\nprompt = 'The history of artificial intelligence'\\\\ngenerated_text = text_generator(prompt)\\\\nprint(generated_text[0]['generated_text'])\\\", 'api_call': \\\"pipeline('text-generation', model='bigscience/bloom-560m')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bigscience/bloom-560m', 'api_call': \\\"pipeline('text-generation', model='bigscience/bloom-560m')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline\\\\n\\\\nmodel_name = 'bigscience/bloom-560m'\\\\napi = pipeline('text-generation', model=model_name)\\\\n\\\\ntext = 'The history of artificial intelligence began in the '\\\\noutput = api(text)\\\\nprint(output[0]['generated_text'])\\\", 'performance': {'dataset': 'Validation', 'accuracy': {'Training Loss': 2.0, 'Validation Loss': 2.2, 'Perplexity': 8.9}}, 'description': 'BLOOM LM is a large open-science, open-access multilingual language model developed by BigScience. It is a transformer-based language model trained on 45 natural languages and 12 programming languages. The model has 559,214,592 parameters, 24 layers, and 16 attention heads.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import M2M100ForConditionalGeneration and M2M100Tokenizer classes from transformers.\\\\n2. Load the pre-trained M2M100 model and its corresponding tokenizer using the from_pretrained method and 'facebook/m2m100_418M' as the model identifier.\\\\n3. Set the source language in the tokenizer by assigning the language code (i.e., 'fr' for French, 'de' for German, etc.) to the src_lang attribute of the tokenizer.\\\\n4. Tokenize the input text using the tokenizer, and include the return_tensors parameter to produce a PyTorch tensor.\\\\n5. Use the generate method on the loaded model, forcing the first generated token to be the target language code (i.e., 'en' for English) using forced_bos_token_id parameter.\\\\n6. Decode the generated tokens using the batch_decode method of the tokenizer, setting skip_special_tokens parameter to True. This will provide the English-translated text.\\\", '<<<code>>>': 'from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\\\nsource_text = \\\"User comment text in their native language\\\"\\\\ntokenizer = M2M100Tokenizer.from_pretrained(\\\\'facebook/m2m100_418M\\\\')\\\\nmodel = M2M100ForConditionalGeneration.from_pretrained(\\\\'facebook/m2m100_418M\\\\')\\\\ntokenizer.src_lang = \\\"native_language_code\\\" # replace with the appropriate code of the native language\\\\nencoded_input = tokenizer(source_text, return_tensors=\\\\'pt\\\\')\\\\ngenerated_tokens = model.generate(**encoded_input, forced_bos_token_id=tokenizer.get_lang_id(\\\\'en\\\\'))\\\\ntranslated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)', 'api_call': \\\"M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/m2m100_418M', 'api_call': \\\"M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')\\\", 'api_arguments': {'encoded_input': 'Encoded input text', 'target_lang': 'Target language code'}, 'python_environment_requirements': [['transformers', 'sentencepiece']], 'example_code': ['from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer', 'hi_text = ', 'chinese_text = ', 'model = M2M100ForConditionalGeneration.from_pretrained(facebook/m2m100_418M)', 'tokenizer = M2M100Tokenizer.from_pretrained(facebook/m2m100_418M)', 'tokenizer.src_lang = hi', 'encoded_hi = tokenizer(hi_text, return_tensors=pt)', 'generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(fr))', 'tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)'], 'performance': {'dataset': 'WMT', 'accuracy': 'Not provided'}, 'description': 'M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token.' </s>\"\n"}
{"text": "<s>[INST]قم بتوليد أسئلة تصنيفية تتعلق ببيانات الجداول، وضع الأسئلة بحيث تساعد في فهم وتصنيف البيانات الكمية والنصوص الواردة في الجداول بشكل دقيق وفعال.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"load_model(cached_download(hf_hub_url('danupurnomo/dummy-titanic', 'titanic_model.h5')))\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': \\\"1. Import the required libraries: huggingface_hub for loading the model, joblib for deserializing the saved model, pandas and numpy for data manipulation, and tensorflow for using the model.\\\\n2. Use the REPO_ID 'danupurnomo/dummy-titanic' to specify the model repository and PIPELINE_FILENAME and TF_FILENAME to specify the model file names.\\\\n3. Load the preprocessing pipeline and model from the Hugging Face model hub using hf_hub_url, cached_download, and load_model functions.\\\\n4. With the loaded model and pipeline, pass in new data as a pandas DataFrame. It needs to contain features such as passenger class, age, sex, fare, and others as specified during model training.\\\\n5. Use the preprocessing pipeline to transform the new data and obtain predictions using the TensorFlow model.\\\", '<<<code>>>': \\\"from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nfrom tensorflow.keras.models import load_model\\\\nREPO_ID = 'danupurnomo/dummy-titanic'\\\\nPIPELINE_FILENAME = 'final_pipeline.pkl'\\\\nTF_FILENAME = 'titanic_model.h5'\\\\nmodel_pipeline = joblib.load(cached_download(\\\\n    hf_hub_url(REPO_ID, PIPELINE_FILENAME)\\\\n))\\\\nmodel_seq = load_model(cached_download(\\\\n    hf_hub_url(REPO_ID, TF_FILENAME)\\\\n))\\\\nnew_data = pd.DataFrame(new_data, columns=['Pclass', 'Age', 'Sex', 'Fare',...])\\\\ntransformed_data = model_pipeline.transform(new_data)\\\\npredictions = model_seq.predict(transformed_data)\\\", 'api_call': \\\"load_model(cached_download(hf_hub_url('danupurnomo/dummy-titanic', 'titanic_model.h5')))\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'danupurnomo/dummy-titanic', 'api_call': \\\"load_model(cached_download(hf_hub_url('danupurnomo/dummy-titanic', 'titanic_model.h5')))\\\", 'api_arguments': ['new_data'], 'python_environment_requirements': [['huggingface_hub', 'joblib', 'pandas', 'numpy', 'tensorflow']], 'example_code': \\\"from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nfrom tensorflow.keras.models import load_model\\\\nREPO_ID = 'danupurnomo/dummy-titanic'\\\\nPIPELINE_FILENAME = 'final_pipeline.pkl'\\\\nTF_FILENAME = 'titanic_model.h5'\\\\nmodel_pipeline = joblib.load(cached_download(\\\\n hf_hub_url(REPO_ID, PIPELINE_FILENAME)\\\\n))\\\\nmodel_seq = load_model(cached_download(\\\\n hf_hub_url(REPO_ID, TF_FILENAME)\\\\n))\\\", 'performance': {'dataset': 'Titanic', 'accuracy': 'Not provided'}, 'description': 'This model is a binary classifier for predicting whether a passenger on the Titanic survived or not, based on features such as passenger class, age, sex, fare, and more.' </s>\"\n"}
{"text": "<s>[INST]قم بتثبيت بيئة تطوير مشتركة لتشغيل نماذج العمق بالرؤية الحاسوبية. استخدم البيانات المتاحة لتدريب النماذج وقم بتقييم أداء العمق المستنتج. قم بمشاركة النتائج والبرمجيات الخاصة بك مع مشاركي النادي للحصول على تعليقات وتحسين الأداء.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To achieve depth estimation, we will use a pre-trained model from the Hugging Face Model Hub, specifically 'sayakpaul/glpn-nyu-finetuned-diode-221116-104421'.\\\\n2. We first import the required package AutoModel from transformers and load the pre-trained depth estimation model using the AutoModel.from_pretrained() method.\\\\n3. Once you have the model, you can feed the required input image (acquired from the camera feed) into the model, which will then process the image and provide an output depth map.\\\\n4. This depth map can be used to determine the distance of various objects in the scene from the camera.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\\\\n# Process the input image and get the depth map prediction\\\\n# depth_map = model(<input_image_prepared>)\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221116-104421', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3736, 'Mae': 0.3079, 'Rmse': 0.4321, 'Abs Rel': 0.3666, 'Log Mae': 0.1288, 'Log Rmse': 0.1794, 'Delta1': 0.4929, 'Delta2': 0.7934, 'Delta3': 0.9234}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى إنشاء نموذج قادر على الجمع بين معالجة اللغات الطبيعية والإجابة عن الأسئلة. يجب أن يكون النموذج قادراً على تصنيف الرموز اللغوية بدقة لضمان كفاءة العملية ودقتها.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"AutoModel.from_pretrained('deepset/bert-medium-squad2-distilled')\\\", 'api_provider': 'PyTorch Transformers', 'explanation': '', 'code': \\\"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\\\\ntokenizer = AutoTokenizer.from_pretrained('deepset/bert-medium-squad2-distilled')\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained('deepset/bert-medium-squad2-distilled')\\\\ninputs = tokenizer(question, context, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nanswer_start = outputs.start_logits.argmax().item()\\\\nanswer_end = outputs.end_logits.argmax().item() + 1\\\\nanswer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\\\", 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'PyTorch Transformers', 'functionality': 'Question Answering', 'api_name': 'deepset/bert-medium-squad2-distilled', 'api_call': \\\"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\\\\ntokenizer = AutoTokenizer.from_pretrained('deepset/bert-medium-squad2-distilled')\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained('deepset/bert-medium-squad2-distilled')\\\\ninputs = tokenizer(question, context, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nanswer_start = outputs.start_logits.argmax().item()\\\\nanswer_end = outputs.end_logits.argmax().item() + 1\\\\nanswer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\\\", 'api_arguments': {'model': 'deepset/bert-medium-squad2-distilled'}, 'python_environment_requirements': {'transformers': '>=4.0.0'}, 'example_code': \\\"qa({'context': 'This is an example context.', 'question': 'What is this example about?'})\\\", 'performance': {'dataset': 'squad_v2', 'exact': 68.6431398972458, 'f1': 72.7637083790805}, 'description': 'This model is a distilled version of deepset/bert-large-uncased-whole-word-masking-squad2, trained on the SQuAD 2.0 dataset for question answering tasks. It is based on the BERT-medium architecture and uses the Hugging Face Transformers library.'}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء جدول بيانات لعملية تحليل تابعه لتوقع القيم المستقبلية باستخدام الاحتساب التابع.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': \\\"1. We first import the necessary function from the Scikit-learn package. This includes pipeline, a function that allows us to quickly create machine learning pipelines.\\\\n2. We then use the pipeline function to create a tabular regression pipeline. We specify the type of pipeline as 'tabular-regression' and load the pre-trained model 'merve/tips9y0jvt5q-tip-regression', which has been trained on tips data for regression tasks.\\\\n3. This pipeline can then be used to perform regression on the customer tipping data, helping us better understand the patterns present in the data.\\\\n4. You will need to install the `dabl` package if it's not already present.\\\", '<<<code>>>': \\\"from sklearn.pipeline import pipeline\\\\nimport dabl\\\\n\\\\nmodel = pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\\\\n# You can then use model.fit(X_train, y_train) and model.predict(X_test) to train and predict, respectively.\\\", 'api_call': \\\"pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'merve/tips9y0jvt5q-tip-regression', 'api_call': \\\"pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['dabl'], 'example_code': 'N/A', 'performance': {'dataset': 'tips9y0jvt5q', 'accuracy': {'r2': 0.41524, 'neg_mean_squared_error': -1.098792}}, 'description': 'Baseline Model trained on tips9y0jvt5q to apply regression on tip. The model uses Ridge(alpha=10) and is trained with dabl library as a baseline. For better results, use AutoTrain.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى تدريب نموذج لتصنيف النصوص في مجال معالجة اللغات الطبيعية. يرجى توجيه الجهود نحو جمع وتصنيف كمية كبيرة من البيانات ، ثم تقسيمها إلى تجمعات ، وبناء نموذج يقوم بتصنيف النصوص بدقة. هل يمكنك القيام بذلك؟? [/INST]{'domain': 'Natural Language Processing Text Classification', 'api_call': \\\"pipeline('text-classification', model='roberta-base-openai-detector')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the pipeline function from the transformers library.', 'code': 'from transformers import pipeline\\\\ndetector = pipeline(\\\\'text-classification\\\\', model=\\\\'roberta-base-openai-detector\\\\')\\\\nresult = detector(\\\"Hello world! Is this content AI-generated?\\\")', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Transformers', 'functionality': 'Detect GPT-2 generated text', 'api_name': 'roberta-base-openai-detector', 'api_call': 'from transformers import pipeline\\\\ndetector = pipeline(\\\\'text-classification\\\\', model=\\\\'roberta-base-openai-detector\\\\')\\\\nresult = detector(\\\"Hello world! Is this content AI-generated?\\\")', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import pipeline\\\\npipe = pipeline(text-classification, model=roberta-base-openai-detector)\\\\nprint(pipe(Hello world! Is this content AI-generated?))', 'performance': {'dataset': 'WebText', 'accuracy': '95%'}, 'description': 'RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model.'}} </s>\"\n"}
{"text": "<s>[INST]نحتاج منك لتطوير نموذج لمعالجة اللغة الطبيعية لتلخيص النصوص بشكل أكثر دقة. الرجاء توضيح الخوارزميات والتقنيات التي تعتزم استخدامها في المهمة.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"pipeline('summarization', model='google/pegasus-xsum')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. The pipeline function is used to create a summarization model which is capable of summarizing large text.\\\\n3. We specify the model 'google/pegasus-xsum' to be loaded. This is a state-of-the-art model for abstractive text summarization based on the Transformer architecture.\\\\n4. Using the created summarizer, we can input the lengthy news article and get a summarized version of the content in a brief format.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nsummarizer = pipeline('summarization', model='google/pegasus-xsum')\\\\nsummary = summarizer(long_news_article, max_length=100, min_length=25, do_sample=False)[0]['summary_text']\\\", 'api_call': \\\"pipeline('summarization', model='google/pegasus-xsum')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/pegasus-xsum', 'api_call': \\\"pipeline('summarization', model='google/pegasus-xsum')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': [{'name': 'xsum', 'accuracy': {'ROUGE-1': 46.862, 'ROUGE-2': 24.453, 'ROUGE-L': 39.055, 'ROUGE-LSUM': 39.099}}, {'name': 'cnn_dailymail', 'accuracy': {'ROUGE-1': 22.206, 'ROUGE-2': 7.67, 'ROUGE-L': 15.405, 'ROUGE-LSUM': 19.218}}, {'name': 'samsum', 'accuracy': {'ROUGE-1': 21.81, 'ROUGE-2': 4.253, 'ROUGE-L': 17.447, 'ROUGE-LSUM': 18.891}}]}, 'description': 'PEGASUS is a pre-trained model for abstractive summarization, developed by Google. It is based on the Transformer architecture and trained on both C4 and HugeNews datasets. The model is designed to extract gap sentences and generate summaries by stochastically sampling important sentences.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء صورة نصية لمشهد يصور الحيوانات في البرية، مع إستخدام تقنيات الذكاء الاصطناعي لتحويل الصور إلى نصوص توصف المشهد بدقة وإبداع? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the Hugging Face Transformers library.\\\\n2. Use the pipeline function to create an optical character recognition (OCR) model for converting the Japanese manga image to text.\\\\n3. Specify the model 'kha-white/manga-ocr-base'. This model is specifically designed for Japanese text in manga, making it a perfect fit for the publisher's requirements.\\\\n4. The OCR pipeline can be used to process the manga page image, extract the Japanese text, and create its digital representation. This could then be used for their online release.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nocr_pipeline = pipeline('ocr', model='kha-white/manga-ocr-base')\\\\nmanga_text = ocr_pipeline(manga_page_image)\\\", 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'api_arguments': 'image', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>\"\n"}
{"text": "<s>[INST]يركز تطبيقنا على تطوير نماذج لتحويل النصوص إلى صور متعددة الوسائط، يجب أن تكون هذه النماذج قادرة على تحديد عمق الأشياء المعروضة في الصور بدقة عالية من خلال تحليل النصوص المقابلة لها.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting', revision=fp16, torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting', revision=fp16, torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \\\"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting', revision=fp16, torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': [{'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}], 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء وصف نصي للصورة المعروضة يأخذ في الاعتبار المحتوى البصري والمعلومات التي يمكن استخراجها منها.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries from the transformers and PIL packages, including TrOCRProcessor and VisionEncoderDecoderModel for OCR processing, and Image for image processing.\\\\n2. Load the pre-trained model 'microsoft/trocr-small-printed' using the from_pretrained method of the VisionEncoderDecoderModel class. This model is designed for optical character recognition tasks.\\\\n3. Use the TrOCRProcessor.from_pretrained method to load the OCR processor and convert an input image to a format that the model can understand.\\\\n4. Use the model to generate text from the processed image.\\\\n5. Decode the generated text to a human-readable format using the processor's batch_decode method.\\\", '<<<code>>>': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimage = Image.open('path_to_image.jpg')\\\\n# replace 'path_to_image.jpg' with the path to your image\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\\nprint(generated_text)\\\", 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/trocr-small-printed', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\\", 'api_arguments': {'image': \\\"Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\", 'processor': \\\"TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\\"}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج لتحويل النص إلى كلام لهوية تطبيقنا الجديد الخاص بمعالجة اللغة الطبيعية والتفاعلية.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary libraries from Hugging Face Transformers, which are AutoModelForCausalLM and AutoTokenizer.\\\\n2. Load the tokenizer and model 'microsoft/DialoGPT-small', which is a pre-trained model designed for conversational applications.\\\\n3. When receiving a user input, tokenize it and generate a response using the model.\\\\n4. Decode the generated tokens to obtain the model's response as text.\\\\n5. Use the generated response to answer users' questions on the support page.\\\", '<<<code>>>': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'microsoft/DialoGPT-small\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'microsoft/DialoGPT-small\\\\')\\\\n\\\\nuser_input = \\\"User question or input here...\\\"\\\\ninput_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\\\\'pt\\\\')\\\\ngenerated_ids = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.eos_token_id)\\\\nresponse = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\\\\n\\\\nprint(\\\"Support response:\\\", response)', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/DialoGPT-small', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')\\\", 'api_arguments': ['bot_input_ids', 'max_length', 'pad_token_id'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-small)\\\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-small)\\\\nfor step in range(5):\\\\n new_user_input_ids = tokenizer.encode(input(>> User:) + tokenizer.eos_token, return_tensors='pt')\\\\n bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\\\\n chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n print(DialoGPT: {}.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\\\", 'performance': {'dataset': 'Reddit discussion thread', 'accuracy': 'Comparable to human response quality under a single-turn conversation Turing test'}, 'description': 'DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multiturn conversations. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتحويل الصور إلى نصوص متعددة الوسائط، ونشجع على توفير بيانات وسائط متعددة لتحسين جودة الإخراج.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary classes from the transformers library provided by Hugging Face: TrOCRProcessor for image processing, and VisionEncoderDecoderModel for the pretrained model.\\\\n2. We load the pre-trained model, called 'microsoft/trocr-large-printed', with VisionEncoderDecoderModel.from_pretrained method.\\\\n3. Using the Image class from the PIL library, we read the image data and convert the image to the RGB color space.\\\\n4. We preprocess the image using TrOCRProcessor.from_pretrained to create pixel_values for the model.\\\\n5. The model takes the pixel_values as input and generates the corresponding ids for the text.\\\\n6. Finally, processor.batch_decode is used to decode the generated ids, converting the handwritten text in the images to digital text.\\\", '<<<code>>>': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\n\\\\nimage = Image.open('your_image_path.jpg').convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/trocr-large-printed', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\\", 'api_arguments': {'TrOCRProcessor': \\\"from_pretrained('microsoft/trocr-large-printed')\\\", 'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': [{'transformers': 'pip install transformers', 'PIL': 'pip install pillow', 'requests': 'pip install requests'}], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطبيق تقنيات معالجة اللغات الطبيعية وملء الفجوة. هل يمكنك توفير دليل شامل أو توجيهات عملية لمساعدتي على كيفية إنشاء نظام لمعالجة اللغات الطبيعية باستخدام تقنيات ملء الفجوة؟? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='roberta-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the necessary \\\\'pipeline\\\\' function from the transformers library provided by Hugging Face.\\\\n2. Create an instance of the \\\"fill-mask\\\" pipeline with the pre-trained model \\\\'roberta-base\\\\'.\\\\n3. This model is based on the RoBERTa architecture, which is an advanced masked language model that helps in filling missing words in the given text, completing the sentences in context.\\\\n4. You can use this pipeline to fill in missing words (represented by <mask>) in an input text, making it easier to complete sentences while writing articles.', '<<<code>>>': 'from transformers import pipeline\\\\nfill_mask = pipeline(\\\\'fill-mask\\\\', model=\\\\'roberta-base\\\\')\\\\ninput_text = \\\"Artificial intelligence is a rapidly growing field that has the potential to <mask> many industries.\\\"\\\\nresult = fill_mask(input_text)', 'api_call': \\\"pipeline('fill-mask', model='roberta-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'roberta-base', 'api_call': \\\"pipeline('fill-mask', model='roberta-base')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='roberta-base')\\\\nunmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': [{'name': 'MNLI', 'accuracy': 87.6}, {'name': 'QQP', 'accuracy': 91.9}, {'name': 'QNLI', 'accuracy': 92.8}, {'name': 'SST-2', 'accuracy': 94.8}, {'name': 'CoLA', 'accuracy': 63.6}, {'name': 'STS-B', 'accuracy': 91.2}, {'name': 'MRPC', 'accuracy': 90.2}, {'name': 'RTE', 'accuracy': 78.7}]}, 'description': 'RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using the Masked language modeling (MLM) objective. This model is case-sensitive and can be fine-tuned on a downstream task.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا ببناء نموذج تصنيف جدولي لتصنيف البيانات في مجال معالجة اللغة الطبيعية والإجابة على الأسئلة، ونريد التنبؤ بتصنيف البيانات بناءً على مجموعات من البيانات المتاحة.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model='deepset/roberta-large-squad2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary libraries, which include the 'pipeline' function from the Transformers library provided by Hugging Face.\\\\n2. We specify the RoBERTa model 'deepset/roberta-large-squad2' to be used for question-answering purposes.\\\\n3. We can provide questions related to programming languages, and specific contexts containing the desired information to be extracted.\\\\n4. The model will be able to answer the questions based on the given context, providing brief descriptions and examples of programming languages.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nnlp_qa = pipeline(\\\\'question-answering\\\\', model=\\\\'deepset/roberta-large-squad2\\\\')\\\\nquestion = \\\\'What is the main use of JavaScript?\\\\'\\\\ncontext = \\\"JavaScript is a programming language that is primarily used for web development. It allows for dynamic content updates and interactions in web browsers.\\\"\\\\nanswer = nlp_qa({\\\\'question\\\\': question, \\\\'context\\\\': context})', 'api_call': \\\"pipeline('question-answering', model='deepset/roberta-large-squad2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'deepset/roberta-large-squad2', 'api_call': \\\"pipeline('question-answering', model='deepset/roberta-large-squad2')\\\", 'api_arguments': ['question', 'context'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; nlp = pipeline('question-answering', model='deepset/roberta-large-squad2'); nlp({'question': 'What is the capital of Germany?', 'context': 'Berlin is the capital of Germany.'})\\\", 'performance': {'dataset': 'squad_v2', 'accuracy': 'Not provided'}, 'description': 'A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتوليد تصنيف مميز للرموز في مجال معالجة اللغة الطبيعية من خلال تحليل البيانات بشكل دقيق وتحسين جودة النصوص المنشأة.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import AutoModel and BertTokenizerFast from the transformers library provided by Hugging Face.\\\\n2. Load the tokenizer 'bert-base-chinese' and the model 'ckiplab/bert-base-chinese-pos'. The tokenizer is used to tokenize the traditional Chinese text, and the model is used to identify the part-of-speech tags associated with each token.\\\\n3. Create a function that takes a traditional Chinese text as input, tokenizes it using the BertTokenizerFast, and then generates part-of-speech tags using the AutoModel.\\\\n4. The output should be the original tokenized text along with its associated part-of-speech tags.\\\", '<<<code>>>': \\\"from transformers import BertTokenizerFast, AutoModel\\\\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\\\\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\\n\\\\ndef tag_POS(text):\\\\n    inputs = tokenizer(text, return_tensors='pt')\\\\n    outputs = model(**inputs)\\\\n    predictions = outputs.logits.argmax(axis=-1).squeeze().tolist()\\\\n    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze())\\\\n    pos_tags = [tokenizer.decode([pred], skip_special_tokens=True) for pred in predictions]\\\\n    return list(zip(tokens, pos_tags))\\\", 'api_call': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ckiplab/bert-base-chinese-pos', 'api_call': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'api_arguments': {'tokenizer': \\\"BertTokenizerFast.from_pretrained('bert-base-chinese')\\\"}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import (\\\\n  BertTokenizerFast,\\\\n  AutoModel,\\\\n)\\\\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\\\\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).' </s>\"\n"}
{"text": "<s>[INST]تعمل شركة تطوير عميلنا على تطوير تطبيق يستهدف ذوي الإعاقة البصرية بمجال Vision Computer Object Detection. نحتاج إلى برنامج يمكنه ملء الفجوات في النص التلقائي لتحسين تجربة المستخدمين في هذا التطبيق.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries and classes: YolosFeatureExtractor and YolosForObjectDetection from transformers, Image from PIL, and requests for downloading the image from a URL.\\\\n2. Define the source of the image to be processed. In this case, it's a URL containing a sample image.\\\\n3. Open the image using the Image library and get the model ready. To do that, first load the feature_extractor from YolosFeatureExtractor.from_pretrained() and the model from YolosForObjectDetection.from_pretrained().\\\\n4. Process the image with the feature extractor and pass it to the model to get the object detection results in the form of bounding boxes (pred_boxes) and corresponding logits.\\\\n5. The resulting bounding boxes and logits can be used to identify and track different object types in the image, which can be useful for a self-navigating robotics framework.\\\", '<<<code>>>': \\\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\n\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\", 'api_call': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hustvl/yolos-tiny', 'api_call': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': \\\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '28.7 AP'}, 'description': 'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا ببناء نظام لمعالجة اللغة الطبيعية لترجمة النصوص، حيث نحتاج إلى ترجمة النصوص بناءً على البيانات المقدمة. يُرجى تقديم النصوص لفريق العمل لتصميم نظام الترجمة بشكل دقيق وفعال.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the pipeline function from Hugging Face's transformers library.\\\", 'code': \\\"from transformers import pipeline\\\\ntranslator = pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\\nenglish_product_description = 'A high-quality and affordable gaming chair with ergonomic design...'\\\\narabic_product_description = translator(english_product_description)[0]['translation_text']\\\", 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Translation', 'api_name': 'Helsinki-NLP/opus-mt-en-ar', 'api_call': \\\"from transformers import pipeline\\\\ntranslator = pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\\nenglish_product_description = 'A high-quality and affordable gaming chair with ergonomic design...'\\\\narabic_product_description = translator(english_product_description)[0]['translation_text']\\\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\\ntranslated_text = translation('Hello World')\\\", 'performance': {'dataset': 'Tatoeba-test.eng.ara', 'accuracy': {'BLEU': 14.0, 'chr-F': 0.437}}, 'description': \\\"A Hugging Face Transformers model for English to Arabic translation, trained on the Tatoeba dataset. It uses a transformer architecture and requires a sentence initial language token in the form of '>>id<<' (id = valid target language ID).\\\"}} </s>\"\n"}
{"text": "<s>[INST]عميلنا يرغب في تطوير تطبيق يعتمد على تقنية جدولية لتحقيق توقعات دقيقة للبيانات. نحتاج إلى تطبيق قادر على تحليل البيانات الجدولية وتحقيق التنبؤات بدقة عالية.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. First, we import the necessary libraries, including joblib and pandas.\\\\n2. Load the pre-trained model by calling joblib.load('model.joblib').\\\\n3. Load the configuration file 'config.json' to get the names of the input features.\\\\n4. Load the input data in a CSV file (e.g., 'data.csv') as a pandas DataFrame, selecting only the specified input features and renaming the columns as 'feat_' + the original column name.\\\\n5. Use the model.predict() method to obtain predictions for carbon emissions for each row of input data.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nimport json\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\n\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\n\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pcoloc/autotrain-mikrotik-7-7-1860563588', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['json', 'joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'pcoloc/autotrain-data-mikrotik-7-7', 'accuracy': {'Loss': 48.213, 'R2': 0.654, 'MSE': 2324.518, 'MAE': 32.634, 'RMSLE': 0.586}}, 'description': 'A tabular regression model trained with AutoTrain to predict carbon emissions based on input features.' </s>\"\n"}
{"text": "<s>[INST]يتطلب تطوير تطبيق لذوي الإعاقة البصرية في مجال تحويل النص إلى كلام استخدام أساليب معالجة اللغة الطبيعية لإنتاج خوارزميات تقوم بذلك.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"PegasusForConditionalGeneration.from_pretrained('tuner007/pegasus_paraphrase')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import PegasusForConditionalGeneration and PegasusTokenizer from the transformers library provided by Hugging Face.\\\\n2. Load the 'tuner007/pegasus_paraphrase' model, which is a PEGASUS fine-tuned for paraphrasing tasks such as rewriting paragraphs.\\\\n3. Create a function that takes input_text, num_return_sequences, and num_beams as arguments. The input_text is the paragraph you want to rewrite, num_return_sequences is the number of different paraphrases you want to receive, and num_beams is a parameter that affects the quality and diversity of generated paraphrases.\\\\n4. Tokenize the input text and use the model to generate paraphrased output using generate() method.\\\\n5. Decode the generated output to receive readable paraphrased text.\\\", '<<<code>>>': 'from transformers import PegasusForConditionalGeneration, PegasusTokenizer\\\\nmodel_name = \\\\'tuner007/pegasus_paraphrase\\\\'\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\n\\\\ndef paraphrase_text(input_text, num_return_sequences=1, num_beams=5):\\\\n    batch = tokenizer([input_text], truncation=True, padding=\\\\'longest\\\\', max_length=60, return_tensors=\\\\'pt\\\\')\\\\n    translated = model.generate(**batch, max_length=60, num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\\\\n    paraphrased_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\\\\n    return paraphrased_text\\\\n\\\\nparaphrased_text = paraphrase_text(\\\"an_original_paragraph\\\", num_return_sequences=1, num_beams=5)', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('tuner007/pegasus_paraphrase')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tuner007/pegasus_paraphrase', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('tuner007/pegasus_paraphrase')\\\", 'api_arguments': {'input_text': 'string', 'num_return_sequences': 'integer', 'num_beams': 'integer'}, 'python_environment_requirements': [{'torch': 'any', 'transformers': 'any'}], 'example_code': \\\"import torch\\\\nfrom transformers import PegasusForConditionalGeneration, PegasusTokenizer\\\\nmodel_name = 'tuner007/pegasus_paraphrase'\\\\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\\\\ndef get_response(input_text,num_return_sequences,num_beams):\\\\n batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=pt).to(torch_device)\\\\n translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\\\\n tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\\\\n return tgt_text\\\", 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'PEGASUS fine-tuned for paraphrasing' </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذجًا لتصنيف النصوص بدون تدريب في مجال معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We need to import AutoTokenizer and AutoModelForSequenceClassification from the transformers library provided by Hugging Face.\\\\n2. Then, we load the pre-trained model 'MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary', which is proficient in natural language inference (NLI) tasks.\\\\n3. We create the input by encoding the given news headline as a hypothesis and a suitable predefined premise for whether the headline is clickbait or not.\\\\n4. The model then outputs probabilities for both entailment and not_entailment cases.\\\\n5. The final step is to compute and print the confidence levels for both clickbait and not_clickbait labels based on these output probabilities.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nimport torch\\\\n\\\\nmodel_name = \\\\'MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\\\\n\\\\nheadline = \\\"You won\\\\'t believe what happened when this celebrity was spotted in Beverly Hills!\\\"\\\\npremise_clickbait = \\\"The headline is clickbait.\\\"\\\\npremise_not_clickbait = \\\"The headline is not clickbait.\\\"\\\\ninput_clickbait = tokenizer(premise_clickbait, headline, truncation=True, return_tensors=\\\\'pt\\\\')\\\\ninput_not_clickbait = tokenizer(premise_not_clickbait, headline, truncation=True, return_tensors=\\\\'pt\\\\')\\\\n\\\\noutput_clickbait = model(**input_clickbait)\\\\noutput_not_clickbait = model(**input_not_clickbait)\\\\n\\\\npredictions = {\\\\n    \\\\'clickbait\\\\': torch.softmax(output_clickbait.logits, -1)[0][0].item() * 100,\\\\n    \\\\'not_clickbait\\\\': torch.softmax(output_not_clickbait.logits, -1)[0][0].item() * 100\\\\n}\\\\n\\\\nprint(predictions)', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary')\\\", 'api_arguments': {'premise': 'string', 'hypothesis': 'string'}, 'python_environment_requirements': [{'transformers': '>=4.13', 'torch': 'any'}], 'example_code': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nimport torch\\\\ndevice = torch.device(cuda) if torch.cuda.is_available() else torch.device(cpu)\\\\nmodel_name = MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\\\\npremise = I first thought that I liked the movie, but upon second thought it was actually disappointing.\\\\nhypothesis = The movie was good.\\\\ninput = tokenizer(premise, hypothesis, truncation=True, return_tensors=pt)\\\\noutput = model(input[input_ids].to(device))\\\\nprediction = torch.softmax(output[logits][0], -1).tolist()\\\\nlabel_names = [entailment, not_entailment]\\\\nprediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\\\\nprint(prediction)', 'performance': {'dataset': {'mnli-m-2c': {'accuracy': 0.925}, 'mnli-mm-2c': {'accuracy': 0.922}, 'fever-nli-2c': {'accuracy': 0.892}, 'anli-all-2c': {'accuracy': 0.676}, 'anli-r3-2c': {'accuracy': 0.665}, 'lingnli-2c': {'accuracy': 0.888}}}, 'description': 'This model was trained on 782 357 hypothesis-premise pairs from 4 NLI datasets: MultiNLI, Fever-NLI, LingNLI and ANLI. The base model is DeBERTa-v3-xsmall from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي لتقسيم الصور.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries such as OneFormerProcessor, OneFormerForUniversalSegmentation from transformers, Image from PIL, and requests.\\\\n2. Load the image from the warehouse surveillance camera.\\\\n3. Initialize the OneFormerProcessor and OneFormerForUniversalSegmentation with the 'shi-labs/oneformer_ade20k_swin_tiny' model.\\\\n4. Perform multi-task segmentation of the image by preparing the required task inputs like 'semantic', 'instance', and 'panoptic'. Pass these as task_inputs to the processor and then to the model.\\\\n5. Obtain the segmented outputs for each task and use the processor's post-processing functions to generate the final segmentation maps for each task separately.\\\", '<<<code>>>': \\\"from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://path_to_warehouse_image.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nprocessor = OneFormerProcessor.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\\n\\\\nsemantic_inputs = processor(images=image, task_inputs=['semantic'], return_tensors='pt')\\\\nsemantic_outputs = model(**semantic_inputs)\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(semantic_outputs, target_sizes=[image.size[::-1]])[0]\\\\n\\\\ninstance_inputs = processor(images=image, task_inputs=['instance'], return_tensors='pt')\\\\ninstance_outputs = model(**instance_inputs)\\\\npredicted_instance_map = processor.post_process_instance_segmentation(instance_outputs, target_sizes=[image.size[::-1]])[0]['segmentation']\\\\n\\\\npanoptic_inputs = processor(images=image, task_inputs=['panoptic'], return_tensors='pt')\\\\npanoptic_outputs = model(**panoptic_inputs)\\\\npredicted_panoptic_map = processor.post_process_panoptic_segmentation(panoptic_outputs, target_sizes=[image.size[::-1]])[0]['segmentation']\\\", 'api_call': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'shi-labs/oneformer_ade20k_swin_tiny', 'api_call': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\\\n\\\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\\\nsemantic_outputs = model(**semantic_inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\n\\\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\\\ninstance_outputs = model(**instance_inputs)\\\\n\\\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\\\n\\\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\\\npanoptic_outputs = model(**panoptic_inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج محادثة يستخدم معالجة اللغة الطبيعية للرد على الاستفسارات حول جدول البيانات. يجب أن يشمل النموذج بداية الرد بجملة \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"TapasForCovid.from_pretrained('lysandre/tiny-tapas-random-sqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary libraries and classes from the transformers package, including AutoTokenizer and TapasForSequenceClassification.\\\\n2. We then use the from_pretrained method of the TapasForSequenceClassification class to load the pre-trained model 'lysandre/tiny-tapas-random-sqa'. This model is specifically designed for table question answering tasks, making it suitable for the hotel reservation system.\\\\n3. To use the model, we will first need a tokenizer to convert the input text (questions and table) into a format that the model can understand. For this, we use the AutoTokenizer.from_pretrained function with the same model name.\\\\n4. We can then use the tokenized input to make predictions using the model, allowing us to answer customers' questions about hotel prices and room availability.\\\", '<<<code>>>': \\\"from transformers import TapasForSequenceClassification, AutoTokenizer\\\\nmodel = TapasForSequenceClassification.from_pretrained('lysandre/tiny-tapas-random-sqa')\\\\ntokenizer = AutoTokenizer.from_pretrained('lysandre/tiny-tapas-random-sqa')\\\", 'api_call': \\\"TapasForCovid.from_pretrained('lysandre/tiny-tapas-random-sqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lysandre/tiny-tapas-random-sqa', 'api_call': \\\"TapasForCovid.from_pretrained('lysandre/tiny-tapas-random-sqa')\\\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'A tiny TAPAS model for table question answering tasks.' </s>\"\n"}
{"text": "<s>[INST]قم بتحميل موديل شبكة جنراتيف متدفقة لتوليد صور بشكل غير مشروط في مجال رؤية الحاسوب.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary library, which is DDPMPipeline from diffusers.', 'code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\\ngenerated_images = pipeline(number_of_images=5)\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'sd-class-butterflies-32', 'api_call': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\\ngenerated_images = pipeline(number_of_images=5)\\\", 'api_arguments': {'model_id': 'clp/sd-class-butterflies-32'}, 'python_environment_requirements': ['diffusers'], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': None, 'accuracy': None}, 'description': 'This model is a diffusion model for unconditional image generation of cute butterflies.'}} </s>\"\n"}
{"text": "<s>[INST]نحن مهتمون بإنشاء نموذج لتحويل مقاطع الفيديو إلى تصنيفات بواسطة تقنية رؤية الحاسوب، حيث يقوم النموذج بتصنيف المحتوى المرئي في المقاطع الفيديو. هل لديك الخبرة اللازمة في مجال تقسيم الصور بواسطة تقنية رؤية الحاسوب؟? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\\n\\\\nimage = Image.open('aerial_photo.jpg')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-ade-640-640', 'api_call': \\\"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\\n\\\\nimage = Image.open('aerial_photo.jpg')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20K', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]يوفر مشروع توليد النصوص لمعالجة اللغة الطبيعية أداة جديدة لتوليد النصوص بشكل تلقائي باستخدام تقنيات متقدمة. قم بتخصيص النماذج والبيانات الخاصة بك للحصول على نتائج أكثر دقة وتنوعًا في التوليد.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary libraries, including the transformers package and its 'AutoModelForCausalLM' and 'AutoTokenizer' classes, as well as the 'set_seed' function for reproducibility.\\\", 'code': 'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\n\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'facebook/opt-66b\\\\', torch_dtype=torch.float16)\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/opt-66b\\\\', use_fast=False)\\\\n\\\\nprompt = \\\"Once upon a time in a small village,\\\"\\\\ninput_ids = tokenizer(prompt, return_tensors=\\\\'pt\\\\').input_ids\\\\nset_seed(42)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=50)\\\\n\\\\nstory_prompts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\\\\nprint(story_prompts)', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Generation', 'api_name': 'facebook/opt-66b', 'api_call': 'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\n\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'facebook/opt-66b\\\\', torch_dtype=torch.float16)\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/opt-66b\\\\', use_fast=False)\\\\n\\\\nprompt = \\\"Once upon a time in a small village,\\\"\\\\ninput_ids = tokenizer(prompt, return_tensors=\\\\'pt\\\\').input_ids\\\\nset_seed(42)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=50)\\\\n\\\\nstory_prompts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\\\\nprint(story_prompts)', 'api_arguments': ['input_ids', 'do_sample', 'num_return_sequences', 'max_length'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-66b, torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-66b, use_fast=False)\\\\nprompt = Hello, I am conscious and\\\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\\\nset_seed(32)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\\\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)', 'performance': {'dataset': 'GPT-3', 'accuracy': 'roughly matched'}, 'description': 'OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. OPT models are trained to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطوير نص تلقائي لمحتوى اللغة الطبيعية لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary components from the Hugging Face Transformers library, such as BartForConditionalGeneration for the text summarization model.\\\\n2. Load the pre-trained model using the BartForConditionalGeneration.from_pretrained method. The 'sshleifer/distilbart-cnn-12-6' checkpoint is trained on the cnn_dailymail dataset and provides a suitable summarization model for summarizing news articles.\\\\n3. Create an instance of the model, and for a given news article text, generate a short summary.\\\\n4. The generated summary can be used to represent the main points of the news article to readers.\\\", '<<<code>>>': 'from transformers import BartForConditionalGeneration, BartTokenizer\\\\nmodel = BartForConditionalGeneration.from_pretrained(\\\\'sshleifer/distilbart-cnn-12-6\\\\')\\\\ntokenizer = BartTokenizer.from_pretrained(\\\\'sshleifer/distilbart-cnn-12-6\\\\')\\\\narticle = \\\"The news article text goes here...\\\"\\\\ninput_tokens = tokenizer(article, return_tensors=\\\\'pt\\\\')\\\\nsummary_output = model.generate(**input_tokens)\\\\nsummary = tokenizer.decode(summary_output[0], skip_special_tokens=True)', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sshleifer/distilbart-cnn-12-6', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\\\", 'api_arguments': '', 'python_environment_requirements': ['huggingface/transformers'], 'example_code': '', 'performance': {'dataset': [{'name': 'cnn_dailymail', 'accuracy': {'Rouge 2': '22.12', 'Rouge-L': '36.99'}}]}, 'description': \\\"DistilBART is a distilled version of BART, a model for text summarization. This specific checkpoint, 'sshleifer/distilbart-cnn-12-6', is trained on the cnn_dailymail dataset and provides a fast and effective way to generate summaries of text. The model can be loaded using the Hugging Face Transformers library.\\\" </s>\"\n"}
{"text": "<s>[INST]لدينا مجموعة طلابية تلتقي كل أسبوعين لتحسين مهارات توليد النصوص في مجال معالجة اللغات الطبيعية. قدم لهم برنامجاً يستخدم تقنيات التعلم الآلي لتوليد نصوص فعالة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('conversational', model='PygmalionAI/pygmalion-350m')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class 'pipeline' from the transformers package.\\\\n2. We then use the 'pipeline' function to create a conversational pipeline with the 'PygmalionAI/pygmalion-350m' model.\\\\n3. With this conversational pipeline, we can initiate a conversation with the prompt as the input and receive generated responses.\\\\n4. The assistant can provide supporting information to various discussions based on the user's input.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nconversational_pipeline = pipeline(\\\\'conversational\\\\', model=\\\\'PygmalionAI/pygmalion-350m\\\\')\\\\nuser_input = \\\"What\\\\'s the primary source of renewable energy?\\\"\\\\ngenerated_response = conversational_pipeline([user_input])\\\\nprint(generated_response)', 'api_call': \\\"pipeline('conversational', model='PygmalionAI/pygmalion-350m')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pygmalion-350m', 'api_call': \\\"pipeline('conversational', model='PygmalionAI/pygmalion-350m')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['transformers'], 'example_code': 'N/A', 'performance': {'dataset': 'The Pile', 'accuracy': 'N/A'}, 'description': \\\"This is a proof-of-concept fine-tune of Facebook's OPT-350M model optimized for dialogue, to be used as a stepping stone to higher parameter models. Disclaimer: NSFW data was included in the fine-tuning of this model. Although SFW inputs will usually result in SFW outputs, you are advised to chat at your own risk. This model is not suitable for use by minors.\\\" </s>\"\n"}
{"text": "<s>[INST]يقوم فريقنا بتطوير نماذج لغوية متقدمة لتحسين جودة الترجمة الآلية، ونحتاج إلى استخدام تقنيات الذكاء الاصطناعي وتعلم الآلة لتحسين كفاءة ترجمة النصوص بدقة وفعالية عالية.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\\\", '<<<api_provider>>>': 'PyTorch Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library by Hugging Face.\\\\n2. Use the pipeline function to create an English to French translation model.\\\\n3. Specify the model 'facebook/nllb-200-distilled-600M' to be loaded. This model has been trained to translate text between 200 languages.\\\\n4. Provide the input text to the translator function, which is the English product description, and it will return the translation in French.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_en_to_fr\\\\', model=\\\\'facebook/nllb-200-distilled-600M\\\\')\\\\ninput_text = \\\"Our state-of-the-art smartwatch offers numerous features, including GPS tracking, heart rate monitoring, sleep tracking, and smartphone notifications. With a sleek design and customizable watch faces, it is perfect for any occasion.\\\"\\\\ntranslation = translator(input_text)[0][\\\\'translation_text\\\\']', 'api_call': \\\"pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\\\", 'provider': 'PyTorch Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/nllb-200-distilled-600M', 'api_call': \\\"pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\\\", 'api_arguments': ['model', 'text'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline; translator = pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M'); translator('Hello World')\\\", 'performance': {'dataset': 'Flores-200', 'accuracy': 'BLEU, spBLEU, chrF++'}, 'description': 'NLLB-200 is a machine translation model primarily intended for research in machine translation, especially for low-resource languages. It allows for single sentence translation among 200 languages. The model was trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation.' </s>\"\n"}
{"text": "<s>[INST]قم ببناء نموذج لتعبئة الفجوات في معالجة اللغة الطبيعية لتحليل النصوص باستخدام تقنيات \\\"Fill-Mask\\\" باللغة العربية وتحسين تفاعل الحوار مع المستخدم.? [/INST]{'domain': 'Natural Language Processing Fill-Mask', 'api_call': \\\"pipeline('fill-mask', model='bert-large-uncased')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': '', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Transformers', 'functionality': 'Masked Language Modeling and Next Sentence Prediction', 'api_name': 'bert-large-uncased', 'api_call': '', 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='bert-large-uncased')\\\\nunmasker(Hello I'm a [MASK] model.)\\\", 'performance': {'dataset': {'SQUAD 1.1 F1/EM': '91.0/84.3', 'Multi NLI Accuracy': '86.05'}}, 'description': 'BERT large model (uncased) is a transformer model pretrained on a large corpus of English data using a masked language modeling (MLM) objective. It has 24 layers, 1024 hidden dimensions, 16 attention heads, and 336M parameters. The model is intended to be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نماذج الذكاء الاصطناعي لتحسين تحديد وتصنيف الصور، ونحتاج إلى تقدير دقة تصنيف الأشياء في الصور التي نحللها.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, such as ConvNextFeatureExtractor, ConvNextForImageClassification from transformers, torch, and datasets.\\\\n2. Load an image dataset containing living room images. In your case, you can load the image of your living room.\\\\n3. The pre-trained model 'facebook/convnext-tiny-224' is made for image classification task. Before using it, you need to preprocess the image with the ConvNextFeatureExtractor.\\\\n4. Use the preprocessed image to predict the logits using the ConvNextForImageClassification model.\\\\n5. Select the index of the highest logit value, which corresponds to the predicted label.\\\\n6. Print the predicted label.\\\", '<<<code>>>': \\\"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\\\nimport torch\\\\n# Replace the image file path with the path of your living room image\\\\nimage = 'path/to/your/image/file'\\\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\\\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\\\ninputs = feature_extractor(image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/convnext-tiny-224', 'api_call': \\\"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-tiny-224'}, 'python_environment_requirements': [['transformers', 'torch', 'datasets']], 'example_code': \\\"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset('huggingface/cats-image')\\\\nimage = dataset['test']['image'][0]\\\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\\\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\\\ninputs = feature_extractor(image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نموذج في مجال معالجة اللغات الطبيعية لترجمة النصوص، حيث نستخدم بعض مجموعات البيانات المتاحة للتنبؤ بالتوظيف في هذا المجال.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. First, import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create a translation pipeline with model \\\"Helsinki-NLP/opus-mt-en-ru\\\" to translate text from English to Russian.\\\\n3. Pass the English promotional content into the translation pipeline to obtain the Russian translation.\\\\n4. The translated Russian text can then be edited if necessary, and it can be used in your marketing materials targeting Russian audiences.', '<<<code>>>': 'from transformers import pipeline\\\\ntranslation = pipeline(\\\\'translation_en_to_ru\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-ru\\\\')\\\\nenglish_promo_content = \\\"Your English promotional content here...\\\"\\\\nrussian_translation = translation(english_promo_content)\\\\nrussian_text = russian_translation[0][\\\\'translation_text\\\\']', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-en-ru', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers', 'torch', 'tensorflow']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ru', model='Helsinki-NLP/opus-mt-en-ru')\\\\ntranslated_text = translation('Hello, how are you?')\\\", 'performance': {'dataset': 'newstest2019-enru', 'accuracy': '27.1'}, 'description': 'Helsinki-NLP/opus-mt-en-ru is a translation model trained on the OPUS dataset, which translates English text to Russian. It is based on the Marian NMT framework and can be used with Hugging Face Transformers.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد نص فيديو من شرح نصي لمشهد يصور الحيوانات في البرية باستخدام تقنيات رؤية الحواسيب لاكتشاف الكائنات. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first install the required packages 'ultralyticsplus' and 'ultralytics' from the package manager.\\\\n2. Then, we import the YOLO class from the ultralyticsplus package.\\\\n3. We load the model 'keremberke/yolov8s-csgo-player-detection' by creating an instance of the YOLO class.\\\\n4. We update the model's parameters such as confidence threshold (conf), Intersection over Union threshold (iou), agnostic_nms, and maximum detections (max_det).\\\\n5. We pass an image URL from the game Counter-Strike: Global Offensive (CS:GO) to the model for prediction.\\\\n6. The model will detect the players in the image and return the detected objects with their bounding boxes.\\\\n7. Finally, we render the result on the original image and display it with the bounding boxes drawn around the detected players.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://example.com/csgo_image.jpg'\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \\\"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\\\" </s>\"\n"}
{"text": "<s>[INST]أنشئ سؤال لغوي يتعلق بتصنيف النص بدون تصنيف في معالجة اللغة الطبيعية والإجابة عنه.? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"pipeline('zero-shot-classification', model='BaptisteDoyen/camembert-base-xnli')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\", 'code': \\\"from transformers import pipeline\\\\nclassifier = pipeline('zero-shot-classification', model='BaptisteDoyen/camembert-base-xnli')\\\\nfrench_article_text = 'Sample French article text...'\\\\nthemes = ['sports', 'politique', 'science', 'culture', 'économie']\\\\n# Candidate themes for articles\\\\nresult = classifier(french_article_text, themes)\\\", 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Classification', 'api_name': 'BaptisteDoyen/camembert-base-xnli', 'api_call': \\\"from transformers import pipeline\\\\nclassifier = pipeline('zero-shot-classification', model='BaptisteDoyen/camembert-base-xnli')\\\\nfrench_article_text = 'Sample French article text...'\\\\nthemes = ['sports', 'politique', 'science', 'culture', 'économie']\\\\n# Candidate themes for articles\\\\nresult = classifier(french_article_text, themes)\\\", 'api_arguments': {'sequence': 'str', 'candidate_labels': 'List[str]', 'hypothesis_template': 'str'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"sequence = L'équipe de France joue aujourd'hui au Parc des Princes\\\\ncandidate_labels = [sport,politique,science]\\\\nhypothesis_template = Ce texte parle de {}.\\\\nclassifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)\\\", 'performance': {'dataset': 'xnli', 'accuracy': {'validation': 81.4, 'test': 81.7}}, 'description': 'Camembert-base model fine-tuned on french part of XNLI dataset. One of the few Zero-Shot classification models working on French.'}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لمعالجة اللغة الطبيعية والحوارات دون تدريب مسبق باستخدام تقنيات اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a text-generation pipeline with the model 'PygmalionAI/pygmalion-1.3b'.\\\\n3. This model is trained to generate conversational responses and can be used with a specific input format. The input format includes character persona, dialogue history, and user input message.\\\\n4. You can now use this text-generation pipeline to generate responses for your web app given a character persona and user input.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ndialogue_pipeline = pipeline(\\\\'text-generation\\\\', \\\\'PygmalionAI/pygmalion-1.3b\\\\')\\\\n\\\\ninput_prompt = \\\"\\\"\\\"\\\\n[CHARACTER]\\\\'s Persona: [A few sentences about the character you want the model to play]\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\\\\n\\\"\\\"\\\"\\\\n\\\\nresponse = dialogue_pipeline(input_prompt)', 'api_call': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pygmalion-1.3b', 'api_call': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", 'api_arguments': 'input_prompt', 'python_environment_requirements': [['transformers']], 'example_code': \\\"[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\\\", 'performance': {'dataset': '56MB of dialogue data', 'accuracy': 'Not provided'}, 'description': \\\"Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\\\" </s>\"\n"}
{"text": "<s>[INST]يجب على الباحثين في مجال معالجة اللغة الطبيعية ملء الفجوة أن يعرفوا كيفية تجهيز البيانات لنماذج اللغة الطبيعية للتصنيف دون الحاجة للتدريب على الشبكات العصبونية العميقة والتعلم التلقائي.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"DebertaModel.from_pretrained('microsoft/deberta-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create a fill-mask pipeline using the 'microsoft/deberta-base' model, which is an advanced pre-trained transformer specifically designed for filling in textual masks.\\\\n3. Use the pipeline to fill in the masked word in the given sentence to get the most suitable replacement for the [MASK] token based on the context.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nfill_mask = pipeline(\\\\'fill-mask\\\\', model=\\\\'microsoft/deberta-base\\\\')\\\\nsentence_with_mask = \\\"In order to succeed, one must have [MASK].\\\"\\\\ncompleted_sentence = fill_mask(sentence_with_mask)', 'api_call': \\\"DebertaModel.from_pretrained('microsoft/deberta-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/deberta-base', 'api_call': \\\"DebertaModel.from_pretrained('microsoft/deberta-base')\\\", 'api_arguments': 'text', 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline\\\\nfill_mask = pipeline('fill-mask', model='microsoft/deberta-base')\\\\nfill_mask('The capital of France is [MASK].')\\\", 'performance': {'dataset': {'SQuAD 1.1': '93.1/87.2', 'SQuAD 2.0': '86.2/83.1', 'MNLI-m': '88.8'}}, 'description': 'DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data.' </s>\"\n"}
{"text": "<s>[INST]لإنشاء نموذج لتصنيف الصوتيات في مجال تصنيف الصوتيات، ابدأ بتجميع بيانات التدريب ثم قم بتقسيم الصوتيات إلى فئات مختلفة. بعد ذلك، قم بتدريب نموذج تصنيف الصوتيات باستخدام تقنيات معالجة الصوت. يمكنك اختبار أداء النموذج باستخدام مجموعة بيانات التحقق، وتحسين دقته بضبط المعلمات. وأخيرًا، يمكنك استخدام النموذج لتصنيف صوتيات جديدة في مجال تصنيف الصوتيات.? [/INST]{'domain': 'Audio Audio Classification', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. First, we'll import the necessary classes from the transformers library: Wav2Vec2ForCTC and Wav2Vec2Processor.\\\", 'code': 'from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\\\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\\\')\\\\ntokenizer = Wav2Vec2Processor.from_pretrained(\\\\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\\\')\\\\n# Replace \\\\'audio_path.wav\\\\' with path to your speech sample\\\\ninput_audio = \\\"audio_path.wav\\\"\\\\ninput_values = tokenizer(file_audio_path, return_tensors=\\\"pt\\\").input_values\\\\nlogits = model(input_values).logits\\\\nprobabilities = torch.softmax(logits, dim=-1)\\\\nemotions = [\\\\'angry\\\\', \\\\'calm\\\\', \\\\'disgust\\\\', \\\\'fearful\\\\', \\\\'happy\\\\', \\\\'neutral\\\\', \\\\'sad\\\\', \\\\'surprised\\\\']\\\\nemotion_probs = {emotion: probability for emotion, probability in zip(emotions, probabilities.tolist()[0])}', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Speech Emotion Recognition', 'api_name': 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition', 'api_call': 'from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\\\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\\\')\\\\ntokenizer = Wav2Vec2Processor.from_pretrained(\\\\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\\\')\\\\n# Replace \\\\'audio_path.wav\\\\' with path to your speech sample\\\\ninput_audio = \\\"audio_path.wav\\\"\\\\ninput_values = tokenizer(file_audio_path, return_tensors=\\\"pt\\\").input_values\\\\nlogits = model(input_values).logits\\\\nprobabilities = torch.softmax(logits, dim=-1)\\\\nemotions = [\\\\'angry\\\\', \\\\'calm\\\\', \\\\'disgust\\\\', \\\\'fearful\\\\', \\\\'happy\\\\', \\\\'neutral\\\\', \\\\'sad\\\\', \\\\'surprised\\\\']\\\\nemotion_probs = {emotion: probability for emotion, probability in zip(emotions, probabilities.tolist()[0])}', 'api_arguments': 'wav2vec2, tokenizer', 'python_environment_requirements': 'transformers 4.8.2, pytorch 1.9.0+cu102, datasets 1.9.0, tokenizers 0.10.3', 'example_code': 'from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor', 'performance': {'dataset': 'RAVDESS', 'accuracy': 0.8223}, 'description': \\\"The model is a fine-tuned version of jonatasgrosman/wav2vec2-large-xlsr-53-english for a Speech Emotion Recognition (SER) task. The dataset used to fine-tune the original pre-trained model is the RAVDESS dataset. This dataset provides 1440 samples of recordings from actors performing on 8 different emotions in English, which are: emotions = ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised'].\\\"}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لمعالجة اللغة الطبيعية لتوليد النصوص.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"AutoTokenizer.from_pretrained('Salesforce/codegen-350M-multi')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Salesforce/codegen-350M-multi\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'Salesforce/codegen-350M-multi\\\\')\\\\ntext = \\\"Create a function that returns the square of a number.\\\"\\\\ninput_ids = tokenizer(text, return_tensors=\\\\'pt\\\\').input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\ngenerated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Program Synthesis', 'api_name': 'Salesforce/codegen-350M-multi', 'api_call': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Salesforce/codegen-350M-multi\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'Salesforce/codegen-350M-multi\\\\')\\\\ntext = \\\"Create a function that returns the square of a number.\\\"\\\\ninput_ids = tokenizer(text, return_tensors=\\\\'pt\\\\').input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\ngenerated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval and MTPB', 'accuracy': 'Refer to the paper for accuracy details'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.'}} </s>\"\n"}
{"text": "<s>[INST]عند تقديم الدورات التعليمية حول تقنيات فصل الصور، يُنصح بتنظيم ورش عمل دورية للطلاب لتطبيق ما تعلموه. يمكن توفير أدوات برمجية لتسهيل عملية تقسيم الصور وتسهيل التفاعل مع نتائج الفصل.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes and functions from the ultralyticsplus package. This includes the YOLO class, and the render_result function.\\\\n2. Create a YOLO model using the specific PCB defect segmentation model, 'keremberke/yolov8s-pcb-defect-segmentation'.\\\\n3. Set model override options, such as confidence threshold, IoU threshold, agnostic NMS, and max_det.\\\\n4. Load the image of the PCB. This can be either a URL or a local path to the image.\\\\n5. Use the model to predict the defects and their locations in the image.\\\\n6. Render the results on the original image with the detected defects and locations.\\\\n7. Print the results including bounding boxes and masks.\\\\n8. Finally, show or save the rendered image with the highlighted PCB defects.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path_to_pcb_image.jpg'\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8s-pcb-defect-segmentation', 'api_call': \\\"YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\n\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.515, 'mAP@0.5(mask)': 0.491}}, 'description': 'YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لتوليد الصور من النصوص متعددة الوسائط باستخدام تكنولوجيا معالجة اللغة الطبيعية والصور.? [/INST]{'domain': 'Multimodal Text-to-Image Generation', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler), torch_dtype=torch.float16)\\\", 'api_provider': 'Hugging Face', 'explanation': '1. First, import the necessary classes from the diffusers and transformers libraries to work with the StableDiffusion model.', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = \\\\'stabilityai/stable-diffusion-2\\\\'\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\\\\'scheduler\\\\')\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(\\\\'cuda\\\\')\\\\nprompt = \\\\'A thrilling fantasy book cover for \\\"The Magical Journey\\\" featuring a majestic castle and a mysterious figure\\\\'\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(\\\\'book_cover.png\\\\')', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = \\\\'stabilityai/stable-diffusion-2\\\\'\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\\\\'scheduler\\\\')\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(\\\\'cuda\\\\')\\\\nprompt = \\\\'A thrilling fantasy book cover for \\\"The Magical Journey\\\" featuring a majestic castle and a mysterious figure\\\\'\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(\\\\'book_cover.png\\\\')', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.'}} </s>\"\n"}
{"text": "<s>[INST]نقوم ببناء منتج في مجال ترجمة معالجة اللغة الطبيعية للإجابة على الأسئلة. نود التنبؤ بالإجابات عن الأسئلة بناءً على بعض مجموعات البيانات المتاحة.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"pipeline('translation_en_to_ru', model='Helsinki-NLP/opus-mt-en-ru')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. First, import the necessary libraries, which are 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\ntranslation = pipeline(\\\\'translation_en_to_ru\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-ru\\\\')\\\\n# Example: Translate English text to Russian\\\\nenglish_text = \\\"Hello, how are you?\\\"\\\\nrussian_translation = translation(english_text)[0][\\\\'translation_text\\\\']\\\\n# Process the translated text in your chatbot and generate a response\\\\nenglish_response = \\\"I\\\\'m good. How can I help you?\\\"\\\\n# Translate the English response back to Russian\\\\nrussian_response = translation(english_response)[0][\\\\'translation_text\\\\']', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Translation', 'api_name': 'opus-mt-en-ru', 'api_call': 'from transformers import pipeline\\\\ntranslation = pipeline(\\\\'translation_en_to_ru\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-ru\\\\')\\\\n# Example: Translate English text to Russian\\\\nenglish_text = \\\"Hello, how are you?\\\"\\\\nrussian_translation = translation(english_text)[0][\\\\'translation_text\\\\']\\\\n# Process the translated text in your chatbot and generate a response\\\\nenglish_response = \\\"I\\\\'m good. How can I help you?\\\"\\\\n# Translate the English response back to Russian\\\\nrussian_response = translation(english_response)[0][\\\\'translation_text\\\\']', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers', 'torch', 'tensorflow'], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ru', model='Helsinki-NLP/opus-mt-en-ru')\\\\ntranslated_text = translation('Hello, how are you?')\\\", 'performance': {'dataset': 'newstest2019-enru', 'accuracy': '27.1'}, 'description': 'Helsinki-NLP/opus-mt-en-ru is a translation model trained on the OPUS dataset, which translates English text to Russian. It is based on the Marian NMT framework and can be used with Hugging Face Transformers.'}} </s>\"\n"}
{"text": "<s>[INST]تعمل شركة تطوير عميلنا على تطوير تطبيق يستهدف ذوي الإعاقة البصرية في مجال تصنيف الصور بواسطة الرؤية الحاسوبية. نحتاج إلى برنامج يمكنه إنشاء صور بدون قيود بواسطة الرؤية الحاسوبية لتحسين تجربة المستخدمين في هذا التطبيق.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('google/ddpm-cat-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include DDPMPipeline from diffusers.\\\\n2. Use the DDPMPipeline.from_pretrained() function to load the 'google/ddpm-cat-256' model.\\\\n3. The loaded model will be used to generate random unconditional cat images.\\\\n4. You can use this model to generate a cat image and save it using the .save() method.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\nddpm = DDPMPipeline.from_pretrained('google/ddpm-cat-256')\\\\ngenerated_image = ddpm().images[0]\\\\ngenerated_image.save('cat_image.png')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-cat-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/ddpm-cat-256', 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-cat-256')\\\", 'api_arguments': ['model_id'], 'python_environment_requirements': [['diffusers']], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-cat-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.' </s>\"\n"}
{"text": "<s>[INST]يرجى إنشاء فيديو من وصف نصي لمشهد يصور الحيوانات في البرية باستخدام تقنيات معالجة اللغة الطبيعية. هل يمكنك مساعدتي في هذا؟? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='bert-base-multilingual-cased')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We first import the pipeline function from the transformers package.\\\\n2. We configure the pipeline function for the masked language modeling task using the 'fill-mask' parameter and set the model to 'bert-base-multilingual-cased'.\\\\n3. We can then use this pipeline to provide a sentence with a missing word, represented by [MASK]. The model will predict the most likely word to fill in the gap based on the context of the sentence.\\\\n4. This feature can be integrated into our language learning app, allowing users to practice filling in missing words in sentences in a variety of languages.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'bert-base-multilingual-cased\\\\')\\\\nsentence = \\\"Hello, I\\\\'m a [MASK] model.\\\"\\\\nprediction = unmasker(sentence)', 'api_call': \\\"pipeline('fill-mask', model='bert-base-multilingual-cased')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-base-multilingual-cased', 'api_call': \\\"pipeline('fill-mask', model='bert-base-multilingual-cased')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='bert-base-multilingual-cased')\\\\nunmasker(Hello I'm a [MASK] model.)\\\", 'performance': {'dataset': 'wikipedia', 'accuracy': 'Not provided'}, 'description': 'BERT multilingual base model (cased) is pretrained on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective. The model is case sensitive and can be used for masked language modeling or next sentence prediction. It is intended to be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering.' </s>\"\n"}
{"text": "<s>[INST]نقوم بإنشاء نموذج يستخدم Computer Vision لتصنيف الصور بدون تصنيف سابق، ونريد توقع فئة الصورة بناءً على بيانات التدريب المتاحة.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. To achieve this, you can use transformers from the Hugging Face library. First, import the necessary library 'pipeline'.\\\\n2. Run the 'pipeline()' function with the 'image-classification' task and the model 'laion/CLIP-convnex_base_w-laion_aesthetic-s13B-b82K' to configure the zero-shot image classifier.\\\\n3. Use the model to classify your input image by providing the path to the image and a list of possible class labels. The list of labels provided in the example includes 'car', 'tree', 'cat', and 'building'. Change the label list according to your requirement.\\\\n4. The classifier returns the predicted class along with the confidence score.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nclassifier = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K')\\\\nresult = classifier('path/to/image.jpg', ['car', 'tree', 'cat', 'building'])\\\", 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K', 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K')\\\", 'api_arguments': {'image': 'path to image file', 'class_names': 'list of possible class names (comma-separated)'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; model = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K'); model('path/to/image.jpg', ['cat', 'dog'])\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '70.8% to 71.7%'}, 'description': 'A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. These models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. They can be used for zero-shot image classification, image and text retrieval, and other tasks.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطبيق تقنيات معالجة اللغات الطبيعية لتحسين مهارات تلخيص النصوص. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include BartForConditionalGeneration and BartTokenizer from transformers.\\\\n2. Load the 'sshleifer/distilbart-cnn-6-6' model using the.from_pretrained method of the BartForConditionalGeneration class. This model has been trained for text summarization tasks on the CNN/Daily Mail and XSum datasets.\\\\n3. Load the tokenizer for the same model.\\\\n4. Use the tokenizer to encode the input text, and then feed the encoded inputs to the model to generate summarized text.\\\\n5. Use the tokenizer to decode the model output to obtain the final summary.\\\", '<<<code>>>': 'from transformers import BartForConditionalGeneration, BartTokenizer\\\\nmodel = BartForConditionalGeneration.from_pretrained(\\\\'sshleifer/distilbart-cnn-6-6\\\\')\\\\ntokenizer = BartTokenizer.from_pretrained(\\\\'sshleifer/distilbart-cnn-6-6\\\\')\\\\narticle = \\\"Insert the article text here.\\\"\\\\ninputs = tokenizer(article, return_tensors=\\\\'pt\\\\')\\\\nsummary_ids = model.generate(inputs.input_ids)\\\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sshleifer/distilbart-cnn-6-6', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': {'cnn_dailymail': {'Rouge 2': 20.17, 'Rouge-L': 29.7}, 'xsum': {'Rouge 2': 20.92, 'Rouge-L': 35.73}}}, 'description': 'DistilBART model for text summarization, trained on the CNN/Daily Mail and XSum datasets. It is a smaller and faster version of BART, suitable for summarizing English text.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى تدريب نماذج توليد النصوص إلى الفيديو على إنشاء مقاطع فيديو مبتكرة ومتنوعة لتحسين الأداء العام لنظامنا التقني وضمان جودة النتائج.? [/INST]{'domain': 'Multimodal Text-to-Video', 'api_call': \\\"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b-legacy', torch_dtype=torch.float16)\\\", 'api_provider': 'Hugging Face', 'explanation': '1. Import the necessary libraries, such as torch and DiffusionPipeline from diffusers.', 'code': 'import torch\\\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\\\nfrom diffusers.utils import export_to_video\\\\npipe = DiffusionPipeline.from_pretrained(\\\\'damo-vilab/text-to-video-ms-1.7b-legacy\\\\', torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\nprompt = \\\"Spiderman is surfing\\\"\\\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\\\nvideo_path = export_to_video(video_frames)', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b-legacy', 'api_call': 'import torch\\\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\\\nfrom diffusers.utils import export_to_video\\\\npipe = DiffusionPipeline.from_pretrained(\\\\'damo-vilab/text-to-video-ms-1.7b-legacy\\\\', torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\nprompt = \\\"Spiderman is surfing\\\"\\\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\\\nvideo_path = export_to_video(video_frames)', 'api_arguments': ['prompt', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': 'import torch\\\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\\\nfrom diffusers.utils import export_to_video\\\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b-legacy, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\nprompt = Spiderman is surfing\\\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': ['LAION5B', 'ImageNet', 'Webvid'], 'accuracy': 'Not provided'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.'}} </s>\"\n"}
{"text": "<s>[INST]يرجى كتابة عبارات نصية تتعلق بالمعالجة اللغوية الطبيعية وتشبه في الهيكل والمعنى، ومن ثم توليد جملة أو فقرة نصية تكون مشابهة لتلك العبارات. يرجى توضيح الخطوات التي يتعين اتباعها لتحقيق ذلك.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/all-distilroberta-v1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the SentenceTransformer class from the sentence_transformers library.\\\\n2. Instantiate the SentenceTransformer model with the pre-trained model 'sentence-transformers/all-distilroberta-v1'.\\\\n3. Encode the search query and documents as vectors using the model's 'encode()' method.\\\\n4. Calculate the similarity between the query and document vectors using a similarity measure, like cosine similarity.\\\\n5. Rank the documents based on their similarity scores and return the most relevant documents.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\n\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-distilroberta-v1\\\\')\\\\n\\\\nquery = \\\"search query\\\"\\\\ndocuments = [\\\"document1\\\", \\\"document2\\\", \\\"document3\\\"]\\\\n\\\\nquery_embedding = model.encode([query])\\\\ndoc_embeddings = model.encode(documents)\\\\n\\\\nranking = cosine_similarity(query_embedding, doc_embeddings)[0]\\\\nranking_sorted = sorted(range(len(ranking)), key=lambda i: ranking[i], reverse=True)\\\\n\\\\nmost_relevant = [documents[i] for i in ranking_sorted]', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-distilroberta-v1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/all-distilroberta-v1', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-distilroberta-v1')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': [{'name': 's2orc', 'accuracy': 'Not provided'}, {'name': 'MS Marco', 'accuracy': 'Not provided'}, {'name': 'yahoo_answers_topics', 'accuracy': 'Not provided'}]}, 'description': 'This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا ببناء نموذج يجمع بين الصور المرئية والأسئلة للرد عليها، ونريد تطوير نظام يمكنه الإجابة على الأسئلة المتعلقة بالصور بشكل دقيق وفعال.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary libraries from the transformers package. These include BlipProcessor and BlipForQuestionAnswering, which will be used for image question answering.\\\\n2. We then load the pre-trained model and processor with the from_pretrained method using the model name 'Salesforce/blip-vqa-capfilt-large'.\\\\n3. With the model and the processor in place, we can now process the raw image and a user-provided question.\\\\n4. To process the image and questions, we use the processor with the raw_image and the question text to create input tensors.\\\\n5. These input tensors are then passed to the model's generate method which outputs the answer to the question.\\\\n6. Finally, we decode and output the generated answer with the processor's decode method, which returns a user-friendly answer.\\\", '<<<code>>>': \\\"from PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\\\nprocessor = BlipProcessor.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\\nmodel = BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\\nraw_image = Image.open(image_path).convert('RGB')\\\\nquestion = user_provided_question\\\\ninputs = processor(raw_image, question, return_tensors='pt')\\\\noutput = model.generate(**inputs)\\\\nanswer = processor.decode(output[0], skip_special_tokens=True)\\\", 'api_call': \\\"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Salesforce/blip-vqa-capfilt-large', 'api_call': \\\"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\", 'api_arguments': {'raw_image': 'RGB image', 'question': 'string'}, 'python_environment_requirements': [{'transformers': 'BlipProcessor, BlipForQuestionAnswering'}], 'example_code': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\\\nquestion = how many dogs are in the picture?\\\\ninputs = processor(raw_image, question, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\\", 'performance': {'dataset': 'VQA', 'accuracy': '+1.6% in VQA score'}, 'description': 'BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.' </s>\"\n"}
{"text": "<s>[INST]قم بتوليد تعليمة تفصيلية لمعالجة النصوص في توليد النصوص باستخدام تقنيات معالجة اللغة الطبيعية، حيث يجب أن يكون النص المولد مناسبًا ومتقنًا بناء على سياق النص الأصلي.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Qiliang/bart-large-cnn-samsum-ChatGPT_v3')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary libraries, which are 'AutoModelForSeq2SeqLM' and 'AutoTokenizer' from transformers.\\\\n2. Use the 'from_pretrained' method of the 'AutoModelForSeq2SeqLM' class to load the model, which in this case is 'Qiliang/bart-large-cnn-samsum-ChatGPT_v3'. This model is a fine-tuned version for text-to-text generation tasks.\\\\n3. Initialize the tokenizer with the same model name using 'AutoTokenizer.from_pretrained()'.\\\\n4. The loaded model will be used to generate responses for customer support inquiries through a conversational interface.\\\", '<<<code>>>': 'from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\\\n\\\\nmodel_name = \\\\'Qiliang/bart-large-cnn-samsum-ChatGPT_v3\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\n\\\\ndef generate_response(input_text):\\\\n    inputs = tokenizer.encode(input_text, return_tensors=\\\\'pt\\\\')\\\\n    outputs = model.generate(inputs, max_length=150, num_return_sequences=1)\\\\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n\\\\n# Example usage:\\\\n# input_text = \\\"My internet is not working properly. Can you help me?\\\"\\\\n# response = generate_response(input_text)\\\\n# print(response)', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Qiliang/bart-large-cnn-samsum-ChatGPT_v3')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bart-large-cnn-samsum-ChatGPT_v3', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Qiliang/bart-large-cnn-samsum-ChatGPT_v3')\\\", 'api_arguments': ['input_text'], 'python_environment_requirements': [['transformers==4.24.0', 'torch==1.12.1', 'datasets==2.6.1', 'tokenizers==0.13.2']], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'This model is a fine-tuned version of philschmid/bart-large-cnn-samsum on an unknown dataset.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام تجاوب أسئلة الجداول باستخدام معالجة اللغة الطبيعية، ونحتاج إلى تقدير عمق الأشياء في الجداول التي نتعامل معها.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the required libraries: AutoModelForTableQuestionAnswering, AutoTokenizer, and pipeline from transformers.\\\\n2. Load the TAPAS model and tokenizer using from_pretrained method, specifying 'navteca/tapas-large-finetuned-wtq' as the pre-trained model.\\\\n3. Create a table-question-answering pipeline using the loaded model and tokenizer.\\\\n4. Use the pipeline to get the desired answer by providing the table data and a question/query about the table.\\\\n5. The result will contain the required information (in this case, the number of stars for a specific repository).\\\", '<<<code>>>': 'from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\\\\n\\\\ntapas_model = AutoModelForTableQuestionAnswering.from_pretrained(\\\\'navteca/tapas-large-finetuned-wtq\\\\')\\\\ntapas_tokenizer = AutoTokenizer.from_pretrained(\\\\'navteca/tapas-large-finetuned-wtq\\\\')\\\\n\\\\nnlp = pipeline(\\\\'table-question-answering\\\\', model=tapas_model, tokenizer=tapas_tokenizer)\\\\n\\\\ntable_data = {\\\\n\\\"Repository\\\": [\\\"Transformers\\\", \\\"Datasets\\\", \\\"Tokenizers\\\"],\\\\n\\\"Stars\\\": [\\\"36542\\\", \\\"4512\\\", \\\"3934\\\"],\\\\n\\\"Contributors\\\": [\\\"651\\\", \\\"77\\\", \\\"34\\\"],\\\\n\\\"Programming language\\\": [\\\"Python\\\", \\\"Python\\\", \\\"Rust, Python and NodeJS\\\"]\\\\n}\\\\nquery = \\\"How many stars does the transformers repository have?\\\"\\\\n\\\\nresult = nlp({\\\\'table\\\\': table_data, \\\\'query\\\\': query})\\\\nprint(result)', 'api_call': \\\"AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'navteca/tapas-large-finetuned-wtq', 'api_call': \\\"AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\", 'api_arguments': {'table': 'table_data', 'query': 'query'}, 'python_environment_requirements': [{'transformers': 'AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline'}], 'example_code': \\\"from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\\\\n\\\\n# Load model & tokenizer\\\\ntapas_model = AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\\ntapas_tokenizer = AutoTokenizer.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\\n\\\\n# Get predictions\\\\nnlp = pipeline('table-question-answering', model=tapas_model, tokenizer=tapas_tokenizer)\\\\nresult = nlp({'table': {'Repository': ['Transformers', 'Datasets', 'Tokenizers'], 'Stars': ['36542', '4512', '3934'], 'Contributors': ['651', '77', '34'], 'Programming language': ['Python', 'Python', 'Rust, Python and NodeJS']}, 'query': 'How many stars does the transformers repository have?'})\\\\nprint(result)\\\", 'performance': {'dataset': 'wikisql', 'accuracy': 'Not provided'}, 'description': 'TAPAS large model fine-tuned on WikiTable Questions (WTQ). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table.' </s>\"\n"}
{"text": "<s>[INST]يجب عليك بناء نموذج لتوليد النصوص في مجال معالجة اللغة الطبيعية Text2Text Generation، بحيث يقوم النموذج بتوليد نصوص جديدة استنادًا إلى النصوص المتاحة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"Parrot(model_tag='prithivida/parrot_paraphraser_on_T5', use_gpu=False)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary package and the Parrot class from the module.\\\\n2. Initialize the Parrot object with the model using the 'prithivida/parrot_paraphraser_on_T5' model for paraphrasing text.\\\\n3. A list of phrases should be available, each of which represents a possible response to a customer query.\\\\n4. Iterate through the list of phrases and use the 'parrot.augment' function to generate paraphrasable variations of the text. This can provide more diverse responses to customer queries while preserving the meaning.\\\", '<<<code>>>': 'from parrot import Parrot\\\\n\\\\nparrot = Parrot(model_tag=\\\\'prithivida/parrot_paraphraser_on_T5\\\\', use_gpu=False)\\\\nphrases = [\\\\n    \\\"Sure, I can help you with that.\\\",\\\\n    \\\"We appreciate your feedback.\\\",\\\\n]\\\\n\\\\nfor phrase in phrases:\\\\n    para_phrases = parrot.augment(input_phrase=phrase)\\\\n    for para_phrase in para_phrases:\\\\n        print(para_phrase)', 'api_call': \\\"Parrot(model_tag='prithivida/parrot_paraphraser_on_T5', use_gpu=False)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'prithivida/parrot_paraphraser_on_T5', 'api_call': \\\"Parrot(model_tag='prithivida/parrot_paraphraser_on_T5', use_gpu=False)\\\", 'api_arguments': ['input_phrase', 'diversity_ranker', 'do_diverse', 'max_return_phrases', 'max_length', 'adequacy_threshold', 'fluency_threshold'], 'python_environment_requirements': [['torch', 'transformers']], 'example_code': 'from parrot import Parrot\\\\nimport torch\\\\nimport warnings\\\\nwarnings.filterwarnings(ignore)\\\\n\\\\nparrot = Parrot(model_tag=prithivida/parrot_paraphraser_on_T5, use_gpu=False)\\\\nphrases = [Can you recommed some upscale restaurants in Newyork?,\\\\n What are the famous places we should not miss in Russia?\\\\n]\\\\nfor phrase in phrases:\\\\n print(-*100)\\\\n print(Input_phrase: , phrase)\\\\n print(-*100)\\\\n para_phrases = parrot.augment(input_phrase=phrase)\\\\n for para_phrase in para_phrases:\\\\n  print(para_phrase)', 'performance': {'dataset': 'Not mentioned', 'accuracy': 'Not mentioned'}, 'description': 'Parrot is a paraphrase based utterance augmentation framework purpose built to accelerate training NLU models. It offers knobs to control Adequacy, Fluency, and Diversity as per your needs. It mainly focuses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بالبحث والتطوير في مجال رؤية الحاسوب وتحويل الصور إلى صور أخرى، نحن نرغب في إنشاء نموذج يمكنه تحويل الصور بدقة عالية، مع الالتزام بالطابع التلقائي في عملية تقسيم الصور.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8m-pcb-defect-segmentation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include YOLO and render_result from the ultralyticsplus package.\\\\n2. Call the 'YOLO' function from the ultralyticsplus package to load the pre-trained model 'keremberke/yolov8m-pcb-defect-segmentation'. This model can detect and segment defects in Printed Circuit Board (PCB) images.\\\\n3. Set some model configuration options, such as confidence score threshold, IOU threshold, etc.\\\\n4. Provide the path to the image of the PCB that you want to analyze for defects.\\\\n5. Use the 'predict' method of the model to get the detection and segmentation results of the defects in the PCB image.\\\\n6. Finally, use the 'render_result' function to visualize the results.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-pcb-defect-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'URL or local path to the PCB image'\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8m-pcb-defect-segmentation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8m-pcb-defect-segmentation', 'api_call': \\\"YOLO('keremberke/yolov8m-pcb-defect-segmentation')\\\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': [['ultralyticsplus==0.0.24', 'ultralytics==8.0.23']], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8m-pcb-defect-segmentation')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'print(results[0].masks)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.568, 'mAP@0.5(mask)': 0.557}}, 'description': 'A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير تقنيات معالجة اللغة الطبيعية للمساعدة في إنشاء نماذج محادثة ذكية، ونحتاج إلى تدريب النماذج على مجموعة كبيرة من البيانات لضمان دقة وفعالية عالية في التفاعل مع المستخدمين.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('ruDialoGpt3-medium-finetuned-telegram')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import AutoTokenizer and AutoModelForCausalLM from the transformers library by Hugging Face.\\\\n2. Use the pretrained model checkpoint 'ruDialoGpt3-medium-finetuned-telegram' to load both the tokenizer and the model.\\\\n3. The model is a DialoGPT trained on Russian language data and fine-tuned on a telegram chat dataset. It is designed for generating conversational responses in Russian.\\\\n4. Use the model and tokenizer to generate responses for a specific user input using the appropriate method.\\\", '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ncheckpoint = 'Kirili4ik/ruDialoGpt3-medium-finetuned-telegram'\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\\\nmodel.eval()\\\\ninput_text = 'привет, как дела?'\\\\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\\\\noutput_tokens = model.generate(input_ids, max_length=100, no_repeat_ngram_size=3, do_sample=True, top_k=50, top_p=0.9, temperature=0.6)\\\\nresponse = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\\\", 'api_call': \\\"AutoModelForCausalLM.from_pretrained('ruDialoGpt3-medium-finetuned-telegram')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ruDialoGpt3-medium-finetuned-telegram', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('ruDialoGpt3-medium-finetuned-telegram')\\\", 'api_arguments': {'num_return_sequences': 1, 'max_length': 512, 'no_repeat_ngram_size': 3, 'do_sample': True, 'top_k': 50, 'top_p': 0.9, 'temperature': 0.6, 'mask_token_id': 'tokenizer.mask_token_id', 'eos_token_id': 'tokenizer.eos_token_id', 'unk_token_id': 'tokenizer.unk_token_id', 'pad_token_id': 'tokenizer.pad_token_id', 'device': 'cpu'}, 'python_environment_requirements': [{'huggingface_transformers': 'AutoTokenizer, AutoModelForCausalLM', 'torch': 'zeros, cat'}], 'example_code': 'checkpoint = Kirili4ik/ruDialoGpt3-medium-finetuned-telegram\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\\\nmodel.eval()', 'performance': {'dataset': 'Russian forums and Telegram chat', 'accuracy': 'Not available'}, 'description': 'DialoGPT trained on Russian language and fine tuned on my telegram chat. This model was created by sberbank-ai and trained on Russian forums. It has been fine-tuned on a 30mb json file of exported telegram chat data.' </s>\"\n"}
{"text": "<s>[INST]لإنشاء نموذج تلخيص باستخدام معالجة اللغة الطبيعية، ابدأ بتجهيز الجمل والنصوص ثم استخدم تقنيات التعلم الآلي لتدريب النموذج. يمكنك اختبار أداء النموذج عن طريق تطبيقه على نصوص لم تستخدم في التدريب. لتحسين جودة التلخيص، يمكنك تطبيق تقنيات مثل تعزيز النموذج وظائف الخسارة متعددة المهام.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('plguillou/t5-base-fr-sum-cnndm')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the T5Tokenizer and T5ForConditionalGeneration from the transformers package.', 'code': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'plguillou/t5-base-fr-sum-cnndm\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'plguillou/t5-base-fr-sum-cnndm\\\\')\\\\ninput_text = \\\"summarize: Hier, une banque dans le centre de Paris a été la cible d\\\\'un braquage audacieux. Trois hommes armés ont pénétré dans la banque et ont pris plusieurs otages. La police est rapidement intervenue et a réussi à libérer tous les otages après une courte négociation avec les braqueurs. Les malfaiteurs ont été arrêtés et personne n\\\\'a été blessé lors de l\\\\'incident.\\\"\\\\ninput_tokens = tokenizer.encode(input_text, return_tensors=\\\\'pt\\\\')\\\\nsummary_output = model.generate(input_tokens, num_beams=4, max_length=30, early_stopping=True)\\\\nsummary_text = tokenizer.decode(summary_output[0], skip_special_tokens=True)', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Abstractive Text Summarization', 'api_name': 'plguillou/t5-base-fr-sum-cnndm', 'api_call': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'plguillou/t5-base-fr-sum-cnndm\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'plguillou/t5-base-fr-sum-cnndm\\\\')\\\\ninput_text = \\\"summarize: Hier, une banque dans le centre de Paris a été la cible d\\\\'un braquage audacieux. Trois hommes armés ont pénétré dans la banque et ont pris plusieurs otages. La police est rapidement intervenue et a réussi à libérer tous les otages après une courte négociation avec les braqueurs. Les malfaiteurs ont été arrêtés et personne n\\\\'a été blessé lors de l\\\\'incident.\\\"\\\\ninput_tokens = tokenizer.encode(input_text, return_tensors=\\\\'pt\\\\')\\\\nsummary_output = model.generate(input_tokens, num_beams=4, max_length=30, early_stopping=True)\\\\nsummary_text = tokenizer.decode(summary_output[0], skip_special_tokens=True)', 'api_arguments': {'input_text': 'summarize: ARTICLE'}, 'python_environment_requirements': {'transformers': 'from transformers import T5Tokenizer, T5ForConditionalGeneration'}, 'example_code': 'tokenizer = T5Tokenizer.from_pretrained(plguillou/t5-base-fr-sum-cnndm)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(plguillou/t5-base-fr-sum-cnndm)', 'performance': {'dataset': 'cnn_dailymail', 'ROUGE-1': 44.5252, 'ROUGE-2': 22.652, 'ROUGE-L': 29.8866}, 'description': 'This model is a T5 Transformers model (JDBN/t5-base-fr-qg-fquad) that was fine-tuned in French for abstractive text summarization.'}} </s>\"\n"}
{"text": "<s>[INST]أنشئ نادي طلابي لعلوم معالجة اللغة الطبيعية، حيث يجتمع الأعضاء كل أسبوعين للتدريب على إنشاء نصوص بشكل توليدي. يجب تزويدهم بأداة لتوليد النصوص ضد وكيل التعلم الآلي.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fr-es')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To perform the translations, we need to import the necessary libraries from the Hugging Face Transformers package, specifically AutoModelForSeq2SeqLM and AutoTokenizer.\\\\n2. We then load the French to Spanish translation model, 'Helsinki-NLP/opus-mt-fr-es', using the from_pretrained method of the AutoModelForSeq2SeqLM class.\\\\n3. After loading the model, we create the tokenizer for pre-processing the text using the from_pretrained method of the AutoTokenizer class.\\\\n4. With the model and tokenizer prepared, we can now use them to translate the product description from French to Spanish.\\\", '<<<code>>>': 'from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\\\n\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'Helsinki-NLP/opus-mt-fr-es\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Helsinki-NLP/opus-mt-fr-es\\\\')\\\\n\\\\nfrench_text = \\\"Description du produit en français ici...\\\"\\\\n\\\\ninputs = tokenizer(french_text, return_tensors=\\\"pt\\\", padding=True, truncation=True)\\\\noutputs = model.generate(inputs[\\\"input_ids\\\"], max_length=50, num_return_sequences=1)\\\\nspanish_translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fr-es')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-fr-es', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fr-es')\\\", 'api_arguments': {'source_languages': 'fr', 'target_languages': 'es'}, 'python_environment_requirements': [{'PyTorch': '1.0.0', 'TensorFlow': '2.0', 'Transformers': '4.0.0'}], 'example_code': \\\"translation('Bonjour, comment ça va?')\\\", 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': {'newssyscomb2009.fr.es': 34.3, 'news-test2008.fr.es': 32.5, 'newstest2009.fr.es': 31.6, 'newstest2010.fr.es': 36.5, 'newstest2011.fr.es': 38.3, 'newstest2012.fr.es': 38.1, 'newstest2013.fr.es': 34.0, 'Tatoeba.fr.es': 53.2}, 'chr-F': {'newssyscomb2009.fr.es': 0.601, 'news-test2008.fr.es': 0.583, 'newstest2009.fr.es': 0.586, 'newstest2010.fr.es': 0.616, 'newstest2011.fr.es': 0.622, 'newstest2012.fr.es': 0.619, 'newstest2013.fr.es': 0.587, 'Tatoeba.fr.es': 0.709}}}, 'description': 'A French to Spanish translation model trained on the OPUS dataset using the Hugging Face Transformers library. The model is based on the transformer-align architecture and uses normalization and SentencePiece for pre-processing.' </s>\"\n"}
{"text": "<s>[INST]عند استخدام خوارزميات معالجة اللغة الطبيعية لملء الفجوة في مجال تحليل الأسئلة والإجابات، يجب عليك توفير النص الأصلي الذي تتعامل معه والكلمات المفقودة المستهدف جمعها. سنقوم بمعالجة النص المدخل وإكمال الفجوات باستخدام تقنيات متقدمة في مجال معالجة اللغة الطبيعية.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"pipeline('question-answering', model='deepset/deberta-v3-base-squad2', tokenizer='deepset/deberta-v3-base-squad2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the required libraries, which are 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=\\\\'deepset/deberta-v3-base-squad2\\\\', tokenizer=\\\\'deepset/deberta-v3-base-squad2\\\\')\\\\nquestion = \\\"Your question here\\\"\\\\ncontext = \\\"Long paragraph of text or context\\\"\\\\nQA_input = {\\\\'question\\\\': question, \\\\'context\\\\': context}\\\\nresult = nlp(QA_input)', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'deepset/deberta-v3-base-squad2', 'api_call': 'from transformers import pipeline\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=\\\\'deepset/deberta-v3-base-squad2\\\\', tokenizer=\\\\'deepset/deberta-v3-base-squad2\\\\')\\\\nquestion = \\\"Your question here\\\"\\\\ncontext = \\\"Long paragraph of text or context\\\"\\\\nQA_input = {\\\\'question\\\\': question, \\\\'context\\\\': context}\\\\nresult = nlp(QA_input)', 'api_arguments': {'model_name': 'deepset/deberta-v3-base-squad2', 'tokenizer': 'deepset/deberta-v3-base-squad2'}, 'python_environment_requirements': ['transformers'], 'example_code': {'import': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline', 'initialize': [\\\"model_name = 'deepset/deberta-v3-base-squad2'\\\", \\\"nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\\\"], 'example_input': {'question': 'Why is model conversion important?', 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'}, 'example_output': 'res = nlp(QA_input)'}, 'performance': {'dataset': 'squad_v2', 'accuracy': {'Exact Match': 83.825, 'F1': 87.41}}, 'description': \\\"This is the deberta-v3-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering.\\\"}} </s>\"\n"}
{"text": "<s>[INST]يجب على المطوّر تطوير برنامج للترجمة الآلية باستخدام تقنيات معالجة اللغة الطبيعية، لتحويل النصوص من لغة معينة إلى لغة أخرى بشكل فعال ودقيق، خاصة في مجال معالجة النصوص الطبية وتعبئة الفراغات.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library.\\\\n2. Use the pipeline function to create a fill-mask model with the 'emilyalsentzer/Bio_ClinicalBERT' model, which has been trained on clinical text and is suitable for medical-related text completion tasks.\\\\n3. Use the created model to fill in the missing word in the provided text.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nfill_mask = pipeline(\\\"fill-mask\\\", model=\\\"emilyalsentzer/Bio_ClinicalBERT\\\")\\\\ntext = \\\"The patient was recently diagnosed with <mask> disease.\\\"\\\\nresult = fill_mask(text)', 'api_call': \\\"AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'emilyalsentzer/Bio_ClinicalBERT', 'api_call': \\\"AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\\\", 'api_arguments': ['AutoTokenizer', 'AutoModel', 'from_pretrained'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\\\\nmodel = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\\\", 'performance': {'dataset': 'MIMIC III', 'accuracy': 'Not provided'}, 'description': 'Bio_ClinicalBERT is a model initialized with BioBERT and trained on all MIMIC notes. It can be used for various NLP tasks in the clinical domain, such as Named Entity Recognition (NER) and Natural Language Inference (NLI).' </s>\"\n"}
{"text": "<s>[INST]قم بتطبيق نماذج معالجة لغوية طبيعية لمقارنة الأشكال الهندسية في مجال الرؤية الحاسوبية الصورة-إلى-صورة. هل يمكنك مساعدتي في تنفيذ هذا؟? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We first need to import the required classes and libraries. We will need ControlNetModel from the diffusers package. Ensure transformers and diffusers packages are also installed.\\\\n2. We load the pre-trained model 'lllyasviel/sd-controlnet-seg' by using ControlNetModel.from_pretrained().\\\\n3. This model can be used to predict segmentation maps for the images of houses. The pipeline function is used to process the input images and generate segmentation maps.\\\\n4. After obtaining the predicted segmentation maps, the images can be saved in the required format.\\\", '<<<code>>>': 'from diffusers import ControlNetModel\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/sd-controlnet-seg\\\\')\\\\nfor house_image in house_images:\\\\n    segmented_image = controlnet(house_image, num_inference_steps=20).images[0]\\\\n    segmented_image.save(f\\\"{house_image}_seg_out.png\\\")', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/sd-controlnet-seg', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", 'api_arguments': ['torch_dtype'], 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate']], 'example_code': \\\"image = pipe(house, image, num_inference_steps=20).images[0]\\\\nimage.save('./images/house_seg_out.png')\\\", 'performance': {'dataset': 'ADE20K', 'accuracy': 'Trained on 164K segmentation-image, caption pairs'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.' </s>\"\n"}
{"text": "<s>[INST]انشئ فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنكم مساعدتي في تصنيف الصوت؟? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a question-answering model using the 'tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa' model. This model has been fine-tuned for question-answering tasks and can assist in answering various questions related to different topics.\\\\n3. The created model can be used to answer questions within the given context.\\\\n4. By providing the question and context to the model, it will return the most probable answer to the question.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nqa_model = pipeline(\\\\'question-answering\\\\', model=\\\\'tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa\\\\')\\\\nquestion = \\\"What is the capital of France?\\\"\\\\ncontext = \\\"Paris is the largest city of France and it is also the capital of the country.\\\"\\\\nanswer = qa_model(question=question, context=context)', 'api_call': \\\"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa', 'api_call': \\\"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\\\", 'api_arguments': 'question, context', 'python_environment_requirements': [['transformers==4.15.0', 'torch==1.8.0+cu101', 'datasets==1.17.0', 'tokenizers==0.10.3']], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 4.3332}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>\"\n"}
{"text": "<s>[INST]يجب على المطور تطوير برنامج لتحويل النص إلى كلام بجودة عالية لمستخدمي تطبيق ذوي الإعاقة البصرية.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", '<<<api_provider>>>': 'speechbrain', '<<<explanation>>>': '1. We import the necessary libraries: torchaudio, speechbrain\\\\'s Tacotron2, and HIFIGAN.\\\\n2. We then load the pre-trained Tacotron2 and HIFIGAN models for German texts using the \\\\'padmalcom/tts-tacotron2-german\\\\' and \\\\'padmalcom/tts-hifigan-german\\\\' sources respectively.\\\\n3. We encode the text \\\"Mary hatte ein kleines Lamm\\\" using the Tacotron2 model to produce a mel spectrogram as the output.\\\\n4. Next, we use the HIFIGAN model to decode the mel spectrogram and generate a waveform.\\\\n5. We finally save the waveform as an audio file \\\\'example_TTS.wav\\\\' in the working directory.', '<<<code>>>': 'import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2, HIFIGAN\\\\n\\\\ntacotron2 = Tacotron2.from_hparams(source=\\\\'padmalcom/tts-tacotron2-german\\\\')\\\\nhifi_gan = HIFIGAN.from_hparams(source=\\\\'padmalcom/tts-hifigan-german\\\\')\\\\ntext = \\\"Mary hatte ein kleines Lamm\\\"\\\\nmel_output, mel_length, alignment = tacotron2.encode_text(text)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save(\\\\'example_TTS.wav\\\\', waveforms.squeeze(1), 22050)', 'api_call': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", 'provider': 'speechbrain', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tts-hifigan-german', 'api_call': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", 'api_arguments': ['mel_output'], 'python_environment_requirements': [['speechbrain']], 'example_code': \\\"import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2\\\\nfrom speechbrain.pretrained import HIFIGAN\\\\ntacotron2 = Tacotron2.from_hparams(source=padmalcom/tts-tacotron2-german, savedir=tmpdir_tts)\\\\nhifi_gan = HIFIGAN.from_hparams(source=padmalcom/tts-hifigan-german, savedir=tmpdir_vocoder)\\\\nmel_output, mel_length, alignment = tacotron2.encode_text(Mary had a little lamb)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\\\", 'performance': {'dataset': 'custom German dataset', 'accuracy': 'Not specified'}, 'description': 'A HiFIGAN vocoder trained on a generated German dataset using mp3_to_training_data. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram.' </s>\"\n"}
{"text": "<s>[INST]عميلنا يرغب في تطوير تطبيق للتصنيف الآلي الذي لا يتطلب تدريب سابق. نحتاج إلى برنامج يستطيع تصنيف النصوص تلقائيا دون الحاجة لقائمة مسبقة من التصنيفات.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the 'transformers' library.\\\\n2. Create an instance of the pipeline using the 'zero-shot-classification' task by specifying the pre-trained model 'valhalla/distilbart-mnli-12-6'.\\\\n3. Define the candidate labels which in this case would be ['professional', 'casual', 'negative'].\\\\n4. Use the created pipeline to pass your marketing article and the candidate labels to determine the tone of the article based on the highest probability.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'valhalla/distilbart-mnli-12-6\\\\')\\\\narticle_text = \\\"Your marketing article here...\\\"\\\\ncandidate_labels = [\\\\'professional\\\\', \\\\'casual\\\\', \\\\'negative\\\\']\\\\nresult = classifier(article_text, candidate_labels)\\\\narticle_tone = result[\\\\'labels\\\\'][0]', 'api_call': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'valhalla/distilbart-mnli-12-6', 'api_call': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')\\\", 'api_arguments': 'text, candidate_labels', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nnlp = pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')\\\\nresult = nlp('The movie was great!', ['positive', 'negative'])\\\\nprint(result)\\\", 'performance': {'dataset': 'MNLI', 'accuracy': {'matched_acc': '89.19', 'mismatched_acc': '89.01'}}, 'description': 'distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is designed for zero-shot classification tasks.' </s>\"\n"}
{"text": "<s>[INST]يوفر التعلم الآلي ناديًا طلابيًا يُعقد كل أسبوعين لممارسة تصنيف النصوص بتقنيات معالجة اللغة الطبيعية. زودهم بأداة ليتمكنوا من التنافس ضد وكيل تصنيف النصوص.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the 'pipeline' function from the transformers library provided by Hugging Face.\\\", 'code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\\nresult = qa_pipeline({'context': 'your_context_here', 'question': 'your_question_here'})\\\", 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large', 'api_call': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\\nresult = qa_pipeline({'context': 'your_context_here', 'question': 'your_question_here'})\\\", 'api_arguments': 'context, question', 'python_environment_requirements': 'transformers', 'example_code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\\nresult = qa_pipeline({'context': 'your_context_here', 'question': 'your_question_here'})\\\", 'performance': {'dataset': 'Dureader-2021', 'accuracy': '83.1'}, 'description': 'A Chinese MRC roberta_wwm_ext_large model trained on a large amount of Chinese MRC data. This model has significantly improved performance on reading comprehension and classification tasks. It has helped multiple users achieve top 5 results in the Dureader-2021 competition.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج تعلم تعزيزي لتدريب العمليات الذكية للقيام بمهام معينة بناءً على التجربة والمكافأة.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"load_from_hub(repo_id='sb3/dqn-Acrobot-v1',filename='{MODEL FILENAME}.zip',)\\\", '<<<api_provider>>>': 'Stable-Baselines3', '<<<explanation>>>': \\\"1. To train your robot to balance the Acrobot-v1, we recommend using the DQN algorithm from Stable-Baselines3 library in combination with the RL Zoo framework.\\\\n2. You can load pre-trained Acrobot-v1 model using the 'load_from_hub' method with its associated parameters.\\\\n3. Utilize this API to perform training and evaluation of the robot via different training options, such as 'train.py', 'enjoy.py', and 'push_to_hub' provided in 'Stable-Baselines3'.\\\\n4. Monitor your training progress and results by storing logs in the specified logs folder.\\\\n5. Finally, you can fine-tune the model based on your specific problem if necessary.\\\", '<<<code>>>': \\\"from rl_zoo3 import load_from_hub\\\\ntrained_agent = load_from_hub(repo_id='sb3/dqn-Acrobot-v1', filename='{MODEL FILENAME}.zip')\\\", 'api_call': \\\"load_from_hub(repo_id='sb3/dqn-Acrobot-v1',filename='{MODEL FILENAME}.zip',)\\\", 'provider': 'Stable-Baselines3', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sb3/dqn-Acrobot-v1', 'api_call': \\\"load_from_hub(repo_id='sb3/dqn-Acrobot-v1',filename='{MODEL FILENAME}.zip',)\\\", 'api_arguments': {'algo': 'dqn', 'env': 'Acrobot-v1', 'logs_folder': 'logs/'}, 'python_environment_requirements': [['rl_zoo3', 'stable-baselines3', 'stable-baselines3-contrib']], 'example_code': ['python -m rl_zoo3.load_from_hub --algo dqn --env Acrobot-v1 -orga sb3 -f logs/', 'python enjoy.py --algo dqn --env Acrobot-v1 -f logs/', 'python train.py --algo dqn --env Acrobot-v1 -f logs/', 'python -m rl_zoo3.push_to_hub --algo dqn --env Acrobot-v1 -f logs/ -orga sb3'], 'performance': {'dataset': 'Acrobot-v1', 'accuracy': '-72.10 +/- 6.44'}, 'description': 'This is a trained model of a DQN agent playing Acrobot-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نموذج لتقنية تحويل النص إلى نص بدون عنصر الصورة في مجال معالجة اللغة الطبيعية لتوليد النص. هل يمكنك مساعدتي في هذا المجال؟? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='gpt2-large')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a text generation model using the \\\\'gpt2-large\\\\' model. This is a powerful, pre-trained GPT-2 model with 774 million parameters that is excellent at generating human-like text based on the input.\\\\n3. Set a seed (optional) for deterministic results on consecutive runs.\\\\n4. Provide a prompt to start the story, such as \\\"Once upon a time in a magical forest,\\\" and set the desired maximum length and number of sequences. The model will generate text based on this prompt in the form of a new children\\\\'s story.', '<<<code>>>': 'from transformers import pipeline, set_seed\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'gpt2-large\\\\')\\\\nset_seed(42)\\\\nstory_start = \\\"Once upon a time in a magical forest,\\\"\\\\ngenerated_story = generator(story_start, max_length=100, num_return_sequences=1)', 'api_call': \\\"pipeline('text-generation', model='gpt2-large')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'gpt2-large', 'api_call': \\\"pipeline('text-generation', model='gpt2-large')\\\", 'api_arguments': {'text': \\\"Hello, I'm a language model,\\\", 'max_length': 30, 'num_return_sequences': 5}, 'python_environment_requirements': [{'transformers': 'pipeline, set_seed', 'PyTorch': 'GPT2Tokenizer, GPT2Model', 'TensorFlow': 'GPT2Tokenizer, TFGPT2Model'}], 'example_code': \\\"from transformers import pipeline, set_seed\\\\ngenerator = pipeline('text-generation', model='gpt2-large')\\\\nset_seed(42)\\\\ngenerator(Hello, I'm a language model,, max_length=30, num_return_sequences=5)\\\", 'performance': {'dataset': {'LAMBADA': {'PPL': 10.87}, 'CBT-CN': {'ACC': 93.45}, 'CBT-NE': {'ACC': 88.0}, 'WikiText2': {'PPL': 19.93}, 'PTB': {'PPL': 40.31}, 'enwiki8': {'BPB': 0.97}, 'text8': {'BPC': 1.02}, 'WikiText103': {'PPL': 22.05}, '1BW': {'PPL': 44.575}}}, 'description': 'GPT-2 Large is the 774M parameter version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective.' </s>\"\n"}
{"text": "<s>[INST]نقدم دوريًا تقنيات استخراج الميزات متعددة الوسائط. قم بتجهيز فريقك بالمعدات اللازمة للتدريب والاختبار.? [/INST]'<<<domain>>>': 'Multimodal Feature Extraction', '<<<api_call>>>': \\\"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, i.e. 'AutoModel' and 'AutoTokenizer' from transformers.\\\\n2. Load the pretrained model 'GanjinZero/UMLSBert_ENG' using AutoModel.from_pretrained().\\\\n3. Load the tokenizer using AutoTokenizer.from_pretrained().\\\\n4. To obtain the term embeddings, tokenize the input medical terms using the loaded tokenizer.\\\\n5. Pass the tokenized input to the loaded model to get the embeddings for the medical terms.\\\\n6. These embeddings can then be used to analyze, classify, or represent medical terms in the app's glossary.\\\", '<<<code>>>': \\\"from transformers import AutoModel, AutoTokenizer\\\\nmodel = AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\\\\ntokenizer = AutoTokenizer.from_pretrained('GanjinZero/UMLSBert_ENG')\\\\nmedical_terms = ['medical term 1', 'medical term 2']\\\\ntokenized_input = tokenizer(medical_terms, return_tensors='pt', padding=True, truncation=True)\\\\nembeddings = model(**tokenized_input).last_hidden_state\\\", 'api_call': \\\"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \\\"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>\"\n"}
{"text": "<s>[INST]يجب عليك تحميل مجموعة البيانات الخاصة بتعلم معالجة اللغة الطبيعية للتصنيف النصي ثم تقديم تنبؤات معقدة باستخدام تقنية التصنيف النصي. العب ضد وكيل التعلم الآلي لتحسين الدقة والأداء.? [/INST]{'domain': 'Natural Language Processing Text Classification', 'api_call': \\\"pipeline(model='martin-ha/toxic-comment-model')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\\\\nmodel_path = \\\\'martin-ha/toxic-comment-model\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\\\\ntoxic_comment_detector = TextClassificationPipeline(model=model, tokenizer=tokenizer)\\\\n\\\\ncomment = \\\"This is a test text.\\\"\\\\nclassification = toxic_comment_detector(comment)\\\\nif classification[0][\\\\'label\\\\'] == \\\\'LABEL_1\\\\' and classification[0][\\\\'score\\\\'] > 0.5:\\\\n    print(\\\"Toxic comment detected and removed.\\\")\\\\nelse:\\\\n    print(\\\"Non-toxic comment.\\\")', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'martin-ha/toxic-comment-model', 'api_call': 'from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\\\\nmodel_path = \\\\'martin-ha/toxic-comment-model\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\\\\ntoxic_comment_detector = TextClassificationPipeline(model=model, tokenizer=tokenizer)\\\\n\\\\ncomment = \\\"This is a test text.\\\"\\\\nclassification = toxic_comment_detector(comment)\\\\nif classification[0][\\\\'label\\\\'] == \\\\'LABEL_1\\\\' and classification[0][\\\\'score\\\\'] > 0.5:\\\\n    print(\\\"Toxic comment detected and removed.\\\")\\\\nelse:\\\\n    print(\\\"Non-toxic comment.\\\")', 'api_arguments': {'model_path': 'martin-ha/toxic-comment-model'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\\\\nmodel_path = martin-ha/toxic-comment-model\\\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\\\\npipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\\\\nprint(pipeline('This is a test text.'))\\\", 'performance': {'dataset': 'held-out test set', 'accuracy': 0.94, 'f1-score': 0.59}, 'description': 'This model is a fine-tuned version of the DistilBERT model to classify toxic comments.'}} </s>\"\n"}
{"text": "<s>[INST]يتم تطوير نماذج للإجابة على الأسئلة المرئية متعددة الأوساط، وذلك عن طريق تحليل الصور والنصوص معًا، ونحتاج إلى تطبيق تقنيات التعلم العميق وذلك لتحسين القدرة على فهم النصوص والصور بشكل أفضل.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the necessary classes, BlipProcessor and BlipForQuestionAnswering, from the transformers library.\\\\n2. Load the pretrained model and processor by calling the from_pretrained() method on both classes with 'Salesforce/blip-vqa-capfilt-large' as the argument. The processor will handle the raw image and question, while the model is responsible for generating the answer.\\\\n3. Fetch the image using a library like requests and convert it to an RGB format with PIL.Image.\\\\n4. Pass the raw image and the user's question to the processor, which will prepare the input for the model.\\\\n5. Feed the processed input to the model and generate an output.\\\\n6. Decode the output to obtain the answer to the user's question about the image content.\\\", '<<<code>>>': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\\\nprocessor = BlipProcessor.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\\nmodel = BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\\nimg_url = 'https://example.com/image.jpg'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\\\nquestion = 'How many dogs are in the picture?'\\\\ninputs = processor(raw_image, question, return_tensors='pt')\\\\nout = model.generate(**inputs)\\\\nanswer = processor.decode(out[0], skip_special_tokens=True)\\\", 'api_call': \\\"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Salesforce/blip-vqa-capfilt-large', 'api_call': \\\"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\\\", 'api_arguments': {'raw_image': 'RGB image', 'question': 'string'}, 'python_environment_requirements': [{'transformers': 'BlipProcessor, BlipForQuestionAnswering'}], 'example_code': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\\\nquestion = how many dogs are in the picture?\\\\ninputs = processor(raw_image, question, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\\", 'performance': {'dataset': 'VQA', 'accuracy': '+1.6% in VQA score'}, 'description': 'BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.' </s>\"\n"}
{"text": "<s>[INST]يجب على المستخدم تنفيذ عملية تصنيف للغة الطبيعية بدون استخدام مصنف قبلي، حيث تقوم النماذج بتحديد تصنيف المستندات تلقائيًا وفقًا للسياق المعطى.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('Recognai/bert-base-spanish-wwm-cased-xnli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library.\\\\n2. Use the pipeline function to create a zero-shot classification model, which can categorize text into predefined categories without being explicitly trained on those categories.\\\\n3. Specify the model 'Recognai/bert-base-spanish-wwm-cased-xnli', which is a fine-tuned version of the Spanish BERT model, trained on the Spanish portion of the XNLI dataset.\\\\n4. The created classifier can be used to classify the news article in Spanish into different categories, such as 'culture', 'society', 'economy', 'health', and 'sports'.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'Recognai/bert-base-spanish-wwm-cased-xnli\\\\')\\\\narticle_text = \\\"Texto del artículo en español...\\\"\\\\ncandidate_labels = [\\\\'cultura\\\\', \\\\'sociedad\\\\', \\\\'economia\\\\', \\\\'salud\\\\', \\\\'deportes\\\\']\\\\nresult = classifier(article_text, candidate_labels)', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('Recognai/bert-base-spanish-wwm-cased-xnli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Recognai/bert-base-spanish-wwm-cased-xnli', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('Recognai/bert-base-spanish-wwm-cased-xnli')\\\", 'api_arguments': ['sequence', 'candidate_labels', 'hypothesis_template'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=Recognai/bert-base-spanish-wwm-cased-xnli)\\\\nclassifier(\\\\nEl autor se perfila, a los 50 años de su muerte, como uno de los grandes de su siglo,\\\\ncandidate_labels=[cultura, sociedad, economia, salud, deportes],\\\\nhypothesis_template=Este ejemplo es {}. \\\\n)', 'performance': {'dataset': 'XNLI-es', 'accuracy': '79.9%'}, 'description': 'This model is a fine-tuned version of the spanish BERT model with the Spanish portion of the XNLI dataset. You can have a look at the training script for details of the training.' </s>\"\n"}
{"text": "<s>[INST]نحن بحاجة إلى تطوير برنامج يستخدم استخراج الميزات متعددة الوسائط، لتحويل النص إلى كلام لمستخدمي التطبيق الخاص بذوي الإعاقة البصرية، في سياق الأسئلة والأجوبة على المستندات المتعددة الوسائط.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import required classes from transformers, such as AutoModelForDocumentQuestionAnswering and AutoTokenizer.\\\\n2. Load the pre-trained model 'tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa' using the from_pretrained() method. This model is trained on document question answering tasks, which is suitable for extracting information from historical documents.\\\\n3. Load the tokenizer using the from_pretrained() method to process the textual data.\\\\n4. Tokenize the input document and the question you want to ask.\\\\n5. Pass the tokenized input to the model and obtain the answer.\\\\n6. Decode the answer using the tokenizer.\\\", '<<<code>>>': 'from transformers import AutoModelForDocumentQuestionAnswering, AutoTokenizer\\\\nmodel = AutoModelForDocumentQuestionAnswering.from_pretrained(\\\\'tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\\\\')\\\\ndocument = \\\"Document text here...\\\"\\\\nquestion = \\\"Your question here...\\\"\\\\ninputs = tokenizer(question, document, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\\\noutputs = model(**inputs)\\\\nanswer_start = outputs.start_logits.argmax().item()\\\\nanswer_end = outputs.end_logits.argmax().item() + 1\\\\nanswer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs.input_ids[0, answer_start:answer_end])).strip()', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'layoutlmv2-base-uncased-finetuned-docvqa', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers==4.12.2', 'torch==1.8.0+cu101', 'datasets==1.14.0', 'tokenizers==0.10.3']], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 1.194}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>\"\n"}
{"text": "<s>[INST]أنشئ تقسيم صورة في مجال رؤية الحاسوب باستخدام تقنيات الفصل التلقائي على الصورة المُقدمة.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the required libraries - SegformerFeatureExtractor and SegformerForSemanticSegmentation from transformers, Image from PIL, and requests.\\\\n2. Load the segformer-b0-finetuned-cityscapes-1024-1024 model using the SegformerFeatureExtractor and SegformerForSemanticSegmentation classes from Hugging Face transformers.\\\\n3. Load an image of a cityscape using the PIL library and download the image using the requests library.\\\\n4. Preprocess the image using the SegformerFeatureExtractor to create the input tensors.\\\\n5. Pass the input tensors to the model to obtain segmentation logits.\\\\n6. The output can later be processed to obtain the final segmentation map that can be used to analyze different objects in the cityscape image.', '<<<code>>>': \\\"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\", 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024', 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير تقنية التحول النصي إلى صور متعددة الوسائط، ونريد إنشاء الصور بناءً على النصوص والبيانات المتاحة.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"pipeline('text-to-image', model='prompthero/openjourney-v4')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. You will need to import the necessary library, the \\\\'pipeline\\\\' function from transformers.\\\\n2. Load the text-to-image model \\\\'prompthero/openjourney-v4\\\\' using the \\\\'pipeline\\\\' function with \\\\'text-to-image\\\\' as the task and specify the model name.\\\\n3. Provide a text description for the book cover, such as \\\"Alice in Wonderland book cover featuring Alice, the White Rabbit, and the Cheshire Cat in a colorful, whimsical setting.\\\"\\\\n4. Run the text-to-image pipeline with the input text to generate an image based on the provided instructions.\\\\n5. Save the generated image to be used as the book cover.', '<<<code>>>': 'from transformers import pipeline\\\\ntext2image = pipeline(\\\\'text-to-image\\\\', model=\\\\'prompthero/openjourney-v4\\\\')\\\\ndescription = \\\"Alice in Wonderland book cover featuring Alice, the White Rabbit, and the Cheshire Cat in a colorful, whimsical setting.\\\"\\\\ngenerated_image = text2image(description)\\\\n# Save the generated image as the book cover\\\\ngenerated_image.save(\\\\'alice_in_wonderland_cover.png\\\\')', 'api_call': \\\"pipeline('text-to-image', model='prompthero/openjourney-v4')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'prompthero/openjourney-v4', 'api_call': \\\"pipeline('text-to-image', model='prompthero/openjourney-v4')\\\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"generate_image('your text here')\\\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا ببناء منتج للبحث عن الوظائف في مجال معالجة اللغة الطبيعية للإجابة على الأسئلة، ونريد التنبؤ براتب الوظيفة بناءً على بعض مجموعات البيانات المتوفرة.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required pipeline function from the transformers package.\\\\n2. Call the pipeline function with the task 'question-answering' and the pre-trained model 'distilbert-base-cased-distilled-squad' to create a question-answer instance.\\\\n3. Provide the question and the context paragraph to the instance, which will analyze the given text and come up with the best answer.\\\\n4. Print the answer and the associated score, start, and end positions.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nquestion_answerer = pipeline(\\\\'question-answering\\\\', model=\\\\'distilbert-base-cased-distilled-squad\\\\')\\\\ncontext = \\\"\\\"\\\"Your context paragraph here...\\\"\\\"\\\"\\\\nquestion = \\\"Your question here...\\\"\\\\nresult = question_answerer(question=question, context=context)\\\\n\\\\nprint(f\\\"Answer: \\\\'{result[\\\\'answer\\\\']}\\\\', score: {round(result[\\\\'score\\\\'], 4)}, start: {result[\\\\'start\\\\']}, end: {result[\\\\'end\\\\']}\\\")', 'api_call': \\\"DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilbert-base-cased-distilled-squad', 'api_call': \\\"DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\\\", 'api_arguments': ['question', 'context'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model='distilbert-base-cased-distilled-squad')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\\\\n...)\\\", 'performance': {'dataset': 'SQuAD v1.1', 'accuracy': {'Exact Match': 79.6, 'F1': 86.996}}, 'description': \\\"DistilBERT base cased distilled SQuAD is a fine-tuned checkpoint of DistilBERT-base-cased, trained using knowledge distillation on SQuAD v1.1 dataset. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark. This model can be used for question answering.\\\" </s>\"\n"}
{"text": "<s>[INST]نحتاج في شركتنا المتخصصة في مجال معالجة اللغة الطبيعية وتصنيف الصفر إلى تصميم نماذج لتصنيف النصوص بدون تدريب مسبق، ونحتاج إلى تقدير عمق المحتوى والمفهوم في النصوص التي سيتم معالجتها.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import necessary modules, including PIL for image processing, requests for loading images, and ChineseCLIPModel and ChineseCLIPProcessor from transformers.\\\\n2. Load the pre-trained ChineseCLIPModel using the from_pretrained method. This model is designed for zero-shot image classification, allowing it to understand the context and contents of an image without being explicitly trained for specific categories.\\\\n3. Load the ChineseCLIPProcessor, which will be used to process both the image and text data.\\\\n4. Load the image data, either from a file or through a URL using the requests library.\\\\n5. Create a list of texts that might describe the image or its elements.\\\\n6. Process the image and texts using the ChineseCLIPProcessor, and pass the resulting tensors to the ChineseCLIPModel.\\\\n7. Obtain the image and text features, and compare their similarity using the cosine similarity method.\\\\n8. The highest similarity score indicates the most likely description for the image content.', '<<<code>>>': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\n\\\\nmodel = ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\\nprocessor = ChineseCLIPProcessor.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\\n\\\\nurl = 'https://image_url.com/image.jpg'\\\\n# Replace with the URL of the image you would like to process\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\ntexts = ['Text description example 1', 'Text description example 2', 'Text description example 3']\\\\n\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nimage_features = model.get_image_features(**inputs)\\\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\\\\n\\\\ninputs = processor(text=texts, padding=True, return_tensors='pt')\\\\ntext_features = model.get_text_features(**inputs)\\\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\\\\n\\\\ncosine_scores = (text_features @ image_features.T).softmax(1)\\\\nbest_score, best_index = cosine_scores.max(dim=-1)\\\\nbest_description = texts[best_index]\\\", 'api_call': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'OFA-Sys/chinese-clip-vit-large-patch14-336px', 'api_call': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\", 'api_arguments': {'images': 'image', 'text': 'texts', 'return_tensors': 'pt', 'padding': 'True'}, 'python_environment_requirements': [['PIL', 'requests', 'transformers']], 'example_code': 'from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\\\\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\\\\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = []\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nimage_features = model.get_image_features(**inputs)\\\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\\\\ninputs = processor(text=texts, padding=True, return_tensors=pt)\\\\ntext_features = model.get_text_features(**inputs)\\\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\\\\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)', 'performance': {'dataset': {'CIFAR10': 96.0, 'CIFAR100': 79.75, 'DTD': 51.2, 'EuroSAT': 52.0, 'FER': 55.1, 'FGVC': 26.2, 'KITTI': 49.9, 'MNIST': 79.4, 'PC': 63.5, 'VOC': 84.9}, 'accuracy': 'various'}, 'description': 'Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-L/14@336px as the image encoder and RoBERTa-wwm-base as the text encoder.' </s>\"\n"}
{"text": "<s>[INST]يوفر مجال استخراج الميزات المتعددة أدوات وتقنيات لاستخلاص السمات من وسائط متعددة مثل النصوص والصور ومقاطع الفيديو. قد تحتاج إلى تطوير أداة تسمح بإستخلاص سمات من وسائط متعددة، مثل استخراج سمات من نص يصاحب الصور للاستفادة منها في وكيل التعلم الآلي للتعرف على الأنماط المعقدة.? [/INST]'<<<domain>>>': 'Multimodal Feature Extraction', '<<<api_call>>>': \\\"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, such as the HubertModel class from the transformers package.\\\\n2. Load the pre-trained model named 'facebook/hubert-large-ll60k' by using the from_pretrained() method of the HubertModel class. This model is designed to extract features from speech and has been pretrained on a large variety of speech data.\\\\n3. Stream the student's speech recording in the desired format (such as WAV, MP3, or other format) and process it accordingly.\\\\n4. Use the loaded model to extract features from the student's speech recording.\\\\n5. Based on the extracted features, analyze the vocal nuances and fluency of the students.\\\", '<<<code>>>': \\\"from transformers import HubertModel\\\\nfrom pathlib import Path\\\\nhubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\\n\\\\nfile_path = 'path/to/student_recording.wav'\\\\n# replace with the path to your audio file\\\\naudio_input = Path(file_path)\\\\nfeatures = hubert(audio_input)\\\", 'api_call': \\\"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \\\"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': ['transformers'], 'example_code': \\\"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\\\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير برنامج يستخدم تقنيات معالجة اللغات الطبيعية (NLP) لتلخيص النصوص بشكل فعال. يجب على هذا البرنامج إنشاء ملخصات مفيدة وموجزة للنصوص المدخلة، مما يساعد على تبسيط قراءة وفهم النصوص للمستخدمين.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required classes (PegasusTokenizer, PegasusForConditionalGeneration) from the transformers library provided by Hugging Face.\\\\n2. Load the pre-trained financial summarization model 'human-centered-summarization/financial-summarization-pegasus'. This model is trained using a dataset of financial news articles and is ideal for summarizing articles in the financial sector.\\\\n3. Tokenize the financial news article using the PegasusTokenizer class, which provides methods for encoding and decoding text data.\\\\n4. Generate a summary of the input text using the PegasusForConditionalGeneration class. You can control the summary's length and other parameters to suit your requirements.\\\", '<<<code>>>': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration\\\\nmodel_name = \\\\'human-centered-summarization/financial-summarization-pegasus\\\\'\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\narticle_text = \\\"Your financial news article content goes here.\\\"\\\\ninput_ids = tokenizer(article_text, return_tensors=\\\\'pt\\\\').input_ids\\\\noutput = model.generate(input_ids, max_length=100, num_beams=5, early_stopping=True)\\\\nsummary = tokenizer.decode(output[0], skip_special_tokens=True)', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'financial-summarization-pegasus', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabia’s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba’s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region’s third-largest lender. The entity’s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East’s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))', 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 35.206, 'ROUGE-2': 16.569, 'ROUGE-L': 30.128, 'ROUGE-LSUM': 30.171}}, 'description': 'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.' </s>\"\n"}
{"text": "<s>[INST]نحن هنا لمساعدتك في تحويل نص وصفي إلى فيديو يُصوّر مشاهد للحيوانات في البرية باستخدام تقنيات معالجة اللغات الطبيعية. يرجى الاتصال بنا للمزيد من المعلومات والمساعدة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-large-ntp-py')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary transformers classes, including AutoTokenizer and T5ForConditionalGeneration.\\\\n2. Load the pre-trained model 'Salesforce/codet5-large-ntp-py' using the T5ForConditionalGeneration class. This model can generate Python code based on a given text input describing the requirements.\\\\n3. Tokenize your text using the pre-trained tokenizer from the AutoTokenizer class.\\\\n4. Use the generated tokens as input and call the generate function of the model with the provided input ids and the desired output length.\\\\n5. Decode the generated token ids to get the Python code example as output.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, T5ForConditionalGeneration\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Salesforce/codet5-large-ntp-py\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'Salesforce/codet5-large-ntp-py\\\\')\\\\ntext = \\\"Create a simple python function to calculate the square of a given number\\\"\\\\ninput_ids = tokenizer(text, return_tensors=\\\\'pt\\\\').input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\ngenerated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-large-ntp-py')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'codet5-large-ntp-py', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-large-ntp-py')\\\", 'api_arguments': ['text', 'return_tensors', 'input_ids', 'max_length'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoTokenizer, T5ForConditionalGeneration\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codet5-large-ntp-py)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(Salesforce/codet5-large-ntp-py)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'APPS benchmark', 'accuracy': 'See Table 5 of the paper'}, 'description': 'CodeT5 is a family of encoder-decoder language models for code from the paper: CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation by Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. The checkpoint included in this repository is denoted as CodeT5-large-ntp-py (770M), which is introduced by the paper: CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning by Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi.' </s>\"\n"}
{"text": "<s>[INST]يمكنك البدء في تحويل النصوص إلى تسجيل صوتي باللغة العربية عبر استخدام تقنيات معالجة اللغة الطبيعية. يُرجى تقديم النص الذي ترغب في تحويله إلى ملف صوتي، وسأقوم بمساعدتك في إنشاء الملف الصوتي. هل تحتاج لأي مزيد من المعلومات أو المساعدة؟? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the pipeline function from the transformers library.', 'code': 'from transformers import pipeline\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'EleutherAI/gpt-neo-2.7B\\\\')\\\\ntext = generator(\\\"Plastic pollution and climate change connection\\\", do_sample=True, min_length=100)[0][\\\\'generated_text\\\\']', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Generation', 'api_name': 'EleutherAI/gpt-neo-2.7B', 'api_call': 'from transformers import pipeline\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'EleutherAI/gpt-neo-2.7B\\\\')\\\\ntext = generator(\\\"Plastic pollution and climate change connection\\\", do_sample=True, min_length=100)[0][\\\\'generated_text\\\\']', 'api_arguments': ['text', 'do_sample', 'min_length'], 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n generator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\\\\n generator(EleutherAI has, do_sample=True, min_length=50)\\\", 'performance': {'dataset': 'the_pile', 'accuracy': {'Lambada_Acc': '62.22%', 'Winogrande': '56.50%', 'Hellaswag': '42.73%'}}, 'description': \\\"GPT-Neo 2.7B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. It was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model. This model is best suited for generating texts from a prompt and can be used directly with a pipeline for text generation.\\\"}} </s>\"\n"}
{"text": "<s>[INST]يحتاج العميل إلى برنامج يقوم بتوليد صور بدون شرط في مجال رؤية الحاسوب، وذلك لتصميم تطبيقات وأدوات تستخدم في التعرف على الأشياء والأنماط البصرية بشكل تلقائي.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-ema-cat-256')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from diffusers import DDPMPipeline\\\\nddpm = DDPMPipeline.from_pretrained('google/ddpm-ema-cat-256')\\\\nimage = ddpm().images[0]\\\\nimage.save('ddpm_generated_image.png')\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-ema-cat-256', 'api_call': \\\"from diffusers import DDPMPipeline\\\\nddpm = DDPMPipeline.from_pretrained('google/ddpm-ema-cat-256')\\\\nimage = ddpm().images[0]\\\\nimage.save('ddpm_generated_image.png')\\\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['!pip install diffusers'], 'example_code': 'from diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-ema-cat-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images, and supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. On the unconditional CIFAR10 dataset, it achieves an Inception score of 9.46 and a state-of-the-art FID score of 3.17.'}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لتصنيف بدون تدريب في مجال معالجة اللغة الطبيعية.? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary libraries, which are PIL for image handling, requests for fetching image data, and various classes from transformers.', 'code': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\n\\\\nmodel = ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\\nprocessor = ChineseCLIPProcessor.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\\nimage = Image.open('user_provided_image.jpg')  # replace with user-provided image path\\\\ntexts = ['Category 1', 'Category 2', 'Category 3']  # replace with actual category descriptions\\\\n\\\\ninputs = processor(images=image, text=texts, return_tensors='pt', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\", 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'OFA-Sys/chinese-clip-vit-large-patch14-336px', 'api_call': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\n\\\\nmodel = ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\\nprocessor = ChineseCLIPProcessor.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\\\\nimage = Image.open('user_provided_image.jpg')  # replace with user-provided image path\\\\ntexts = ['Category 1', 'Category 2', 'Category 3']  # replace with actual category descriptions\\\\n\\\\ninputs = processor(images=image, text=texts, return_tensors='pt', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\", 'api_arguments': {'images': 'image', 'text': 'texts', 'return_tensors': 'pt', 'padding': 'True'}, 'python_environment_requirements': ['PIL', 'requests', 'transformers'], 'example_code': 'from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\\\\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\\\\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = []\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nimage_features = model.get_image_features(**inputs)\\\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\\\\ninputs = processor(text=texts, padding=True, return_tensors=pt)\\\\ntext_features = model.get_text_features(**inputs)\\\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\\\\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)', 'performance': {'dataset': {'CIFAR10': 96.0, 'CIFAR100': 79.75, 'DTD': 51.2, 'EuroSAT': 52.0, 'FER': 55.1, 'FGVC': 26.2, 'KITTI': 49.9, 'MNIST': 79.4, 'PC': 63.5, 'VOC': 84.9}, 'accuracy': 'various'}, 'description': 'Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-L/14@336px as the image encoder and RoBERTa-wwm-base as the text encoder.'}} </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذجًا لمعالجة اللغة الطبيعية للإجابة عن الأسئلة.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We import the appropriate function from the transformers library developed by Hugging Face.\\\\n2. We create a question-answer pipeline using the pre-trained model 'distilbert-base-cased-distilled-squad', which has been fine-tuned on the SQuAD v1.1 dataset. This allows for question answering in a wide range of contexts.\\\\n3. The resulting model, named question_answerer, can understand and answer questions about popular places and attractions given an appropriate context (provided in a text format).\\\\n4. By providing the model with a question and a context (text containing the answer), the model will return the answer to the query.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nquestion_answerer = pipeline(\\\\'question-answering\\\\', model=\\\\'distilbert-base-cased-distilled-squad\\\\')\\\\ncontext = \\\"Paris is famous for its many attractions including the Eiffel Tower, the Louvre Museum, and the Notre-Dame Cathedral.\\\"\\\\nresult = question_answerer(question=\\\"What famous museum is located in Paris?\\\", context=context)', 'api_call': \\\"DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilbert-base-cased-distilled-squad', 'api_call': \\\"DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\\\", 'api_arguments': ['question', 'context'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model='distilbert-base-cased-distilled-squad')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\\\\n...)\\\", 'performance': {'dataset': 'SQuAD v1.1', 'accuracy': {'Exact Match': 79.6, 'F1': 86.996}}, 'description': \\\"DistilBERT base cased distilled SQuAD is a fine-tuned checkpoint of DistilBERT-base-cased, trained using knowledge distillation on SQuAD v1.1 dataset. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark. This model can be used for question answering.\\\" </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير تقنية مرئية للحوسبة لإنشاء صور من صور أخرى، ونرغب في تقديم تنبؤات دقيقة بالصور المنتجة على أساس البيانات المتاحة.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We first import the pipeline class from the transformers package.\\\\n2. We then use the pipeline function to create an instance of the image-to-image model with the model identifier 'GreeneryScenery/SheepsControlV3'. This model is designed to generate images based on an input image and optional text guidance.\\\\n3. We can provide the input image and optional text guidance to the model instance, and the model will generate an advertisement image based on the given inputs.\\\\n4. The generated advertisement image can be used for the digital marketing purposes of our client.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nmodel = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\\\\nresult = model({'image': 'path/to/image.jpg', 'text_guidance': 'Create an advertisement with a vibrant background and a catchy phrase'})\\\", 'api_call': \\\"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GreeneryScenery/SheepsControlV3', 'api_call': \\\"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\\\", 'api_arguments': {'image': 'Path to image file', 'text_guidance': 'Optional text guidance for the model'}, 'python_environment_requirements': [{'transformers': 'latest', 'torch': 'latest'}], 'example_code': ['from transformers import pipeline', \\\"model = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\\\", \\\"result = model({'image': 'path/to/image.jpg', 'text_guidance': 'Optional text guidance'})\\\"], 'performance': {'dataset': 'GreeneryScenery/SheepsControlV3', 'accuracy': 'Not provided'}, 'description': 'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.' </s>\"\n"}
{"text": "<s>[INST]أنشئ عبارة جديدة في مجال معالجة اللغات الطبيعية تبدأ بـ \\\"تحسين تشابه الجمل\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We first import the 'pipeline' function from the transformers package.\\\\n2. We then use the pipeline function to load the pre-trained GPT-2 model 'sshleifer/tiny-gpt2'. This model is specifically designed for text generation tasks and can generate creative text-based content.\\\\n3. We provide a starting phrase for the Sci-Fi story and let the model generate a continuation of the text, which can serve as the beginning of the story.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntext_generator = pipeline(\\\\'text-generation\\\\', model=\\\\'sshleifer/tiny-gpt2\\\\')\\\\nstory_start = \\\"In a distant galaxy, a mysterious planet...\\\"\\\\ngenerated_text = text_generator(story_start, max_length=100)\\\\nsci_fi_story_start = generated_text[0][\\\\'generated_text\\\\']', 'api_call': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sshleifer/tiny-gpt2', 'api_call': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", 'api_arguments': {'model': 'sshleifer/tiny-gpt2'}, 'python_environment_requirements': [{'huggingface_transformers': '>=4.0.0'}], 'example_code': \\\"from transformers import pipeline\\\\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\\\\nresult = nlp('Once upon a time')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A tiny GPT-2 model for text generation, suitable for low-resource environments and faster inference. This model is part of the Hugging Face Transformers library and can be used for generating text given a prompt.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد صور من دون شروط باستخدام تقنية رؤية الكمبيوتر. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-church-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we have to install the diffusers package.\\\\n2. Next, we import the DiffusionPipeline class from the diffusers package to create the necessary model for generating unconditional images.\\\\n3. We then use the from_pretrained method of the DiffusionPipeline class to load the pre-trained model, and we pass the model_id 'google/ncsnpp-church-256' that has been trained for unconditional image synthesis tasks.\\\\n4. The model can then be used to generate an image of the church, which can be used for postcards.\\\\n5. Finally, store the generated image to a file called 'sde_ve_generated_image.png', which can be printed on the postcards.\\\", '<<<code>>>': \\\"from diffusers import DiffusionPipeline\\\\nmodel_id = 'google/ncsnpp-church-256'\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[sample]\\\\nimage[0].save('sde_ve_generated_image.png')\\\", 'api_call': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-church-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/ncsnpp-church-256', 'api_call': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-church-256')\\\", 'api_arguments': 'model_id', 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = google/ncsnpp-church-256\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[sample]\\\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and can generate high fidelity images of size 1024 x 1024.' </s>\"\n"}
{"text": "<s>[INST]سيقوم بتصميم نظام لتحليل اللغة الطبيعية للإجابة على الأسئلة، وسيحتاج إلى تقدير عمق الأشياء في النصوص التي يتم استخدامها كمدخلات.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. We then use the pipeline function to create a question-answering model, specifically for Chinese text.\\\\n3. We load the 'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large' model, which is a pre-trained model designed for machine reading comprehension tasks, and has a strong performance on Chinese text.\\\\n4. The resulting model can be used to answer questions based on the given context (paragraph) in Chinese.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\\nresult = qa_pipeline({'context': '提供的文字段落', 'question': '有关问题'})\\\", 'api_call': \\\"pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large', 'api_call': \\\"pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\", 'api_arguments': 'context, question', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\\\nresult = qa_pipeline({'context': 'your_context_here', 'question': 'your_question_here'})\\\", 'performance': {'dataset': 'Dureader-2021', 'accuracy': '83.1'}, 'description': 'A Chinese MRC roberta_wwm_ext_large model trained on a large amount of Chinese MRC data. This model has significantly improved performance on reading comprehension and classification tasks. It has helped multiple users achieve top 5 results in the Dureader-2021 competition.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى تطوير برنامج يستخدم تقنيات معالجة اللغة الطبيعية لتحسين التفاعل والمحادثات بين النظام والمستخدمين في تطبيق عميلنا لتحقيق تجربة مستخدم سلسة ومناسبة.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"pipeline('text-generation', model='PygmalionAI/pygmalion-2.7b')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We will use the Hugging Face Transformers library to load the PygmalionAI/pygmalion-2.7b model using the pipeline function for text-generation.\\\\n2. To create a chatbot for a medieval knight character, we will define the character's persona and attributes such as being brave, loyal, and following the code of chivalry.\\\\n3. For each user input message, we will format the input, including the character's persona, dialogue history, and user input message, according to the API's specifications.\\\\n4. The model will generate a response from the knight character in the context of the specified persona and the dialogue history.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nchatbot = pipeline(\\\\'text-generation\\\\', model=\\\\'PygmalionAI/pygmalion-2.7b\\\\')\\\\npersona = \\\"Knight\\\\'s Persona: A brave, loyal, and chivalrous medieval knight.\\\"\\\\ndialogue_history = \\\\'\\\\'  # Initialize an empty dialogue history\\\\nyou_message = \\\\'What do you think about honor?\\\\'\\\\nchat_input = f\\\"{persona}<START>{dialogue_history}You: {you_message}[Knight]:\\\"\\\\nresponse = chatbot(chat_input)', 'api_call': \\\"pipeline('text-generation', model='PygmalionAI/pygmalion-2.7b')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pygmalion-2.7b', 'api_call': \\\"pipeline('text-generation', model='PygmalionAI/pygmalion-2.7b')\\\", 'api_arguments': {'input_prompt': 'string'}, 'python_environment_requirements': [{'transformers': 'latest'}], 'example_code': \\\"model([CHARACTER]'s Persona: [A few sentences about the character you want the model to play]<START>[DIALOGUE HISTORY]You: [Your input message here][CHARACTER]:)\\\", 'performance': {'dataset': '56MB of dialogue data', 'accuracy': 'N/A'}, 'description': \\\"Pygmalion 2.7B is a proof-of-concept dialogue model based on EleutherAI's gpt-neo-2.7B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, including real and partially machine-generated conversations. The model is intended for use in generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\\\" </s>\"\n"}
{"text": "<s>[INST]يمكنك استخدام تقنيات معالجة اللغة الطبيعية لتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل تستطيع مساعدتك في هذا المجال؟? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='roberta-large')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a model for masked language modeling using 'roberta-large', which is a large, pretrained model that understands the context of English language data.\\\\n3. With this 'fill-mask' model, we can provide a partially written sentence containing a special <mask> token. The model will then use its knowledge of the language to suggest the best possible completion for the masked part of the sentence.\\\\n4. The created pipeline can be passed a partially written sentence with the <mask> token, and it will return a list of possible completions ranked by likelihood.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'roberta-large\\\\')\\\\ninput_text = \\\"The quick brown fox jumps over the <mask> dog.\\\"\\\\ncompletions = unmasker(input_text)', 'api_call': \\\"pipeline('fill-mask', model='roberta-large')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'roberta-large', 'api_call': \\\"pipeline('fill-mask', model='roberta-large')\\\", 'api_arguments': {'model': 'roberta-large'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='roberta-large')\\\\nunmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': 'GLUE', 'accuracy': {'MNLI': 90.2, 'QQP': 92.2, 'QNLI': 94.7, 'SST-2': 96.4, 'CoLA': 68.0, 'STS-B': 96.4, 'MRPC': 90.9, 'RTE': 86.6}}, 'description': 'RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using the Masked language modeling (MLM) objective. It can be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering.' </s>\"\n"}
{"text": "<s>[INST]يهدف العمل في هذا المجال إلى تطوير نماذج متعددة الوسائط للإجابة عن أسئلة عن الوثائق. ابدأ بفهم المفاهيم اللازمة ثم قم بإنشاء نموذج تصنيف الصور دون الاعتماد على بيانات معينة. هل ترغب في الحصول على مساعدة في تحقيق ذلك؟? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"LayoutLMv2ForQuestionAnswering.from_pretrained('dperales/layoutlmv2-base-uncased_finetuned_docvqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To load the pre-trained LayoutLMv2 model, first import the necessary class from the transformers package. In this case, it's LayoutLMv2ForQuestionAnswering.\\\\n2. Next, use the from_pretrained method to load the pre-trained model with the identifier 'dperales/layoutlmv2-base-uncased_finetuned_docvqa'. This is a document question answering model based on the LayoutLMv2 architecture and fine-tuned on the DocVQA dataset.\\\\n3. Once the model is loaded, you can use it to answer questions about the content of scanned documents by providing the document image and the corresponding question as input.\\\", '<<<code>>>': \\\"from transformers import LayoutLMv2ForQuestionAnswering\\\\nmodel = LayoutLMv2ForQuestionAnswering.from_pretrained('dperales/layoutlmv2-base-uncased_finetuned_docvqa')\\\", 'api_call': \\\"LayoutLMv2ForQuestionAnswering.from_pretrained('dperales/layoutlmv2-base-uncased_finetuned_docvqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dperales/layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \\\"LayoutLMv2ForQuestionAnswering.from_pretrained('dperales/layoutlmv2-base-uncased_finetuned_docvqa')\\\", 'api_arguments': {'model': 'dperales/layoutlmv2-base-uncased_finetuned_docvqa'}, 'python_environment_requirements': [{'transformers': 'latest'}], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for Document Question Answering based on the LayoutLMv2 architecture, fine-tuned on the DocVQA dataset.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتحويل صورة غير شرطية إلى صوت باللغة العربية. هل يمكنك مساعدتي في تحويل الصورة إلى مقطع صوتي؟? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the required 'diffusers' package.\\\\n2. Import the DDPMPipeline class from the diffusers package.\\\\n3. Use the from_pretrained method of the DDPMPipeline class to load the pre-trained model 'google/ddpm-bedroom-256'. This model has been trained for high-quality image synthesis, specifically for generating bedroom interior designs.\\\\n4. Once the model is loaded, generate an image using the loaded model. This image represents a computer-generated bedroom interior design.\\\\n5. Save the generated image to a file, such as 'ddpm_generated_image.png'.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\nddpm = DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\\nimage = ddpm().images[0]\\\\nimage.save('ddpm_generated_image.png')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/ddpm-bedroom-256', 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\", 'api_arguments': 'None', 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج لتحويل النص في مجال معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. We import the necessary classes from the transformers package. This includes AutoModelForSeq2SeqLM for the translation model and AutoTokenizer for tokenizing inputs.\\\\n2. We first load the Helsinki-NLP/opus-mt-en-de model by using the from_pretrained method of the AutoModelForSeq2SeqLM class. This model has been trained on a large corpus of text and can translate English text to German.\\\\n3. We create an instance of the AutoTokenizer, specifying the same Hugging Face model.\\\\n4. We use the tokenizer to convert the input English text into a format suitable for the model, and then use the model to generate the translated German text.\\\\n5. Finally, we tokenize and reconstruct the translated German text to make it human-readable.', '<<<code>>>': 'from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-de\\\\')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-de\\\\')\\\\ninput_text = \\\"response in English\\\"\\\\ninput_tokenized = tokenizer.prepare_seq2seq_batch(src_texts=[input_text], return_tensors=\\\\'pt\\\\')\\\\noutput_tokenized = model.generate(**input_tokenized)\\\\ntranslated_text = tokenizer.batch_decode(output_tokenized, skip_special_tokens=True)[0]', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-en-de', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'Helsinki-NLP/opus-mt-en-de'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'performance': {'dataset': 'newstest2018-ende.en.de', 'accuracy': {'BLEU': 45.2, 'chr-F': 0.69}}, 'description': 'The Helsinki-NLP/opus-mt-en-de model is a translation model developed by the Language Technology Research Group at the University of Helsinki. It translates English text to German using the Hugging Face Transformers library. The model is trained on the OPUS dataset and has a BLEU score of 45.2 on the newstest2018-ende.en.de dataset.' </s>\"\n"}
{"text": "<s>[INST]قم بتحويل النص التالي إلى صوت: \\\"أريدك أن تولد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية لاستخدامها في تصنيف الصور بواسطة رؤية الحاسوب. هل يمكنك مساعدتي في هذا؟\\\"? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur')\\\", 'api_provider': 'Fairseq', 'explanation': \\\"1. Import the necessary libraries, such as 'fairseq', 'hub_utils', 'huggingface_hub', 'IPython.display', and 'torchaudio'.\\\", 'code': 'from fairseq import hub_utils, checkpoint_utils\\\\nfrom fairseq.models.speech_to_text import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur\\\\')\\\\nmodel = models[0].cpu()\\\\naudio_file_path = \\\"path_to_english_speech_input.wav\\\"\\\\naudio, _ = torchaudio.load(audio_file_path)\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\n\\\\n# Get the target language code, e.g., \\\\'es\\\\' for Spanish, \\\\'fr\\\\' for French, or \\\\'it\\\\' for Italian\\\\ntarget_language_code = \\\"es\\\"\\\\nunit = S2THubInterface.get_prediction(task, model, generator, sample, target_language_code)\\\\n\\\\nvocoder_dir = snapshot_download(\\\"facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur\\\")\\\\nvocoder, vocoder_cfg = CodeHiFiGANVocoder.from_pretrained(vocoder_dir)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\ntranslated_speech, sample_rate = tts_model.get_prediction(tts_sample)', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Fairseq', 'functionality': 'Speech-to-speech translation', 'api_name': 'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur', 'api_call': 'from fairseq import hub_utils, checkpoint_utils\\\\nfrom fairseq.models.speech_to_text import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur\\\\')\\\\nmodel = models[0].cpu()\\\\naudio_file_path = \\\"path_to_english_speech_input.wav\\\"\\\\naudio, _ = torchaudio.load(audio_file_path)\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\n\\\\n# Get the target language code, e.g., \\\\'es\\\\' for Spanish, \\\\'fr\\\\' for French, or \\\\'it\\\\' for Italian\\\\ntarget_language_code = \\\"es\\\"\\\\nunit = S2THubInterface.get_prediction(task, model, generator, sample, target_language_code)\\\\n\\\\nvocoder_dir = snapshot_download(\\\"facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur\\\")\\\\nvocoder, vocoder_cfg = CodeHiFiGANVocoder.from_pretrained(vocoder_dir)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\ntranslated_speech, sample_rate = tts_model.get_prediction(tts_sample)', 'api_arguments': {'audio': '16000Hz mono channel audio'}, 'python_environment_requirements': ['fairseq', 'hub_utils', 'huggingface_hub', 'IPython.display', 'torchaudio'], 'example_code': ['import json', 'import os', 'from pathlib import Path', 'import IPython.display as ipd', 'from fairseq import hub_utils', 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub', 'from fairseq.models.speech_to_text.hub_interface import S2THubInterface', 'from fairseq.models.text_to_speech import CodeHiFiGANVocoder', 'from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface', 'from huggingface_hub import snapshot_download', 'import torchaudio', 'cache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)', 'models, cfg, task = load_model_ensemble_and_task_from_hf_hub(', 'facebook/xm_transformer_s2ut_800m-es-en-st-asr-bt_h1_2022,', 'arg_overrides={config_yaml: config.yaml, task: speech_to_text},', 'cache_dir=cache_dir,', ')', 'model = models[0].cpu()', 'cfg[task].cpu = True', 'generator = task.build_generator([model], cfg)', '# requires 16000Hz mono channel audio', 'audio, _ = torchaudio.load(/Users/lpw/git/api-inference-community/docker_images/fairseq/tests/samples/sample2.flac)', 'sample = S2THubInterface.get_model_input(task, audio)', 'unit = S2THubInterface.get_prediction(task, model, generator, sample)', 'library_name = fairseq', 'cache_dir = (', ' cache_dir or (Path.home() / .cache / library_name).as_posix()', ')', 'cache_dir = snapshot_download(', ' ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name', ')', 'x = hub_utils.from_pretrained(', ' cache_dir,', ' model.pt,', ' .,', ' archive_map=CodeHiFiGANVocoder.hub_models(),', ' config_yaml=config.json,', ' fp16=False,', ' is_vocoder=True,', ')', \\\"with open(f{x['args']['data']}/config.json) as f:\\\", ' vocoder_cfg = json.load(f)', 'assert (', ' len(x[args][model_path]) == 1', '), Too many vocoder models in the input', 'vocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)', 'tts_model = VocoderHubInterface(vocoder_cfg, vocoder)', 'tts_sample = tts_model.get_model_input(unit)', 'wav, sr = tts_model.get_prediction(tts_sample)', 'ipd.Audio(wav, rate=sr)'], 'performance': {'dataset': 'covost2', 'accuracy': None}, 'description': 'Speech-to-speech translation model from fairseq S2UT (paper/code) for Spanish-English. Trained on mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتحميل مجموعة البيانات المناسبة للتدريب من مصدر موثوق على جهاز الكمبيوتر الخاص بك. بعد ذلك، قم بتحليل الصور البصرية باستخدام نموذج الرؤية الحاسوبية Image-to-Image لتحويل الصور من نوع إلى آخر. يرجى ضبط معلمات النموذج بحيث تحقق أفضل أداء ممكن. في النهاية، قم بتقييم أداء النموذج وضبطه حسب الحاجة.? [/INST]{'domain': 'Computer Vision Image-to-Image', 'api_call': \\\"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV5')\\\", 'api_provider': 'Hugging Face', 'explanation': \\\"1. Import the 'pipeline' function from the transformers library, which is provided by Hugging Face.\\\", 'code': \\\"from transformers import pipeline\\\\nimage_transformer = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV5')\\\\ninput_image_path = 'path/to/image/file'\\\\noutput_image = image_transformer(input_image=input_image_path)[0]\\\", 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'GreeneryScenery/SheepsControlV5', 'api_call': \\\"from transformers import pipeline\\\\nimage_transformer = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV5')\\\\ninput_image_path = 'path/to/image/file'\\\\noutput_image = image_transformer(input_image=input_image_path)[0]\\\", 'api_arguments': {'input_image': 'path/to/image/file'}, 'python_environment_requirements': {'huggingface_hub': '>=0.0.17', 'transformers': '>=4.13.0', 'torch': '>=1.10.0'}, 'example_code': '', 'performance': {'dataset': 'poloclub/diffusiondb', 'accuracy': 'Not provided'}, 'description': 'SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.'}} </s>\"\n"}
{"text": "<s>[INST]الرجاء الاستفادة من واجهة برمجة التطبيقات التي تتعلق بتحليل اللغة الطبيعية لترجمة النصوص. يمكنك اتباع التعليمات الواردة لتحويل النصوص إلى لغة أخرى، ثم يمكنك إنشاء تسلسل لتوليد ترجمة دقيقة وفعالة. يُرجى اتباع الخطوات الواردة في الواجهة لتنفيذ هذا العمل.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"MarianMTModel.from_pretrained('pytorch-models/opus-mt-tc-big-en-pt')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import MarianMTModel, MarianTokenizer\\\\nsrc_text = [\\\"&gt;&gt;por&lt;&lt; O produto não está funcionando corretamente.\\\"] # Replace with the actual customer question\\\\nmodel_name = \\\\'pytorch-models/opus-mt-tc-big-en-pt\\\\'\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\\\\'pt\\\\', padding=True))\\\\ntranslation = tokenizer.decode(translated[0], skip_special_tokens=True)', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Neural machine translation', 'api_name': 'opus-mt-tc-big-en-pt', 'api_call': 'from transformers import MarianMTModel, MarianTokenizer\\\\nsrc_text = [\\\"&gt;&gt;por&lt;&lt; O produto não está funcionando corretamente.\\\"] # Replace with the actual customer question\\\\nmodel_name = \\\\'pytorch-models/opus-mt-tc-big-en-pt\\\\'\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\\\\'pt\\\\', padding=True))\\\\ntranslation = tokenizer.decode(translated[0], skip_special_tokens=True)', 'api_arguments': {'src_text': 'list of text strings with language tokens'}, 'python_environment_requirements': {'transformers': '4.16.2'}, 'example_code': 'from transformers import MarianMTModel, MarianTokenizer\\\\nsrc_text = [\\\\n &gt;&gt;por&lt;&lt; Tom tried to stab me.,\\\\n &gt;&gt;por&lt;&lt; He has been to Hawaii several times.\\\\n]\\\\nmodel_name = pytorch-models/opus-mt-tc-big-en-pt\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\ntranslated = model.generate(**tokenizer(src_text, return_tensors=pt, padding=True))\\\\nfor t in translated:\\\\n print( tokenizer.decode(t, skip_special_tokens=True) )', 'performance': {'dataset': [{'name': 'flores101-devtest', 'accuracy': 50.4}, {'name': 'tatoeba-test-v2021-08-07', 'accuracy': 49.6}]}, 'description': 'Neural machine translation model for translating from English (en) to Portuguese (pt). This model is part of the OPUS-MT project, an effort to make neural machine translation models widely available and accessible for many languages in the world.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتصميم نظم رؤية حاسوبية لتصنيف الصور، ونحتاج إلى تحديد عمق الأشياء في الصور التي تتم التصنيف عليها.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include 'AutoFeatureExtractor', 'RegNetForImageClassification', and 'load_dataset'.\\\\n2. Load the dataset containing images of kitchen appliances.\\\\n3. Use the 'AutoFeatureExtractor.from_pretrained' and 'RegNetForImageClassification.from_pretrained' methods to create a feature extractor and RegNet model.\\\\n4. Process an image of a kitchen appliance using the feature extractor and feed it into the RegNet model.\\\\n5. Retrieve the model's prediction and use it to categorize the kitchen appliance in question.\\\", '<<<code>>>': \\\"from transformers import AutoFeatureExtractor, RegNetForImageClassification\\\\nfrom PIL import Image\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset('some_dataset_with_kitchen_appliances') \\\\nimage = Image.open('kitchen_appliance_image.jpg')\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('zuppif/regnet-y-040')\\\\nmodel = RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/regnet-y-008', 'api_call': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'zuppif/regnet-y-040'}, 'python_environment_requirements': [{'transformers': 'AutoFeatureExtractor, RegNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}], 'example_code': 'from transformers import AutoFeatureExtractor, RegNetForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset(huggingface/cats-image)\\\\nimage = dataset[test][image][0]\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\\\ninputs = feature_extractor(image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n... logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج تصنيف جدولي باستخدام بيانات الدوري الطلابي. استخدم خوارزمية تعلم آلي لتدريب النموذج على تصنيف الفرق الطلابية حسب النتائج والإحصائيات.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. Import joblib to load the trained model and pandas for handling data.\\\\n2. Load the pre-trained carbon emissions classification model using joblib.load() function and store it in a variable (e.g., model).\\\\n3. Read a CSV file containing the features for which the predictions are to be made using pandas. This can include factors such as the manufacturing process, materials used, and transportation variables.\\\\n4. Process the input data according to the model's configuration and extract the required features.\\\\n5. Use the trained model to predict the carbon emissions for the given product features, which will help guide the creation of more environmentally friendly products.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\ndata = pd.read_csv('product_features.csv')\\\\nprocessed_data = preprocess_data(data)  # Replace preprocess_data with the actual preprocessing function\\\\npredictions = model.predict(processed_data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tejas23/autotrain-amx2-1702259725', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': {'accuracy': 0.827}}, 'description': 'Multi-class Classification Model for Carbon Emissions' </s>\"\n"}
{"text": "<s>[INST]يقوم فريق البحث بتطوير نموذج لتصنيف الصور بدون تدريب مسبق في مجال رؤية الحاسوب.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library for this task, which is 'pipeline' from transformers.\\\\n2. Initialize the zero-shot image classification pipeline using the 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k' model.\\\\n3. Use the initialized classifier for classifying the product images by providing the image and relevant category labels (e.g., 'shirt', 'jeans', 'dress') as input.\\\\n4. The model will return the most probable category predictions for the given product image.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\\\\nimage_path = 'path/to/fashion_product_image.jpg'\\\\ncategories = ['shirt', 'jeans', 'dress', 'shoes', 'accessories', 'jacket']\\\\n# possible fashion product categories\\\\nresult = image_classifier(image_path, possible_class_names=categories)\\\", 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k', 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\\\", 'api_arguments': ['image', 'possible_class_names'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'); classifier(image, possible_class_names=['cat', 'dog'])\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '80.1'}, 'description': 'A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. The model is intended for research purposes and enables researchers to better understand and explore zero-shot, arbitrary image classification. It can be used for interdisciplinary studies of the potential impact of such models. The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k.' </s>\"\n"}
{"text": "<s>[INST]يمكن للمستخدمين استخدام تقنيات معالجة اللغة الطبيعية لتصنيف النصوص بدون تدريب في مجال معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required transformer package, which includes the 'pipeline' function.\\\\n2. Load the model from the Hugging Face model hub using the 'pipeline' function, and provide the model name 'svalabs/gbert-large-zeroshot-nli'.\\\\n3. The model is designed for zero-shot text classification, which means it can classify text into labels that it has not seen before.\\\\n4. Provide the German text and the labels including the urgency labels as input to the model.\\\\n5. The model will return the classification results that indicate the theme of the text and whether it is urgent or not.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsequence = \\\"Ich habe ein Problem mit meinem Iphone das so schnell wie möglich gelöst werden muss\\\"\\\\nlabels = [\\\"Computer\\\", \\\"Handy\\\", \\\"Tablet\\\", \\\"dringend\\\", \\\"nicht dringend\\\"]\\\\nhypothesis_template = \\\"In diesem Satz geht es um das Thema {}.\\\"\\\\nzero_shot_pipeline = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'svalabs/gbert-large-zeroshot-nli\\\\')\\\\nresult = zero_shot_pipeline(sequence, labels, hypothesis_template=hypothesis_template)', 'api_call': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'svalabs/gbert-large-zeroshot-nli', 'api_call': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", 'api_arguments': ['sequence', 'labels', 'hypothesis_template'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\nzershot_pipeline = pipeline(zero-shot-classification, model=svalabs/gbert-large-zeroshot-nli)\\\\nsequence = Ich habe ein Problem mit meinem Iphone das so schnell wie möglich gelöst werden muss\\\\nlabels = [Computer, Handy, Tablet, dringend, nicht dringend]\\\\nhypothesis_template = In diesem Satz geht es um das Thema {}. ', 'performance': {'dataset': 'XNLI TEST-Set', 'accuracy': '85.6%'}, 'description': 'A German zeroshot classification model based on the German BERT large model from deepset.ai and finetuned for natural language inference using machine-translated nli sentence pairs from mnli, anli, and snli datasets.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتصنيف الصوتيات، ونرغب في تحسين أداء النموذج في التعرف على الأصوات المختلفة.? [/INST]'<<<domain>>>': 'Audio Classification', '<<<api_call>>>': \\\"HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, install the required libraries, including audio_models, transformers, torch, numpy, and pydub.\\\\n2. Load the HubertForSpeechClassification model using the from_pretrained method with the provided model name 'Rajaram1996/Hubert_emotion'.\\\\n3. Also load the feature extractor 'facebook/hubert-base-ls960' to transform the audio input into suitable format for the model.\\\\n4. Create a function to convert an audio file to numpy array with the specified sampling rate.\\\\n5. Use the feature extractor to process the audio array and generate input tensors for the model.\\\\n6. Pass the tensors into the model to get the emotion predictions along with their confidence scores.\\\\n7. You can now analyze the detected emotions from the user's voice input while interacting with the voice assistant.\\\", '<<<code>>>': 'from audio_models import HubertForSpeechClassification\\\\nfrom transformers import Wav2Vec2FeatureExtractor, AutoConfig\\\\nimport torch\\\\nimport numpy as np\\\\nfrom pydub import AudioSegment\\\\n\\\\naudio_file = \\\\'voice_input.wav\\\\' # replace this with the path to your voice input file\\\\npath = \\\\'voice_input.wav\\\\'\\\\nmodel = HubertForSpeechClassification.from_pretrained(\\\\'Rajaram1996/Hubert_emotion\\\\')\\\\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\\\\'facebook/hubert-base-ls960\\\\')\\\\n\\\\nsampling_rate = 16000\\\\n\\\\ndef speech_file_to_array(path, sampling_rate):\\\\n    sound = AudioSegment.from_file(path)\\\\n    sound = sound.set_frame_rate(sampling_rate)\\\\n    sound_array = np.array(sound.get_array_of_samples())\\\\n    return sound_array\\\\n\\\\nsound_array = speech_file_to_array(audio_file, sampling_rate)\\\\ninputs = feature_extractor(sound_array, sampling_rate=sampling_rate, return_tensors=\\\\'pt\\\\', padding=True)\\\\ninputs = {key: inputs[key].to(torch.device(\\\"cpu\\\")).float() for key in inputs}\\\\n\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\nscores = torch.nn.functional.softmax(logits, dim=1).detach().cpu().numpy()[0]', 'api_call': \\\"HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Rajaram1996/Hubert_emotion', 'api_call': \\\"HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')\\\", 'api_arguments': {'audio_file': 'string'}, 'python_environment_requirements': [['audio_models', 'transformers', 'torch', 'numpy', 'pydub']], 'example_code': \\\"def predict_emotion_hubert(audio_file):\\\\n from audio_models import HubertForSpeechClassification\\\\n from transformers import Wav2Vec2FeatureExtractor, AutoConfig\\\\n import torch.nn.functional as F\\\\n import torch\\\\n import numpy as np\\\\n from pydub import AudioSegment\\\\nmodel = HubertForSpeechClassification.from_pretrained(Rajaram1996/Hubert_emotion)\\\\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(facebook/hubert-base-ls960)\\\\nsampling_rate=16000\\\\nconfig = AutoConfig.from_pretrained(Rajaram1996/Hubert_emotion)\\\\ndef speech_file_to_array(path, sampling_rate):\\\\n sound = AudioSegment.from_file(path)\\\\n sound = sound.set_frame_rate(sampling_rate)\\\\n sound_array = np.array(sound.get_array_of_samples())\\\\n return sound_array\\\\nsound_array = speech_file_to_array(audio_file, sampling_rate)\\\\ninputs = feature_extractor(sound_array, sampling_rate=sampling_rate, return_tensors=pt, padding=True)\\\\ninputs = {key: inputs[key].to(cpu).float() for key in inputs}\\\\nwith torch.no_grad():\\\\n logits = model(**inputs).logits\\\\nscores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\\\\noutputs = [{\\\\n emo: config.id2label[i],\\\\n score: round(score * 100, 1)}\\\\n for i, score in enumerate(scores)\\\\n]\\\\nreturn [row for row in sorted(outputs, key=lambda x:x[score], reverse=True) if row['score'] != '0.0%'][:2]\\\\nresult = predict_emotion_hubert(male-crying.mp3)\\\\nresult\\\", 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'A pretrained model for predicting emotion in local audio files using Hubert.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج جديد لتوجيه اللغة الطبيعية لإنشاء قصة قصيرة تبدأ بالجملة \\\"كان ياما كان في أرض بعيدة\\\". الجملة يمكن أن تحتوي على إبداع في توليف الكلمات وفهم سياقها لإنتاج نص مميز ومفهوم، بما يتناسب مع مجال معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('summarization', model='lidiya/bart-large-xsum-samsum')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required library pipeline from transformers package.\\\\n2. Initialize the summarizer with the pipeline function that uses the 'lidiya/bart-large-xsum-samsum' model for abstractive summarization.\\\\n3. Pass the conversation text into the summarizer to get the summary.\\\\n4. The summarizer produces a concise summary of the main points without losing the essence of the conversation.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'lidiya/bart-large-xsum-samsum\\\\')\\\\nconversation = \\\"Hey, how are you? Long time no see. I just moved to a new house. I really love the place. It\\\\'s bigger than my previous house and much closer to work. I get to save on commute expenses too. Last week, I got a raise at work for my excellent performance. I can finally take a vacation. I was thinking about visiting Italy during the summertime, and I even booked a flight for it. I hope everything goes well, and I can\\\\'t wait to try their delicious foods.\\\"\\\\nsummary = summarizer(conversation)\\\\nprint(summary[0][\\\\'summary_text\\\\'])', 'api_call': \\\"pipeline('summarization', model='lidiya/bart-large-xsum-samsum')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lidiya/bart-large-xsum-samsum', 'api_call': \\\"pipeline('summarization', model='lidiya/bart-large-xsum-samsum')\\\", 'api_arguments': 'conversation', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=lidiya/bart-large-xsum-samsum)\\\\nconversation = '''Hannah: Hey, do you have Betty's number?\\\\nAmanda: Lemme check\\\\nAmanda: Sorry, can't find it.\\\\nAmanda: Ask Larry\\\\nAmanda: He called her last time we were at the park together\\\\nHannah: I don't know him well\\\\nAmanda: Don't be shy, he's very nice\\\\nHannah: If you say so..\\\\nHannah: I'd rather you texted him\\\\nAmanda: Just text him 🙂\\\\nHannah: Urgh.. Alright\\\\nHannah: Bye\\\\nAmanda: Bye bye <br />\\\\n'''\\\\nsummarizer(conversation)\\\", 'performance': {'dataset': 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization', 'accuracy': {'rouge1': 53.306, 'rouge2': 28.355, 'rougeL': 44.095}}, 'description': 'This model was obtained by fine-tuning facebook/bart-large-xsum on Samsum dataset.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير منتج متعدد الوسائط للإجابة على أسئلة الوثائق، ونحن بحاجة لاستخدام البيانات المتوفرة لتنبؤ بالإجابات المناسبة.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the 'jinhybr/OCR-DocVQA-Donut' model for the 'document-question-answering' task.\\\\n3. Provide an image file path and a relevant question, such as retrieving the report title.\\\\n4. The loaded model will analyze the image and generate a response based on the question.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ndoc_vqa = pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\\nimage_path = 'path/to/report_image.jpg'\\\\nquestion = 'What is the title of the report?'\\\\nanswer = doc_vqa(image_path=image_path, question=question)\\\\nprint(answer['answer'])\\\", 'api_call': \\\"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': \\\"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\", 'api_arguments': 'image_path, question', 'python_environment_requirements': ['transformers'], 'example_code': \\\"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\\\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج معالجة لغوية طبيعية لإنشاء ملخصات قصيرة للنصوص باستخدام تقنيات معالجة اللغات الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-arxiv')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library, specifically BigBirdPegasusForConditionalGeneration for the summarization model, and AutoTokenizer for processing text input.\\\\n2. Create a tokenizer instance using the AutoTokenizer.from_pretrained() method with 'google/bigbird-pegasus-large-arxiv' as the model.\\\\n3. Load the pre-trained BigBirdPegasus model using the BigBirdPegasusForConditionalGeneration.from_pretrained() method with 'google/bigbird-pegasus-large-arxiv'.\\\\n4. Use the tokenizer to tokenize the input scientific paper text.\\\\n5. Pass the tokenized inputs to the model's generate() method to generate a summary.\\\\n6. Decode the generated summary tokens back into human-readable text.\\\", '<<<code>>>': 'from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'google/bigbird-pegasus-large-arxiv\\\\')\\\\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(\\\\'google/bigbird-pegasus-large-arxiv\\\\')\\\\ntext = \\\"<Scientific_Paper_Text>\\\"\\\\ninputs = tokenizer(text, return_tensors=\\\\'pt\\\\')\\\\nprediction = model.generate(**inputs)\\\\nsummary = tokenizer.batch_decode(prediction)[0]', 'api_call': \\\"BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-arxiv')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/bigbird-pegasus-large-arxiv', 'api_call': \\\"BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-arxiv')\\\", 'api_arguments': ['attention_type', 'block_size', 'num_random_blocks'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(google/bigbird-pegasus-large-arxiv)\\\\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(google/bigbird-pegasus-large-arxiv)\\\\ntext = Replace me by any text you'd like.\\\\ninputs = tokenizer(text, return_tensors='pt')\\\\nprediction = model.generate(**inputs)\\\\nprediction = tokenizer.batch_decode(prediction)\\\", 'performance': {'dataset': 'scientific_papers', 'accuracy': {'ROUGE-1': 36.028, 'ROUGE-2': 13.417, 'ROUGE-L': 21.961, 'ROUGE-LSUM': 29.648}}, 'description': \\\"BigBird, is a sparse-attention based transformer which extends Transformer based models, such as BERT to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle. BigBird was introduced in this paper and first released in this repository. BigBird relies on block sparse attention instead of normal attention (i.e. BERT's attention) and can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts.\\\" </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لتوليد النصوص من بيانات اللغة الطبيعية باستخدام تقنيات معالجة اللغة الطبيعية.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-large-ntp-py')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the AutoTokenizer and T5ForConditionalGeneration classes from the transformers package.', 'code': 'from transformers import AutoTokenizer, T5ForConditionalGeneration\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Salesforce/codet5-large-ntp-py\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'Salesforce/codet5-large-ntp-py\\\\')\\\\ntext = \\\"Generate a Python code snippet to implement iterator pattern for a simple list.\\\"\\\\ninput_ids = tokenizer(text, return_tensors=\\\\'pt\\\\').input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\ncode_snippet = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\\\\nprint(code_snippet)', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'codet5-large-ntp-py', 'api_call': 'from transformers import AutoTokenizer, T5ForConditionalGeneration\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Salesforce/codet5-large-ntp-py\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'Salesforce/codet5-large-ntp-py\\\\')\\\\ntext = \\\"Generate a Python code snippet to implement iterator pattern for a simple list.\\\"\\\\ninput_ids = tokenizer(text, return_tensors=\\\\'pt\\\\').input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\ncode_snippet = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\\\\nprint(code_snippet)', 'api_arguments': ['text', 'return_tensors', 'input_ids', 'max_length'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, T5ForConditionalGeneration\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codet5-large-ntp-py)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(Salesforce/codet5-large-ntp-py)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'APPS benchmark', 'accuracy': 'See Table 5 of the paper'}, 'description': 'CodeT5 is a family of encoder-decoder language models for code from the paper: CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation by Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. The checkpoint included in this repository is denoted as CodeT5-large-ntp-py (770M), which is introduced by the paper: CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning by Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتنفيذ تدريبات نموذج معالجة اللغة الطبيعية الخاص بك لتحويل النص إلى نص بإستخدام مجموعة البيانات المتعددة الوسائط الخاصة بنا. استخدم النصوص الأساسية والنصوص المولدة معًا لتحسين الأداء. جرب تدريب نماذجك لفهم كيفية تحويل النصوص بشكل مثلى. استمتع بتحسين أداء نماذجك وتعلم كيفية التفاعل بشكل أفضل بين النصوص الأساسية والمولدة في مهمة تحويل النص.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary libraries, including the BlenderbotForConditionalGeneration class from transformers.\\\\n2. Utilize the from_pretrained method to load the model 'facebook/blenderbot_small-90M'.\\\\n3. The loaded model is specialized for generating conversations and will be used as the backbone of your chatbot.\\\\n4. Receive user input (questions or comments), and pass them to the chatbot.\\\\n5. The chatbot will generate appropriate responses that can be sent back to the user.\\\", '<<<code>>>': 'from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\\\\nmodel = BlenderbotForConditionalGeneration.from_pretrained(\\\\'facebook/blenderbot_small-90M\\\\')\\\\ntokenizer = BlenderbotTokenizer.from_pretrained(\\\\'facebook/blenderbot_small-90M\\\\')\\\\ndef generate_response(message):\\\\n    inputs = tokenizer([message], return_tensors=\\\\'pt\\\\')\\\\n    reply_ids = model.generate(**inputs)\\\\n    response = tokenizer.batch_decode(reply_ids)[0].replace(\\\"<s>\\\", \\\"\\\").replace(\\\"</s>\\\", \\\"\\\").strip()\\\\n    return response', 'api_call': \\\"BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/blenderbot_small-90M', 'api_call': \\\"BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')\\\", 'api_arguments': ['message'], 'python_environment_requirements': [['transformers']], 'example_code': 'Input a message to start chatting with facebook/blenderbot_small-90M.', 'performance': {'dataset': 'blended_skill_talk', 'accuracy': 'Not provided'}, 'description': 'Blenderbot is a chatbot model that provides engaging talking points and listens to their partners, both asking and answering questions, and displaying knowledge, empathy, and personality appropriately, depending on the situation.' </s>\"\n"}
{"text": "<s>[INST]تقوم الشركة ببناء نموذج لتقنية معالجة اللغة الطبيعية لتحويل النصوص إلى نصوص أخرى، وتهدف الى التنبؤ بالنصوص وتصنيفها بدون تدريب مسبق من خلال البيانات المتاحة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To achieve language translation for multiple languages, import the M2M100ForConditionalGeneration class from the transformers package.\\\\n2. Load the 'facebook/m2m100_1.2B' model using M2M100ForConditionalGeneration's `from_pretrained` method. This model is designed specifically for many-to-many multilingual translation, and it can directly translate between the 9,900 directions of 100 languages.\\\\n3. Also, import and load the M2M100Tokenizer from the transformers package.\\\\n4. Set the source language of the text in the tokenizer.\\\\n5. Encode the source text using the tokenizer and generate the translated tokens using the model.\\\\n6. Finally, decode the generated tokens back into human-readable form using the tokenizer, specifying the target language.\\\", '<<<code>>>': 'from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\\\nsrc_text = \\\"Enter your source text here\\\"\\\\nsrc_lang = \\\"en\\\"  # Replace with the source language code\\\\ntgt_lang = \\\"fr\\\"  # Replace with the target language code\\\\n\\\\nmodel = M2M100ForConditionalGeneration.from_pretrained(\\\\'facebook/m2m100_1.2B\\\\')\\\\ntokenizer = M2M100Tokenizer.from_pretrained(\\\\'facebook/m2m100_1.2B\\\\')\\\\ntokenizer.src_lang = src_lang\\\\n\\\\nencoded_src = tokenizer(src_text, return_tensors=\\\\'pt\\\\')\\\\ngenerated_tokens = model.generate(**encoded_src, forced_bos_token_id=tokenizer.get_lang_id(tgt_lang))\\\\ntgt_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]', 'api_call': \\\"M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/m2m100_1.2B', 'api_call': \\\"M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\\\", 'api_arguments': {'encoded_input': 'encoded text', 'forced_bos_token_id': 'target language id'}, 'python_environment_requirements': ['sentencepiece'], 'example_code': 'from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\\\nhi_text = chinese_text = model = M2M100ForConditionalGeneration.from_pretrained(facebook/m2m100_1.2B)\\\\ntokenizer = M2M100Tokenizer.from_pretrained(facebook/m2m100_1.2B)\\\\ntokenizer.src_lang = hi\\\\nencoded_hi = tokenizer(hi_text, return_tensors=pt)\\\\ngenerated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(fr))\\\\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)', 'performance': {'dataset': 'M2M100', 'accuracy': 'Not specified'}, 'description': 'M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نموذج للإجابة على الأسئلة المرئية متعددة الوسائط، ونحتاج إلى تجديد التواصل ببناء أنظمة لغوية قادرة على فهم الحوار بين الأفراد.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library.\\\\n2. Use the pipeline function to create a visual question answering model.\\\\n3. Load the model 'ivelin/donut-refexp-combined-v1'. This model is trained for multimodal tasks that involve both images and text, such as visual question answering.\\\\n4. Given an image and a corresponding question (e.g., about hazardous materials or their colors), the model can be used to analyze the image and provide an answer based on the visual content in the context of the question.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nvqa = pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\\\\nimage_path = 'warehouse_image.jpg'\\\\nquestion = 'What is the color of the hazardous material?'\\\\nanswer = vqa(image=image_path, question=question)\\\", 'api_call': \\\"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ivelin/donut-refexp-combined-v1', 'api_call': \\\"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\\\", 'api_arguments': 'image, question', 'python_environment_requirements': ['transformers'], 'example_code': \\\"vqa(image='path/to/image.jpg', question='What is the color of the object?')\\\", 'performance': {'dataset': 'ivelin/donut-refexp-combined-v1', 'accuracy': 'N/A'}, 'description': 'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.' </s>\"\n"}
{"text": "<s>[INST]نظرًا لاهتمامي بمجال تحويل النصوص إلى كلام، والرغبة في تحسين جودة الصوت المتولد من النص المكتوب، أود أن اطلب منك توليد تعليمات يمكنها مساعدتي في تحسين جودة الإخراج الصوتي بدون الحاجة إلى عينات صوتية كبيرة. هل يمكنك مساعدتي في تحقيق ذلك؟? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': 'run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori', 'api_provider': 'ESPnet', 'explanation': '1. The audio text-to-speech processing is done using the ESPnet framework, which provides functionality to convert text to speech.', 'code': \\\"Install required libraries: pip install espnet2 huggingface_hub\\\\nRun the following command in your terminal: ./run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori\\\\n# Use the downloaded model and ESPnet framework for text-to-speech conversion, following ESPnet's documentation and examples.\\\", 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'ESPnet', 'functionality': 'Text-to-Speech', 'api_name': 'mio/tokiwa_midori', 'api_call': \\\"Install required libraries: pip install espnet2 huggingface_hub\\\\nRun the following command in your terminal: ./run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori\\\\n# Use the downloaded model and ESPnet framework for text-to-speech conversion, following ESPnet's documentation and examples.\\\", 'api_arguments': ['espnet', 'mio/tokiwa_midori', 'Text2Speech', 'config', 'model_path'], 'python_environment_requirements': ['espnet2', 'huggingface_hub'], 'example_code': './run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori', 'performance': {'dataset': 'amadeus', 'accuracy': 'Not provided'}, 'description': 'This model was trained by mio using amadeus recipe in espnet.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتحويل النصوص إلى كلام مسموع في مجال تقنية الصوت، حيث نهدف إلى إنشاء نماذج لتحويل النصوص إلى كلام مسموع بشكل آلي باستخدام تقنيات متقدمة، ونتطلع إلى تحسين أداء هذه النماذج من خلال تجميع واستخدام مجموعات بيانات متنوعة ومتاحة.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\\\", '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'Text2Speech' from espnet2.bin.tts_inference and 'soundfile' for handling audio files.\\\\n2. Use the 'Text2Speech.from_pretrained()' function to load the specified pre-trained model for Chinese language text-to-speech conversion.\\\\n3. The loaded model is then used to convert the input text (usually a Chinese article content) into speech.\\\\n4. Save the generated speech as an audio file (e.g., PCM 16-bit format) with 'soundfile.write()' function.\\\\n**Note**: This model is specific to the Chinese language. Replace it with a suitable model for other languages.\\\\n5. The generated audio can then be integrated into the website for visually impaired visitors to enjoy audible content.\\\", '<<<code>>>': \\\"import soundfile\\\\nfrom espnet2.bin.tts_inference import Text2Speech\\\\n\\\\ntext2speech = Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\\\\ntext = 'Chinese article content here...'\\\\nspeech = text2speech(text)['wav']\\\\nsoundfile.write('out.wav', speech.numpy(), text2speech.fs, 'PCM_16')\\\", 'api_call': \\\"Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\\\", 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best', 'api_call': \\\"Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['torch', 'espnet_model_zoo']], 'example_code': 'import soundfile\\\\nfrom espnet2.bin.tts_inference import Text2Speech\\\\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\\\\ntext = 春江潮水连海平，海上明月共潮生\\\\nspeech = text2speech(text)[wav]\\\\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)', 'performance': {'dataset': 'csmsc', 'accuracy': 'Not specified'}, 'description': 'A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج للتصنيف الصوري الخاص بالرؤية الحاسوبية \\\"Computer Vision\\\" بدون توجيه \\\"Zero-Shot Image Classification\\\"، وتهدف هذه الخدمة إلى تحليل الصور وتصنيفها بشكل تلقائي بناءً على الفئات الموجودة وذلك دون الحاجة إلى بيانات تدريبية مسبقة.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary function 'pipeline' from the transformers package.\\\\n2. Create an image classifier by calling the 'pipeline' function and specifying the 'image-classification' task and model name 'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft'.\\\\n3. Provide the image file path and a list of animal classes you want to recognize in the image.\\\\n4. The model will return the predicted class with its confidence percentage.\\\\n5. You can display the results in the app for kids to learn about the recognized animal.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nclassifier = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\\\\nimage_path = 'path/to/animal_image.jpg'\\\\nanimal_classes = ['elephant', 'lion', 'giraffe', 'zebra', 'rhinoceros']\\\\nresult = classifier(image_path, animal_classes)\\\", 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft', 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\\\", 'api_arguments': {'image_path': 'Path to the image file', 'class_names': 'List of comma-separated class names'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft'); classifier('path/to/image.jpg', ['class1', 'class2'])\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '75.9-76.9%'}, 'description': 'A series of CLIP ConvNeXt-Large models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models achieve between 75.9 and 76.9 top-1 zero-shot accuracy on ImageNet-1k.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج معالجة لغوية طبيعية لتحليل التشابه بين الجمل في النصوص.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('nikcheerla/nooks-amd-detection-realtime')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the 'sentence-transformers' library.\\\\n2. Import the 'SentenceTransformer' class from the sentence_transformers library.\\\\n3. Create a SentenceTransformer instance by specifying the model 'nikcheerla/nooks-amd-detection-realtime'.\\\\n4. Encode the given texts into 768-dimensional embeddings using the 'encode' method of the SentenceTransformer instance.\\\\n5. Compute the cosine similarity between the embeddings to measure the semantic similarity between the texts.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer(\\\\'nikcheerla/nooks-amd-detection-realtime\\\\')\\\\ntext1 = \\\"This is an example sentence.\\\"\\\\ntext2 = \\\"Each sentence is converted to a vector.\\\"\\\\nembeddings = model.encode([text1, text2])\\\\n# Calculate cosine similarity between the embeddings to measure semantic similarity', 'api_call': \\\"SentenceTransformer('nikcheerla/nooks-amd-detection-realtime')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nikcheerla/nooks-amd-detection-realtime', 'api_call': \\\"SentenceTransformer('nikcheerla/nooks-amd-detection-realtime')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('{MODEL_NAME}')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('dslim/bert-base-NER')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers package. This includes AutoTokenizer, AutoModelForTokenClassification, and pipeline function.\\\\n2. Use the from_pretrained method of the AutoTokenizer and AutoModelForTokenClassification classes to load the pre-trained tokenizer and model. The model and tokenizer are 'dslim/bert-base-NER', which is fine-tuned for Named Entity Recognition (NER) tasks.\\\\n3. Create an NER pipeline using the pipeline function, with the model and tokenizer.\\\\n4. Provide the text that describes the client's request and pass it through the pipeline.\\\\n5. The pipeline will return a list of named entities and their recognized class labels (e.g., person, location, organization, or miscellaneous).\\\\n6. Use this information to create a document summary, highlighting the key persons, locations, organizations and miscellaneous entities.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'dslim/bert-base-NER\\\\')\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'dslim/bert-base-NER\\\\')\\\\nnlp = pipeline(\\\\'ner\\\\', model=model, tokenizer=tokenizer)\\\\ntext = \\\"The client\\\\'s request describes a meeting between Bill Gates and Elon Musk at Microsoft\\\\'s headquarters in Seattle. They will discuss potential collaboration on AI development for Tesla cars.\\\"\\\\nner_results = nlp(text)\\\\nprint(ner_results)', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('dslim/bert-base-NER')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dslim/bert-base-NER', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('dslim/bert-base-NER')\\\", 'api_arguments': {'model': 'AutoModelForTokenClassification.from_pretrained(dslim/bert-base-NER)', 'tokenizer': 'AutoTokenizer.from_pretrained(dslim/bert-base-NER)'}, 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoTokenizer, AutoModelForTokenClassification\\\\nfrom transformers import pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(dslim/bert-base-NER)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(dslim/bert-base-NER)\\\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\\\nexample = My name is Wolfgang and I live in Berlin\\\\nner_results = nlp(example)\\\\nprint(ner_results)', 'performance': {'dataset': 'conll2003', 'accuracy': {'f1': 91.3, 'precision': 90.7, 'recall': 91.9}}, 'description': 'bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). Specifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لتحويل الصور المتعددة الوسائط إلى نص باستخدام معالجة اللغة الطبيعية في البرمجة اللغوية الحاسوبية.? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the required libraries, which are 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\ntext_generation = pipeline(\\\\'text-generation\\\\', model=\\\\'microsoft/git-large-r-textcaps\\\\')\\\\ninput_image = \\\"path_to_the_image_file\\\"\\\\ndescription = text_generation(input_image)', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-r-textcaps', 'api_call': 'from transformers import pipeline\\\\ntext_generation = pipeline(\\\\'text-generation\\\\', model=\\\\'microsoft/git-large-r-textcaps\\\\')\\\\ninput_image = \\\"path_to_the_image_file\\\"\\\\ndescription = text_generation(input_image)', 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'TextCaps', 'accuracy': ''}, 'description': \\\"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\\\"}} </s>\"\n"}
{"text": "<s>[INST]عليك تطوير تطبيق يمكن من خلاله تحويل النص المدخل إلى صورة أو ابتكار رسومات بيانية توضح المعنى. هذا المطلب يفيد في تطبيقات متعددة كتصميم صفحات الويب وتطوير تطبيقات الهاتف.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': 'StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the necessary classes and libraries, including StableDiffusionPipeline and torch from the diffusers and torch packages.\\\\n2. Set your custom prompt as a textual description of the desired image. For example: \\\"High-quality, photorealistic image of a mountain bike on a scenic forest trail, set during sunset with warm colors, and text displaying the bicycle company\\\\'s name in bold, white letters.\\\"\\\\n3. Load the pre-trained model, \\\\'darkstorm2150/Protogen_v5.8_Official_Release\\\\', using the from_pretrained method with torch_dtype set to torch.float16 for optimal performance.\\\\n4. Optionally, configure the scheduler and move the pipeline to the GPU (if available) using the provided code.\\\\n5. Generate an image based on the provided text prompt using the num_inference_steps property to control the quality and detail of the output image.\\\\n6. Save the generated image to a file for use as an advertising banner.', '<<<code>>>': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\n\\\\nprompt = \\\"High-quality, photorealistic image of a mountain bike on a scenic forest trail, set during sunset with warm colors, and text displaying the bicycle company\\\\'s name in bold, white letters.\\\"\\\\nmodel_id = \\\\'darkstorm2150/Protogen_v5.8_Official_Release\\\\'\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to(\\\\'cuda\\\\')\\\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\\\nimage.save(\\\\'./bicycle_ad_banner.jpg\\\\')', 'api_call': 'StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': 'StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': [['torch', 'diffusers']], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\\\nimport torch\\\\nprompt = (\\\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\\\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\\\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\\\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\\\n)\\\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to(cuda)\\\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج تحليل جدولي يستخدم تقنيات معالجة اللغات الطبيعية لتوليد نتائج دقيقة وجذابة بناءً على البيانات المقدمة، وقم بتقييم جودة النتائج المحققة بواسطة المعايير الدقيقة كالانسجام والإتقان والإبداع.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We first need to import the necessary libraries, such as joblib for loading the model and pandas for handling the data.\\\\n2. We load the pre-trained model 'model.joblib' using 'joblib.load'.\\\\n3. Load the input data in CSV format with relevant features into a pandas DataFrame.\\\\n4. We also load the configuration file containing the feature names.\\\\n5. We preprocess the data by selecting the required features and renaming the columns in the DataFrame according to the column naming convention used in the model.\\\\n6. Finally, we use the model to make predictions on the input data and obtain the predicted carbon emissions for the client's business.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('client_data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bibekbehera/autotrain-numeric_prediction-40376105019', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'bibekbehera/autotrain-data-numeric_prediction', 'accuracy': {'Loss': 0.152, 'R2': 0.659, 'MSE': 0.023, 'MAE': 0.062, 'RMSLE': 0.105}}, 'description': 'A tabular regression model trained with AutoTrain to predict carbon emissions based on input features.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية باستخدام تقنيات تقدير العمق. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and classes like PIL for image processing, transformers for the depth estimation model, and numpy for array manipulation.\\\\n2. Use the Global-Local Path Networks (GLPN) model from Hugging Face transformers (vinvino02/glpn-kitti), which has been trained on the KITTI dataset and tailored for monocular depth estimation.\\\\n3. Load the input image from a file or capture it in real-time using the car's camera.\\\\n4. Use the model to estimate the depth of various objects within the input image.\\\\n5. The provided depth image can be used to inform the advanced driver-assistance systems (ADAS) for better decision-making during driving.\\\", '<<<code>>>': \\\"from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimage = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with path to your image\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained('vinvino02/glpn-kitti')\\\\nmodel = GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    predicted_depth = outputs.predicted_depth\\\", 'api_call': \\\"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-kitti', 'api_call': \\\"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\\\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'KITTI', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج تعليمات صوتية للكشف عن النشاط الصوتي عبر API، تبدأ بجملة \\\"ابدأ الرصد الصوتي عندما تكتشف صوتًا\\\".? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"pipeline('voice-activity-detection', model='Eklavya/ZFF_VAD')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the required library pipeline from transformers.\\\\n2. Initialize the voice activity detection model by using the pipeline function with the model parameter set as 'Eklavya/ZFF_VAD'.\\\\n3. Now you can use the voice activity detection model to predict whether there is any human voice present in your given audio file.\\\\n4. The model will provide an output indicating the presence of a human voice in the audio.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\n\\\\nvad = pipeline('voice-activity-detection', model='Eklavya/ZFF_VAD')\\\\naudio_file = 'path/to/your/audio/file.wav'\\\\nresult = vad(audio_file)\\\", 'api_call': \\\"pipeline('voice-activity-detection', model='Eklavya/ZFF_VAD')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Eklavya/ZFF_VAD', 'api_call': \\\"pipeline('voice-activity-detection', model='Eklavya/ZFF_VAD')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['transformers, torch'], 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A Voice Activity Detection model by Eklavya, using the Hugging Face framework.' </s>\"\n"}
{"text": "<s>[INST]عند استخدام خدمة الترجمة الآلية لمعالجة اللغة الطبيعية، يرجى التأكد من تضمين جميع العناصر والتفاصيل اللازمة بوضوح في النص، مثل التعليمات والتوجيهات.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"translate('input_text', model='Helsinki-NLP/opus-mt-en-fr')\\\", 'api_provider': 'Hugging Face', 'explanation': '1. Import the pipeline function from the transformers library provided by Hugging Face.', 'code': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_en_to_fr\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-fr\\\\')\\\\ninput_text = \\\"Enter your English text to be translated\\\"\\\\ntranslated_text = translator(input_text)\\\\nfrench_text = translated_text[0][\\\\'translation_text\\\\']\\\\nprint(french_text)', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face', 'functionality': 'Translation', 'api_name': 'Helsinki-NLP/opus-mt-en-fr', 'api_call': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_en_to_fr\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-fr\\\\')\\\\ninput_text = \\\"Enter your English text to be translated\\\"\\\\ntranslated_text = translator(input_text)\\\\nfrench_text = translated_text[0][\\\\'translation_text\\\\']\\\\nprint(french_text)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': {'newsdiscussdev2015-enfr.en.fr': 33.8, 'newsdiscusstest2015-enfr.en.fr': 40.0, 'newssyscomb2009.en.fr': 29.8, 'news-test2008.en.fr': 27.5, 'newstest2009.en.fr': 29.4, 'newstest2010.en.fr': 32.7, 'newstest2011.en.fr': 34.3, 'newstest2012.en.fr': 31.8, 'newstest2013.en.fr': 33.2, 'Tatoeba.en.fr': 50.5}}}, 'description': 'Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.'}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء تسجيل صوتي لنص يبدأ بـ \\\"كان ياما كان في أرض بعيدة\\\"، وصف فيها صورة متعددة الأوجه.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", '<<<api_provider>>>': 'speechbrain', '<<<explanation>>>': '1. Import necessary libraries such as torchaudio, Tacotron2, and HIFIGAN from speechbrain.\\\\n2. Load the pre-trained Tacotron2 model for German text-to-speech using the \\\\'from_hparams\\\\' method with the appropriate source and save directory.\\\\n3. Load the pre-trained HIFIGAN vocoder model for German using the \\\\'from_hparams\\\\' method with the appropriate source and save directory.\\\\n4. Encode the input text \\\"Guten Morgen, wie geht es Ihnen heute?\\\" using the Tacotron2 model, which will return the Mel spectrogram.\\\\n5. Convert the Mel spectrogram into a waveform using the HIFIGAN vocoder\\\\'s \\\\'decode_batch\\\\' method.\\\\n6. Save the resulting waveform as an audio file using torchaudio\\\\'s \\\\'save\\\\' function.', '<<<code>>>': 'import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2\\\\nfrom speechbrain.pretrained import HIFIGAN\\\\n\\\\ntext = \\\"Guten Morgen, wie geht es Ihnen heute?\\\"\\\\ntmpdir_tts = \\\"tmp_tacotron\\\"\\\\ntmpdir_vocoder = \\\"tmp_hifigan\\\"\\\\n\\\\ntacotron2 = Tacotron2.from_hparams(source=\\\\'padmalcom/tts-tacotron2-german\\\\', savedir=tmpdir_tts)\\\\nhifi_gan = HIFIGAN.from_hparams(source=\\\\'padmalcom/tts-hifigan-german\\\\', savedir=tmpdir_vocoder)\\\\n\\\\nmel_output, mel_length, alignment = tacotron2.encode_text(text)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save(\\\\'example_TTS.wav\\\\', waveforms.squeeze(1), 22050)', 'api_call': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", 'provider': 'speechbrain', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tts-hifigan-german', 'api_call': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", 'api_arguments': ['mel_output'], 'python_environment_requirements': [['speechbrain']], 'example_code': \\\"import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2\\\\nfrom speechbrain.pretrained import HIFIGAN\\\\ntacotron2 = Tacotron2.from_hparams(source=padmalcom/tts-tacotron2-german, savedir=tmpdir_tts)\\\\nhifi_gan = HIFIGAN.from_hparams(source=padmalcom/tts-hifigan-german, savedir=tmpdir_vocoder)\\\\nmel_output, mel_length, alignment = tacotron2.encode_text(Mary had a little lamb)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\\\", 'performance': {'dataset': 'custom German dataset', 'accuracy': 'Not specified'}, 'description': 'A HiFIGAN vocoder trained on a generated German dataset using mp3_to_training_data. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram.' </s>\"\n"}
{"text": "<s>[INST]أنشئ وصفاً نصياً قصيراً يصف الصورة المعروضة بشكل متعدد الوسائط لتصنيف الصور.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary packages, which are AutoImageProcessor and AutoModelForImageClassification from transformers, and Image from PIL.\\\\n2. Use the 'from_pretrained' method to load the Swin Transformer v2 model, which is 'microsoft/swinv2-tiny-patch4-window8-256', for image classification tasks.\\\\n3. Load the input image using the Image object from the PIL library.\\\\n4. Use the processor to preprocess the image, and then pass the preprocessed image to the model for classification.\\\\n5. The outputs from the model can be used to identify the clothing style present in the image.\\\", '<<<code>>>': 'from transformers import AutoModelForImageClassification, AutoImageProcessor\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = \\\\'http://example.com/path/to/image.jpg\\\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = AutoImageProcessor.from_pretrained(\\\\'microsoft/swinv2-tiny-patch4-window8-256\\\\')\\\\nmodel = AutoModelForImageClassification.from_pretrained(\\\\'microsoft/swinv2-tiny-patch4-window8-256\\\\')\\\\ninputs = processor(images=image, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/swinv2-tiny-patch4-window8-256', 'api_call': \\\"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\\\", 'api_arguments': {'image': 'http://images.cocodataset.org/val2017/000000039769.jpg'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطبيق تقنيات التعلّم التعزيزي في مجال معالجة اللغة الطبيعية وإنشاء نصوص، لتحقيق أداء محسن في إنتاج نصوص طبيعية تتماشى مع اللغة العربية. هل يمكنك مساعدتي في هذا؟? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('bigscience/bloomz-560m')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries AutoModelForCausalLM and AutoTokenizer from transformers.', 'code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ncheckpoint = \\\\'bigscience/bloomz-560m\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\\\ninputs = tokenizer.encode(\\\"Translate to English: Je t’aime.\\\", return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs)\\\\ntranslated_text = tokenizer.decode(outputs[0])', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Generation', 'api_name': 'bigscience/bloomz-560m', 'api_call': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ncheckpoint = \\\\'bigscience/bloomz-560m\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\\\ninputs = tokenizer.encode(\\\"Translate to English: Je t’aime.\\\", return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs)\\\\ntranslated_text = tokenizer.decode(outputs[0])', 'api_arguments': {'checkpoint': 'bigscience/bloomz-560m', 'inputs': 'Translate to English: Je t’aime.'}, 'python_environment_requirements': ['transformers', 'accelerate', 'bitsandbytes'], 'example_code': {'CPU': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ncheckpoint = bigscience/bloomz-560m\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\\\ninputs = tokenizer.encode(Translate to English: Je t’aime., return_tensors=pt)\\\\noutputs = model.generate(inputs)\\\\nprint(tokenizer.decode(outputs[0]))'}, 'performance': {'dataset': 'bigscience/xP3', 'accuracy': {'Winogrande XL (xl) validation set': 52.41, 'XWinograd (en) test set': 51.01, 'XWinograd (fr) test set': 51.81, 'XWinograd (jp) test set': 52.03, 'XWinograd (pt) test set': 53.99, 'XWinograd (ru) test set': 53.97, 'XWinograd (zh) test set': 54.76, 'ANLI (r1) validation set': 33.4, 'ANLI (r2) validation set': 33.4, 'ANLI (r3) validation set': 33.5}}, 'description': 'BLOOMZ & mT0 are a family of models capable of following human instructions in dozens of languages zero-shot. Finetuned on the crosslingual task mixture (xP3), these models can generalize to unseen tasks & languages. Useful for tasks expressed in natural language, such as translation, summarization, and question answering.'}} </s>\"\n"}
{"text": "<s>[INST]يجب على المطوّر تطوير نموذج لتصنيف الجداول يستخدم تقنيات معالجة اللغة الطبيعية، وذلك لتحسين دقة تصنيف البيانات المتواجدة في الجداول. يمكن استخدام هذا النموذج في تحسين تجربة المستخدمين في فهم واستخدام البيانات بشكل أكثر فعالية وسهولة.? [/INST]{'domain': 'Tabular Tabular Classification', 'api_call': \\\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv')))\\\", 'api_provider': 'Scikit-learn', 'explanation': '', 'code': \\\"from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\n\\\\nREPO_ID = 'julien-c/wine-quality'\\\\nFILENAME = 'sklearn_model.joblib'\\\\n\\\\nmodel = joblib.load(cached_download(hf_hub_url(REPO_ID, FILENAME)))\\\\ndata_file = cached_download(hf_hub_url(REPO_ID, 'winequality-red.csv'))\\\\nwinedf = pd.read_csv(data_file, sep=';')\\\\n\\\\nX = winedf.drop(['quality'], axis=1)\\\\nY = winedf['quality']\\\\n\\\\nwine_quality_labels = model.predict(X[:3])\\\", 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Scikit-learn', 'functionality': 'Wine Quality classification', 'api_name': 'julien-c/wine-quality', 'api_call': \\\"from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\n\\\\nREPO_ID = 'julien-c/wine-quality'\\\\nFILENAME = 'sklearn_model.joblib'\\\\n\\\\nmodel = joblib.load(cached_download(hf_hub_url(REPO_ID, FILENAME)))\\\\ndata_file = cached_download(hf_hub_url(REPO_ID, 'winequality-red.csv'))\\\\nwinedf = pd.read_csv(data_file, sep=';')\\\\n\\\\nX = winedf.drop(['quality'], axis=1)\\\\nY = winedf['quality']\\\\n\\\\nwine_quality_labels = model.predict(X[:3])\\\", 'api_arguments': ['X'], 'python_environment_requirements': ['huggingface_hub', 'joblib', 'pandas'], 'example_code': 'from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nREPO_ID = julien-c/wine-quality\\\\nFILENAME = sklearn_model.joblib\\\\nmodel = joblib.load(cached_download(\\\\n hf_hub_url(REPO_ID, FILENAME)\\\\n))\\\\ndata_file = cached_download(\\\\n hf_hub_url(REPO_ID, winequality-red.csv)\\\\n)\\\\nwinedf = pd.read_csv(data_file, sep=;)\\\\nX = winedf.drop([quality], axis=1)\\\\nY = winedf[quality]\\\\nprint(X[:3])\\\\nlabels = model.predict(X[:3])\\\\nmodel.score(X, Y)', 'performance': {'dataset': 'julien-c/wine-quality', 'accuracy': 0.6616635397123202}, 'description': 'A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.'}} </s>\"\n"}
{"text": "<s>[INST]يجب على المطوّر تطوير نموذج لتصنيف النصوص بدون تصنيف باستخدام تقنيات معالجة اللغة الطبيعية، لتحويل النصوص من لغة معينة إلى أخرى بدون الحاجة إلى تصنيف مسبق، خاصة في مجال التصنيف الآلي للنصوص والتعامل مع المجالات الجديدة.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"CrossEncoder('cross-encoder/nli-roberta-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary library which is 'pipeline' from the transformers package.\\\\n2. Use the 'pipeline' function to create a zero-shot classification model using the 'cross-encoder/nli-roberta-base' model.\\\\n3. The model will be used for classifying news headlines into various categories like technology, sports, and politics.\\\\n4. Pass the news headline to the model along with candidate labels for classification.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'cross-encoder/nli-roberta-base\\\\')\\\\nheadline = \\\"Apple unveils its latest iPhone model with advanced features\\\"\\\\ncandidate_labels = [\\\\'technology\\\\', \\\\'sports\\\\', \\\\'politics\\\\']\\\\nresult = classifier(headline, candidate_labels)', 'api_call': \\\"CrossEncoder('cross-encoder/nli-roberta-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cross-encoder/nli-roberta-base', 'api_call': \\\"CrossEncoder('cross-encoder/nli-roberta-base')\\\", 'api_arguments': ['sentence1', 'sentence2'], 'python_environment_requirements': [['sentence_transformers', 'transformers']], 'example_code': \\\"from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model='cross-encoder/nli-roberta-base')\\\\nsent = Apple just announced the newest iPhone X\\\\ncandidate_labels = [technology, sports, politics]\\\\nres = classifier(sent, candidate_labels)\\\\nprint(res)\\\", 'performance': {'dataset': ['SNLI', 'MultiNLI'], 'accuracy': 'See SBERT.net - Pretrained Cross-Encoder'}, 'description': 'Cross-Encoder for Natural Language Inference trained on the SNLI and MultiNLI datasets. Outputs three scores corresponding to the labels: contradiction, entailment, neutral.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء برنامج يقوم بتلخيص النصوص بشكل تلقائي في مجال معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"pipeline('summarization', model='philschmid/distilbart-cnn-12-6-samsum')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a text summarization model.\\\\n3. Specify the model 'philschmid/distilbart-cnn-12-6-samsum' for loading. This model is trained on conversational text with the goal of generating shorter summaries.\\\\n4. The created summarizer can be used to process conversation transcripts and generate brief summaries. This can help managers in understanding key points discussed during customer interactions.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'philschmid/distilbart-cnn-12-6-samsum\\\\')\\\\nconversation = \\\"customer: I am having difficulty accessing my account. \\\\\\\\nagent: Can you please provide your email address and the error message you are receiving? \\\\\\\\ncustomer: my email is example@example.com, and the error says \\\\'invalid login credentials\\\\'. \\\\\\\\nagent: I see, it seems like you need to reset your password.\\\"\\\\nsummary = summarizer(conversation)', 'api_call': \\\"pipeline('summarization', model='philschmid/distilbart-cnn-12-6-samsum')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilbart-cnn-12-6-samsum', 'api_call': \\\"pipeline('summarization', model='philschmid/distilbart-cnn-12-6-samsum')\\\", 'api_arguments': {'model': 'philschmid/distilbart-cnn-12-6-samsum'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = '''Jeff: Can I train a 🤗 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n'''\\\\nsummarizer(conversation)\\\", 'performance': {'dataset': 'samsum', 'accuracy': {'ROUGE-1': 41.09, 'ROUGE-2': 20.746, 'ROUGE-L': 31.595, 'ROUGE-LSUM': 38.339}}, 'description': 'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.' </s>\"\n"}
{"text": "<s>[INST]قم بتحميل مجموعة بيانات الفيديو لتصنيف المقاطع البصرية، ثم قم بتدريب نموذج التصنيف باستخدام تقنيات رؤية الحاسوب المناسبة. قم بتقسيم البيانات إلى بيانات تدريب واختبار، وقم بتقييم أداء النموذج باستخدام مقاييس مثل الدقة والاستدلال. يمكنك اختيار النموذج الأمثل بناءً على النتائج المحققة.? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-ssv2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary packages. In this case, we need the TimesformerForVideoClassification model from the transformers library and the numpy and torch libraries to work with the video data.', 'code': \\\"from transformers import TimesformerForVideoClassification, AutoImageProcessor\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = ...  # Load your video data here (list of 16 frames)\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/timesformer-hr-finetuned-ssv2')\\\\nmodel = TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-ssv2')\\\\ninputs = feature_extractor(images=video, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n    predicted_class_idx = logits.argmax(-1).item()\\\\n    action_label = model.config.id2label[predicted_class_idx]\\\", 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'facebook/timesformer-hr-finetuned-ssv2', 'api_call': \\\"from transformers import TimesformerForVideoClassification, AutoImageProcessor\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = ...  # Load your video data here (list of 16 frames)\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/timesformer-hr-finetuned-ssv2')\\\\nmodel = TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-ssv2')\\\\ninputs = feature_extractor(images=video, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n    predicted_class_idx = logits.argmax(-1).item()\\\\n    action_label = model.config.id2label[predicted_class_idx]\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'numpy', 'torch'], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 448, 448))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-hr-finetuned-ssv2)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-hr-finetuned-ssv2)\\\\ninputs = feature_extractor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Something Something v2', 'accuracy': 'Not provided'}, 'description': 'TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتحويل الملف الصوتي إلى ملف صوتي باستخدام أداة التحويل المناسبة.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"Asteroid('JorisCos/ConvTasNet_Libri2Mix_sepclean_16k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To extract speech from a noisy audio file, you can use the 'ConvTasNet_Libri2Mix_sepclean_16k' model from the Asteroid library hosted on Hugging Face model hub.\\\\n2. Import the Asteroid class and create an instance of the model with 'JorisCos/ConvTasNet_Libri2Mix_sepclean_16k'.\\\\n3. This model has been trained to separate clean speech from noisy audio, and it can be applied to your input audio file.\\\\n4. The output will be a clean version of the speech contained in the noisy audio file.\\\", '<<<code>>>': \\\"from asteroid import Asteroid\\\\nmodel = Asteroid('JorisCos/ConvTasNet_Libri2Mix_sepclean_16k')\\\\nclean_speech = model.separate(noisy_audio_file)\\\", 'api_call': \\\"Asteroid('JorisCos/ConvTasNet_Libri2Mix_sepclean_16k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ConvTasNet_Libri2Mix_sepclean_16k', 'api_call': \\\"Asteroid('JorisCos/ConvTasNet_Libri2Mix_sepclean_16k')\\\", 'api_arguments': [], 'python_environment_requirements': [['asteroid']], 'example_code': '', 'performance': {'dataset': 'Libri2Mix', 'accuracy': {'si_sdr': 15.243671356901526, 'si_sdr_imp': 15.243034178473609, 'sdr': 15.668108919568112, 'sdr_imp': 15.578229918028036, 'sir': 25.295100756629957, 'sir_imp': 25.205219921301754, 'sar': 16.307682590197313, 'sar_imp': -51.64989963759405, 'stoi': 0.9394951175291422, 'stoi_imp': 0.22640192740016568}}, 'description': 'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لمعالجة اللغة الطبيعية لمقارنة الجمل بالقرب من \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import SentenceTransformer and util from the sentence_transformers library.\\\\n2. Create a model instance using the SentenceTransformer class and load the \\\\'sentence-transformers/multi-qa-mpnet-base-dot-v1\\\\' model. This model is designed for semantic search and can map sentences or paragraphs to a dense 768-dimensional vector space.\\\\n3. Define a query representing the information they are looking for (e.g. \\\"effects of climate change on biodiversity\\\").\\\\n4. Encode the query and a list of tweets (called \\\\'docs\\\\') using the model.\\\\n5. Calculate the similarity scores (dot product) between the query embedding and the tweet embeddings.\\\\n6. Sort the tweets by their similarity scores in descending order, and return the most relevant tweets.', '<<<code>>>': \\\"from sentence_transformers import SentenceTransformer, util\\\\nmodel = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\\\\nquery = 'effects of climate change on biodiversity'\\\\ntweets = ['Tweet1...', 'Tweet2...', 'Tweet3...']\\\\nquery_emb = model.encode(query)\\\\ntweet_emb = model.encode(tweets)\\\\nscores = util.dot_score(query_emb, tweet_emb)\\\\nsorted_tweets = sorted(zip(tweets, scores), key=lambda x: x[1], reverse=True)\\\\ntop_tweets = [tweet for tweet, score in sorted_tweets[:10]]\\\", 'api_call': \\\"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/multi-qa-mpnet-base-dot-v1', 'api_call': \\\"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\\\", 'api_arguments': ['query', 'docs'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer, util\\\\nquery = How many people live in London?\\\\ndocs = [Around 9 Million people live in London, London is known for its financial district]\\\\nmodel = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\\\\nquery_emb = model.encode(query)\\\\ndoc_emb = model.encode(docs)\\\\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\\\\ndoc_score_pairs = list(zip(docs, scores))\\\\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\\\\nfor doc, score in doc_score_pairs:\\\\n print(score, doc)\\\", 'performance': {'dataset': [{'name': 'WikiAnswers', 'accuracy': 77427422}, {'name': 'PAQ', 'accuracy': 64371441}, {'name': 'Stack Exchange', 'accuracy': 25316456}]}, 'description': 'This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources.' </s>\"\n"}
{"text": "<s>[INST]عميلنا يرغب في إنشاء برنامج تطوير متعدد الوسائط لتحويل النصوص إلى فيديو، بدون قيود، باستخدام تقنية الرؤية الحاسوبية. نحن بحاجة إلى برنامج قادر على توليد الفيديوهات بشكل تلقائي من النصوص المقدمة دون الحاجة إلى إدخال بيانات محددة.? [/INST]{'domain': 'Multimodal Text-to-Video', 'api_call': \\\"pipeline('text-to-video', model='chavinlo/TempoFunk')\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': 'from transformers import pipeline\\\\ntext_to_video_pipeline = pipeline(\\\\'text-to-video\\\\', model=\\\\'chavinlo/TempoFunk\\\\')\\\\ninput_text = \\\"The only way to do great work is to love what you do.\\\"\\\\nvideo = text_to_video_pipeline(input_text)\\\\n# Save or display the generated video for the filmmaker', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'chavinlo/TempoFunk', 'api_call': 'from transformers import pipeline\\\\ntext_to_video_pipeline = pipeline(\\\\'text-to-video\\\\', model=\\\\'chavinlo/TempoFunk\\\\')\\\\ninput_text = \\\"The only way to do great work is to love what you do.\\\"\\\\nvideo = text_to_video_pipeline(input_text)\\\\n# Save or display the generated video for the filmmaker', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتصنيف الفيديوهات باستخدام تقنيات الرؤية الحاسوبية، ونريد تحقيق التصنيف الدقيق لمختلف أنواع الفيديوهات.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes and functions from the transformers package, such as VideoMAEImageProcessor, VideoMAEForPreTraining.\\\\n2. Create a video input, which in this case is a list of frames, each as a 3-channel (RGB) 224x224 image.\\\\n3. Instantiate a VideoMAEImageProcessor using the from_pretrained method, specifying the 'MCG-NJU/videomae-base' model for pre-training.\\\\n4. Create an instance of the VideoMAEForPreTraining class using the same 'MCG-NJU/videomae-base' model.\\\\n5. Preprocess the video using the processor instance, and convert it into a suitable format like tensors.\\\\n6. Perform video classification using the model, providing the tensor-formatted video input and any other necessary inputs.\\\\n7. Retrieve the classification results, and determine which sport is associated with the input video.\\\", '<<<code>>>': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base')\\\\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base')\\\\npixel_values = processor(video, return_tensors='pt').pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss\\\", 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'MCG-NJU/videomae-base', 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base')\\\", 'api_arguments': ['video'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base)\\\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base)\\\\npixel_values = processor(video, return_tensors=pt).pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss', 'performance': {'dataset': 'Kinetics-400', 'accuracy': 'To be provided'}, 'description': 'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches.' </s>\"\n"}
{"text": "<s>[INST]تركز مجال تجهيز اللغة الطبيعية والوسائط المتعددة على مشكلة ملء الفجوات في المستندات. نحن نسعى لتطوير نماذج قادرة على ملء الفجوات بشكل دقيق وفعال. الرجاء تقديم تقدير لقدرة نموذجنا الجديد على ملء الفجوات باستخدام بيانات تجريبية متعلقة بتجهيز اللغة الطبيعية والوسائط المتعددة.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the Hugging Face Transformers library and load the state-of-the-art model and the tokenizer for document question answering.\\\\n2. The model, 'seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', is fine-tuned on the DocVQA dataset and specialized in processing document images and extracting answers based on the provided questions.\\\\n3. Utilize the pipeline function to create a question-answering instance using the model and tokenizer, which will receive both questions and the document context as inputs. This will identify the relevant content in the document that answers the question.\\\\n4. Use this instance to query the document by passing the required information.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ndoc_qa = pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\\\\nresult = doc_qa({'question': 'What is the total amount due?', 'context': document_content})\\\", 'api_call': \\\"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \\\"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'A document question answering model finetuned on the DocVQA dataset using LayoutLMv2-base-uncased.' </s>\"\n"}
{"text": "<s>[INST]قم ببرمجة نموذج لتوليد تحويل صوت إلى صوت باستخدام تقنيات التعلم العميق، يقوم النموذج بتحليل الإشارات الصوتية وتحويلها إلى إشارات صوتية مختلفة باستخدام تقنيات تخيل الصوت.? [/INST]{'domain': 'Audio Audio-to-Audio', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_unity_en-hk')\\\", 'api_provider': 'Fairseq', 'explanation': '', 'code': \\\"import torchaudio\\\\nfrom fairseq import hub_utils, checkpoint_utils\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nfrom IPython.display import Audio\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_unity_en-hk')\\\\nmodel = models[0].cpu()\\\\n\\\\n# Load English audio file\\\\naudio, _ = torchaudio.load('/path/to/english/audio/file')\\\\n\\\\n# Translate English speech to Hokkien UniT text\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\\\\n\\\\ncache_dir = os.getenv('HUGGINGFACE_HUB_CACHE')\\\\ncache_dir = snapshot_download('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS', cache_dir=cache_dir)\\\\n\\\\nx = hub_utils.from_pretrained(\\\\n    cache_dir,\\\\n    'model.pt',\\\\n    '.',\\\\n    archive_map=CodeHiFiGANVocoder.hub_models(),\\\\n    config_yaml='config.json',\\\\n    fp16=False,\\\\n    is_vocoder=True\\\\n)\\\\n\\\\nvocoder = CodeHiFiGANVocoder(x['args']['model_path'][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\n\\\\n# Generate Hokkien speech from UniT text\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\n\\\\n# Play the generated Hokkien speech\\\\nAudio(wav, rate=sr)\\\", 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Fairseq', 'functionality': 'speech-to-speech-translation', 'api_name': 'xm_transformer_unity_en-hk', 'api_call': \\\"import torchaudio\\\\nfrom fairseq import hub_utils, checkpoint_utils\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nfrom IPython.display import Audio\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_unity_en-hk')\\\\nmodel = models[0].cpu()\\\\n\\\\n# Load English audio file\\\\naudio, _ = torchaudio.load('/path/to/english/audio/file')\\\\n\\\\n# Translate English speech to Hokkien UniT text\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\\\\n\\\\ncache_dir = os.getenv('HUGGINGFACE_HUB_CACHE')\\\\ncache_dir = snapshot_download('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS', cache_dir=cache_dir)\\\\n\\\\nx = hub_utils.from_pretrained(\\\\n    cache_dir,\\\\n    'model.pt',\\\\n    '.',\\\\n    archive_map=CodeHiFiGANVocoder.hub_models(),\\\\n    config_yaml='config.json',\\\\n    fp16=False,\\\\n    is_vocoder=True\\\\n)\\\\n\\\\nvocoder = CodeHiFiGANVocoder(x['args']['model_path'][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\n\\\\n# Generate Hokkien speech from UniT text\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\n\\\\n# Play the generated Hokkien speech\\\\nAudio(wav, rate=sr)\\\", 'api_arguments': {'config_yaml': 'config.yaml', 'task': 'speech_to_text', 'cache_dir': 'cache_dir'}, 'python_environment_requirements': ['fairseq', 'hub_utils', 'torchaudio', 'IPython.display', 'huggingface_hub'], 'example_code': ['import json', 'import os', 'from pathlib import Path', 'import IPython.display as ipd', 'from fairseq import hub_utils', 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub', 'from fairseq.models.speech_to_text.hub_interface import S2THubInterface', 'from fairseq.models.text_to_speech import CodeHiFiGANVocoder', 'from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface', 'from huggingface_hub import snapshot_download', 'import torchaudio', 'cache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)', 'models, cfg, task = load_model_ensemble_and_task_from_hf_hub(', ' facebook/xm_transformer_unity_en-hk,', ' arg_overrides={config_yaml: config.yaml, task: speech_to_text},', ' cache_dir=cache_dir,', ')', 'model = models[0].cpu()', 'cfg[task].cpu = True', 'generator = task.build_generator([model], cfg)', 'audio, _ = torchaudio.load(/path/to/an/audio/file)', 'sample = S2THubInterface.get_model_input(task, audio)', 'unit = S2THubInterface.get_prediction(task, model, generator, sample)', 'library_name = fairseq', 'cache_dir = (', ' cache_dir or (Path.home() / .cache / library_name).as_posix()', ')', 'cache_dir = snapshot_download(', ' ffacebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS, cache_dir=cache_dir, library_name=library_name', ')', 'x = hub_utils.from_pretrained(', ' cache_dir,', ' model.pt,', ' .,', ' archive_map=CodeHiFiGANVocoder.hub_models(),', ' config_yaml=config.json,', ' fp16=False,', ' is_vocoder=True,', ')', \\\"with open(f{x['args']['data']}/config.json) as f:\\\", ' vocoder_cfg = json.load(f)', 'assert (', ' len(x[args][model_path]) == 1', '), Too many vocoder models in the input', 'vocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)', 'tts_model = VocoderHubInterface(vocoder_cfg, vocoder)', 'tts_sample = tts_model.get_model_input(unit)', 'wav, sr = tts_model.get_prediction(tts_sample)', 'ipd.Audio(wav, rate=sr)'], 'performance': {'dataset': 'MuST-C', 'accuracy': None}, 'description': 'Speech-to-speech translation model with two-pass decoder (UnitY) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain.'}} </s>\"\n"}
{"text": "<s>[INST]عميلنا يرغب في تطوير تطبيق يستخدم تقنية التعرف التلقائي على الكلام الصوتي. نحن بحاجة إلى برنامج يقوم بتحويل النص إلى كلام بصوت واضح ودقيق لتحسين تجربة المستخدمين الذين يعانون من إعاقات بصرية.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"WhisperModel('large-v2')\\\", '<<<api_provider>>>': 'CTranslate2', '<<<explanation>>>': \\\"1. Import the required WhisperModel class from the faster_whisper library.\\\\n2. Instantiate the WhisperModel with the 'large-v2' model which has been trained on a dataset of 99 languages and is suitable for multilingual Automatic Speech Recognition (ASR).\\\\n3. Use the transcribe method of the WhisperModel instance to convert audio samples into text segments. The resulting segments will contain the start and end times as well as the transcriptions.\\\\n4. Iterate through the segments to obtain the text transcriptions and timestamps for further processing, such as displaying them in a customer support app.\\\", '<<<code>>>': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(\\\\'large-v2\\\\')\\\\nsegments, info = model.transcribe(\\\\'audio.mp3\\\\')\\\\nfor segment in segments:\\\\n    print(\\\"[%.2fs -> %.2fs] %s\\\" % (segment.start, segment.end, segment.text))', 'api_call': \\\"WhisperModel('large-v2')\\\", 'provider': 'CTranslate2', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': \\\"WhisperModel('large-v2')\\\", 'api_arguments': ['audio.mp3'], 'python_environment_requirements': [['faster_whisper']], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.' </s>\"\n"}
{"text": "<s>[INST]قم بتمرير النص المراد معالجته إلى نموذج معالجة اللغة الطبيعية Fill-Mask لدينا للحصول على تقدير عمق الأشياء في المشهد الذي تلتقطه الكاميرات.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"DebertaV2ForMaskedLM.from_pretrained('microsoft/deberta-v2-xxlarge')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. We import the necessary classes from the transformers package, including DebertaV2ForMaskedLM for the fill-mask functionality.\\\\n2. We use the from_pretrained method of the DebertaV2ForMaskedLM class to load the pre-trained model \\\\'microsoft/deberta-v2-xxlarge\\\\'. This model is designed to fill in masked parts of text, which can help complete sentences or phrases.\\\\n3. We then prepare the input sentence by adding the mask token \\\"<mask>\\\" in place of the missing word in the sentence, i.e., \\\"During summer, the weather is usually very <mask>.\\\"\\\\n4. The classifier can then be used to predict the most plausible word that completes the statement.', '<<<code>>>': 'from transformers import DebertaV2ForMaskedLM, DebertaV2Tokenizer\\\\nmodel = DebertaV2ForMaskedLM.from_pretrained(\\\\'microsoft/deberta-v2-xxlarge\\\\')\\\\ntokenizer = DebertaV2Tokenizer.from_pretrained(\\\\'microsoft/deberta-v2-xxlarge\\\\')\\\\n\\\\ninput_sentence = \\\"During summer, the weather is usually very <mask>.\\\"\\\\ninputs = tokenizer.encode_plus(input_sentence, return_tensors=\\\"pt\\\")\\\\npredictions = model(**inputs)[0]\\\\npredicted_index = predictions.argmax(dim=2)[:, -2]\\\\npredicted_word = tokenizer.decode(predicted_index).strip()', 'api_call': \\\"DebertaV2ForMaskedLM.from_pretrained('microsoft/deberta-v2-xxlarge')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/deberta-v2-xxlarge', 'api_call': \\\"DebertaV2ForMaskedLM.from_pretrained('microsoft/deberta-v2-xxlarge')\\\", 'api_arguments': {'model_name_or_path': 'microsoft/deberta-v2-xxlarge'}, 'python_environment_requirements': [{'pip_install': ['datasets', 'deepspeed']}], 'example_code': 'python -m torch.distributed.launch --nproc_per_node=${num_gpus} run_glue.py --model_name_or_path microsoft/deberta-v2-xxlarge --task_name $TASK_NAME --do_train --do_eval --max_seq_length 256 --per_device_train_batch_size ${batch_size} --learning_rate 3e-6 --num_train_epochs 3 --output_dir $output_dir --overwrite_output_dir --logging_steps 10 --logging_dir $output_dir --deepspeed ds_config.json', 'performance': {'dataset': [{'name': 'SQuAD 1.1', 'accuracy': 'F1/EM: 96.1/91.4'}, {'name': 'SQuAD 2.0', 'accuracy': 'F1/EM: 92.2/89.7'}, {'name': 'MNLI-m/mm', 'accuracy': 'Acc: 91.7/91.9'}, {'name': 'SST-2', 'accuracy': 'Acc: 97.2'}, {'name': 'QNLI', 'accuracy': 'Acc: 96.0'}, {'name': 'CoLA', 'accuracy': 'MCC: 72.0'}, {'name': 'RTE', 'accuracy': 'Acc: 93.5'}, {'name': 'MRPC', 'accuracy': 'Acc/F1: 93.1/94.9'}, {'name': 'QQP', 'accuracy': 'Acc/F1: 92.7/90.3'}, {'name': 'STS-B', 'accuracy': 'P/S: 93.2/93.1'}]}, 'description': 'DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data. This is the DeBERTa V2 xxlarge model with 48 layers, 1536 hidden size. The total parameters are 1.5B and it is trained with 160GB raw data.' </s>\"\n"}
{"text": "<s>[INST]يتطلب استخراج الميزات في معالجة اللغة الطبيعية القدرة على التعرف على السياق وتحليل وتقدير عمق المعنى في النصوص المدخلة لمعالجة جدول أسئلة اللغة الطبيعية.? [/INST]{'domain': 'Natural Language Processing Feature Extraction', 'api_call': \\\"pipeline('feature-extraction', model='YituTech/conv-bert-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary libraries, which are 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\nfeature_extractor = pipeline(\\\\'feature-extraction\\\\', model=\\\\'YituTech/conv-bert-base\\\\')\\\\ntext = \\\"Sample text for feature extraction.\\\"\\\\nfeatures = feature_extractor(text)', 'api_data': {'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': 'from transformers import pipeline\\\\nfeature_extractor = pipeline(\\\\'feature-extraction\\\\', model=\\\\'YituTech/conv-bert-base\\\\')\\\\ntext = \\\"Sample text for feature extraction.\\\"\\\\nfeatures = feature_extractor(text)', 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.'}} </s>\"\n"}
{"text": "<s>[INST]يحتاج العميل إلى برنامج يستخدم تقنيات معالجة اللغة الطبيعية لتحويل النص إلى كلام وإلى مقاطع فيديو، وذلك لتطوير تطبيق لذوي الإعاقة البصرية في مجال نص إلى فيديو متعدد الوسائط.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Video', '<<<api_call>>>': \\\"BaseModel.from_pretrained('duncan93/video')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"BaseModel.from_pretrained('duncan93/video')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'duncan93/video', 'api_call': \\\"BaseModel.from_pretrained('duncan93/video')\\\", 'api_arguments': '', 'python_environment_requirements': ['Asteroid'], 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير معالج نصوص متعدد الوسائط للإجابة على الأسئلة، نريد القدرة على استرجاع المعلومات من وثائق متنوعة مثل النصوص والصور والفيديوهات.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library, such as LayoutLMv3Processor and LayoutLMv3ForQuestionAnswering.\\\\n2. Instantiate the processor and model by calling the from_pretrained method with the appropriate model name: 'rubentito/layoutlmv3-base-mpdocvqa'.\\\\n3. The model can be used for multiple documents, converting images of documents to the appropriate format (RGB).\\\\n4. The system is then able to identify the specific information the legal firm is looking for by inputting relevant questions and context from each document.\\\\n5. The model finds the optimal start and end indices within the input text to answer the question, and the answer can be decoded for further analysis.\\\", '<<<code>>>': 'from transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\\\\nimport torch\\\\nprocessor = LayoutLMv3Processor.from_pretrained(\\\\'rubentito/layoutlmv3-base-mpdocvqa\\\\', apply_ocr=False)\\\\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(\\\\'rubentito/layoutlmv3-base-mpdocvqa\\\\')\\\\nquestion = \\\"Find the contract\\\\'s start date\\\"\\\\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=\\\\'pt\\\\')\\\\n\\\\noutputs = model(**document_encoding)\\\\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\\\\nend_idx = torch.argmax(outputs.end_logits, axis=1)\\\\nanswer = processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()', 'api_call': \\\"LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'layoutlmv3-base-mpdocvqa', 'api_call': \\\"LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')\\\", 'api_arguments': ['image', 'question', 'context', 'boxes'], 'python_environment_requirements': [['torch', 'transformers']], 'example_code': 'import torch\\\\nfrom transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\\\\nprocessor = LayoutLMv3Processor.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa, apply_ocr=False)\\\\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)\\\\nimage = Image.open(example.jpg).convert(RGB)\\\\nquestion = Is this a question?\\\\ncontext = [Example]\\\\nboxes = [0, 0, 1000, 1000]\\\\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=pt)\\\\noutputs = model(**document_encoding)\\\\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\\\\nend_idx = torch.argmax(outputs.end_logits, axis=1)\\\\nanswers = self.processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()', 'performance': {'dataset': 'rubentito/mp-docvqa', 'accuracy': {'ANLS': 0.4538, 'APPA': 51.9426}}, 'description': 'This is pretrained LayoutLMv3 from Microsoft hub and fine-tuned on Multipage DocVQA (MP-DocVQA) dataset. This model was used as a baseline in Hierarchical multimodal transformers for Multi-Page DocVQA.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام يقوم بتصنيف مقاطع الفيديو باستخدام تقنيات رؤية الحاسوب، ونحتاج إلى تحديد النصوص والأشكال الموجودة في الصور والملفات البصرية المختلفة بدقة عالية لتحسين أداء النظام.? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We will import the necessary classes and libraries from the Transformers package. This includes TimesformerForVideoClassification for video classification and AutoImageProcessor for pre-processing video data.', 'code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(8, 3, 224, 224))  # Replace with video frames from CCTV\\\\nprocessor = AutoImageProcessor.from_pretrained(\\\\'facebook/timesformer-base-finetuned-k600\\\\')\\\\nmodel = TimesformerForVideoClassification.from_pretrained(\\\\'facebook/timesformer-base-finetuned-k600\\\\')\\\\n\\\\ninputs = processor(images=video, return_tensors=\\\\'pt\\\\')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'facebook/timesformer-base-finetuned-k600', 'api_call': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(8, 3, 224, 224))  # Replace with video frames from CCTV\\\\nprocessor = AutoImageProcessor.from_pretrained(\\\\'facebook/timesformer-base-finetuned-k600\\\\')\\\\nmodel = TimesformerForVideoClassification.from_pretrained(\\\\'facebook/timesformer-base-finetuned-k600\\\\')\\\\n\\\\ninputs = processor(images=video, return_tensors=\\\\'pt\\\\')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_arguments': ['images'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k600)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k600)\\\\ninputs = processor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-600', 'accuracy': None}, 'description': 'TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]يجب على المطور تطوير نموذج تصنيف الصوت لتطبيق خاص بتصنيف مقاطع الصوت المختلفة. نحتاج إلى برنامج قادر على تصنيف أصوات مختلفة بدقة وفعالية لتلبية احتياجات مستخدمي التطبيق.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"Wav2Vec2ForCTC.from_pretrained('ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, Wav2Vec2ForCTC and Wav2Vec2Processor, from the transformers library.\\\\n2. Load the pre-trained model 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition', which has been fine-tuned for the Speech Emotion Recognition task on the RAVDESS dataset.\\\\n3. Use the Wav2Vec2Processor to convert your input audio file into a format that the model can understand.\\\\n4. Pass the processed input to the model and obtain the output logits.\\\\n5. Decode the logits to get a predicted emotion label.\\\", '<<<code>>>': 'from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\\\nimport torchaudio\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\\\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\\\')\\\\nprocessor = Wav2Vec2Processor.from_pretrained(\\\\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\\\')\\\\naudio_file_path = \\\"path/to/your/audio/file.wav\\\"\\\\nwaveform, _ = torchaudio.load(audio_file_path)\\\\ninput_values = processor(waveform.numpy(), sampling_rate=16_000, return_tensors=\\\"pt\\\").input_values\\\\nlogits = model(input_values).logits\\\\npredicted_id = torch.argmax(logits, dim=-1)\\\\nemotion = processor.decode(predicted_id[0])', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition')\\\", 'api_arguments': 'wav2vec2, tokenizer', 'python_environment_requirements': ['transformers 4.8.2, pytorch 1.9.0+cu102, datasets 1.9.0, tokenizers 0.10.3'], 'example_code': 'from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor', 'performance': {'dataset': 'RAVDESS', 'accuracy': 0.8223}, 'description': \\\"The model is a fine-tuned version of jonatasgrosman/wav2vec2-large-xlsr-53-english for a Speech Emotion Recognition (SER) task. The dataset used to fine-tune the original pre-trained model is the RAVDESS dataset. This dataset provides 1440 samples of recordings from actors performing on 8 different emotions in English, which are: emotions = ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised'].\\\" </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لتصنيف الصور بدون تصنيف متعدد الوسائط. ابدأ ببرمجة محرك للبحث يستخدم البيانات من مصادر متنوعة وتحليل الصور لتصنيفها بدقة.? [/INST]{'domain': 'Multimodal Zero-Shot Image Classification', 'api_call': \\\"pipeline('zero-shot-image-classification', model='microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\ncell_type_classifier = pipeline('zero-shot-image-classification', model='microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\\\\nimage_path = 'path/to/microscopy_image.png'\\\\npossible_cell_types = ['neuron', 'astrocyte', 'microglia', 'oligodendrocyte']\\\\nclassification_result = cell_type_classifier(image_path, possible_cell_types)\\\", 'api_data': {'domain': 'Multimodal Zero-Shot Image Classification', 'framework': 'Hugging Face', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224', 'api_call': \\\"from transformers import pipeline\\\\ncell_type_classifier = pipeline('zero-shot-image-classification', model='microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\\\\nimage_path = 'path/to/microscopy_image.png'\\\\npossible_cell_types = ['neuron', 'astrocyte', 'microglia', 'oligodendrocyte']\\\\nclassification_result = cell_type_classifier(image_path, possible_cell_types)\\\", 'api_arguments': 'image, possible_class_names', 'python_environment_requirements': 'transformers, torch, torchvision', 'example_code': \\\"from transformers import pipeline\\\\nclip = pipeline('zero-shot-image-classification', model='microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\\\\nimage = 'path/to/image.png'\\\\npossible_class_names = ['class1', 'class2', 'class3']\\\\nresult = clip(image, possible_class_names)\\\", 'performance': {'dataset': 'PMC-15M', 'accuracy': 'State of the art'}, 'description': 'BiomedCLIP is a biomedical vision-language foundation model pretrained on PMC-15M, a dataset of 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central, using contrastive learning. It uses PubMedBERT as the text encoder and Vision Transformer as the image encoder, with domain-specific adaptations. It can perform various vision-language processing (VLP) tasks such as cross-modal retrieval, image classification, and visual question answering.'}} </s>\"\n"}
{"text": "<s>[INST]اقترح في اجتماع النادي الطلابي امكانية استخدام أداة تعتمد على تقنيات التعلم الآلي لتحسين ترجمة النصوص وتوفير تصنيف غير مسبق لها.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"translation_pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_de_to_en\\\\', model=\\\\'Helsinki-NLP/opus-mt-de-en\\\\')\\\\ntranslated_text = translator(\\\"German text goes here\\\")[0][\\\\'translation_text\\\\']', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Translation', 'api_name': 'opus-mt-de-en', 'api_call': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_de_to_en\\\\', model=\\\\'Helsinki-NLP/opus-mt-de-en\\\\')\\\\ntranslated_text = translator(\\\"German text goes here\\\")[0][\\\\'translation_text\\\\']', 'api_arguments': ['source languages: de', 'target languages: en'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'opus', 'accuracy': {'newssyscomb2009.de.en': 29.4, 'news-test2008.de.en': 27.8, 'newstest2009.de.en': 26.8, 'newstest2010.de.en': 30.2, 'newstest2011.de.en': 27.4, 'newstest2012.de.en': 29.1, 'newstest2013.de.en': 32.1, 'newstest2014-deen.de.en': 34.0, 'newstest2015-ende.de.en': 34.2, 'newstest2016-ende.de.en': 40.4, 'newstest2017-ende.de.en': 35.7, 'newstest2018-ende.de.en': 43.7, 'newstest2019-deen.de.en': 40.1, 'Tatoeba.de.en': 55.4}}, 'description': 'A German to English translation model trained on the OPUS dataset using the Hugging Face Transformers library.'}} </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج لاكتشاف النشاط الصوتي عن طريق الصوت.? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"VoiceActivityDetection(segmentation='anilbs/segmentation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class from the pyannote.audio package. This includes VoiceActivityDetection for detecting segments when someone is speaking.\\\\n2. We then use the VoiceActivityDetection class with the 'anilbs/segmentation' segmentation to create a voice activity detection pipeline.\\\\n3. Set default hyperparameters like 'onset', 'offset', 'min_duration_on', and 'min_duration_off' as specified.\\\\n4. This pipeline can then be used to analyze an audio file, in your case, a recorded call, and identify the stages when someone is speaking.\\\", '<<<code>>>': \\\"from pyannote.audio.pipelines import VoiceActivityDetection\\\\npipeline = VoiceActivityDetection(segmentation='anilbs/segmentation')\\\\nHYPER_PARAMETERS = {\\\\n    'onset': 0.5,\\\\n    'offset': 0.5,\\\\n    'min_duration_on': 0.0,\\\\n    'min_duration_off': 0.0\\\\n}\\\\npipeline.instantiate(HYPER_PARAMETERS)\\\\nvad = pipeline('audio.wav')\\\\n# replace 'audio.wav' with path to your recorded call file\\\", 'api_call': \\\"VoiceActivityDetection(segmentation='anilbs/segmentation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'anilbs/segmentation', 'api_call': \\\"VoiceActivityDetection(segmentation='anilbs/segmentation')\\\", 'api_arguments': {'onset': 0.5, 'offset': 0.5, 'min_duration_on': 0.0, 'min_duration_off': 0.0}, 'python_environment_requirements': ['pyannote.audio 2.0'], 'example_code': {'voice_activity_detection': 'from pyannote.audio.pipelines import VoiceActivityDetection\\\\npipeline = VoiceActivityDetection(segmentation=anilbs/segmentation)\\\\nHYPER_PARAMETERS = {\\\\n onset: 0.5, offset: 0.5,\\\\n min_duration_on: 0.0,\\\\n min_duration_off: 0.0\\\\n}\\\\npipeline.instantiate(HYPER_PARAMETERS)\\\\nvad = pipeline(audio.wav)'}, 'performance': {'dataset': [{'name': 'AMI Mix-Headset', 'accuracy': {'onset': 0.684, 'offset': 0.577, 'min_duration_on': 0.181, 'min_duration_off': 0.037}}, {'name': 'DIHARD3', 'accuracy': {'onset': 0.767, 'offset': 0.377, 'min_duration_on': 0.136, 'min_duration_off': 0.067}}, {'name': 'VoxConverse', 'accuracy': {'onset': 0.767, 'offset': 0.713, 'min_duration_on': 0.182, 'min_duration_off': 0.501}}]}, 'description': 'Model from End-to-end speaker segmentation for overlap-aware resegmentation, by Hervé Bredin and Antoine Laurent. Online demo is available as a Hugging Face Space.' </s>\"\n"}
{"text": "<s>[INST]يجب على الباحثين في مجال معالجة اللغة الطبيعية إنشاء نصوص توليدية عالية الجودة تستخدم للتدريب المعمق لنماذج اللغة؛ لذا، ينبغي لهم تطوير نموذج جيل النصوص المحفوظ يتيح لمستخدميه الوصول إلى إمكانيات التنبؤ بالنص المتقدم والمناسب للسياق.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('af1tang/personaGPT')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library, including GPT2Tokenizer and GPT2LMHeadModel.\\\\n2. Load the pre-trained model 'af1tang/personaGPT' using the AutoModelForCausalLM class.\\\\n3. Create a tokenizer using AutoTokenizer.from_pretrained.\\\\n4. Feed the persona facts and action codes (e.g., description of the bot's personality and turn-level goals) into the model. These will help generate appropriate responses for a personalized conversation about a vacation.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\nimport torch\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'af1tang/personaGPT\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'af1tang/personaGPT\\\\')\\\\n\\\\nif torch.cuda.is_available():\\\\n   model = model.cuda()\\\\n\\\\npersonas = \\\\'<persona facts>\\\\'\\\\ndialog_hx = [<action codes>]\\\\n\\\\nbot_input_ids = tokenizer.encode(personas + \\\\' \\\\'.join(dialog_hx), return_tensors=\\\\'pt\\\\')\\\\nresponse = model.generate(bot_input_ids, do_sample=True, top_k=10, top_p=0.92, max_length=1000)\\\\ngenerated_response = tokenizer.decode(response[0], skip_special_tokens=True)\\\\nprint(\\\"Bot:\\\", generated_response)', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('af1tang/personaGPT')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'af1tang/personaGPT', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('af1tang/personaGPT')\\\", 'api_arguments': {'do_sample': 'True', 'top_k': '10', 'top_p': '.92', 'max_length': '1000', 'pad_token': 'tokenizer.eos_token_id'}, 'python_environment_requirements': [{'transformers': 'GPT2Tokenizer, GPT2LMHeadModel', 'torch': 'torch'}], 'example_code': {'load_model': 'tokenizer = AutoTokenizer.from_pretrained(af1tang/personaGPT)\\\\nmodel = AutoModelForCausalLM.from_pretrained(af1tang/personaGPT)\\\\nif torch.cuda.is_available():\\\\n model = model.cuda()', 'generate_response': 'bot_input_ids = to_var([personas + flatten(dialog_hx)]).long()\\\\nmsg = generate_next(bot_input_ids)\\\\ndialog_hx.append(msg)\\\\nprint(Bot: {}.format(tokenizer.decode(msg, skip_special_tokens=True)))'}, 'performance': {'dataset': 'Persona-Chat', 'accuracy': 'Not provided'}, 'description': 'PersonaGPT is an open-domain conversational agent designed to do 2 tasks: decoding personalized responses based on input personality facts (the persona profile of the bot) and incorporating turn-level goals into its responses through action codes (e.g., talk about work, ask about favorite music). It builds on the DialoGPT-medium pretrained model based on the GPT-2 architecture.' </s>\"\n"}
{"text": "<s>[INST]قم بتضمين حقل اللغة الأصلية في النص لاستكشاف تعزيز التعلم وتقديم جملة معنوية متكاملة في مجال معالجة اللغة الطبيعية والترجمة.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"pipeline('translation_en_to_ROMANCE', model='Helsinki-NLP/opus-mt-en-ROMANCE')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import pipeline\\\\ntranslation = pipeline(\\\\'translation_en_to_ROMANCE\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-ROMANCE\\\\')\\\\nsocial_media_content = \\\"Check out our newest product line!\\\"\\\\ntranslated_content = translation(social_media_content, tgt_lang=\\\\'it\\\\')', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Translation', 'api_name': 'opus-mt-en-ROMANCE', 'api_call': 'from transformers import pipeline\\\\ntranslation = pipeline(\\\\'translation_en_to_ROMANCE\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-ROMANCE\\\\')\\\\nsocial_media_content = \\\"Check out our newest product line!\\\"\\\\ntranslated_content = translation(social_media_content, tgt_lang=\\\\'it\\\\')', 'api_arguments': 'source languages, target languages', 'python_environment_requirements': 'transformers, torch', 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ROMANCE', model='Helsinki-NLP/opus-mt-en-ROMANCE')\\\\ntranslated_text = translation('Hello, how are you?', tgt_lang='es')\\\", 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': 50.1, 'chr-F': 0.693}}, 'description': 'A translation model trained on the OPUS dataset that supports translation between English and various Romance languages. It uses a transformer architecture and requires a sentence initial language token in the form of >>id<< (id = valid target language ID).'}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لتدريب الروبوتات باستخدام تقنيات تعزيز الجمل في مجال معالجة اللغة الطبيعية، حيث يقوم النموذج بتعلم كيفية اتخاذ القرارات الصحيحة في ظل البيئة المتغيرة باستخدام التعزيز. يمكنك استخدام خوارزميات تعلم الآلة التي تعتمد على التعزيز مثل Q-learning أو Deep Q Networks لتدريب النموذج. كما يُنصح بتقديم بيئة افتراضية للروبوت لاختبار أداء النموذج وتحسين استجابته في مهام الروبوتات.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModel.from_pretrained('typeform/squeezebert-mnli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'typeform/squeezebert-mnli'.\\\\n3. The loaded model specializes in zero-shot classification, which predicts the category of an unseen example without having matching training data.\\\\n4. Provide the input text (meeting email invitation), as well as a list of candidate categories ('operations', 'marketing', 'finance', 'technology').\\\\n5. Use the loaded model to classify the given text into the appropriate category.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'typeform/squeezebert-mnli\\\\')\\\\nemail_invitation = \\\"Dear Marketing Team, Please join us for a meeting to discuss our upcoming new marketing campaign. We will review the objectives, budget, and timeline to ensure a successful launch. Looking forward to your participation. Best, [Your Name]\\\"\\\\ncategories = [\\\\'operations\\\\', \\\\'marketing\\\\', \\\\'finance\\\\', \\\\'technology\\\\']\\\\nresult = classifier(email_invitation, categories)', 'api_call': \\\"AutoModel.from_pretrained('typeform/squeezebert-mnli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'typeform/squeezebert-mnli', 'api_call': \\\"AutoModel.from_pretrained('typeform/squeezebert-mnli')\\\", 'api_arguments': 'text, candidate_labels, multi_label', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nnlp = pipeline('zero-shot-classification', model='typeform/squeezebert-mnli')\\\\nresult = nlp('The quick brown fox jumps over the lazy dog', candidate_labels=['sports', 'language', 'animals'])\\\", 'performance': {'dataset': 'mulit_nli', 'accuracy': 'not provided'}, 'description': 'SqueezeBERT is a transformer model designed for efficient inference on edge devices. This specific model, typeform/squeezebert-mnli, is fine-tuned on the MultiNLI dataset for zero-shot classification tasks.' </s>\"\n"}
{"text": "<s>[INST]قم بتنفيذ تصنيف الصور باستخدام تقنيات الرؤية الحاسوبية. ابدأ بتحميل مجموعة بيانات الصور ثم قم بتدريب نموذج تصنيف الصور واختبر أداء النموذج على مجموعة بيانات الاختبار.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries from the transformers package, PIL, and requests. This includes AutoFeatureExtractor, SwinForImageClassification, and Image for processing image data.\\\\n2. Use the from_pretrained method of the SwinForImageClassification class to load the pre-trained model 'microsoft/swin-tiny-patch4-window7-224'. This model has been trained for image classification tasks, which is exactly what we need for detecting the main object in an image.\\\\n3. Load the image data, either from a file or by fetching it from a URL.\\\\n4. Next, use the from_pretrained method of the AutoFeatureExtractor class to load a feature extractor for the same model.\\\\n5. Use the feature extractor to preprocess the image and create input tensors.\\\\n6. Pass these input tensors to the image classification model, which outputs logits representing the confidence scores for each possible object class.\\\\n7. Find the class with the highest confidence score and print the detected object's name.\\\", '<<<code>>>': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = \\\\'http://images.cocodataset.org/val2017/000000039769.jpg\\\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\\\\'microsoft/swin-tiny-patch4-window7-224\\\\')\\\\nmodel = SwinForImageClassification.from_pretrained(\\\\'microsoft/swin-tiny-patch4-window7-224\\\\')\\\\ninputs = feature_extractor(images=image, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/swin-tiny-patch4-window7-224', 'api_call': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': [{'transformers': 'AutoFeatureExtractor', 'PIL': 'Image', 'requests': 'requests'}], 'example_code': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء صور بدون شروط لمشاهد تصور الحيوانات في البرية بشكل أوتوماتيكي. هل يمكنك مساعدتي في تصميم النموذج الذي يقوم بتكوين الصور؟? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import DDPMPipeline from the diffusers library.\\\\n2. Use the DDPMPipeline.from_pretrained method to load the model 'clp/sd-class-butterflies-32'. This model is designed to generate high-quality images of butterflies.\\\\n3. Call the pipeline to generate an unconditional image of a butterfly. You can access the generated image by indexing the pipeline output.\\\\n4. The resulting image can be used as a high-quality graphic for the publishing house's upcoming biology book.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\\ngenerated_butterfly_image = pipeline().images[0]\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sd-class-butterflies-32', 'api_call': \\\"DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\", 'api_arguments': {'model_id': 'clp/sd-class-butterflies-32'}, 'python_environment_requirements': [['diffusers']], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': None, 'accuracy': None}, 'description': 'This model is a diffusion model for unconditional image generation of cute butterflies.' </s>\"\n"}
{"text": "<s>[INST]يتعلق زبوننا بتطوير تطبيق للأشخاص ذوي الإعاقة البصرية. نحن بحاجة إلى برنامج يقوم بتحويل النص إلى كلام لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/fastspeech2-en-200_speaker-cv4',arg_overrides={'vocoder': 'hifigan', 'fp16': False})\\\", '<<<api_provider>>>': 'Fairseq', '<<<explanation>>>': \\\"1. Import the necessary packages, including fairseq and IPython.\\\\n2. Use the load_model_ensemble_and_task_from_hf_hub function to create the FastSpeech 2 text-to-speech model. The function takes the model 'facebook/fastspeech2-en-200_speaker-cv4', which is trained on English male and female voices.\\\\n3. Update the configuration with the data and build a generator.\\\\n4. Use the TTSHubInterface functions to convert a text input (e.g., the weather forecast) into an audio waveform, and play the audio using IPython.display.Audio.\\\", '<<<code>>>': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\'facebook/fastspeech2-en-200_speaker-cv4\\\\', arg_overrides={\\\\'vocoder\\\\': \\\\'hifigan\\\\', \\\\'fp16\\\\': False})\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ntext = \\\"Today\\\\'s weather is partly cloudy with a high of 75 degrees.\\\"\\\\nmodel = models[0]\\\\ngenerator = task.build_generator(model, cfg)\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\n\\\\nipd.Audio(wav, rate=rate)', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/fastspeech2-en-200_speaker-cv4',arg_overrides={'vocoder': 'hifigan', 'fp16': False})\\\", 'provider': 'Fairseq', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'fastspeech2-en-male1', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/fastspeech2-en-200_speaker-cv4',arg_overrides={'vocoder': 'hifigan', 'fp16': False})\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['fairseq', 'IPython']], 'example_code': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n facebook/fastspeech2-en-200_speaker-cv4,\\\\n arg_overrides={vocoder: hifigan, fp16: False}\\\\n)\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\ntext = Hello, this is a test run.\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)', 'performance': {'dataset': 'common_voice', 'accuracy': None}, 'description': 'FastSpeech 2 text-to-speech model from fairseq S^2. English, 200 male/female voices, trained on Common Voice v4.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام لتصنيف الصور بدون زاد مستخدماً تقنيات التعرف البصري. نهدف إلى توجيه النموذج لتصنيف الصور بشكل دقيق باستخدام البيانات النصية والبصرية المتاحة.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the required libraries and functions such as pipeline from the transformers package, PIL.Image and os.\\\\n2. Instantiate the zero-shot image classification pipeline with the model 'laion/CLIP-ViT-g-14-laion2B-s34B-b88K' from Hugging Face.\\\\n3. Load the image you want to classify into the PIL.Image object.\\\\n4. Define a list of insect categories, such as ants, bees, butterflies, etc., that you want to classify the captured images into.\\\\n5. Use the zero-shot classification pipeline to classify the image into the provided insect categories.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nfrom PIL import Image\\\\nimport os\\\\n\\\\nimage_path = \\\\'path/to/image/file\\\\'  # replace with the path to your image\\\\nimage = Image.open(image_path)\\\\nclass_names = [\\\"ant\\\", \\\"bee\\\", \\\"butterfly\\\", \\\"spider\\\", \\\"ladybug\\\", \\\"grasshopper\\\"]\\\\n\\\\nclassifier = pipeline(\\\\'zero-shot-image-classification\\\\', model=\\\\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\\\')\\\\nprediction = classifier(image, class_names)\\\\nprint(prediction)', 'api_call': \\\"pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-ViT-g-14-laion2B-s34B-b88K', 'api_call': \\\"pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\\\", 'api_arguments': {'image': 'path/to/image/file', 'class_names': 'list_of_class_names'}, 'python_environment_requirements': [{'huggingface_hub': '0.0.17', 'transformers': '4.11.3', 'torch': '1.9.0', 'torchvision': '0.10.0'}], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نموذج في مجال تحويل النصوص الصوتية إلى نصوص، حيث نقوم بتوليد النصوص الصوتية بشكل آلي باستخدام تقنيات تعالج اللغات الطبيعية وتقنيات توليد النصوص من قواعد البيانات الصوتية المتاحة.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"api.load('ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS').\\\", '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. Import the necessary libraries, which in this case are 'Api' from 'huggingface_hub' and the 'espnet_model_zoo'.\\\\n2. Load the Marathi Male Text-to-Speech model using the api.load() function from huggingface_hub, passing the identifier 'ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS'.\\\\n3. After loading the pre-trained Marathi Male TTS model, you can use it to generate speech given a Marathi text input. This generated voice over can be used for advertisements or any other multimedia content.\\\", '<<<code>>>': \\\"from huggingface_hub import Api\\\\nfrom espnet_model_zoo import apimp\\\\napi = apimp.Api()\\\\ntts_model = api.load('ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS')\\\\n# text_input = Insert your Marathi text here for generating speech\\\\n# generated_speech = tts_model.generate_speech(text_input)\\\", 'api_call': \\\"api.load('ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS').\\\", 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'SYSPIN/Marathi_Male_TTS', 'api_call': \\\"api.load('ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS').\\\", 'api_arguments': [], 'python_environment_requirements': [['huggingface_hub']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Marathi Male Text-to-Speech model using ESPnet framework.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج للإجابة على أسئلة الجدول في مجال معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"pipeline('table-question-answering', model='dsba-lab/koreapas-finetuned-korwikitq')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' function from the transformers library.\\\\n2. Use the 'pipeline' function to create a table-question-answering pipeline, specifying the pre-trained Korean model 'dsba-lab/koreapas-finetuned-korwikitq' as the model to be loaded.\\\\n3. The created model can be used to answer queries provided in the Korean language by using 'table' and 'query' as input for the table-question-answering pipeline.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntable_qa = pipeline('table-question-answering', model='dsba-lab/koreapas-finetuned-korwikitq')\\\\nanswer = table_qa(table=table, query='질문')\\\", 'api_call': \\\"pipeline('table-question-answering', model='dsba-lab/koreapas-finetuned-korwikitq')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dsba-lab/koreapas-finetuned-korwikitq', 'api_call': \\\"pipeline('table-question-answering', model='dsba-lab/koreapas-finetuned-korwikitq')\\\", 'api_arguments': {}, 'python_environment_requirements': [{'transformers': '>=4.0.0'}], 'example_code': \\\"from transformers import pipeline; table_qa = pipeline('table-question-answering', model='dsba-lab/koreapas-finetuned-korwikitq'); table_qa(table=table, query='질문')\\\", 'performance': {'dataset': 'korwikitq', 'accuracy': None}, 'description': 'A Korean Table Question Answering model finetuned on the korwikitq dataset.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد تسجيل صوتي من النص الصوتي للنص الذي تم توفيره. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': 'TextToSpeech(NeuML/ljspeech-jets-onnx)', '<<<api_provider>>>': 'ONNX', '<<<explanation>>>': \\\"1. First, import the necessary libraries, which include soundfile, txtai.pipeline, onnxruntime, yaml, and ttstokenizer.\\\\n2. We then use the TextToSpeech function from the txtai.pipeline library to load the 'NeuML/ljspeech-jets-onnx' model. This model has been trained for text-to-speech tasks, which is exactly what we need for converting text into speech for a podcast project.\\\\n3. With the model loaded, we can provide it with the text we wish to convert into speech. The model will then generate the speech, which can be saved into a file using the soundfile.write function, with a sample rate of 22,050 Hz.\\\", '<<<code>>>': 'import soundfile as sf\\\\nfrom txtai.pipeline import TextToSpeech\\\\ntts = TextToSpeech(\\\\'NeuML/ljspeech-jets-onnx\\\\')\\\\nspeech = tts(\\\"Text to convert to speech\\\")\\\\nsf.write(\\\\'out.wav\\\\', speech, 22050)', 'api_call': 'TextToSpeech(NeuML/ljspeech-jets-onnx)', 'provider': 'ONNX', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'NeuML/ljspeech-jets-onnx', 'api_call': 'TextToSpeech(NeuML/ljspeech-jets-onnx)', 'api_arguments': ['text'], 'python_environment_requirements': [['soundfile', 'txtai.pipeline', 'onnxruntime', 'yaml', 'ttstokenizer']], 'example_code': 'import soundfile as sf\\\\nfrom txtai.pipeline import TextToSpeech\\\\n\\\\n# Build pipeline\\\\ntts = TextToSpeech(NeuML/ljspeech-jets-onnx)\\\\n\\\\n# Generate speech\\\\nspeech = tts(Say something here)\\\\n\\\\n# Write to file\\\\nsf.write(out.wav, speech, 22050)', 'performance': {'dataset': 'ljspeech', 'accuracy': None}, 'description': 'ESPnet JETS Text-to-Speech (TTS) Model for ONNX exported using the espnet_onnx library. Can be used with txtai pipeline or directly with ONNX.' </s>\"\n"}
{"text": "<s>[INST]أنشئ جملة مفتاحية تستخدم في توليد النصوص التلقائي في معالجة اللغات الطبيعية تبدأ بـ \\\"بناء نموذج توليد النصوص\\\" لمجال تحويل الصور المتعددة الى نص.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and classes, such as TrOCRProcessor, VisionEncoderDecoderModel from transformers, Image from PIL, and requests.\\\\n2. Download the image using the supplied URL and convert it into an RGB format using the Image class from the PIL library.\\\\n3. Load the pre-trained 'microsoft/trocr-small-handwritten' model using the VisionEncoderDecoderModel.from_pretrained() and TrOCRProcessor.from_pretrained() methods.\\\\n4. Process the image using the TrOCRProcessor to obtain the pixel values.\\\\n5. Use the generated pixel values as input to the VisionEncoderDecoderModel, and use the generate() method to obtain the generated_ids.\\\\n6. Finally, decode the generated_ids using the processor's batch_decode() function to obtain the extracted text from the handwritten image.\\\", '<<<code>>>': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]يرجى توليد إعادة انشاء صوتية من النص الوصفي لمشهد يصور الحيوانات في البرية باستخدام تقنيات التعرف التلقائي على الكلام والرد على الأسئلة المتعلقة بالمحتوى. هل يمكنك توجيهي نحو الخطوات اللازمة لتنفيذ هذه المهمة؟? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-xlsr-53-espeak-cv-ft')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. These include Wav2Vec2Processor and Wav2Vec2ForCTC.\\\\n2. We then use the from_pretrained method of the Wav2Vec2Processor and Wav2Vec2ForCTC classes to load the pre-trained model 'facebook/wav2vec2-xlsr-53-espeak-cv-ft'. This model has been trained for automatic speech recognition tasks.\\\\n3. Use the processor to process the audio clip into input values for the model.\\\\n4. Pass the input values to the model, and it will output logits.\\\\n5. Finally, get the predicted_ids by taking the argmax of logits, and use the processor to batch_decode the predicted_ids into a transcription (text).\\\", '<<<code>>>': 'from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nprocessor = Wav2Vec2Processor.from_pretrained(\\\\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\\\')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\\\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\\\')\\\\naudio_clip = \\\"path/to/audio_clip.wav\\\"\\\\ninput_values = processor(audio_clip, return_tensors=\\\"pt\\\").input_values\\\\nwith torch.no_grad():\\\\n    logits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-xlsr-53-espeak-cv-ft')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/wav2vec2-xlsr-53-espeak-cv-ft', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-xlsr-53-espeak-cv-ft')\\\", 'api_arguments': {'model_name': 'facebook/wav2vec2-xlsr-53-espeak-cv-ft'}, 'python_environment_requirements': [{'transformers': '4.13.0', 'torch': '1.10.0', 'datasets': '1.14.0'}], 'example_code': 'processor = Wav2Vec2Processor.from_pretrained(facebook/wav2vec2-xlsr-53-espeak-cv-ft)\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(facebook/wav2vec2-xlsr-53-espeak-cv-ft)\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\ninput_values = processor(ds[0][audio][array], return_tensors=pt).input_values\\\\nwith torch.no_grad():\\\\n    logits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'performance': {'dataset': 'common_voice', 'accuracy': 'Not specified'}, 'description': 'Wav2Vec2-Large-XLSR-53 finetuned on multi-lingual Common Voice for phonetic label recognition in multiple languages. The model outputs a string of phonetic labels, and a dictionary mapping phonetic labels to words has to be used to map the phonetic output labels to output words.' </s>\"\n"}
{"text": "<s>[INST]عميلنا يرغب في تطوير تطبيق صوتي إلى صوتي. نحتاج إلى بشكلٍ أساسي برنامج يتمكن من تحويل الصوت إلى صوت بحيث يكون ملائماً للمستخدمين الذين يعانون من إعاقة بصرية.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import pipeline function from the transformers library provided by Hugging Face.\\\\n2. Load the audio source separation model 'mpariente/DPRNNTasNet-ks2_WHAM_sepclean' using the pipeline function with the task 'audio-source-separation'.\\\\n3. Use the created pipeline to separate the sources in the input music clip.\\\\n4. Examine the output of the pipeline, which will tell you if the music clip contains multiple sources or not.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsource_separator = pipeline(\\\\'audio-source-separation\\\\', model=\\\\'mpariente/DPRNNTasNet-ks2_WHAM_sepclean\\\\')\\\\nsources = source_separator(\\\"path_to_audio_file.ogg\\\")\\\\nif len(sources) > 1:\\\\n    print(\\\"The music clip contains multiple sources.\\\")\\\\nelse:\\\\n    print(\\\"The music clip contains only a single source.\\\")', 'api_call': \\\"pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mpariente/DPRNNTasNet-ks2_WHAM_sepclean', 'api_call': \\\"pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')\\\", 'api_arguments': 'audio_file', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'WHAM!', 'si_sdr': 19.316743490695334, 'si_sdr_imp': 19.317895273889842, 'sdr': 19.68085347190952, 'sdr_imp': 19.5298092932871, 'sir': 30.362213998701232, 'sir_imp': 30.21116982007881, 'sar': 20.15553251343315, 'sar_imp': -129.02091762351188, 'stoi': 0.97772664309074, 'stoi_imp': 0.23968091518217424}, 'description': 'This model was trained by Manuel Pariente using the wham/DPRNN recipe in Asteroid. It was trained on the sep_clean task of the WHAM! dataset.' </s>\"\n"}
{"text": "<s>[INST]إنشاء نادي طلابي متعدد الوسائط في مجال إجابة أسئلة المستندات، حيث يلتقي الأعضاء كل أسبوعين لممارسة لعبة كرة القدم الافتراضية. يُنصح بتوفير أداة تمكنهم من اللعب ضد وكيل التعلم الآلي لحل أسئلة حول الجداول.? [/INST]{'domain': 'Multimodal Document Question Answer', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import AutoModelForDocumentQuestionAnswering, AutoTokenizer\\\\nfrom PIL import Image\\\\n\\\\nmodel = AutoModelForDocumentQuestionAnswering.from_pretrained(\\\\'tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\\\\')\\\\n\\\\nquestion = \\\"What is the invoice number?\\\"\\\\nimage = Image.open(\\\\'path/to/invoice/image.jpg\\\\')\\\\n\\\\ntokens = tokenizer(question, image, return_tensors=\\\\'pt\\\\')\\\\nlogits = model(**tokens)[0].squeeze()\\\\nanswer = tokenizer.decode(logits.argmax())', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': 'from transformers import AutoModelForDocumentQuestionAnswering, AutoTokenizer\\\\nfrom PIL import Image\\\\n\\\\nmodel = AutoModelForDocumentQuestionAnswering.from_pretrained(\\\\'tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\\\\')\\\\n\\\\nquestion = \\\"What is the invoice number?\\\"\\\\nimage = Image.open(\\\\'path/to/invoice/image.jpg\\\\')\\\\n\\\\ntokens = tokenizer(question, image, return_tensors=\\\\'pt\\\\')\\\\nlogits = model(**tokens)[0].squeeze()\\\\nanswer = tokenizer.decode(logits.argmax())', 'api_arguments': 'question, image', 'python_environment_requirements': 'transformers, torch, datasets, tokenizers', 'example_code': '', 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.3167}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتحليل النص وترجمته باستخدام معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_en_to_it', model='Helsinki-NLP/opus-mt-en-it')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"pipeline('translation_en_to_it', model='Helsinki-NLP/opus-mt-en-it')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-en-it', 'api_call': \\\"pipeline('translation_en_to_it', model='Helsinki-NLP/opus-mt-en-it')\\\", 'api_arguments': {'source_language': 'en', 'target_language': 'it'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; translator = pipeline('translation_en_to_it', model='Helsinki-NLP/opus-mt-en-it'); translator('Hello, world!')\\\", 'performance': {'dataset': 'opus', 'accuracy': {'newssyscomb2009.en.it': {'BLEU': 30.9, 'chr-F': 0.606}, 'newstest2009.en.it': {'BLEU': 31.9, 'chr-F': 0.604}, 'Tatoeba.en.it': {'BLEU': 48.2, 'chr-F': 0.695}}}, 'description': 'A Transformer-based English to Italian translation model trained on the OPUS dataset. This model can be used for translation tasks using the Hugging Face Transformers library.' </s>\"\n"}
{"text": "<s>[INST]قم بإعداد وثيقة نصية تتضمن نصوص وصور ومقاطع صوتية في مجال تصنيف الصور دون الحاجة لأي بيانات مسبقة في حوزتك، ثم أعطها للطلاب لقراءتها ومشاهدة محتواها. بعد ذلك، اطلب منهم تقديم الأسئلة المتعلقة بالوثيقة وإجابتها باستخدام أداة متعددة الوسائط.? [/INST]{'domain': 'Computer Vision Zero-Shot Image Classification', 'api_call': \\\"CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary classes and libraries, including Image from PIL, requests, CLIPProcessor, and CLIPModel from transformers.', 'code': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\n\\\\nmodel = CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')\\\\nprocessor = CLIPProcessor.from_pretrained('flax-community/clip-rsicd-v2')\\\\nimage_url = 'https://example.com/satellite_image.jpg'\\\\nimage = Image.open(requests.get(image_url, stream=True).raw)\\\\nlabels = ['a photo of a forest', 'a photo of deforestation']\\\\n\\\\ninputs = processor(text=labels, images=image, return_tensors='pt', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\\n\\\\nresult = dict(zip(labels, probs[0]))\\\\nhighest_prob_label = max(result, key=result.get)\\\", 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'flax-community/clip-rsicd-v2', 'api_call': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\n\\\\nmodel = CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')\\\\nprocessor = CLIPProcessor.from_pretrained('flax-community/clip-rsicd-v2')\\\\nimage_url = 'https://example.com/satellite_image.jpg'\\\\nimage = Image.open(requests.get(image_url, stream=True).raw)\\\\nlabels = ['a photo of a forest', 'a photo of deforestation']\\\\n\\\\ninputs = processor(text=labels, images=image, return_tensors='pt', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\\n\\\\nresult = dict(zip(labels, probs[0]))\\\\nhighest_prob_label = max(result, key=result.get)\\\", 'api_arguments': {'text': ['a photo of a residential area', 'a photo of a playground', 'a photo of a stadium', 'a photo of a forest', 'a photo of an airport'], 'images': 'image', 'return_tensors': 'pt', 'padding': 'True'}, 'python_environment_requirements': ['PIL', 'requests', 'transformers'], 'example_code': 'from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(flax-community/clip-rsicd-v2)\\\\nprocessor = CLIPProcessor.from_pretrained(flax-community/clip-rsicd-v2)\\\\nurl = https://raw.githubusercontent.com/arampacha/CLIP-rsicd/master/data/stadium_1.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nlabels = [residential area, playground, stadium, forest, airport]\\\\ninputs = processor(text=[fa photo of a {l} for l in labels], images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\\nfor l, p in zip(labels, probs[0]):\\\\n print(f{l:&lt;16} {p:.4f})', 'performance': {'dataset': {'RSICD': {'original CLIP': {'k=1': 0.572, 'k=3': 0.745, 'k=5': 0.837, 'k=10': 0.939}, 'clip-rsicd-v2 (this model)': {'k=1': 0.883, 'k=3': 0.968, 'k=5': 0.982, 'k=10': 0.998}}}}, 'description': 'This model is a fine-tuned CLIP by OpenAI. It is designed with an aim to improve zero-shot image classification, text-to-image and image-to-image retrieval specifically on remote sensing images.'}} </s>\"\n"}
{"text": "<s>[INST]نقدم لكم تطبيقًا مبتكرًا للتعامل مع الملفات الصوتية، يتيح للمستخدمين تحويل الصوت إلى نص والنص إلى صوت بسهولة، مع إمكانية تحسين دقة التعرف على الصوت وفهم المحتوى الصوتي بشكل أوسع وأعمق. اكتشف تحسينات على التعرف الصوتي مع تطبيق Text-to-Speech الخاص بنا في مجال الصوتيات.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804')\\\", '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. Import the necessary libraries, including transformers and AutoModelForCausalLM.\\\\n2. Use AutoModelForCausalLM.from_pretrained() method from transformers to load the Japanese text-to-speech model 'espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804'.\\\\n3. The model can then be used to convert a given text input into natural-sounding speech in Japanese.\\\\n4. A suitable text-to-speech pipeline can be built around this model to create a speechbot for converting Japanese news articles into speech efficiently.\\\", '<<<code>>>': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nmodel_checkpoint = \\\\'espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(model_checkpoint)\\\\n\\\\ntext = \\\"Japanese news article text…\\\"\\\\ninputs = tokenizer(text, return_tensors=\\\"pt\\\")\\\\noutputs = model(**inputs)\\\\ngenerated_speech = outputs.generated_speech', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804')\\\", 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Japanese text-to-speech model trained using the ESPnet framework. It is designed to convert text input into natural-sounding speech.' </s>\"\n"}
{"text": "<s>[INST]نحن بحاجة إلى تطوير نماذج لتحليل تشابه الجمل في معالجة اللغة الطبيعية، وذلك من خلال تحويل الجمل إلى بيانات قياسية قابلة للمقارنة.? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"BertModel.from_pretrained('setu4993/LaBSE')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the necessary classes from the torch and transformers packages. This includes BertModel for sentence similarity and BertTokenizerFast for tokenizing.', 'code': 'import torch\\\\nfrom transformers import BertModel, BertTokenizerFast\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\\\'setu4993/LaBSE\\\\')\\\\nmodel = BertModel.from_pretrained(\\\\'setu4993/LaBSE\\\\')\\\\nmodel = model.eval()\\\\nexample_speaker_interests = [\\\\'dog\\\\', \\\"Interazione uomo-cane\\\", \\\"犬とのコミュニケーション\\\"]\\\\ninputs = tokenizer(example_speaker_interests, return_tensors=\\\\'pt\\\\', padding=True)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nembeddings = outputs.pooler_output', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': 'import torch\\\\nfrom transformers import BertModel, BertTokenizerFast\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\\\'setu4993/LaBSE\\\\')\\\\nmodel = BertModel.from_pretrained(\\\\'setu4993/LaBSE\\\\')\\\\nmodel = model.eval()\\\\nexample_speaker_interests = [\\\\'dog\\\\', \\\"Interazione uomo-cane\\\", \\\"犬とのコミュニケーション\\\"]\\\\ninputs = tokenizer(example_speaker_interests, return_tensors=\\\\'pt\\\\', padding=True)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nembeddings = outputs.pooler_output', 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \\\"import torch\\\\nfrom transformers import BertModel, BertTokenizerFast\\\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\\\nmodel = model.eval()\\\\nenglish_sentences = [\\\\n 'dog',\\\\n 'Puppies are nice.',\\\\n 'I enjoy taking long walks along the beach with my dog.',\\\\n]\\\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\\\nwith torch.no_grad():\\\\n english_outputs = model(**english_inputs)\\\\nenglish_embeddings = english_outputs.pooler_output\\\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.'}} </s>\"\n"}
{"text": "<s>[INST]يحتاج عميلنا في مجال الصوت من الصوت إلى برنامج يمكنه تحويل الصوت إلى صوت.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create an audio source separation model.\\\\n3. Specify the model 'mpariente/DPRNNTasNet-ks2_WHAM_sepclean' to be loaded. This model is trained for separating sources in audio files or recordings, specifically, it has been trained on the sep_clean task of the WHAM! dataset.\\\\n4. The created pipeline can be used to separate the different sources in the given audio file, such as background noise, voice, and speech.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nseparator = pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')\\\\nseparated_audio_sources = separator(audio_file)\\\", 'api_call': \\\"pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mpariente/DPRNNTasNet-ks2_WHAM_sepclean', 'api_call': \\\"pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')\\\", 'api_arguments': 'audio_file', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'WHAM!', 'si_sdr': 19.316743490695334, 'si_sdr_imp': 19.317895273889842, 'sdr': 19.68085347190952, 'sdr_imp': 19.5298092932871, 'sir': 30.362213998701232, 'sir_imp': 30.21116982007881, 'sar': 20.15553251343315, 'sar_imp': -129.02091762351188, 'stoi': 0.97772664309074, 'stoi_imp': 0.23968091518217424}, 'description': 'This model was trained by Manuel Pariente using the wham/DPRNN recipe in Asteroid. It was trained on the sep_clean task of the WHAM! dataset.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد نصوص تلقائياً في مجال معالجة اللغات الطبيعية. أريد توليد وصف دقيق لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في تحقيق ذلك باستخدام تقنيات توليد النصوص بالنصوص بطريقة مبتكرة ودقيقة؟? [/INST]{'domain': 'Natural Language Processing, Text2Text Generation', 'api_call': \\\"EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import BertTokenizerFast, EncoderDecoderModel\\\\ntokenizer = BertTokenizerFast.from_pretrained('kykim/bertshared-kor-base')\\\\nmodel = EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\\\", 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kykim/bertshared-kor-base', 'api_call': \\\"from transformers import BertTokenizerFast, EncoderDecoderModel\\\\ntokenizer = BertTokenizerFast.from_pretrained('kykim/bertshared-kor-base')\\\\nmodel = EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\\\", 'api_arguments': {'tokenizer': 'BertTokenizerFast.from_pretrained(kykim/bertshared-kor-base)'}, 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import BertTokenizerFast, EncoderDecoderModel\\\\ntokenizer = BertTokenizerFast.from_pretrained(kykim/bertshared-kor-base)\\\\nmodel = EncoderDecoderModel.from_pretrained(kykim/bertshared-kor-base)', 'performance': {'dataset': '70GB Korean text dataset', 'accuracy': '42000 lower-cased subwords'}, 'description': 'Bert base model for Korean, trained on a 70GB Korean text dataset and 42000 lower-cased subwords. Can be used for Text2Text Generation tasks.'}} </s>\"\n"}
{"text": "<s>[INST]يعمل عميلنا على تطوير تطبيق للإجابة على الأسئلة المرئية المتعددة الوسائط. نحتاج إلى برنامج قادر على تحويل النص إلى كلام لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"AutoModel.from_pretrained('microsoft/git-large-vqav2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'microsoft/git-large-vqav2'.\\\\n3. The loaded model will be used for visual-question-answering, which is a task where the model answers a given text question by processing a given image.\\\\n4. The model will process an image of the room and then answer a question related to some objects or situations in it.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nvqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0)\\\\n# Replace 'path_to_image' with the path to the image in question\\\\n# Replace 'your_question' with the question you want to ask about the image\\\\nresults = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})\\\", 'api_call': \\\"AutoModel.from_pretrained('microsoft/git-large-vqav2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/git-large-vqav2', 'api_call': \\\"AutoModel.from_pretrained('microsoft/git-large-vqav2')\\\", 'api_arguments': {'model': 'microsoft/git-large-vqav2', 'task': 'visual-question-answering', 'device': 0}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; vqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0); results = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})\\\", 'performance': {'dataset': 'VQAv2', 'accuracy': 'Refer to the paper'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لتصنيف مقاطع الفيديو باستخدام تقنيات رؤية الحاسوب.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries from transformers, such as AutoImageProcessor and TimesformerForVideoClassification.\\\\n2. Load the pretrained model 'facebook/timesformer-base-finetuned-k600' using TimesformerForVideoClassification.from_pretrained() method.\\\\n3. Process the video frames using AutoImageProcessor and create a torch tensor of the frames.\\\\n4. Pass the processed video tensor into the model to get the logits.\\\\n5. Find the predicted class index with the highest probability in the logits.\\\\n6. Finally, use the predicted class index to obtain the label of the action that is classified by the model.\\\", '<<<code>>>': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(\\\\'facebook/timesformer-base-finetuned-k600\\\\')\\\\nmodel = TimesformerForVideoClassification.from_pretrained(\\\\'facebook/timesformer-base-finetuned-k600\\\\')\\\\ninputs = processor(images=video, return_tensors=\\\\'pt\\\\')\\\\n\\\\n with torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n predicted_class_idx = logits.argmax(-1).item()\\\\n print(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/timesformer-base-finetuned-k600', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\", 'api_arguments': ['images'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k600)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k600)\\\\ninputs = processor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-600', 'accuracy': None}, 'description': 'TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]يجب على العميل تطوير تطبيق لمعالجة اللغة الطبيعية لقياس تشابه الجمل. نحن بحاجة إلى برنامج يقوم بتحويل النص إلى كلام لمستخدمي هذا التطبيق في مجال معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'SentenceTransformer' from sentence_transformers.\\\\n2. Instantiate the SentenceTransformer model with the name 'sentence-transformers/bert-base-nli-mean-tokens'.\\\\n3. The loaded model is trained to convert sentences into 768-dimensional embeddings which can be used to compute their semantic similarity.\\\\n4. Use the 'encode()' method to get the embeddings of input sentences.\\\\n5. Calculate the cosine similarity between the embeddings to determine the similarity of the two sentences.\\\\n6. A higher cosine similarity score indicates that the sentences are semantically similar.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/bert-base-nli-mean-tokens\\\\')\\\\nsentence1 = \\\"This is an example sentence.\\\"\\\\nsentence2 = \\\"Each sentence is converted.\\\"\\\\nembeddings = model.encode([sentence1, sentence2])\\\\nsimilarity = cosine_similarity(embeddings[0].reshape(1, -1), embeddings[1].reshape(1, -1))[0][0]\\\\nprint(f\\\"Cosine similarity: {similarity}\\\")', 'api_call': \\\"SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/bert-base-nli-mean-tokens', 'api_call': \\\"SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نماذج للكشف التلقائي عن الصوت بإستخدام تقنيات الرؤية الحاسوبية ونحتاج إلى تحديد نشاط الصوت بدقة عالية.? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"Model.from_pretrained('pyannote/brouhaha', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the pyannote.audio and brouhaha-vad libraries.\\\\n2. Import the Model class from pyannote.audio and use the from_pretrained method to load the 'pyannote/brouhaha' pre-trained model. Provide your Hugging Face API access token as the 'use_auth_token' parameter.\\\\n3. Create an Inference object using the imported model.\\\\n4. Use the Inference object to estimate voice activity detection, speech-to-noise ratio, and C50 room acoustics for the meeting audio.\\\\n5. The output can be further processed to identify voice activity segments, filter out noise, and provide a clean transcript of the meeting.\\\", '<<<code>>>': 'from pyannote.audio import Model\\\\nfrom pyannote.audio import Inference\\\\nmodel = Model.from_pretrained(\\\\'pyannote/brouhaha\\\\', use_auth_token=\\\\'ACCESS_TOKEN_GOES_HERE\\\\')\\\\ninference = Inference(model)\\\\noutput = inference(\\\\'audio.wav\\\\')\\\\nfor frame, (vad, snr, c50) in output:\\\\n    t = frame.middle\\\\n    print(f\\\"{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f}\\\")', 'api_call': \\\"Model.from_pretrained('pyannote/brouhaha', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pyannote/brouhaha', 'api_call': \\\"Model.from_pretrained('pyannote/brouhaha', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", 'api_arguments': ['audio.wav'], 'python_environment_requirements': [['pyannote-audio', 'brouhaha-vad']], 'example_code': ['from pyannote.audio import Model', 'model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)', 'from pyannote.audio import Inference', 'inference = Inference(model)', 'output = inference(audio.wav)', 'for frame, (vad, snr, c50) in output:', '  t = frame.middle', '  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})'], 'performance': {'dataset': 'LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene', 'accuracy': 'Not provided'}, 'description': 'Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نموذج لتصنيف الصوتيات باستخدام تقنيات الرؤية الحاسوبية الخاصة بعدم التصنيف المسبق للصور. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, including Image from PIL and ChineseCLIPModel and ChineseCLIPProcessor from transformers.\\\\n2. Load the pretrained Chinese-CLIP-ViT-Large-Patch14 model using the from_pretrained() method. This is a powerful model trained on a large-scale dataset of around 200 million Chinese image-text pairs.\\\\n3. Create a processor for preprocessing images and texts using the ChineseCLIPProcessor class.\\\\n4. Load the image to classify and preprocess the image using the processor.\\\\n5. Define a list of texts corresponding to the possible categories.\\\\n6. Compute image and text features and perform normalization on the computed features.\\\\n7. Use the model's output to calculate softmax probabilities for each category.\\\", '<<<code>>>': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\n\\\\nmodel = ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\\nprocessor = ChineseCLIPProcessor.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\\nurl = 'https://your-image-url.jpg'  # replace with your image URL\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\ntexts = ['category1', 'category2', 'category3']  # replace with your categories\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nimage_features = model.get_image_features(**inputs)\\\\nimage_features /= image_features.norm(p=2, dim=-1, keepdim=True)\\\\n\\\\ninputs = processor(text=texts, padding=True, return_tensors='pt')\\\\ntext_features = model.get_text_features(**inputs)\\\\ntext_features /= text_features.norm(p=2, dim=-1, keepdim=True)\\\\n\\\\ninputs = processor(text=texts, images=image, return_tensors='pt', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\", 'api_call': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'chinese-clip-vit-large-patch14', 'api_call': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\", 'api_arguments': {'model_name': 'OFA-Sys/chinese-clip-vit-large-patch14'}, 'python_environment_requirements': [{'libraries': ['transformers', 'PIL', 'requests']}], 'example_code': 'from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\\\\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\\\\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = []\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nimage_features = model.get_image_features(**inputs)\\\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True) # normalize\\\\ninputs = processor(text=texts, padding=True, return_tensors=pt)\\\\ntext_features = model.get_text_features(**inputs)\\\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True) # normalize\\\\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\\\\nprobs = logits_per_image.softmax(dim=1) # probs: [[0.0066, 0.0211, 0.0031, 0.9692]]', 'performance': {'dataset': 'MUGE Text-to-Image Retrieval, Flickr30K-CN Retrieval, COCO-CN Retrieval, CIFAR10, CIFAR100, DTD, EuroSAT, FER, FGV, KITTI, MNIST, PASCAL VOC', 'accuracy': 'Varies depending on the dataset'}, 'description': 'Chinese-CLIP-ViT-Large-Patch14 is a large version of the Chinese CLIP model, with ViT-L/14 as the image encoder and RoBERTa-wwm-base as the text encoder. Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It is designed for zero-shot image classification tasks.' </s>\"\n"}
{"text": "<s>[INST]يقوم العميل بتطوير تطبيق للتحليل الاحصائي. نحتاج الى برنامج لتنفيذ تحليل الانحدار لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': \\\"1. Import the 'pipeline' function from the transformers library.\\\\n2. Create a regression pipeline with the specified model 'merve/tips9y0jvt5q-tip-regression'. This model is a Ridge Regression trained on tips9y0jvt5q dataset.\\\\n3. Use the pipeline to make predictions based on the customer's total bill amount. The output will be an estimation of the tip amount.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nregression_pipeline = pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\\\\ntotal_bill = [[45.23]]\\\\npredicted_tip = regression_pipeline(total_bill)\\\", 'api_call': \\\"pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'merve/tips9y0jvt5q-tip-regression', 'api_call': \\\"pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['dabl'], 'example_code': 'N/A', 'performance': {'dataset': 'tips9y0jvt5q', 'accuracy': {'r2': 0.41524, 'neg_mean_squared_error': -1.098792}}, 'description': 'Baseline Model trained on tips9y0jvt5q to apply regression on tip. The model uses Ridge(alpha=10) and is trained with dabl library as a baseline. For better results, use AutoTrain.' </s>\"\n"}
{"text": "<s>[INST]على فريق التعقب عمل إلى صور لوحة مرسومة يدويًا وتحويلها إلى صور حقيقية لوحات فنية باستخدام واجهة التعليم الآلي للرؤية الحاسوبية.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, such as 'torch' and 'Swin2SRForConditionalGeneration' from the transformers library provided by Hugging Face.\\\\n2. Create an instance of the Swin2SRForConditionalGeneration class by loading the pretrained model 'condef/Swin2SR-lightweight-x2-64', which is designed for lightweight image super-resolution tasks.\\\\n3. Use the model to upscale low-resolution images by passing the input image through the model, which will generate a higher-resolution output image.\\\\n4. The higher-resolution image can then be saved or displayed, as required by the photography application.\\\", '<<<code>>>': \\\"from transformers import Swin2SRForConditionalGeneration\\\\nfrom PIL import Image\\\\nimport torch\\\\n# Load pretrained model\\\\nmodel = Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64')\\\\nmodel.eval()\\\\n# Load input image as PIL Image\\\\ninput_image = Image.open('path/to/low_resolution_image.jpg')\\\\n# Convert PIL image to tensor format\\\\ninput_tensor = torch.FloatTensor(input_image).unsqueeze(0)\\\\n# Upscale the image using the model\\\\nwith torch.no_grad():\\\\n  upscaled_tensor = model(input_tensor)\\\\n# Convert upscaled tensor back to PIL image\\\\nupscaled_image = Image.from_numpy(upscaled_tensor.squeeze(0).numpy())\\\\n# Save or display upscaled image\\\\nupscaled_image.save('path/to/upscaled_image.jpg')\\\", 'api_call': \\\"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'swin2SR-lightweight-x2-64', 'api_call': \\\"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\\\", 'api_arguments': 'feature_extractor, model', 'python_environment_requirements': ['transformers, torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.' </s>\"\n"}
{"text": "<s>[INST]استخدم تقنيات معالجة اللغة الطبيعية لملء الفجوات في النصوص الطبية لتحسين التوصيف وفهم النص. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='xlm-roberta-large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary pipeline function from the transformers library provided by Hugging Face.\\\\n2. We use the pipeline function to create a fill-mask model, 'xlm-roberta-large', which is capable of predicting masked words in a text.\\\\n3. This model is a multilingual version of RoBERTa pre-trained on a large corpus of text and can be fine-tuned for various NLP tasks, such as sequence classification, token classification, or question answering.\\\\n4. In our case, we'll use the model to complete missing words in essays and documents for our online educational platform.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'xlm-roberta-large\\\\')\\\\nfilled_sentence = unmasker(\\\"In the future, artificial intelligence will play a crucial role in <mask> fields.\\\")', 'api_call': \\\"pipeline('fill-mask', model='xlm-roberta-large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'xlm-roberta-large', 'api_call': \\\"pipeline('fill-mask', model='xlm-roberta-large')\\\", 'api_arguments': {'model': 'xlm-roberta-large'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='xlm-roberta-large')\\\\nunmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': 'CommonCrawl', 'accuracy': 'N/A'}, 'description': 'XLM-RoBERTa is a multilingual version of RoBERTa pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It is designed for masked language modeling and can be fine-tuned on downstream tasks such as sequence classification, token classification, or question answering.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء جدول تصنيفي لنظام التعرف التلقائي على الكلام الصوتي يحتوي على البيانات التالية وقم بتصنيفها وفقًا لمعايير تصنيف الرموز المحددة.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-large-960h-lv60-self')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nmodel = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-large-960h-lv60-self')\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-large-960h-lv60-self')\\\\n\\\\ninput_values = processor(audio_data, return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'wav2vec2', 'api_name': 'facebook/wav2vec2-large-960h-lv60-self', 'api_call': \\\"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nmodel = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-large-960h-lv60-self')\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-large-960h-lv60-self')\\\\n\\\\ninput_values = processor(audio_data, return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers', 'datasets', 'torch', 'jiwer'], 'example_code': 'from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained(facebook/wav2vec2-large-960h-lv60-self)\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(facebook/wav2vec2-large-960h-lv60-self)\\\\n\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\n\\\\ninput_values = processor(ds[0][audio][array], return_tensors=pt, padding=longest).input_values\\\\n\\\\nlogits = model(input_values).logits\\\\n\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'performance': {'dataset': 'librispeech_asr', 'accuracy': {'clean': 1.9, 'other': 3.9}}, 'description': \\\"Facebook's Wav2Vec2 model pretrained and fine-tuned on 960 hours of Libri-Light and Librispeech on 16kHz sampled speech audio. The model was trained with Self-Training objective. The model is used for Automatic Speech Recognition and can be used as a standalone acoustic model.\\\"}} </s>\"\n"}
{"text": "<s>[INST]أنشئ تعليمة جديدة في مجال كشف النشاط الصوتي تبدأ بـ \\\"توسيع قاعدة الأصوات\\\".? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"VoiceActivityDetection(segmentation='anilbs/segmentation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class from the pyannote.audio package. This includes VoiceActivityDetection for detecting segments when someone is speaking.\\\\n2. We then use the VoiceActivityDetection class with the 'anilbs/segmentation' segmentation to create a voice activity detection pipeline.\\\\n3. Set default hyperparameters like 'onset', 'offset', 'min_duration_on', and 'min_duration_off' as specified.\\\\n4. This pipeline can then be used to analyze an audio file, in your case, a recorded call, and identify the stages when someone is speaking.\\\", '<<<code>>>': \\\"from pyannote.audio.pipelines import VoiceActivityDetection\\\\npipeline = VoiceActivityDetection(segmentation='anilbs/segmentation')\\\\nHYPER_PARAMETERS = {\\\\n    'onset': 0.5,\\\\n    'offset': 0.5,\\\\n    'min_duration_on': 0.0,\\\\n    'min_duration_off': 0.0\\\\n}\\\\npipeline.instantiate(HYPER_PARAMETERS)\\\\nvad = pipeline('audio.wav')\\\\n# replace 'audio.wav' with path to your recorded call file\\\", 'api_call': \\\"VoiceActivityDetection(segmentation='anilbs/segmentation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'anilbs/segmentation', 'api_call': \\\"VoiceActivityDetection(segmentation='anilbs/segmentation')\\\", 'api_arguments': {'onset': 0.5, 'offset': 0.5, 'min_duration_on': 0.0, 'min_duration_off': 0.0}, 'python_environment_requirements': ['pyannote.audio 2.0'], 'example_code': {'voice_activity_detection': 'from pyannote.audio.pipelines import VoiceActivityDetection\\\\npipeline = VoiceActivityDetection(segmentation=anilbs/segmentation)\\\\nHYPER_PARAMETERS = {\\\\n onset: 0.5, offset: 0.5,\\\\n min_duration_on: 0.0,\\\\n min_duration_off: 0.0\\\\n}\\\\npipeline.instantiate(HYPER_PARAMETERS)\\\\nvad = pipeline(audio.wav)'}, 'performance': {'dataset': [{'name': 'AMI Mix-Headset', 'accuracy': {'onset': 0.684, 'offset': 0.577, 'min_duration_on': 0.181, 'min_duration_off': 0.037}}, {'name': 'DIHARD3', 'accuracy': {'onset': 0.767, 'offset': 0.377, 'min_duration_on': 0.136, 'min_duration_off': 0.067}}, {'name': 'VoxConverse', 'accuracy': {'onset': 0.767, 'offset': 0.713, 'min_duration_on': 0.182, 'min_duration_off': 0.501}}]}, 'description': 'Model from End-to-end speaker segmentation for overlap-aware resegmentation, by Hervé Bredin and Antoine Laurent. Online demo is available as a Hugging Face Space.' </s>\"\n"}
{"text": "<s>[INST]عند تدريب نموذج رؤية كمبيوترية على تصنيف مقاطع الفيديو بدون مشاهدة \\\"Zero-Shot Video Classification\\\"، استخدم البيانات النوعية لتعليمه فهم المقاطع التي لم يشاهدها من قبل.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package, like TimesformerForVideoClassification for the video classification model.\\\\n2. Use the TimesformerForVideoClassification.from_pretrained() function to load the pre-trained model 'facebook/timesformer-hr-finetuned-k600'. This model has been trained for video classification tasks on the Kinetics-600 dataset.\\\\n3. Acquire the video frames in real-time or load the video data from a file.\\\\n4. The loaded model can be used to analyze the video frames and classify the actions taking place in it, which can be useful when identifying dangerous actions in a factory setting.\\\", '<<<code>>>': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\n# Replace \\\"video\\\" with your actual video frames as a list of numpy arrays\\\\nvideo = list(np.random.randn(16, 3, 448, 448))\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained(\\\\'facebook/timesformer-hr-finetuned-k600\\\\')\\\\nmodel = TimesformerForVideoClassification.from_pretrained(\\\\'facebook/timesformer-hr-finetuned-k600\\\\')\\\\n\\\\ninputs = processor(images=video, return_tensors=\\\\'pt\\\\')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/timesformer-hr-finetuned-k600', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')\\\", 'api_arguments': {'images': 'video', 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'numpy', 'torch']], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 448, 448))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-hr-finetuned-k600)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-hr-finetuned-k600)\\\\ninputs = processor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-600', 'accuracy': 'Not provided'}, 'description': 'TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 600 possible Kinetics-600 labels.' </s>\"\n"}
{"text": "<s>[INST]قم بتنفيذ الخطوات التالية لتدريب نموذج الانحدار الجدولي في منطقة البيانات الخاصة بك: 1. استيراد البيانات الجدولية المطلوبة إلى بيئة البرمجة الخاصة بك. 2. تنظيف وتحليل البيانات للتأكد من مطابقتها لمتطلبات النموذج. 3. اختيار الخوارزمية المناسبة لتطبيق عملية الانحدار الجدولي. 4. تدريب النموذج باستخدام البيانات وضبط الهايبرباراميترز إذا لزم الأمر. 5. قيم أداء النموذج باستخدام مقاييس الأداء الخاصة بالانحدار الجدولي. اتبع التعليمات بدقة وضبط النموذج بكفاءة لتحقيق أداء ممتاز. حظًا سعيدًا!? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Install and import joblib and Pandas libraries.\\\\n2. Use the joblib.load() function to load the pre-trained model, which can predict CO2 emissions of vehicles.\\\\n3. Load any required input data using Pandas, such as a CSV file containing various vehicle features.\\\\n4. Pass the input data through the trained model to obtain the predicted CO2 emissions, which can aid in emissions management.', '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\ndata = pd.read_csv('data.csv')\\\\nfeatures = ['feature1', 'feature2', 'feature3']\\\\ndata = data[features]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': '45473113800', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'samvelkoch/autotrain-data-prknsn-2', 'accuracy': {'Loss': 5.079, 'R2': 0.109, 'MSE': 25.795, 'MAE': 3.78, 'RMSLE': 0.849}}, 'description': 'A tabular regression model trained with AutoTrain for predicting carbon emissions.' </s>\"\n"}
{"text": "<s>[INST]أنشئ قصة مرئية قصيرة تبدأ بصورة محددة واطلب من المستخدم كتابة وصف نصي لهذه الصورة.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package and the Image class from the PIL package.\\\\n2. Use the from_pretrained method of the VisionEncoderDecoderModel and TrOCRProcessor classes to load the pre-trained model 'microsoft/trocr-large-handwritten' and processing tool required for the image-to-text tasks, respectively.\\\\n3. Load the handwritten image using the Image.open method. Convert the image to RGB format.\\\\n4. Preprocess the image using the TrOCRProcessor instance and retrieve pixel_values from the preprocessing result.\\\\n5. Use the pre-trained model to generate image-to-text information, by passing the pixel_values as input to the model's generate method.\\\\n6. Retrieve the generated text by decoding the returned ids and store it in a variable.\\\", '<<<code>>>': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimage = Image.open('handwritten_image_path.jpg').convert('RGB')\\\\n# replace 'handwritten_image_path.jpg' with path to your handwritten image\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': [{'packages': ['transformers', 'PIL', 'requests']}], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>\"\n"}
{"text": "<s>[INST]عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى برنامج يقوم بتطبيق تصنيف حسب الفئة دون تدريب مسبق على معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': 'classifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)', '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class from the transformers package. This includes pipeline for zero-shot classification.\\\\n2. We then use the pipeline function to create a zero-shot classifier with the model 'Sahajtomar/German_Zeroshot'. This pre-trained model is fine-tuned specifically for classifying German language text.\\\\n3. With this classifier, we can process German news articles by inputting the article text as 'sequence', and the classifier will predict the most relevant category from the list of provided candidate labels. The hypothesis_template needs to be adjusted to German for better results.\\\\n4. The output will be a dictionary containing the most relevant category along with the confidence score. You can use this output to categorize German news articles.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nclassifier = pipeline('zero-shot-classification', model='Sahajtomar/German_Zeroshot')\\\\nsequence = 'Letzte Woche gab es einen Selbstmord in einer nahe gelegenen kolonie'\\\\ncandidate_labels = ['Verbrechen', 'Tragödie', 'Stehlen']\\\\nhypothesis_template = 'In deisem geht es um {}'\\\\nresult = classifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)\\\", 'api_call': 'classifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)', 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': \\\"AutoModelForSequenceClassification.from_pretrained('Sahajtomar/German_Zeroshot')\\\", 'api_call': 'classifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)', 'api_arguments': {'sequence': 'string', 'candidate_labels': 'list of strings', 'hypothesis_template': 'string'}, 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=Sahajtomar/German_Zeroshot)\\\\nsequence = Letzte Woche gab es einen Selbstmord in einer nahe gelegenen kolonie\\\\ncandidate_labels = [Verbrechen,Tragödie,Stehlen]\\\\nhypothesis_template = In deisem geht es um {}. ## Since monolingual model,its sensitive to hypothesis template. This can be experimented\\\\nclassifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)', 'performance': {'dataset': {'XNLI DEV (german)': {'accuracy': 85.5}, 'XNLI TEST (german)': {'accuracy': 83.6}}}, 'description': 'This model has GBERT Large as base model and fine-tuned it on xnli de dataset. The default hypothesis template is in English: This text is {}. While using this model, change it to In deisem geht es um {}. or something different. While inferencing through huggingface api may give poor results as it uses by default english template. Since model is monolingual and not multilingual, hypothesis template needs to be changed accordingly.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد بيانات جدولية لتصنيف البيانات. هل يمكنك مساعدتي في تطبيق تقنيات التصنيف على البيانات الجدولية؟? [/INST]{'domain': 'Tabular Tabular Classification', 'api_call': \\\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv')))\\\", 'api_provider': 'Scikit-learn', 'explanation': '', 'code': \\\"from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\n\\\\nmodel = joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'sklearn_model.joblib')))\\\\ndata_file = cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv'))\\\\nwine_df = pd.read_csv(data_file, sep=';')\\\\nX = wine_df.drop(['quality'], axis=1)\\\\nY = wine_df['quality']\\\\nlabels = model.predict(X[:3])\\\", 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Scikit-learn', 'functionality': 'Wine Quality classification', 'api_name': 'julien-c/wine-quality', 'api_call': \\\"from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\n\\\\nmodel = joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'sklearn_model.joblib')))\\\\ndata_file = cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv'))\\\\nwine_df = pd.read_csv(data_file, sep=';')\\\\nX = wine_df.drop(['quality'], axis=1)\\\\nY = wine_df['quality']\\\\nlabels = model.predict(X[:3])\\\", 'api_arguments': ['X'], 'python_environment_requirements': ['huggingface_hub', 'joblib', 'pandas'], 'example_code': 'from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nREPO_ID = julien-c/wine-quality\\\\nFILENAME = sklearn_model.joblib\\\\nmodel = joblib.load(cached_download(\\\\n hf_hub_url(REPO_ID, FILENAME)\\\\n))\\\\ndata_file = cached_download(\\\\n hf_hub_url(REPO_ID, winequality-red.csv)\\\\n)\\\\nwinedf = pd.read_csv(data_file, sep=;)\\\\nX = winedf.drop([quality], axis=1)\\\\nY = winedf[quality]\\\\nprint(X[:3])\\\\nlabels = model.predict(X[:3])\\\\nmodel.score(X, Y)', 'performance': {'dataset': 'julien-c/wine-quality', 'accuracy': 0.6616635397123202}, 'description': 'A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.'}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج جديد لتصنيف البيانات الجدولية. استخدم تقنيات تعلم الآلة والبيانات الجدولية. ابدأ بجمع مجموعة بيانات وتدريب النموذج عليها.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': '1. Import required libraries: joblib, json, and pandas.\\\\n2. Load the pre-trained carbon emissions classification model using joblib.load().\\\\n3. Open the configuration file for the model (config.json) and extract required features.\\\\n4. Preprocess the input dataset by applying the necessary transformations and selecting only required features.\\\\n5. Use the model.predict() method to get the category predictions for the given dataset entries.\\\\n6. As a result, you will get the category predictions for the carbon emissions of the given dataset.', '<<<code>>>': \\\"import joblib\\\\nimport json\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tejas23/autotrain-amx2-1702259729', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'json', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'tejas23/autotrain-data-amx2', 'accuracy': 0.824}, 'description': 'A multi-class classification model for predicting carbon emissions.' </s>\"\n"}
{"text": "<s>[INST]عميلنا يقوم بتطوير تطبيق لتحويل الصور إلى صور أخرى بواسطة الحاسوب. نحن بحاجة إلى برنامج يمكنه تحليل الصور وتقديمها بطريقة منظمة لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the necessary libraries and modules, including the ControlNetModel and LineartAnimeDetector.\\\\n2. Load the model (lllyasviel/control_v11p_sd15s2_lineart_anime) using `ControlNetModel.from_pretrained()`\\\\n3. Create an instance of LineartAnimeDetector using `LineartAnimeDetector.from_pretrained(\\\\'lllyasviel/Annotators\\\\')`\\\\n4. Provide a text prompt that will inspire the anime illustration, such as \\\"a magical girl flying in the sky\\\"\\\\n5. Prepare the image, and process it into a control image using the LineartAnimeDetector\\\\n6. Generate the output image using the StableDiffusionControlNetPipeline and the text prompt\\\\n7. Save the output image to a file for further review and refinement', '<<<code>>>': 'import torch\\\\nfrom controlnet_aux import LineartAnimeDetector\\\\nfrom diffusers import ControlNetModel, StableDiffusionControlNetPipeline, UniPCMultistepScheduler\\\\nfrom transformers import CLIPTextModel\\\\n\\\\nprompt = \\\"a magical girl flying in the sky\\\"\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/control_v11p_sd15s2_lineart_anime\\\\', torch_dtype=torch.float16)\\\\nprocessor = LineartAnimeDetector.from_pretrained(\\\\'lllyasviel/Annotators\\\\')\\\\ntext_encoder = CLIPTextModel.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', subfolder=\\\\'text_encoder\\\\', num_hidden_layers=11, torch_dtype=torch.float16)\\\\n\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\n\\\\ngenerator = torch.manual_seed(0)\\\\ncontrol_image = processor(None)\\\\noutput_image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\noutput_image.save(\\\\'output_image.png\\\\')', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\\\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': [['pip install diffusers transformers accelerate', 'pip install controlnet_aux==0.3.0']], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import LineartAnimeDetector', 'from transformers import CLIPTextModel', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15s2_lineart_anime', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15s2_lineart_anime/resolve/main/images/input.png', ')', 'image = image.resize((512, 512))', 'prompt = A warrior girl in the jungle', 'processor = LineartAnimeDetector.from_pretrained(lllyasviel/Annotators)', 'control_image = processor(image)', 'control_image.save(./images/control.png)', 'text_encoder = CLIPTextModel.from_pretrained(runwayml/stable-diffusion-v1-5, subfolder=text_encoder, num_hidden_layers=11, torch_dtype=torch.float16)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \\\"image.save('images/image_out.png')\\\"], 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.' </s>\"\n"}
{"text": "<s>[INST]يرجى استخدام تقنية تعقب عيون الأشخاص لتحديد الجواب الصحيح للسؤال المرئي متعدد الوسائط. عندما تكونوا على استعداد، ابدأوا بتنفيذ السؤال مع النموذج المعني قائلين: \\\"أين ينظر هذا الشخص؟\\\". اقترح استخدام تقنية الشبكات العصبية العميقة لتحسين دقة الإجابة.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary class from the transformers library provided by Hugging Face.\\\\n2. Use the from_pretrained method of the AutoModel class to load the pre-trained model 'sheldonxxxx/OFA_model_weights'.\\\\n3. This model can be used to answer questions about products by combining information from both visual and textual sources.\\\\n4. By providing the appropriate input data - an image of the product and the text question from customers, the AI model can generate the most likely answer. This answer can then be utilized by the chat support agents to provide accurate and timely information to customers.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nofa_model = AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\\n# Further model preparation and usage with the associated library from https://github.com/OFA-Sys/OFA\\\", 'api_call': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sheldonxxxx/OFA_model_weights', 'api_call': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطبيق تعلم الإعتماد على توليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في توليد الصور؟? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-church-256')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. First, we import the necessary classes from the diffusers package. This includes DDPMPipeline for the unconditional image generation model.', 'code': \\\"from diffusers import DDPMPipeline\\\\nddpm = DDPMPipeline.from_pretrained('google/ddpm-church-256')\\\\nimage = ddpm().images[0]\\\\nimage.save('ddpm_generated_image.png')\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-church-256', 'api_call': \\\"from diffusers import DDPMPipeline\\\\nddpm = DDPMPipeline.from_pretrained('google/ddpm-church-256')\\\\nimage = ddpm().images[0]\\\\nimage.save('ddpm_generated_image.png')\\\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-church-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.'}} </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي في المجال الروبوتيات التعزيزي.? [/INST]'<<<domain>>>': 'Reinforcement Learning Robotics', '<<<api_call>>>': \\\"AutoModel.from_pretrained('camusean/grasp_diffusion')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We first import the AutoModel class from the transformers library provided by Hugging Face.\\\\n2. We then use the from_pretrained method of the AutoModel class to load the pre-trained model 'camusean/grasp_diffusion'. This model is designed for 6D grasping tasks, which can help the robotic hand in picking and sorting objects optimally.\\\\n3. The loaded model can be used to generate the optimal grasping pose and force to pick an object in real-life scenarios. The generated grasping and motion strategies can be used to control the robotic hand more effectively.\\\\n4. The robot can then execute these strategies to pick and sort objects in the warehouse.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\ngrasp_model = AutoModel.from_pretrained('camusean/grasp_diffusion')\\\\n# Perform the necessary steps to generate the required grasping pose for the robotic hand\\\", 'api_call': \\\"AutoModel.from_pretrained('camusean/grasp_diffusion')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Reinforcement Learning Robotics', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'camusean/grasp_diffusion', 'api_call': \\\"AutoModel.from_pretrained('camusean/grasp_diffusion')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['transformers'], 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Trained Models for Grasp SE(3) DiffusionFields. Check SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion for additional details.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام للفهم الآلي للنصوص يمكنه الإجابة عن الأسئلة المتعلقة بالجداول، ونحتاج إلى بناء نموذج للتنبوء بالإجابة الصحيحة على أسس دقيقة وموثوقة.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary classes from the transformers package. This includes TapasTokenizer for tokenizing table data and TapasForQuestionAnswering for the table question answering model.\\\\n2. We then use the from_pretrained function to load the pre-trained model 'lysandre/tapas-temporary-repo'. This model specializes in answering questions related to table data which is ideal for productivity apps that deal with task tables.\\\\n3. We process the table and question input with the tokenizer to convert them into arrays of tokens that the model can accept.\\\\n4. Finally, we feed these tokenized inputs to the model and get the answer in the form of answer coordinates and aggregation indices. These indices are then translated back into the answer using the tokenizer.\\\", '<<<code>>>': \\\"from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\ntokenizer = TapasTokenizer.from_pretrained('lysandre/tapas-temporary-repo')\\\\nmodel = TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\\ninputs = tokenizer(table=table_data, queries=question, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\npredicted_answer_coordinates, _ = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach())\\\", 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lysandre/tapas-temporary-repo', 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\", 'api_arguments': ['model_name', 'question', 'table'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"tokenizer = TapasTokenizer.from_pretrained('lysandre/tapas-temporary-repo')\\\\nmodel = TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\\ninputs = tokenizer(table=table, queries=question, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())\\\", 'performance': {'dataset': 'SQA', 'accuracy': 'Not provided'}, 'description': 'TAPAS base model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion and can be used for answering questions related to a table in a conversational set-up.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج في مجال الرؤية الحاسوبية لاكتشاف الأشياء لتنبؤ بالكائنات المتواجدة في الصور، ونريد استخدام بعض مجموعات البيانات المتاحة لتدريب النماذج وتحسين الأداء.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8n-csgo-player-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first install ultralyticsplus and ultralytics to access the YOLO model. Then, we import YOLO and render_result from ultralyticsplus library.\\\\n2. Next, we load the pre-trained YOLOv8 model 'keremberke/yolov8n-csgo-player-detection' specifically trained for detecting CS:GO players. This model can recognize the following labels: ['ct', 'cthead', 't', 'thead'].\\\\n3. Configure the required confidences, inter-cell overlap, and maximum detected objects allowed by the model.\\\\n4. Provide the image URL or local path containing CS:GO players and make the predictions using the predict method.\\\\n5. To render and visualize results, use the render_result function with the model, image, and detected results.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path_to_your_csgo_image.jpg'\\\\n# replace 'path_to_your_csgo_image.jpg' with your image URL or local path\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8n-csgo-player-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8n-csgo-player-detection', 'api_call': \\\"YOLO('keremberke/yolov8n-csgo-player-detection')\\\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\\\n\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\n\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\n\\\\nresults = model.predict(image)\\\\n\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.844}, 'description': \\\"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\\\" </s>\"\n"}
{"text": "<s>[INST]يقوم نادي تحليل السؤال المرئي المتعدد بتجميع مجموعة من الصور والأسئلة على هذه الصور ليتم تحليلها والإجابة عليها. قدموا لهم واجهة برمجية للتواصل مع نظام التعلم الآلي للمساعدة في تحقيق الأداء المثلى.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. First, import the AutoModel class from the transformers library provided by Hugging Face.\\\\n2. We load the pre-trained model- 'sheldonxxxx/OFA_model_weights' using the from_pretrained method of the AutoModel class. This particular model is capable of performing visual question-answering tasks.\\\\n3. In order to use this model in your project, you will need to process the given image and question as input to the model.\\\\n4. Please note that this API does not directly support processing image data. To use this model with images, you will need to integrate it with the OFA-Sys/OFA library which can be found at https://github.com/OFA-Sys/OFA.\\\\n5. With the integration, you can use the model to answer questions about the content of the images provided as input.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", 'api_call': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sheldonxxxx/OFA_model_weights', 'api_call': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام لتصنيف النصوص الطبيعية والمحادثات باستخدام تقنيات معالجة اللغات الطبيعية، حيث نريد تصنيف النصوص والمحادثات في مجال استجابة الأسئلة باستخدام نماذج تعلم آلي متعددة. يتطلب ذلك تدريب نماذج الذكاء الاصطناعي على مجموعة كبيرة من البيانات المصنفة مسبقًا.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"AutoModel.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. Import the necessary libraries, which in this case are 'pipeline' and 'AutoModel' from transformers.\\\", 'code': \\\"from transformers import pipeline, AutoModel\\\\nmodel = AutoModel.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')\\\\nqa_pipeline = pipeline('question-answering', model=model)\\\\nresult = qa_pipeline({'context': 'The product XYZ has a 1-year warranty on parts and 90 days on labor.', 'question': 'How long is the warranty on parts?'})\\\\nprint(result)\\\", 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Question Answering', 'api_name': 'bert-large-cased-whole-word-masking-finetuned-squad', 'api_call': \\\"from transformers import pipeline, AutoModel\\\\nmodel = AutoModel.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')\\\\nqa_pipeline = pipeline('question-answering', model=model)\\\\nresult = qa_pipeline({'context': 'The product XYZ has a 1-year warranty on parts and 90 days on labor.', 'question': 'How long is the warranty on parts?'})\\\\nprint(result)\\\", 'api_arguments': {'model_name_or_path': 'bert-large-cased-whole-word-masking', 'dataset_name': 'squad', 'do_train': True, 'do_eval': True, 'learning_rate': 3e-05, 'num_train_epochs': 2, 'max_seq_length': 384, 'doc_stride': 128, 'output_dir': './examples/models/wwm_cased_finetuned_squad/', 'per_device_eval_batch_size': 3, 'per_device_train_batch_size': 3}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='bert-large-cased-whole-word-masking-finetuned-squad')\\\\nresult = qa_pipeline({'context': 'This is a context example.', 'question': 'What is this example for?'})\\\\nprint(result)\\\", 'performance': {'dataset': [{'name': 'BookCorpus', 'accuracy': 'N/A'}, {'name': 'English Wikipedia', 'accuracy': 'N/A'}]}, 'description': 'BERT large model (cased) whole word masking finetuned on SQuAD. This model is cased and trained with a new technique: Whole Word Masking. After pre-training, this model was fine-tuned on the SQuAD dataset.'}} </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى برنامج في مجال معالجة اللغة الطبيعية يقوم بتحويل النصوص المكتوبة إلى كلام بصوت واضح ودقيق لدعم مستخدمي تطبيق عميلنا الذين يعانون من إعاقة بصرية، خاصة عند الإجابة على أسئلة في جداول. يجب أن يكون الصوت واضحًا وسهل الفهم ليساعد الأشخاص ذوي الإعاقة البصرية في فهم المحتوى بشكل أفضل.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. The pipeline function is used to create a table-question-answering model, which is capable of answering questions based on the information provided in tabular data.\\\\n3. We specify the model 'google/tapas-large-finetuned-wtq' to be loaded. This model has been trained on a large corpus of text and can extract information from tables to answer questions.\\\\n4. The created table-question-answering pipeline can be used to query a table for specific information, given a natural language question, and will return the answer based on the table data.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\\nresult = qa_pipeline(table=table, query=query)\\\", 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/tapas-large-finetuned-wtq', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", 'api_arguments': {'model': 'google/tapas-large-finetuned-wtq', 'task': 'table-question-answering'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\nqa_pipeline = pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\\n\\\\nresult = qa_pipeline(table=table, query=query)\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.5097}, 'description': 'TAPAS large model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).' </s>\"\n"}
{"text": "<s>[INST]يقوم نظامنا بتحويل الملفات الصوتية إلى صيغة أخرى من الملفات الصوتية، ونحن بحاجة لإنشاء نماذج للتحويل التلقائي بين هذه الصيغ لضمان جودة الصوت المحسنة.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"BaseModel.from_pretrained('JorisCos/DCUNet_Libri1Mix_enhsingle_16k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import transformers and the necessary libraries.\\\\n2. Use the BaseModel.from_pretrained() function provided by the transformers library to create an audio enhancement model.\\\\n3. Specify the model 'JorisCos/DCUNet_Libri1Mix_enhsingle_16k' to be loaded. This model has been trained to enhance speech signals in noisy environments.\\\\n4. The created enhancer will process the input audio file and generate an output file with enhanced speech.\\\", '<<<code>>>': \\\"from transformers import BaseModel \\\\nfrom asteroid import AudioFileProcessor\\\\nmodel = BaseModel.from_pretrained('JorisCos/DCUNet_Libri1Mix_enhsingle_16k')\\\\nenhancer = AudioFileProcessor(model)\\\\ninput_audio_file = 'path/to/noisy_audio.wav'\\\\noutput_audio_file = 'path/to/enhanced_audio.wav'\\\\nenhancer_process = enhancer.process_file(input_audio_file, output_audio_file)\\\", 'api_call': \\\"BaseModel.from_pretrained('JorisCos/DCUNet_Libri1Mix_enhsingle_16k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DCUNet_Libri1Mix_enhsingle_16k', 'api_call': \\\"BaseModel.from_pretrained('JorisCos/DCUNet_Libri1Mix_enhsingle_16k')\\\", 'api_arguments': 'audio', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Libri1Mix', 'accuracy': {'si_sdr': 13.154035391645971, 'si_sdr_imp': 9.704254085786271, 'sdr': 13.568058873121435, 'sdr_imp': 10.065396073908367, 'sar': 13.568058873121435, 'sar_imp': 10.065396073908367, 'stoi': 0.9199373340235417, 'stoi_imp': 0.12401751048300132}}, 'description': 'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى استخدام تقنية تحويل النص إلى كلام لتطوير تطبيقنا لذوي الإعاقة البصرية، تأكد من اختيار برنامج يوفر جودة صوتية عالية ودقة في التعبير لتعزيز تجربة المستخدمين.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\\\", '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. First, we import the necessary pipeline class from the transformers package.\\\\n2. Next, create a text-to-speech pipeline using the pretrained model 'espnet/kan-bayashi_ljspeech_vits'. This model has been trained on the ljspeech dataset and can be utilized for converting text input into synthesized speech.\\\\n3. Now, just pass any text input to this pipeline function to synthesize speech audio. You can then play the audio or incorporate it into your application for visually impaired users.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\\\\naudio = tts('Hello World')\\\", 'api_call': \\\"pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\\\", 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kan-bayashi_ljspeech_vits', 'api_call': \\\"pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')\\\", 'performance': {'dataset': 'ljspeech', 'accuracy': 'Not mentioned'}, 'description': 'A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech.' </s>\"\n"}
{"text": "<s>[INST]أنشئ وصفاً نصياً قصيراً يصف البيانات الموجودة في الجدول الذي يُعرض بشكل شامل ومفصل.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': 'TabTransformer.from_config()', '<<<api_provider>>>': 'Keras', '<<<explanation>>>': \\\"1. First, use Keras' TabTransformer API to create a custom-configured TabTransformer model. It uses a self-attention-based Transformers structure followed by multiple feed-forward layers for structured data learning tasks.\\\\n2. This model accepts both numerical and categorical inputs, which can include mileage, make, model, year, and installed options.\\\\n3. Categorical features are encoded into embedding vectors and then fed through Transformer blocks. The output contextual embeddings are concatenated with numerical features and passed through an MLP block.\\\\n4. A SoftMax function is applied at the end of the model for final predictions.\\\\n5. Train this TabTransformer model on your historical automobile data and use the trained model to predict car prices based on the given features.\\\", '<<<code>>>': 'from keras_io.tab_transformer import TabTransformer, TabTransformerConfig\\\\nconfig = TabTransformerConfig()\\\\nmodel = TabTransformer.from_config(config)\\\\n# Train the model on historical automobile data and use the trained model to predict car prices based on given features', 'api_call': 'TabTransformer.from_config()', 'provider': 'Keras', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keras-io/tab_transformer', 'api_call': 'TabTransformer.from_config()', 'api_arguments': 'N/A', 'python_environment_requirements': ['Hugging Face'], 'example_code': 'N/A', 'performance': {'dataset': 'United States Census Income Dataset', 'accuracy': 'N/A'}, 'description': \\\"This model uses self-attention based Transformers structure followed by multiple feed forward layers to serve supervised and semi-supervised learning tasks on tabular data. The model's inputs can contain both numerical and categorical features. Categorical features are encoded into embedding vectors before being fed into a stack of Transformer blocks. The contextual embeddings of the categorical features after the final Transformer layer are concatenated with the input numerical features and fed into a final MLP block. A SoftMax function is applied at the end of the model.\\\" </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد صورة من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. First, we\\\\'ll import the necessary python packages and the ControlNetModel to generate images with specific patterns or designs using a text description as input.\\\\n2. Then, we will use the pre-trained lllyasviel/control_v11p_sd15_normalbae model combined with the Stable Diffusion ControlNetPipeline for generating the images.\\\\n3. Provide the text prompt describing the specific pattern or design of the garment that we want to generate. For example, \\\"A blue dress with floral pattern.\\\"\\\\n4. Run the pipeline with the given prompt to generate the image, and save it for later use.', '<<<code>>>': 'from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\\\\nfrom controlnet_aux import NormalBaeDetector\\\\nimport torch\\\\nfrom PIL import Image\\\\n\\\\nprompt = \\\"A blue dress with floral pattern\\\"  # replace with desired pattern description\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/control_v11p_sd15_normalbae\\\\', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', controlnet=controlnet, torch_dtype=torch.float16)\\\\n\\\\ngenerator = torch.manual_seed(33)\\\\ncontrol_image = controlnet(control_image)\\\\ngenerated_image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\ngenerated_image.save(\\\\'generated_garment_pattern.png\\\\')', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/control_v11p_sd15_normalbae', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae')\\\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate', 'controlnet_aux']], 'example_code': \\\"import torch\\\\nimport os\\\\nfrom huggingface_hub import HfApi\\\\nfrom pathlib import Path\\\\nfrom diffusers.utils import load_image\\\\nfrom PIL import Image\\\\nimport numpy as np\\\\nfrom controlnet_aux import NormalBaeDetector\\\\nfrom diffusers import (\\\\n ControlNetModel,\\\\n StableDiffusionControlNetPipeline,\\\\n UniPCMultistepScheduler,\\\\n)\\\\ncheckpoint = lllyasviel/control_v11p_sd15_normalbae\\\\nimage = load_image(\\\\n https://huggingface.co/lllyasviel/control_v11p_sd15_normalbae/resolve/main/images/input.png\\\\n)\\\\nprompt = A head full of roses\\\\nprocessor = NormalBaeDetector.from_pretrained(lllyasviel/Annotators)\\\\ncontrol_image = processor(image)\\\\ncontrol_image.save(./images/control.png)\\\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(33)\\\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\nimage.save('images/image_out.png')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لمعالجة اللغة الطبيعية لتحويل النص إلى نص، حيث نريد إنشاء تحول نصي لتحقيق الهدف المطلوب.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import required libraries, such as \\\\'T5Tokenizer\\\\' and \\\\'T5ForConditionalGeneration\\\\' from transformers.\\\\n2. Load the tokenizer and the model \\\\'google/flan-t5-base\\\\', which is a state-of-the-art language model fine-tuned on multiple tasks including text generation.\\\\n3. Prepare a prompt like \\\"Generate creative titles for an article about The History of Artificial Intelligence\\\" and tokenize it to create input_ids using the loaded tokenizer.\\\\n4. Pass the input_ids to the model\\\\'s \\\\'generate()\\\\' function, which will generate a token sequence that can be decoded to produce creative article title suggestions.', '<<<code>>>': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'google/flan-t5-base\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'google/flan-t5-base\\\\')\\\\nprompt = \\\"Generate creative titles for an article about The History of Artificial Intelligence\\\"\\\\ninput_ids = tokenizer(prompt, return_tensors=\\\"pt\\\").input_ids\\\\noutputs = model.generate(input_ids)\\\\narticle_titles = tokenizer.decode(outputs[0])', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/flan-t5-base', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\\\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-base)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-base)\\\\ninput_text = translate English to German: How old are you?\\\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))', 'performance': {'dataset': [{'name': 'MMLU', 'accuracy': '75.2%'}]}, 'description': 'FLAN-T5 is a language model fine-tuned on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering.' </s>\"\n"}
{"text": "<s>[INST]عند استخدام API لتحويل الصوت إلى صوت، تأكد من توفير مصدر الصوت الأصلي والهدف، وضبط إعدادات الصوت بشكل صحيح. يمكنك اختبار النتائج من خلال تسجيل ومقارنة الصوتين الناتجين. ننصح باستخدام النصائح الصوتية لتحسين جودة الإخراج.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': 'S2THubInterface()', '<<<api_provider>>>': 'Fairseq', '<<<explanation>>>': \\\"1. Import the required libraries and modules, such as fairseq, torchaudio, and huggingface_hub.\\\\n2. Download and load the 'facebook/xm_transformer_s2ut_hk-en' model using the load_model_ensemble_and_task_from_hf_hub function provided by fairseq.\\\\n3. Use the model to build a generator.\\\\n4. Load the input audio file using torchaudio.load().\\\\n5. Use S2THubInterface to convert the input audio into a format the model can understand.\\\\n6. Get the translated text using S2THubInterface.get_prediction().\\\\n7. Download and load the HiFiGAN vocoder model to convert text back to speech.\\\\n8. Use VocoderHubInterface to generate an English speech waveform.\\\\n9. Save the English speech waveform and play it using IPython.display.Audio.\\\", '<<<code>>>': '# For detailed code, refer to the example_code provided in the API. Replace the file path with the path of the audio you want to translate.', 'api_call': 'S2THubInterface()', 'provider': 'Fairseq', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'xm_transformer_s2ut_hk-en', 'api_call': 'S2THubInterface()', 'api_arguments': {'task': 'speech_to_text', 'model': 'facebook/xm_transformer_s2ut_hk-en', 'generator': 'task.build_generator([model], cfg)', 'sample': 'S2THubInterface.get_model_input(task, audio)'}, 'python_environment_requirements': [{'fairseq': 'latest', 'torchaudio': 'latest', 'huggingface_hub': 'latest'}], 'example_code': \\\"import json\\\\nimport os\\\\nfrom pathlib import Path\\\\nimport IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n facebook/xm_transformer_s2ut_hk-en,\\\\n arg_overrides={config_yaml: config.yaml, task: speech_to_text},\\\\n cache_dir=cache_dir,\\\\n)\\\\nmodel = models[0].cpu()\\\\ncfg[task].cpu = True\\\\ngenerator = task.build_generator([model], cfg)\\\\naudio, _ = torchaudio.load(/path/to/an/audio/file)\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\\\\nlibrary_name = fairseq\\\\ncache_dir = (\\\\n cache_dir or (Path.home() / .cache / library_name).as_posix()\\\\n)\\\\ncache_dir = snapshot_download(\\\\n ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name\\\\n)\\\\nx = hub_utils.from_pretrained(\\\\n cache_dir,\\\\n model.pt,\\\\n .,\\\\n archive_map=CodeHiFiGANVocoder.hub_models(),\\\\n config_yaml=config.json,\\\\n fp16=False,\\\\n is_vocoder=True,\\\\n)\\\\nwith open(f{x['args']['data']}/config.json) as f:\\\\n vocoder_cfg = json.load(f)\\\\nassert (\\\\n len(x[args][model_path]) == 1\\\\n), Too many vocoder models in the input\\\\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\nipd.Audio(wav, rate=sr)\\\", 'performance': {'dataset': 'TED, drama, TAT domain', 'accuracy': 'Not provided'}, 'description': 'Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq for Hokkien-English. Trained with supervised data in TED, drama, TAT domain, and weakly supervised data in drama domain.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير برنامج يقوم بتحويل النص إلى كلام لمستخدمي تطبيقنا الذي يهتم بتحليل اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a translation model.\\\\n3. Specify the model 'Helsinki-NLP/opus-mt-es-en' to be loaded. This model is trained specifically for translating Spanish text to English text.\\\\n4. The created translation model can be used to translate the text from the report in Spanish to English, providing understandable text for English-speaking employees.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_es_to_en\\\\', model=\\\\'Helsinki-NLP/opus-mt-es-en\\\\')\\\\nspanish_report = \\\"Texto del informe en español aquí...\\\"\\\\nenglish_translation = translator(spanish_report)', 'api_call': \\\"pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-es-en', 'api_call': \\\"pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')('Hola, ¿cómo estás?')\\\", 'performance': {'dataset': [{'name': 'newssyscomb2009-spaeng.spa.eng', 'accuracy': {'BLEU': 30.6, 'chr-F': 0.57}}, {'name': 'news-test2008-spaeng.spa.eng', 'accuracy': {'BLEU': 27.9, 'chr-F': 0.553}}, {'name': 'newstest2009-spaeng.spa.eng', 'accuracy': {'BLEU': 30.4, 'chr-F': 0.572}}, {'name': 'newstest2010-spaeng.spa.eng', 'accuracy': {'BLEU': 36.1, 'chr-F': 0.614}}, {'name': 'newstest2011-spaeng.spa.eng', 'accuracy': {'BLEU': 34.2, 'chr-F': 0.599}}, {'name': 'newstest2012-spaeng.spa.eng', 'accuracy': {'BLEU': 37.9, 'chr-F': 0.624}}, {'name': 'newstest2013-spaeng.spa.eng', 'accuracy': {'BLEU': 35.3, 'chr-F': 0.609}}, {'name': 'Tatoeba-test.spa.eng', 'accuracy': {'BLEU': 59.6, 'chr-F': 0.739}}]}, 'description': 'Helsinki-NLP/opus-mt-es-en is a machine translation model trained to translate from Spanish to English using the Hugging Face Transformers library. The model is based on the Marian framework and was trained on the OPUS dataset.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج لتوليد ملخصات نصوص من نوع إلى نوع (Text2Text Generation) في مجال معالجة اللغات الطبيعية، يستطيع تحويل النصوص من شكل إلى آخر بشكل مباشر وفعال.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"pipeline('summarization', model='google/pegasus-xsum')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the pipeline function from the transformers package provided by Hugging Face.\\\\n2. We create a summarization pipeline using the 'google/pegasus-xsum' model. Pegasus is a transformer-based model designed for abstractive summarization.\\\\n3. We input the news article text and let the model generate a summary.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'google/pegasus-xsum\\\\')\\\\nsummary = summarizer(\\\"President Biden signed three executive orders on Tuesday aimed at reversing stubborn wage gaps and promoting better pay equity in the federal workforce. The administration estimates that the third of the orders will increase the wages of nearly 23,000 employees by 2023. The latest moves build on previous efforts by the Biden administration to address systemic inequality and ensure workers earn a living wage. The first order would establish an equity and inclusion council for the federal workforce, while the second would set a target of raising the minimum wage for all federal contractors to $15 per hour.\\\")[\\\\'summary_text\\\\']', 'api_call': \\\"pipeline('summarization', model='google/pegasus-xsum')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/pegasus-xsum', 'api_call': \\\"pipeline('summarization', model='google/pegasus-xsum')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': [{'name': 'xsum', 'accuracy': {'ROUGE-1': 46.862, 'ROUGE-2': 24.453, 'ROUGE-L': 39.055, 'ROUGE-LSUM': 39.099}}, {'name': 'cnn_dailymail', 'accuracy': {'ROUGE-1': 22.206, 'ROUGE-2': 7.67, 'ROUGE-L': 15.405, 'ROUGE-LSUM': 19.218}}, {'name': 'samsum', 'accuracy': {'ROUGE-1': 21.81, 'ROUGE-2': 4.253, 'ROUGE-L': 17.447, 'ROUGE-LSUM': 18.891}}]}, 'description': 'PEGASUS is a pre-trained model for abstractive summarization, developed by Google. It is based on the Transformer architecture and trained on both C4 and HugeNews datasets. The model is designed to extract gap sentences and generate summaries by stochastically sampling important sentences.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام لتصنيف النصوص الطبيعية باستخدام تقنيات التصنيف الصفري، حيث نحتاج إلى تصنيف النصوص إلى فئات محددة بناءً على خصائصها اللغوية المختلفة، ونريد استخدام تقنيات التعلم الآلي لتحقيق ذلك.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-9')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to create a zero-shot classification model using the 'valhalla/distilbart-mnli-12-9' model.\\\\n3. We then input the article text and candidate topics (labels) into the model.\\\\n4. The model will analyze the text and provide a score for each topic, indicating the probability that the text belongs to that category.\\\\n5. Based on the topic probabilities, the most appropriate topic can be chosen for the article.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nzero_shot_classifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'valhalla/distilbart-mnli-12-9\\\\')\\\\narticle_text = \\\"Article text goes here...\\\"\\\\ncandidate_labels = [\\\\'politics\\\\', \\\\'technology\\\\', \\\\'sports\\\\', \\\\'entertainment\\\\', \\\\'business\\\\']\\\\nresult = zero_shot_classifier(article_text, candidate_labels)', 'api_call': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-9')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'valhalla/distilbart-mnli-12-9', 'api_call': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-9')\\\", 'api_arguments': 'text, candidate_labels', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'MNLI', 'accuracy': {'matched_acc': 89.56, 'mismatched_acc': 89.52}}, 'description': 'distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is used for zero-shot text classification tasks.' </s>\"\n"}
{"text": "<s>[INST]أنشئ قصة قصيرة تبدأ بـ \\\"باستخدام تقنيات معالجة اللغات الطبيعية،? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='bigscience/bloom-560m')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'pipeline' from the transformers library - this would be used to create a text-generation model.\\\\n2. Use the 'pipeline' function from transformers to load the model 'bigscience/bloom-560m', which is a state-of-the-art text generation model.\\\\n3. Provide the initial text, i.e., the beginning of the article, to the text-generation model, which will then generate the continuation of the article.\\\\n4. We can then print the generated text as the continuation for the article.\\\", '<<<code>>>': 'from transformers import pipeline\\\\n\\\\nmodel_name = \\\\'bigscience/bloom-560m\\\\'\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=model_name)\\\\n\\\\ninitial_text = \\\"Autonomous vehicles have begun to revolutionize the transportation industry, particularly in urban areas where traffic is congested and public transit is often unreliable.\\\"\\\\noutput = generator(initial_text)\\\\ncontinuation = output[0][\\\\'generated_text\\\\']\\\\nprint(continuation)', 'api_call': \\\"pipeline('text-generation', model='bigscience/bloom-560m')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bigscience/bloom-560m', 'api_call': \\\"pipeline('text-generation', model='bigscience/bloom-560m')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline\\\\n\\\\nmodel_name = 'bigscience/bloom-560m'\\\\napi = pipeline('text-generation', model=model_name)\\\\n\\\\ntext = 'The history of artificial intelligence began in the '\\\\noutput = api(text)\\\\nprint(output[0]['generated_text'])\\\", 'performance': {'dataset': 'Validation', 'accuracy': {'Training Loss': 2.0, 'Validation Loss': 2.2, 'Perplexity': 8.9}}, 'description': 'BLOOM LM is a large open-science, open-access multilingual language model developed by BigScience. It is a transformer-based language model trained on 45 natural languages and 12 programming languages. The model has 559,214,592 parameters, 24 layers, and 16 attention heads.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتصميم نظم النص إلى الفيديو متعددة الوسائط باستخدام تقنيات الرؤية الحاسوبية، ونحتاج إلى تصنيف الصور والكائنات ضمنها بدقة عالية.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Video', '<<<api_call>>>': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the required libraries like torch and tuneavideo.\\\\n2. Import the TuneAVideoPipeline class, the UNet3DConditionModel class and the save_videos_grid function.\\\\n3. Load the pretrained \\\\'nitrosocke/mo-di-diffusion\\\\' model using TuneAVideoPipeline.from_pretrained() function.\\\\n4. Load the pretrained UNet3DConditionModel with the prompt \\\"a magical princess playing guitar, modern Disney style\\\".\\\\n5. Set the video_length, height, width, num_inference_steps, and guidance_scale as required.\\\\n6. Generate the video using the loaded pipeline based on the prompt provided.\\\\n7. Save the generated video as a .gif file with the given prompt as the filename.', '<<<code>>>': \\\"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\\\nfrom tuneavideo.util import save_videos_grid\\\\nimport torch\\\\nunet_model_path = 'Tune-A-Video-library/mo-di-bear-guitar'\\\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\\\npipe = TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=unet, torch_dtype=torch.float16).to('cuda')\\\\npipe.enable_xformers_memory_efficient_attention()\\\\nprompt = 'a magical princess is playing guitar, modern disney style'\\\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\\\nsave_videos_grid(video, f'./{prompt}.gif')\\\", 'api_call': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mo-di-bear-guitar', 'api_call': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': [['torch', 'tuneavideo']], 'example_code': \\\"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\\\nfrom tuneavideo.util import save_videos_grid\\\\nimport torch\\\\npretrained_model_path = nitrosocke/mo-di-diffusion\\\\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\\\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\nprompt = a magical princess is playing guitar, modern disney style\\\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\\\nsave_videos_grid(video, f./{prompt}.gif)\\\", 'performance': {'dataset': 'Not mentioned', 'accuracy': 'Not mentioned'}, 'description': 'Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الإفتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي في تصنيف الصور بدون مراقبة في مجال تقدير عمق رؤية الحواسيب.? [/INST]{'domain': 'Computer Vision Depth Estimation', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We import AutoModel from transformers and torchvision.transforms for handling image transformations.', 'code': \\\"import torch\\\\nfrom transformers import AutoModel\\\\nfrom torchvision.transforms import ToTensor, Resize, Compose\\\\nfrom PIL import Image\\\\n\\\\nmodel_checkpoint = 'sayakpaul/glpn-nyu-finetuned-diode-221215-093747'\\\\nmodel = AutoModel.from_pretrained(model_checkpoint)\\\\nif torch.cuda.is_available():\\\\n    model.cuda()\\\\n\\\\ninput_image = Image.open('<image_path_here>').convert('RGB')\\\\ntransforms = Compose([Resize((224, 224)), ToTensor()])\\\\ntransformed_image = transforms(input_image).unsqueeze(0).to(model.device)\\\\n\\\\nwith torch.no_grad():\\\\n    output = model(transformed_image)\\\\ndepth_map = output.cpu().numpy()\\\", 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-093747', 'api_call': \\\"import torch\\\\nfrom transformers import AutoModel\\\\nfrom torchvision.transforms import ToTensor, Resize, Compose\\\\nfrom PIL import Image\\\\n\\\\nmodel_checkpoint = 'sayakpaul/glpn-nyu-finetuned-diode-221215-093747'\\\\nmodel = AutoModel.from_pretrained(model_checkpoint)\\\\nif torch.cuda.is_available():\\\\n    model.cuda()\\\\n\\\\ninput_image = Image.open('<image_path_here>').convert('RGB')\\\\ntransforms = Compose([Resize((224, 224)), ToTensor()])\\\\ntransformed_image = transforms(input_image).unsqueeze(0).to(model.device)\\\\n\\\\nwith torch.no_grad():\\\\n    output = model(transformed_image)\\\\ndepth_map = output.cpu().numpy()\\\", 'api_arguments': [], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.'}} </s>\"\n"}
{"text": "<s>[INST]رجاءً قم بتطوير برنامج يعمل على تحليل وتصنيف البيانات المرئية لتحديد العمق بدقة وفعالية عالية.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries AutoModel from transformers and torch.\\\\n2. Load the pre-trained depth estimation model using the from_pretrained method from the AutoModel class. The model name is 'sayakpaul/glpn-nyu-finetuned-diode-221215-093747'.\\\\n3. The depth estimation model can then be used to analyze input images and produce depth maps, which represent the environment's depth and allow for effective navigation by drones.\\\\n4. In order to use the model, the input images need to be preprocessed and converted into tensors by following the specific model requirements.\\\\n5. Once processed, the depth map can be obtained and used for further analysis and navigation.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nimport torch\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\\\\n# Preprocess input image and convert it into a tensor\\\\n# input_tensor = preprocess_image(input_image)\\\\n# Obtain depth map from the model\\\\n# depth_map = model(input_tensor)\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221215-093747', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطبيق تقنيات الرؤية الحاسوبية على البيانات الجدولية للتنبؤ والتحليل. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load(hf_hub_download('srg/outhimar_64-Close-regression', 'sklearn_model.joblib'))\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': \\\"1. Import the necessary libraries, including joblib and hf_hub_download method.\\\\n2. Download the pre-trained model 'srg/outhimar_64-Close-regression' using hf_hub_download method.\\\\n3. Load the downloaded model using joblib.load().\\\\n4. The loaded model is a regression-based model trained to predict stock closing prices based on historical data.\\\\n5. Pass the feature dataset X (i.e., X_train) to the model to obtain predictions of stock closing prices for the dataset.\\\", '<<<code>>>': \\\"import joblib\\\\nfrom huggingface_hub import hf_hub_download\\\\n\\\\nmodel_path = hf_hub_download('srg/outhimar_64-Close-regression', 'sklearn_model.joblib')\\\\nmodel = joblib.load(model_path)\\\\n\\\\npredicted_closing_prices = model.predict(X_train)\\\", 'api_call': \\\"joblib.load(hf_hub_download('srg/outhimar_64-Close-regression', 'sklearn_model.joblib'))\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'srg/outhimar_64-Close-regression', 'api_call': \\\"joblib.load(hf_hub_download('srg/outhimar_64-Close-regression', 'sklearn_model.joblib'))\\\", 'api_arguments': ['X_train', 'y_train'], 'python_environment_requirements': [['scikit-learn', 'dabl']], 'example_code': \\\"Pipeline(steps=[('easypreprocessor',EasyPreprocessor(types= continuous dirty_float low_card_int ... date free_string useless\\\\nDate False False False ... True False False\\\\nOpen True False False ... False False False\\\\nHigh True False False ... False False False\\\\nLow True False False ... False False False\\\\nAdj Close True False False ... False False False\\\\nVolume True False False ... False False False[6 rows x 7 columns])),('ridge', Ridge(alpha=10))])\\\", 'performance': {'dataset': 'outhimar_64', 'accuracy': {'r2': 0.999858, 'neg_mean_squared_error': -1.067685}}, 'description': 'Baseline Model trained on outhimar_64 to apply regression on Close. Disclaimer: This model is trained with dabl library as a baseline, for better results, use AutoTrain. Logs of training including the models tried in the process can be found in logs.txt.' </s>\"\n"}
{"text": "<s>[INST]أنا خبير في تطوير نظام لمعالجة اللغة الطبيعية التفاعليّة لإنتاج نصوص. هل تستطيع مساعدتي في توجيهي نحو الموارد الضروريّة لذلك؟? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"pipeline('text-generation', model='Pi3141/DialoGPT-medium-elon-3')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We import the pipeline function from the transformers library provided by Hugging Face.', 'code': 'from transformers import pipeline\\\\ntext_generator = pipeline(\\\\'text-generation\\\\', model=\\\\'Pi3141/DialoGPT-medium-elon-3\\\\')\\\\ninput_text = \\\"What do you think about Mars colonization?\\\"\\\\nresponse_output = text_generator(input_text)\\\\nprint(response_output[0][\\\\'generated_text\\\\'])', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Conversational', 'api_name': 'Pi3141/DialoGPT-medium-elon-3', 'api_call': 'from transformers import pipeline\\\\ntext_generator = pipeline(\\\\'text-generation\\\\', model=\\\\'Pi3141/DialoGPT-medium-elon-3\\\\')\\\\ninput_text = \\\"What do you think about Mars colonization?\\\"\\\\nresponse_output = text_generator(input_text)\\\\nprint(response_output[0][\\\\'generated_text\\\\'])', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.', 'performance': {'dataset': 'Twitter tweets by Elon Musk', 'accuracy': 'N/A'}, 'description': 'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.'}} </s>\"\n"}
{"text": "<s>[INST]قم بإظهار مهاراتك في تحويل النص إلى فيديو من خلال إعادة إنتاج المشهد المقدم بتسجيل صوتي جديد. يجب تنفيذ المهمة بدقة واستخدام أدوات تحويل النصوص إلى فيديو ذات جودة عالية. عند الانتهاء، قدم النتائج بفيديو قد يتم تقييمه.? [/INST]{'domain': 'Multimodal Text-to-Video', 'api_call': \\\"pipeline('text-to-video', model='chavinlo/TempoFunk')\\\", 'api_provider': 'Hugging Face', 'explanation': '1. First, we need to import the Hugging Face Transformers library, specifically the pipeline function.', 'code': 'from transformers import pipeline\\\\ntext_to_video = pipeline(\\\\'text-to-video\\\\', model=\\\\'chavinlo/TempoFunk\\\\')\\\\ninput_text = \\\"The daily life of a software engineer starts with checking emails and attending meetings, followed by writing code, debugging, and collaborating with colleagues. The day ends with updating documentation and reporting progress.\\\"\\\\ngenerated_video = text_to_video(input_text)', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'chavinlo/TempoFunk', 'api_call': 'from transformers import pipeline\\\\ntext_to_video = pipeline(\\\\'text-to-video\\\\', model=\\\\'chavinlo/TempoFunk\\\\')\\\\ninput_text = \\\"The daily life of a software engineer starts with checking emails and attending meetings, followed by writing code, debugging, and collaborating with colleagues. The day ends with updating documentation and reporting progress.\\\"\\\\ngenerated_video = text_to_video(input_text)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.'}} </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج لتحليل تشابه الجمل بتقنيات معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We need to use the SentenceTransformer class from the sentence_transformers package.\\\\n2. We load the pre-trained model 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2' to obtain a sentence embedding model capable of mapping sentences to a 768-dimensional dense vector space.\\\\n3. We provide a list of sentences as input to the model.\\\\n4. The model.encode() method takes the sentences as input and produces embeddings or dense vector representations that can be used to measure semantic similarity.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nsentences = [\\\"Shakespeare was a poet and playwright.\\\", \\\"He wrote plays and poems like Romeo and Juliet, and Hamlet.\\\", \\\"The renowned writer, Shakespeare, is well-known for his literary works, such as Hamlet and Romeo and Juliet.\\\"]\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/paraphrase-multilingual-mpnet-base-v2\\\\')\\\\nembeddings = model.encode(sentences)', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]من خلال هذا الواجهة، يمكن للنظام تحويل الصور إلى نصوص وصفية توضح المشهد بالتفصيل، مما يسهل عملية التعرف على الموضوعات والأشياء في الصور. ستقوم شركتنا بتصميم حلول متعددة الحواس لتحويل الصور إلى نصوص بطريقة دقيقة وموثوقة لضمان فهم أفضل للمحتوى المرئي. هذا يعتبر جزءًا من مجال الإجابة على الأسئلة المرئية متعددة الوسائط.? [/INST]{'domain': 'Multimodal Visual Question Answering', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('uclanlp/visualbert-vqa')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import AutoModelForQuestionAnswering, AutoTokenizer\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained('uclanlp/visualbert-vqa')\\\\ntokenizer = AutoTokenizer.from_pretrained('uclanlp/visualbert-vqa')\\\\n# For image feature extraction, we recommend using a method such as faster_rcnn\\\\n# or detectron2.\\\", 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'uclanlp/visualbert-vqa', 'api_call': \\\"from transformers import AutoModelForQuestionAnswering, AutoTokenizer\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained('uclanlp/visualbert-vqa')\\\\ntokenizer = AutoTokenizer.from_pretrained('uclanlp/visualbert-vqa')\\\\n# For image feature extraction, we recommend using a method such as faster_rcnn\\\\n# or detectron2.\\\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A VisualBERT model for Visual Question Answering.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتحليل الصور المتعددة الوسائط وتحويلها إلى نصوص قابلة للقراءة باستخدام تقنيات الرؤية المتعددة.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary modules from the transformers and PIL libraries.\\\\n2. We load the pre-trained model 'microsoft/trocr-base-printed' using the VisionEncoderDecoderModel class from Hugging Face transformers.\\\\n3. This model has been trained specifically for Optical Character Recognition (OCR) and is fine-tuned on the SROIE dataset.\\\\n4. For each image, we use the processor (TrOCRProcessor) to process the images for the model and obtain pixel values.\\\\n5. We then feed these pixel values to the model, which generates IDs corresponding to the extracted text.\\\\n6. Finally, we decode the IDs back into text using the processor, which we can then store in our database.\\\", '<<<code>>>': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nimage = Image.open('scanned_document.jpg')\\\\n# replace 'scanned_document.jpg' with the path to your image\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>\"\n"}
{"text": "<s>[INST]يتم تنظيم نادي طلابي في مجال التحويل النصي إلى صور يجتمع كل أسبوعين للعب كرة القدم الافتراضية. يرجى تزويدهم بأداة للعب ضد وكيل التعلم الآلي.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We need to import the Text2ImagePipeline from the transformers library provided by Hugging Face.\\\\n2. Next, instantiate the Text2ImagePipeline with the model 'Linaqruf/anything-v3.0'. This model generates images from textual descriptions.\\\\n3. Input the textual description into the Text2ImagePipeline's __call__() method to generate the image.\\\\n4. The generated image can be used by Melanie as a reference for her illustrations.\\\\n(Note: GPT models cannot create images and Hugging Face doesn't have an official text-to-image API. But to provide an idea of what it would look like, we have created a fictional example here.)\\\", '<<<code>>>': 'from transformers import Text2ImagePipeline\\\\nimage_generator = Text2ImagePipeline(model=\\\\'Linaqruf/anything-v3.0\\\\')\\\\ngenerated_image = image_generator(\\\"A cat sitting on a windowsill\\\")', 'api_call': \\\"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \\\"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>\"\n"}
{"text": "<s>[INST]لتطوير نظام ملخصات تلقائي باستخدام معالجة اللغة الطبيعية، يرجى تصميم آلية لتحديد الفقرات الرئيسية في النص الأصلي. كما يُنصح بتطوير نموذج تعليم آلي لإنشاء ملخصات مفيدة ومختصرة. يمكن توفير بيانات تدريب واختبار ذات جودة لتحقيق أداء ممتاز. وفي الاجتماع الأسبوعي لفريقك، قدم لهم أدوات قوية وفعالة لإجراء تجارب مع نماذج الملخصات الآلية.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"pipeline('summarization', model='facebook/bart-large-cnn')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the pipeline function from the Transformers library provided by Hugging Face.', 'code': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'facebook/bart-large-cnn\\\\')\\\\narticle = \\\"Insert your long article text here\\\"\\\\nsummary = summarizer(article, max_length=130, min_length=30, do_sample=False)\\\\nprint(summary[0][\\\\'summary_text\\\\'])', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Summarization', 'api_name': 'facebook/bart-large-cnn', 'api_call': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'facebook/bart-large-cnn\\\\')\\\\narticle = \\\"Insert your long article text here\\\"\\\\nsummary = summarizer(article, max_length=130, min_length=30, do_sample=False)\\\\nprint(summary[0][\\\\'summary_text\\\\'])', 'api_arguments': ['ARTICLE', 'max_length', 'min_length', 'do_sample'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=facebook/bart-large-cnn)\\\\nARTICLE = ...\\\\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))', 'performance': {'dataset': 'cnn_dailymail', 'accuracy': {'ROUGE-1': 42.949, 'ROUGE-2': 20.815, 'ROUGE-L': 30.619, 'ROUGE-LSUM': 40.038}}, 'description': 'BART (large-sized model), fine-tuned on CNN Daily Mail. BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.'}} </s>\"\n"}
{"text": "<s>[INST]يمكنك بدء تدريب نماذج لمعالجة اللغة الطبيعية للإجابة على أسئلة عن جداول في مجال استجابة الأسئلة المرتبطة بالجداول. هل ترغب في تعليم خوارزميات المعالجة اللغوية الطبيعية؟? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('microsoft/tapex-large-finetuned-wikisql')\\\", 'api_provider': 'PyTorch Transformers', 'explanation': '', 'code': \\\"from transformers import TapexTokenizer, BartForConditionalGeneration\\\\nimport pandas as pd\\\\n\\\\ntokenizer = TapexTokenizer.from_pretrained('microsoft/tapex-large-finetuned-wikisql')\\\\nmodel = BartForConditionalGeneration.from_pretrained('microsoft/tapex-large-finetuned-wikisql')\\\\ndata = {\\\\n    'year': [1896, 1900, 1904, 2004, 2008, 2012],\\\\n    'city': ['Athens', 'Paris', 'St. Louis', 'Athens', 'Beijing', 'London']\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = 'In which year did Beijing host the Olympic Games?'\\\\nencoding = tokenizer(table=table, query=query, return_tensors='pt')\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\\\", 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'PyTorch Transformers', 'functionality': 'Table Question Answering', 'api_name': 'microsoft/tapex-large-finetuned-wikisql', 'api_call': \\\"from transformers import TapexTokenizer, BartForConditionalGeneration\\\\nimport pandas as pd\\\\n\\\\ntokenizer = TapexTokenizer.from_pretrained('microsoft/tapex-large-finetuned-wikisql')\\\\nmodel = BartForConditionalGeneration.from_pretrained('microsoft/tapex-large-finetuned-wikisql')\\\\ndata = {\\\\n    'year': [1896, 1900, 1904, 2004, 2008, 2012],\\\\n    'city': ['Athens', 'Paris', 'St. Louis', 'Athens', 'Beijing', 'London']\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = 'In which year did Beijing host the Olympic Games?'\\\\nencoding = tokenizer(table=table, query=query, return_tensors='pt')\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\\\", 'api_arguments': {'table': 'pd.DataFrame.from_dict(data)', 'query': 'query'}, 'python_environment_requirements': {'transformers': 'TapexTokenizer, BartForConditionalGeneration', 'pandas': 'pd'}, 'example_code': 'from transformers import TapexTokenizer, BartForConditionalGeneration\\\\nimport pandas as pd\\\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-large-finetuned-wikisql)\\\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-large-finetuned-wikisql)\\\\ndata = {\\\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\\\n city: [athens, paris, st. louis, athens, beijing, london]\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = In which year did beijing host the Olympic Games?\\\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))', 'performance': {'dataset': 'wikisql', 'accuracy': 'N/A'}, 'description': 'TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. This model is the tapex-base model fine-tuned on the WikiSQL dataset.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد نماذج تصنيف الصور إلى نصوص بمساعدة معالجة اللغة الطبيعية. هل يمكنك مساعدتي في ذلك؟? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import 'GenerativeImage2TextModel' from the transformers library provided by Hugging Face.\\\", 'code': 'from transformers import GenerativeImage2TextModel\\\\nmodel = GenerativeImage2TextModel.from_pretrained(\\\\'microsoft/git-large-coco\\\\')\\\\n\\\\n# Function to convert image file to PIL Image or Tensor\\\\ndef image_preprocessing(image_path):\\\\n    # Implement image preprocessing steps here.\\\\n    pass\\\\n\\\\nimage_input = image_preprocessing(image_file_path)\\\\ncontext_description = \\\"A short context for the desired caption\\\"\\\\ngenerated_caption = model.generate_image_caption(image_input, context_description)', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': 'from transformers import GenerativeImage2TextModel\\\\nmodel = GenerativeImage2TextModel.from_pretrained(\\\\'microsoft/git-large-coco\\\\')\\\\n\\\\n# Function to convert image file to PIL Image or Tensor\\\\ndef image_preprocessing(image_path):\\\\n    # Implement image preprocessing steps here.\\\\n    pass\\\\n\\\\nimage_input = image_preprocessing(image_file_path)\\\\ncontext_description = \\\"A short context for the desired caption\\\"\\\\ngenerated_caption = model.generate_image_caption(image_input, context_description)', 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتحويل الصور إلى نص متعدد الوسائط. هل يمكنك مساعدتي في تطوير نموذج يمكنه التعرف على الصور البرية للحيوانات دون تدريب مسبق؟? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"pipeline('text-generation', model='microsoft/git-base-coco')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\ncaption_generator = pipeline('text-generation', model='microsoft/git-base-coco')\\\\n\\\\n# For generating captions on images\\\\nresult = caption_generator(image_path_or_url)\\\\n\\\\n# For videos, extract image frames and process them using the model\\\", 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \\\"from transformers import pipeline\\\\ncaption_generator = pipeline('text-generation', model='microsoft/git-base-coco')\\\\n\\\\n# For generating captions on images\\\\nresult = caption_generator(image_path_or_url)\\\\n\\\\n# For videos, extract image frames and process them using the model\\\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام لتصنيف مقاطع الفيديو بتقنية رؤية الحاسوب، ونريد توقع محتوى الفيديو بناءً على بعض البيانات المتاحة.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary modules from the required libraries, like transformers, numpy, and torch for video classification tasks.\\\\n2. Use the from_pretrained method of the TimesformerForVideoClassification class to load the pre-trained model 'facebook/timesformer-hr-finetuned-k600'. This model has been trained on the Kinetics-600 dataset for video classification tasks.\\\\n3. Load the 16-frame video snippet. The video frames need to be given as numpy arrays with shape (number_of_frames, channel, height, width).\\\\n4. Use the model to analyze the video snippet and identify the action being performed.\\\", '<<<code>>>': 'from transformers import TimesformerForVideoClassification, AutoImageProcessor\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(16, 3, 448, 448))  # Replace this with your video frames\\\\nprocessor = AutoImageProcessor.from_pretrained(\\\\'facebook/timesformer-hr-finetuned-k600\\\\')\\\\nmodel = TimesformerForVideoClassification.from_pretrained(\\\\'facebook/timesformer-hr-finetuned-k600\\\\')\\\\n\\\\ninputs = processor(images=video, return_tensors=\\\\'pt\\\\')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/timesformer-hr-finetuned-k600', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')\\\", 'api_arguments': {'images': 'video', 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'numpy', 'torch']], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 448, 448))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-hr-finetuned-k600)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-hr-finetuned-k600)\\\\ninputs = processor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-600', 'accuracy': 'Not provided'}, 'description': 'TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 600 possible Kinetics-600 labels.' </s>\"\n"}
{"text": "<s>[INST]يقوم الخبراء في مجال التعلم العميق بتطوير نماذج تقوم بتدريب الوكيل لتحسين اتخاذ القرارات في بحث الزيادة في المكافأة، من خلال تفاعله مع بيئته.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"load_from_hub(repo_id='sb3/dqn-CartPole-v1',filename='{MODEL FILENAME}.zip',)\\\", '<<<api_provider>>>': 'Stable-Baselines3', '<<<explanation>>>': \\\"1. Import the necessary libraries, such as 'stable_baselines3' and 'rl_zoo3'.\\\\n2. Utilize the 'load_from_hub' function to obtain the pre-trained DQN (Deep Q-Network) model, designed for the CartPole-v1 environment.\\\\n3. This model has been trained to balance a pole on a cart as it moves, which can serve as a proxy for assessing a self-driving car's performance in a simulated environment.\\\\n4. Deploy the model in your specific environment and evaluate its performance using various metrics or observations.\\\", '<<<code>>>': 'from rl_zoo3 import load_from_hub\\\\nfrom stable_baselines3 import DQN, make_vec_env\\\\n\\\\nenv = make_vec_env(\\\"CartPole-v1\\\")\\\\nmodel = load_from_hub(repo_id=\\\\'sb3/dqn-CartPole-v1\\\\', filename=\\\\'{MODEL FILENAME}.zip\\\\',)\\\\nmodel.set_env(env)\\\\n\\\\nobs = env.reset()\\\\nfor _ in range(1000):\\\\n    action, _states = model.predict(obs, deterministic=True)\\\\n    obs, rewards, dones, info = env.step(action)\\\\n    env.render()', 'api_call': \\\"load_from_hub(repo_id='sb3/dqn-CartPole-v1',filename='{MODEL FILENAME}.zip',)\\\", 'provider': 'Stable-Baselines3', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dqn-CartPole-v1', 'api_call': \\\"load_from_hub(repo_id='sb3/dqn-CartPole-v1',filename='{MODEL FILENAME}.zip',)\\\", 'api_arguments': ['algo', 'env', 'logs'], 'python_environment_requirements': [['rl_zoo3', 'stable-baselines3', 'stable-baselines3-contrib']], 'example_code': 'python train.py --algo dqn --env CartPole-v1 -f logs/', 'performance': {'dataset': 'CartPole-v1', 'accuracy': '500.00 +/- 0.00'}, 'description': 'This is a trained model of a DQN agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا ببناء نظام لمعالجة اللغة الطبيعية لتصنيف النصوص، ونريد التنبؤ بفئة النص بناءً على بعض مجموعات البيانات المتوفرة.? [/INST]{'domain': 'Natural Language Processing Text Classification', 'api_call': \\\"pipeline('sentiment-analysis', model='siebert/sentiment-roberta-large-english')\\\", 'api_provider': 'Transformers', 'explanation': '1. First, import the pipeline function from the transformers library provided by Hugging Face.', 'code': 'from transformers import pipeline\\\\nsentiment_analyzer = pipeline(\\\\'sentiment-analysis\\\\', model=\\\\'siebert/sentiment-roberta-large-english\\\\')\\\\n\\\\n# Analyzing the sentiment of a user post\\\\npost = \\\"I had a great time at the party last night!\\\"\\\\nresult = sentiment_analyzer(post)', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Transformers', 'functionality': 'Sentiment Analysis', 'api_name': 'siebert/sentiment-roberta-large-english', 'api_call': 'from transformers import pipeline\\\\nsentiment_analyzer = pipeline(\\\\'sentiment-analysis\\\\', model=\\\\'siebert/sentiment-roberta-large-english\\\\')\\\\n\\\\n# Analyzing the sentiment of a user post\\\\npost = \\\"I had a great time at the party last night!\\\"\\\\nresult = sentiment_analyzer(post)', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import pipeline\\\\nsentiment_analysis = pipeline(sentiment-analysis, model=siebert/sentiment-roberta-large-english)\\\\nprint(sentiment_analysis(I love this!))', 'performance': {'dataset': [{'name': 'McAuley and Leskovec (2013) (Reviews)', 'accuracy': 98.0}, {'name': 'McAuley and Leskovec (2013) (Review Titles)', 'accuracy': 87.0}, {'name': 'Yelp Academic Dataset', 'accuracy': 96.5}, {'name': 'Maas et al. (2011)', 'accuracy': 96.0}, {'name': 'Kaggle', 'accuracy': 96.0}, {'name': 'Pang and Lee (2005)', 'accuracy': 91.0}, {'name': 'Nakov et al. (2013)', 'accuracy': 88.5}, {'name': 'Shamma (2009)', 'accuracy': 87.0}, {'name': 'Blitzer et al. (2007) (Books)', 'accuracy': 92.5}, {'name': 'Blitzer et al. (2007) (DVDs)', 'accuracy': 92.5}, {'name': 'Blitzer et al. (2007) (Electronics)', 'accuracy': 95.0}, {'name': 'Blitzer et al. (2007) (Kitchen devices)', 'accuracy': 98.5}, {'name': 'Pang et al. (2002)', 'accuracy': 95.5}, {'name': 'Speriosu et al. (2011)', 'accuracy': 85.5}, {'name': 'Hartmann et al. (2019)', 'accuracy': 98.0}], 'average_accuracy': 93.2}, 'description': \\\"This model ('SiEBERT', prefix for 'Sentiment in English') is a fine-tuned checkpoint of RoBERTa-large (Liu et al. 2019). It enables reliable binary sentiment analysis for various types of English-language text. For each instance, it predicts either positive (1) or negative (0) sentiment. The model was fine-tuned and evaluated on 15 data sets from diverse text sources to enhance generalization across different types of texts (reviews, tweets, etc.). Consequently, it outperforms models trained on only one type of text (e.g., movie reviews from the popular SST-2 benchmark) when used on new data as shown below.\\\"}} </s>\"\n"}
{"text": "<s>[INST]حمّل ملفي صوت مختلفين لأشخاص يتحدثون بلغة مختلفة. استخدم واجهة البرمجة الخاصة بالصوت لتحويل الصوت الأول إلى نص? [/INST]{'domain': 'Audio Voice Activity Detection', 'api_call': \\\"Model.from_pretrained('pyannote/brouhaha', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from pyannote.audio import Model, Inference\\\\nmodel = Model.from_pretrained('pyannote/brouhaha', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\\ninference = Inference(model)\\\\noutput = inference('audio.wav')\\\\n# replace 'audio.wav' with path to your audio file\\\", 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Voice Activity Detection, Speech-to-Noise Ratio, and C50 Room Acoustics Estimation', 'api_name': 'pyannote/brouhaha', 'api_call': \\\"from pyannote.audio import Model, Inference\\\\nmodel = Model.from_pretrained('pyannote/brouhaha', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\\ninference = Inference(model)\\\\noutput = inference('audio.wav')\\\\n# replace 'audio.wav' with path to your audio file\\\", 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote-audio', 'brouhaha-vad'], 'example_code': ['from pyannote.audio import Model', 'model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)', 'from pyannote.audio import Inference', 'inference = Inference(model)', 'output = inference(audio.wav)', 'for frame, (vad, snr, c50) in output:', '  t = frame.middle', '  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})'], 'performance': {'dataset': 'LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene', 'accuracy': 'Not provided'}, 'description': 'Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.'}} </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج تصنيف الصور في رؤية الحاسوب.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To achieve this, we first import the necessary packages, such as the pipeline function from the transformers library provided by Hugging Face and the torch library for handling input image tensors.\\\\n2. We use the pipeline function to create an image classification model by specifying the model 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k'. This model is trained on ImageNet-1K and further fine-tuned on ImageNet-12K for image classification tasks, including dog breed classification.\\\\n3. By using this classifier, we can input an image of a dog, and it will be able to predict the breed of the dog with high accuracy.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimport torch\\\\ndog_classifier = pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\\ndog_breed = dog_classifier(dog_image_tensor)\\\", 'api_call': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'api_call': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", 'api_arguments': {'model': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'framework': 'pt'}, 'python_environment_requirements': [['transformers', 'torch']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج للتوليف النصي لمعالجة اللغة الطبيعية الذي يقوم بتحويل النص إلى نص آخر باستخدام عبارة تبدأ بـ \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package, which includes RobertaTokenizer to tokenize the given code snippet and T5ForConditionalGeneration to load the CodeT5 model.\\\\n2. Use the from_pretrained method to load the 'Salesforce/codet5-base' model, which has been pre-trained for code understanding and generation tasks.\\\\n3. Use the tokenizer to convert the provided Python code snippet into a format that can be fed into the model.\\\\n4. Use the generate method to translate the input code snippet into a human-readable description.\\\\n5. Decode the generated output to obtain a plain-text explanation.\\\", '<<<code>>>': 'from transformers import RobertaTokenizer, T5ForConditionalGeneration\\\\ntokenizer = RobertaTokenizer.from_pretrained(\\\\'Salesforce/codet5-base\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'Salesforce/codet5-base\\\\')\\\\ntext = \\\"def greet(user): print(f\\\\'hello {user}!\\\\')\\\"\\\\ninput_ids = tokenizer(text, return_tensors=\\\\'pt\\\\').input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=20)\\\\nhuman_readable_description = tokenizer.decode(generated_ids[0], skip_special_tokens=True)', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Salesforce/codet5-base', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\\\", 'api_arguments': ['text', 'return_tensors', 'input_ids', 'max_length'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import RobertaTokenizer, T5ForConditionalGeneration\\\\ntokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\\\\nmodel = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\\\\ntext = def greet(user): print(f'hello <extra_id_0>!')\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=8)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\\\", 'performance': {'dataset': 'code_search_net', 'accuracy': 'Refer to the paper for evaluation results on several downstream benchmarks'}, 'description': 'CodeT5 is a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. It supports both code understanding and generation tasks and allows for multi-task learning. The model can be used for tasks such as code summarization, code generation, code translation, code refinement, code defect detection, and code clone detection.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد نموذج يستطيع الإجابة على أسئلة متعلقة بمعالجة اللغات الطبيعية. هل يمكنك مساعدتي في ذلك؟ لقد يتم تحقيق ذلك من خلال استخدام تقنيات معالجة اللغات الطبيعية لتدريب نموذج توليد نص مقابل نص.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('t5-3b')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary packages: T5ForConditionalGeneration from transformers, and the tokenizer for the T5-3b model.', 'code': 'from transformers import T5ForConditionalGeneration, T5Tokenizer\\\\n\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'t5-3b\\\\')\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'t5-3b\\\\')\\\\n\\\\ninput_text = \\\"Task-specific prompt with input text\\\"\\\\ninputs = tokenizer.encode(input_text, return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs)\\\\ngenerated_text = tokenizer.decode(outputs[0])', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Translation, Summarization, Question Answering, Sentiment Analysis', 'api_name': 't5-3b', 'api_call': 'from transformers import T5ForConditionalGeneration, T5Tokenizer\\\\n\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'t5-3b\\\\')\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'t5-3b\\\\')\\\\n\\\\ninput_text = \\\"Task-specific prompt with input text\\\"\\\\ninputs = tokenizer.encode(input_text, return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs)\\\\ngenerated_text = tokenizer.decode(outputs[0])', 'api_arguments': 'input_text', 'python_environment_requirements': 'transformers', 'example_code': \\\"input_text = 'translate English to French: The quick brown fox jumps over the lazy dog'; inputs = tokenizer.encode(input_text, return_tensors='pt'); outputs = model.generate(inputs); translated_text = tokenizer.decode(outputs[0])\\\", 'performance': {'dataset': 'c4', 'accuracy': 'See research paper, Table 14'}, 'description': 'T5-3B is a Text-To-Text Transfer Transformer (T5) model with 3 billion parameters. It is designed for various NLP tasks such as translation, summarization, question answering, and sentiment analysis. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on multiple supervised and unsupervised tasks.'}} </s>\"\n"}
{"text": "<s>[INST]يمكن لمطوري اللغة الطبيعية المحادثية الاجتماع في النادي الطلابي كل أسبوعين ليحسنوا مهاراتهم في تطوير نماذج التعلم الآلي. يُنصح بتوفير أداة لعب القدم الافتراضية للمشاركين من أجل تحقيق التحدي والتعلم.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('pygmalion-6b')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import needed transformers library classes such as AutoTokenizer and AutoModelForCausalLM.\\\\n2. Load the Pygmalion 6B model using the from_pretrained method of AutoModelForCausalLM.\\\\n3. Tokenize the input prompt (which includes the persona description and conversation history) using the tokenizer.\\\\n4. Generate the model's response to the input dialogue by passing the encoded tokens to the model.\\\\n5. Decode the generated tokens to create a text-based response for the game character.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'pygmalion-6b\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'pygmalion-6b\\\\')\\\\n\\\\ncharacter_persona = \\\"[A few sentences about the character you want the model to play]\\\"\\\\ndialogue_history = \\\"[DIALOGUE HISTORY]\\\"\\\\ninput_message = \\\"You: [Your input message here]\\\"\\\\ninput_prompt = character_persona + \\\"\\\\\\\\n<START>\\\\\\\\n\\\" + dialogue_history + \\\"\\\\\\\\n\\\" + input_message\\\\n\\\\nencoded_input = tokenizer.encode(input_prompt, return_tensors=\\\"pt\\\")\\\\ngenerated_tokens = model.generate(encoded_input, max_length=100, n=num_responses)\\\\ndecoded_response = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('pygmalion-6b')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pygmalion-6b', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('pygmalion-6b')\\\", 'api_arguments': ['input_prompt'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\\\\n<START>\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\\\", 'performance': {'dataset': '56MB of dialogue data', 'accuracy': 'Not specified'}, 'description': \\\"Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B. The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model was initialized from the uft-6b ConvoGPT model and fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed.\\\" </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج لتقسيم الصور في مجال رؤية الحاسوب.? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"YOLO('keremberke/yolov8n-pothole-segmentation')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the YOLO class from the ultralyticsplus library and render_result function.', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pothole-segmentation', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.995, 'mAP@0.5(mask)': 0.995}}, 'description': 'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية باستخدام تصنيف لغات طبيعية بدون تدريب. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a zero-shot classification model based on the pre-trained model 'svalabs/gbert-large-zeroshot-nli'. This model has been trained for natural language inference and is suitable for text classification tasks.\\\\n3. Given a customer complaint, determine possible categories and urgency levels.\\\\n4. Use the created classifier to categorize the customer complaint based on the provided labels.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nzeroshot_pipeline = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'svalabs/gbert-large-zeroshot-nli\\\\')\\\\nsequence = \\\"Ich habe ein Problem mit meinem Iphone, das so schnell wie möglich gelöst werden muss\\\"\\\\nlabels = [\\\\'Computer\\\\', \\\\'Handy\\\\', \\\\'Tablet\\\\', \\\\'dringend\\\\', \\\\'nicht dringend\\\\']\\\\nhypothesis_template = \\\"In diesem Satz geht es um das Thema {}.\\\"\\\\nclassification_results = zeroshot_pipeline(sequence, labels, hypothesis_template=hypothesis_template)', 'api_call': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'svalabs/gbert-large-zeroshot-nli', 'api_call': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", 'api_arguments': ['sequence', 'labels', 'hypothesis_template'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\nzershot_pipeline = pipeline(zero-shot-classification, model=svalabs/gbert-large-zeroshot-nli)\\\\nsequence = Ich habe ein Problem mit meinem Iphone das so schnell wie möglich gelöst werden muss\\\\nlabels = [Computer, Handy, Tablet, dringend, nicht dringend]\\\\nhypothesis_template = In diesem Satz geht es um das Thema {}. ', 'performance': {'dataset': 'XNLI TEST-Set', 'accuracy': '85.6%'}, 'description': 'A German zeroshot classification model based on the German BERT large model from deepset.ai and finetuned for natural language inference using machine-translated nli sentence pairs from mnli, anli, and snli datasets.' </s>\"\n"}
{"text": "<s>[INST]لتطوير برنامج يقوم بتلخيص النصوص بشكل طبيعي، قم بتجميع فريق من المطورين المهتمين بمعالجة اللغة الطبيعية وتقنيات التحليل اللغوي. ابتكروا خوارزمية تلخيص محتوى النصوص بشكل فعال ودقيق. لا تنسوا إضافة واجهة سهلة الاستخدام للمستخدمين.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the necessary classes from the transformers library provided by Hugging Face, including BartForConditionalGeneration and BartTokenizer.', 'code': \\\"from transformers import BartForConditionalGeneration, BartTokenizer\\\\nmodel = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\\ntokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\\ninputs = tokenizer(article_text, return_tensors='pt', max_length=512, truncation=True)\\\\nsummary_ids = model.generate(inputs['input_ids'], num_beams=4, min_length=30, max_length=100)\\\\nsummary = tokenizer.decode(summary_ids[0])\\\", 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Transformers', 'functionality': 'Text Summarization', 'api_name': 'sshleifer/distilbart-cnn-6-6', 'api_call': \\\"from transformers import BartForConditionalGeneration, BartTokenizer\\\\nmodel = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\\ntokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\\ninputs = tokenizer(article_text, return_tensors='pt', max_length=512, truncation=True)\\\\nsummary_ids = model.generate(inputs['input_ids'], num_beams=4, min_length=30, max_length=100)\\\\nsummary = tokenizer.decode(summary_ids[0])\\\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': {'cnn_dailymail': {'Rouge 2': 20.17, 'Rouge-L': 29.7}, 'xsum': {'Rouge 2': 20.92, 'Rouge-L': 35.73}}}, 'description': 'DistilBART model for text summarization, trained on the CNN/Daily Mail and XSum datasets. It is a smaller and faster version of BART, suitable for summarizing English text.'}} </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'AutoTokenizer' and 'AutoModelForTokenClassification' from transformers.\\\\n2. Load the tokenizer and the model using the 'from_pretrained' method with the mentioned model name, which is 'Babelscape/wikineural-multilingual-ner'.\\\\n3. Create a pipeline for Named Entity Recognition (NER) using the loaded tokenizer and model.\\\\n4. Use the NER pipeline to analyze the medical history of the patient by providing the patient's text as input.\\\\n5. The pipeline will return the recognized named entities, specifically highlighting life events relevant to the patient's medical history.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Babelscape/wikineural-multilingual-ner\\\\')\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'Babelscape/wikineural-multilingual-ner\\\\')\\\\nnlp = pipeline(\\\\'ner\\\\', model=model, tokenizer=tokenizer)\\\\npatient_history = \\\"Medical history text here...\\\"\\\\nner_results = nlp(patient_history)\\\\nprint(ner_results)', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Babelscape/wikineural-multilingual-ner', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\\\", 'api_arguments': {'model': 'AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)', 'tokenizer': 'AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)'}, 'python_environment_requirements': [{'transformers': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline'}], 'example_code': 'tokenizer = AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\\\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\\\nexample = My name is Wolfgang and I live in Berlin\\\\nner_results = nlp(example)\\\\nprint(ner_results)', 'performance': {'dataset': 'Babelscape/wikineural-multilingual-ner', 'accuracy': 'span-based F1-score up to 6 points over previous state-of-the-art systems for data creation'}, 'description': 'A multilingual Named Entity Recognition (NER) model fine-tuned on the WikiNEuRal dataset, supporting 9 languages (de, en, es, fr, it, nl, pl, pt, ru). It is based on the mBERT architecture and trained on all 9 languages jointly. The model can be used with the Hugging Face Transformers pipeline for NER tasks.' </s>\"\n"}
{"text": "<s>[INST]إننا كشركة متخصصة في توليد النصوص باستخدام تقنيات معالجة اللغة الطبيعية، نقوم بتطوير نماذج لإنشاء نصوص منظورة لديها طابع الطبيعية. يجب عليك فهم تدفق الكلمات والعبارات بشكل متقن لضمان أدقية النهج والإنتاجية.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='decapoda-research/llama-13b-hf')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the pipeline function from Hugging Face\\\\'s transformers library.\\\\n2. Instantiate a text generation pipeline using the specified model \\\\'decapoda-research/llama-13b-hf\\\\', which has been trained on a variety of tasks, such as question answering and reading comprehension.\\\\n3. Input the starting phrase \\\"Once upon a time, in a world full of robots,\\\" to the pipeline model, which will generate an interesting story about robots in a children\\\\'s context.', '<<<code>>>': 'from transformers import pipeline\\\\nstory_generator = pipeline(\\\\'text-generation\\\\', model=\\\\'decapoda-research/llama-13b-hf\\\\')\\\\nstory = story_generator(\\\"Once upon a time, in a world full of robots,\\\")', 'api_call': \\\"pipeline('text-generation', model='decapoda-research/llama-13b-hf')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'decapoda-research/llama-13b-hf', 'api_call': \\\"pipeline('text-generation', model='decapoda-research/llama-13b-hf')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"generator('Once upon a time')\\\", 'performance': {'dataset': [{'name': 'BoolQ', 'accuracy': '85.3'}, {'name': 'PIQA', 'accuracy': '82.8'}, {'name': 'SIQA', 'accuracy': '52.3'}, {'name': 'HellaSwag', 'accuracy': '84.2'}, {'name': 'WinoGrande', 'accuracy': '77'}, {'name': 'ARC-e', 'accuracy': '81.5'}, {'name': 'ARC-c', 'accuracy': '56'}, {'name': 'OBQACOPA', 'accuracy': '60.2'}]}, 'description': 'LLaMA-13B is an auto-regressive language model based on the transformer architecture developed by the FAIR team of Meta AI. It is designed for research purposes, such as question answering, natural language understanding, and reading comprehension. The model has been trained on a variety of sources, including web data, GitHub, Wikipedia, and books in 20 languages. It has been evaluated on several benchmarks, including BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC, and OpenBookQA.' </s>\"\n"}
{"text": "<s>[INST]لتحقيق تقدم أكبر في مجال تقنية رؤية الحاسوب وتقسيم الصور، ابتكروا نادي طلابي يجتمع بانتظام لتطوير مهاراتكم. قموا بتنظيم دوريات للعب كرة القدم الافتراضية بشكل دوري، وضعوا خططًا لتحسين قدراتكم التواصلية والتفاعلية. كما يوصى بزيادة تحدي التعلم من خلال توفير أدوات للعب ضد وكيل التعلم الآلي، لزيادة تفاعلكم مع التكنولوجيا وقدرتكم على التكامل معها.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import necessary libraries, including Mask2FormerForUniversalSegmentation from transformers, Image from PIL, and requests for handling URL requests.\\\\n2. Load the pretrained 'facebook/mask2former-swin-tiny-coco-instance' model using the from_pretrained method.\\\\n3. Load the image you want to segment, either from a file or from an image URL.\\\\n4. Process the image using the processor, which resizes and normalizes the input image.\\\\n5. Use the model to make predictions on the processed image, which will generate class queries logits and masks queries logits.\\\\n6. Post-process the outputs to extract the instance segmentation map for the input image.\\\\n7. The result will be an instance segmentation map that highlights and separates vehicles from the rest of the image.\\\", '<<<code>>>': \\\"from transformers import Mask2FormerForUniversalSegmentation, AutoImageProcessor\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\n\\\\nurl = 'https://your_image_url_here.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\ninputs = processor(images=image, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n\\\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_instance_map = result['segmentation']\\\", 'api_call': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/mask2former-swin-tiny-coco-instance', 'api_call': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-tiny-coco-instance'}, 'python_environment_requirements': [['torch', 'transformers', 'PIL', 'requests']], 'example_code': \\\"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_instance_map = result['segmentation']\\\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.' </s>\"\n"}
{"text": "<s>[INST]أنشئ قصة مصورة تتحول إلى فيديو تبدأ بجملة \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Multimodal Text-to-Video', '<<<api_call>>>': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'TuneAVideoPipeline', 'UNet3DConditionModel', and 'save_videos_grid'.\\\\n2. Use the 'TuneAVideoPipeline.from_pretrained()' method to create a Text-to-Video pipeline with the nitrosocke/mo-di-diffusion prebuilt model, and the UNet3DConditionModel in 'Tune-A-Video-library/mo-di-bear-guitar'.\\\\n3. To create video snippets from text descriptions, use the 'pipe()' function with arguments such as 'prompt', 'video_length', 'height', 'width', 'num_inference_steps', and 'guidance_scale'.\\\\n4. Save the generated video by using the 'save_videos_grid()' function from the tuneavideo library.\\\", '<<<code>>>': \\\"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\\\nfrom tuneavideo.util import save_videos_grid\\\\nimport torch\\\\n\\\\nunet_model_path = 'Tune-A-Video-library/mo-di-bear-guitar'\\\\npretrained_model_path = 'nitrosocke/mo-di-diffusion'\\\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to('cuda')\\\\npipe.enable_xformers_memory_efficient_attention()\\\\n\\\\nprompt = 'A conference showcasing the latest technology trends'\\\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\\\nsave_videos_grid(video, f'./{prompt}.gif')\\\", 'api_call': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mo-di-bear-guitar', 'api_call': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': [['torch', 'tuneavideo']], 'example_code': \\\"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\\\nfrom tuneavideo.util import save_videos_grid\\\\nimport torch\\\\npretrained_model_path = nitrosocke/mo-di-diffusion\\\\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\\\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\nprompt = a magical princess is playing guitar, modern disney style\\\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\\\nsave_videos_grid(video, f./{prompt}.gif)\\\", 'performance': {'dataset': 'Not mentioned', 'accuracy': 'Not mentioned'}, 'description': 'Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد نصلخصح فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('plguillou/t5-base-fr-sum-cnndm')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries T5Tokenizer and T5ForConditionalGeneration from the transformers package.\\\\n2. Use the 'from_pretrained' method to load the tokenizer and the T5 model with the provided model name 'plguillou/t5-base-fr-sum-cnndm'.\\\\n3. Tokenize the French news article text using the T5Tokenizer.\\\\n4. Pass the tokenized input to the T5ForConditionalGeneration model by setting 'input_ids' and 'attention_mask' values.\\\\n5. Decode the generated tokens to obtain the summarized text.\\\", '<<<code>>>': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'plguillou/t5-base-fr-sum-cnndm\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'plguillou/t5-base-fr-sum-cnndm\\\\')\\\\ninputs = tokenizer(\\\"summarize: \\\" + french_news_article, return_tensors=\\\"pt\\\")\\\\nsummary_ids = model.generate(inputs[\\\\'input_ids\\\\'], num_return_sequences=1, num_beams=4, no_repeat_ngram_size=2, early_stopping=True, length_penalty=1.5)\\\\nsummaries = [tokenizer.decode(summary_id, skip_special_tokens=True, clean_up_tokenization_spaces=True) for summary_id in summary_ids]', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('plguillou/t5-base-fr-sum-cnndm')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'plguillou/t5-base-fr-sum-cnndm', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('plguillou/t5-base-fr-sum-cnndm')\\\", 'api_arguments': {'input_text': 'summarize: ARTICLE'}, 'python_environment_requirements': [{'transformers': 'from transformers import T5Tokenizer, T5ForConditionalGeneration'}], 'example_code': 'tokenizer = T5Tokenizer.from_pretrained(plguillou/t5-base-fr-sum-cnndm)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(plguillou/t5-base-fr-sum-cnndm)', 'performance': {'dataset': 'cnn_dailymail', 'ROUGE-1': 44.5252, 'ROUGE-2': 22.652, 'ROUGE-L': 29.8866}, 'description': 'This model is a T5 Transformers model (JDBN/t5-base-fr-qg-fquad) that was fine-tuned in French for abstractive text summarization.' </s>\"\n"}
{"text": "<s>[INST]قم ببناء نموذج معالجة لغوية طبيعية للتصنيف الصفري في مجال معالجة اللغة الطبيعية.? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"CrossEncoder('cross-encoder/nli-roberta-base')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': 'from sentence_transformers import CrossEncoder\\\\nencoder = CrossEncoder(\\\\'cross-encoder/nli-roberta-base\\\\')\\\\nmovie_conversation = \\\"Conversation about the movie goes here...\\\"\\\\ncandidate_genres = [\\\\'action\\\\', \\\\'comedy\\\\', \\\\'drama\\\\', \\\\'horror\\\\', \\\\'sci-fi\\\\']\\\\nscores = encoder.predict([(movie_conversation, genre) for genre in candidate_genres])\\\\nresult = {genre: score for genre, score in zip(candidate_genres, scores)}', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Transformers', 'functionality': 'Zero-Shot Classification', 'api_name': 'cross-encoder/nli-roberta-base', 'api_call': 'from sentence_transformers import CrossEncoder\\\\nencoder = CrossEncoder(\\\\'cross-encoder/nli-roberta-base\\\\')\\\\nmovie_conversation = \\\"Conversation about the movie goes here...\\\"\\\\ncandidate_genres = [\\\\'action\\\\', \\\\'comedy\\\\', \\\\'drama\\\\', \\\\'horror\\\\', \\\\'sci-fi\\\\']\\\\nscores = encoder.predict([(movie_conversation, genre) for genre in candidate_genres])\\\\nresult = {genre: score for genre, score in zip(candidate_genres, scores)}', 'api_arguments': ['sentence1', 'sentence2'], 'python_environment_requirements': ['sentence_transformers', 'transformers'], 'example_code': \\\"from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model='cross-encoder/nli-roberta-base')\\\\nsent = Apple just announced the newest iPhone X\\\\ncandidate_labels = [technology, sports, politics]\\\\nres = classifier(sent, candidate_labels)\\\\nprint(res)\\\", 'performance': {'dataset': ['SNLI', 'MultiNLI'], 'accuracy': 'See SBERT.net - Pretrained Cross-Encoder'}, 'description': 'Cross-Encoder for Natural Language Inference trained on the SNLI and MultiNLI datasets. Outputs three scores corresponding to the labels: contradiction, entailment, neutral.'}} </s>\"\n"}
{"text": "<s>[INST]أنشئ جملة بسيطة تحتوي على سؤال وجواب ضمن مجال معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Start by importing the necessary classes from the transformers package, including AutoModelForQuestionAnswering, AutoTokenizer, and pipeline.\\\\n2. Load the pre-trained model 'deepset/roberta-base-squad2' using the from_pretrained method of the AutoModelForQuestionAnswering class. This question-answering model has been trained on the SQuAD 2.0 dataset and is suitable for extracting answers from text content.\\\\n3. Load the corresponding tokenizer using the from_pretrained method of the AutoTokenizer class.\\\\n4. Use the pipeline function to create a question-answering pipeline with the model and tokenizer.\\\\n5. Accept user input for a question and use the pipeline to extract an answer from the webpage's text, given both as input to the pipeline.\\\", '<<<code>>>': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel_name = \\\\'deepset/roberta-base-squad2\\\\'\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=model, tokenizer=tokenizer)\\\\n\\\\nquestion = \\\"What is the main topic of the webpage?\\\"  # Replace with user input\\\\nwebpage_text = \\\"\\\"\\\"This is a webpage about...\\\"\\\"\\\"  # Replace with text content of the webpage\\\\n\\\\nQA_input = {\\\\'question\\\\': question, \\\\'context\\\\': webpage_text}\\\\nanswer = nlp(QA_input)', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'optimum/roberta-base-squad2', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')\\\", 'api_arguments': {'model_name': 'deepset/roberta-base-squad2', 'tokenizer': 'deepset/roberta-base-squad2'}, 'python_environment_requirements': [['transformers']], 'example_code': {'a': {'import': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline', 'example': \\\"nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\\\\nQA_input = {\\\\n 'question': 'Why is model conversion important?',\\\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\\\n}\\\\nres = nlp(QA_input)\\\"}, 'b': {'import': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer', 'example': 'model = AutoModelForQuestionAnswering.from_pretrained(model_name)\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)'}}, 'performance': {'dataset': 'squad_v2', 'accuracy': {'exact': 79.87029394424324, 'f1': 82.91251169582613}}, 'description': 'This is an ONNX conversion of the deepset/roberta-base-squad2 model for extractive question answering. It is trained on the SQuAD 2.0 dataset and is compatible with the Transformers library.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام لتصنيف الصور باستخدام تقنيات الرؤية الحاسوبية، ونود تدريب النظام على تصنيف الصور بدقة وفقاً لتصنيفات محددة مسبقاً.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create an image classification model.\\\\n3. Specify the model 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k' to be loaded. This model has been fine-tuned for image classification tasks based on the ViT (Vision Transformer) architecture.\\\\n4. Once the image_classifier is created, pass an image that needs to be classified into categories as input to the model to get the predictions.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\\nresult = image_classifier(image_path)\\\", 'api_call': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'api_call': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", 'api_arguments': {'model': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'framework': 'pt'}, 'python_environment_requirements': [['transformers', 'torch']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.' </s>\"\n"}
{"text": "<s>[INST]عميلنا يريد تطوير تطبيق لتحويل الصورة إلى نص. نحتاج إلى بحث وتطوير برنامج يقوم بهذه المهمة بكفاءة عالية ويناسب احتياجات مستخدمي هذا التطبيق.? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the pipeline function from the transformers library provided by Hugging Face.', 'code': 'from transformers import pipeline\\\\nimage_captioner = pipeline(\\\\'text2text-generation\\\\', model=\\\\'salesforce/blip2-opt-6.7b\\\\')\\\\nimage_path = \\\\'your_image_path.jpg\\\\'\\\\nwith open(image_path, \\\"rb\\\") as image_file:\\\\n    image_data = image_file.read()  # Read the image file as bytes\\\\ncaption = image_captioner(image_data)', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': 'from transformers import pipeline\\\\nimage_captioner = pipeline(\\\\'text2text-generation\\\\', model=\\\\'salesforce/blip2-opt-6.7b\\\\')\\\\nimage_path = \\\\'your_image_path.jpg\\\\'\\\\nwith open(image_path, \\\"rb\\\") as image_file:\\\\n    image_data = image_file.read()  # Read the image file as bytes\\\\ncaption = image_captioner(image_data)', 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية باستخدام تصنيف بدون تدريب لمعالجة اللغة الطبيعية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-3')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a zero-shot classification model with the pre-trained model \\\\'valhalla/distilbart-mnli-12-3\\\\'. This model is well-suited for zero-shot classification tasks required in this context.\\\\n3. Define the list of possible dispute categories such as: [\\\"financial issue\\\", \\\"delivery issue\\\", \\\"customer support issue\\\"]\\\\n4. Use the created classifier to classify the given dispute text.\\\\n5. Extract the most likely dispute category.', '<<<code>>>': 'from transformers import pipeline\\\\ndispute_text = \\\"Lauren recently bought a dress from our website, and she is complaining that the item she received is not as described. The quality of the fabric is very poor, and the color is completely different from what was shown in the pictures. She is requesting a full refund.\\\"\\\\ncategories = [\\\"financial issue\\\", \\\"delivery issue\\\", \\\"customer support issue\\\"]\\\\nzero_shot_classifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'valhalla/distilbart-mnli-12-3\\\\')\\\\nclassification_result = zero_shot_classifier(dispute_text, categories)\\\\ntop_category = classification_result[\\\\'labels\\\\'][0]', 'api_call': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-3')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'valhalla/distilbart-mnli-12-3', 'api_call': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-3')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': [{'name': 'matched acc', 'accuracy': 88.1}, {'name': 'mismatched acc', 'accuracy': 88.19}]}, 'description': 'distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is a simple and effective technique with very little performance drop.' </s>\"\n"}
{"text": "<s>[INST]استخدم تقنيات معالجة اللغة الطبيعية لتحويل النصوص إلى نصوص أخرى، ابدأ بـ \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the required libraries from the transformers package. In this case, import 'BertTokenizerFast' and 'EncoderDecoderModel'.\\\", 'code': 'from transformers import BertTokenizerFast, EncoderDecoderModel\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\\\'kykim/bertshared-kor-base\\\\')\\\\nmodel = EncoderDecoderModel.from_pretrained(\\\\'kykim/bertshared-kor-base\\\\')\\\\n\\\\nkorean_sentence = \\\"한국어 뉴스 기사 문장\\\"\\\\ninputs = tokenizer.encode(korean_sentence, return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs)\\\\nenglish_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kykim/bertshared-kor-base', 'api_call': 'from transformers import BertTokenizerFast, EncoderDecoderModel\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\\\'kykim/bertshared-kor-base\\\\')\\\\nmodel = EncoderDecoderModel.from_pretrained(\\\\'kykim/bertshared-kor-base\\\\')\\\\n\\\\nkorean_sentence = \\\"한국어 뉴스 기사 문장\\\"\\\\ninputs = tokenizer.encode(korean_sentence, return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs)\\\\nenglish_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)', 'api_arguments': {'tokenizer': 'BertTokenizerFast.from_pretrained(kykim/bertshared-kor-base)'}, 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import BertTokenizerFast, EncoderDecoderModel\\\\ntokenizer = BertTokenizerFast.from_pretrained(kykim/bertshared-kor-base)\\\\nmodel = EncoderDecoderModel.from_pretrained(kykim/bertshared-kor-base)', 'performance': {'dataset': '70GB Korean text dataset', 'accuracy': '42000 lower-cased subwords'}, 'description': 'Bert base model for Korean, trained on a 70GB Korean text dataset and 42000 lower-cased subwords. Can be used for Text2Text Generation tasks.'}} </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي في مجال تقسيم الصور برؤية الحاسوب.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8m-building-segmentation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the 'ultralyticsplus' library using pip, which contains the YOLO object detection model.\\\\n2. Import the necessary classes YOLO and render_result from the 'ultralyticsplus' package.\\\\n3. Initialize the YOLO model with 'keremberke/yolov8m-building-segmentation', which is trained for building segmentation in satellite images.\\\\n4. Set model overrides for 'conf' (object confidence threshold) and 'iou' (IoU threshold for non-maximum suppression) to appropriate values.\\\\n5. Use the 'predict' method to perform building detection and segmentation on a given satellite image URL or local file path.\\\\n6. The result contains detected buildings' bounding boxes and masks that can be used for city planning purposes.\\\\n7. Use the 'render_result' function to visualize the results on the input image.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path_or_url_to_satellite_image'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8m-building-segmentation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8m-building-segmentation', 'api_call': \\\"YOLO('keremberke/yolov8m-building-segmentation')\\\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.623, 'mAP@0.5(mask)': 0.613}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتوليد صور بدون شروط في مجال رؤية الحاسوب، ونريد ابتكار صور جديدة بناءً على البيانات المتاحة لتصنيف الصور بدون شروط في مجال رؤية الحاسوب.? [/INST]{'domain': 'Computer Vision Zero-Shot Image Classification', 'api_call': \\\"CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary libraries, including 'Image' from PIL, 'requests', 'CLIPProcessor', and 'CLIPModel' from transformers.\\\", 'code': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')\\\\nprocessor = CLIPProcessor.from_pretrained('flax-community/clip-rsicd-v2')\\\\nurl = 'https://example.com/path/to/image.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nlabels = ['residential area', 'playground', 'stadium', 'forest', 'airport']\\\\ninputs = processor(text=[f'a photo of a {l}' for l in labels], images=image, return_tensors='pt', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\\nfor l, p in zip(labels, probs[0]):\\\\n    print(f'{l: <16} {p:.4f}')\\\", 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'flax-community/clip-rsicd-v2', 'api_call': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')\\\\nprocessor = CLIPProcessor.from_pretrained('flax-community/clip-rsicd-v2')\\\\nurl = 'https://example.com/path/to/image.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nlabels = ['residential area', 'playground', 'stadium', 'forest', 'airport']\\\\ninputs = processor(text=[f'a photo of a {l}' for l in labels], images=image, return_tensors='pt', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\\nfor l, p in zip(labels, probs[0]):\\\\n    print(f'{l: <16} {p:.4f}')\\\", 'api_arguments': {'text': ['a photo of a residential area', 'a photo of a playground', 'a photo of a stadium', 'a photo of a forest', 'a photo of an airport'], 'images': 'image', 'return_tensors': 'pt', 'padding': 'True'}, 'python_environment_requirements': ['PIL', 'requests', 'transformers'], 'example_code': 'from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(flax-community/clip-rsicd-v2)\\\\nprocessor = CLIPProcessor.from_pretrained(flax-community/clip-rsicd-v2)\\\\nurl = https://raw.githubusercontent.com/arampacha/CLIP-rsicd/master/data/stadium_1.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nlabels = [residential area, playground, stadium, forest, airport]\\\\ninputs = processor(text=[fa photo of a {l} for l in labels], images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\\nfor l, p in zip(labels, probs[0]):\\\\n print(f{l:&lt;16} {p:.4f})', 'performance': {'dataset': {'RSICD': {'original CLIP': {'k=1': 0.572, 'k=3': 0.745, 'k=5': 0.837, 'k=10': 0.939}, 'clip-rsicd-v2 (this model)': {'k=1': 0.883, 'k=3': 0.968, 'k=5': 0.982, 'k=10': 0.998}}}}, 'description': 'This model is a fine-tuned CLIP by OpenAI. It is designed with an aim to improve zero-shot image classification, text-to-image and image-to-image retrieval specifically on remote sensing images.'}} </s>\"\n"}
{"text": "<s>[INST]قم ببناء نموذج لتحويل الصوت إلى نص باستخدام مجموعة البيانات المعينة. قُم بتجهيز النموذج لاستقبال التوجيهات بصوتك الخاص. ضع النموذج في وضع الاستماع المستمر. قم بتشغيل التطبيق وتجربة استدعاء الأوامر بواسطة الصوت. تحقق من دقة التعرف على الكلمات. أصلح أي أخطاء وقم بضبط النموذج لتحسين الأداء لاحقا.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"Wav2Vec2ForCTC.from_pretrained('cpierse/wav2vec2-large-xlsr-53-esperanto')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import required libraries, including torchaudio, torch, Wav2Vec2ForCTC, and Wav2Vec2Processor from transformers.\\\\n2. Create a processor instance by calling Wav2Vec2Processor.from_pretrained with the model name 'cpierse/wav2vec2-large-xlsr-53-esperanto'.\\\\n3. Create a model instance by calling Wav2Vec2ForCTC.from_pretrained with the model name 'cpierse/wav2vec2-large-xlsr-53-esperanto'.\\\\n4. Load the provided speech_recording_path using torchaudio.load function, which returns a tuple of speech_array and sampling_rate.\\\\n5. Resample the audio to 16kHz using torchaudio.transforms.Resample.\\\\n6. Pass the resampled audio to the processor instance and call its __call__ function to create the input tensors needed for the model.\\\\n7. Feed the input tensors to the model to get logits, and then find the most likely text using torch.argmax.\\\\n8. Decode the predicted IDs using the processor instance and print the predicted text.\\\", '<<<code>>>': 'import torch\\\\nimport torchaudio\\\\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\\\nprocessor = Wav2Vec2Processor.from_pretrained(\\\\'cpierse/wav2vec2-large-xlsr-53-esperanto\\\\')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\\\'cpierse/wav2vec2-large-xlsr-53-esperanto\\\\')\\\\nspeech_array, sampling_rate = torchaudio.load(\\\\'path_to_the_speech_recording.wav\\\\')\\\\nresampler = torchaudio.transforms.Resample(sampling_rate, 16000)\\\\nresampled_audio = resampler(speech_array)\\\\ninput_values = processor(resampled_audio.squeeze().numpy(), sampling_rate=16000, return_tensors=\\\"pt\\\", padding=True)\\\\nlogits = model(input_values.input_values, attention_mask=input_values.attention_mask).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\npredicted_text = processor.batch_decode(predicted_ids)\\\\nprint(\\\"Predicted text:\\\", predicted_text)', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('cpierse/wav2vec2-large-xlsr-53-esperanto')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cpierse/wav2vec2-large-xlsr-53-esperanto', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('cpierse/wav2vec2-large-xlsr-53-esperanto')\\\", 'api_arguments': ['path', 'sentence', 'speech', 'sampling_rate'], 'python_environment_requirements': [['torch', 'torchaudio', 'datasets', 'transformers', 're', 'jiwer']], 'example_code': 'import torch\\\\nimport torchaudio\\\\nfrom datasets import load_dataset\\\\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\\\ntest_dataset = load_dataset(common_voice, eo, split=test[:2%]) \\\\nprocessor = Wav2Vec2Processor.from_pretrained(cpierse/wav2vec2-large-xlsr-53-esperanto) \\\\nmodel = Wav2Vec2ForCTC.from_pretrained(cpierse/wav2vec2-large-xlsr-53-esperanto) \\\\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\\\\ndef speech_file_to_array_fn(batch):\\\\n speech_array, sampling_rate = torchaudio.load(batch[path])\\\\n batch[speech] = resampler(speech_array).squeeze().numpy()\\\\n return batch\\\\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\\\\ninputs = processor(test_dataset[speech][:2], sampling_rate=16_000, return_tensors=pt, padding=True)\\\\nwith torch.no_grad():\\\\n logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\nprint(Prediction:, processor.batch_decode(predicted_ids))\\\\nprint(Reference:, test_dataset[sentence][:2])', 'performance': {'dataset': 'common_voice', 'accuracy': '12.31%'}, 'description': 'Fine-tuned facebook/wav2vec2-large-xlsr-53 on esperanto using the Common Voice dataset. When using this model, make sure that your speech input is sampled at 16kHz.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتعلم كيفية تصنيف الصوتيات وفهمها باستخدام الذكاء الاصطناعي في مجال تحويل النصوص إلى صوت. هل يمكنك توجيهي على كيفية البدء في هذا المجال؟? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"WhisperModel('large-v2')\\\", '<<<api_provider>>>': 'CTranslate2', '<<<explanation>>>': \\\"1. Import the WhisperModel class from the faster_whisper library.\\\\n2. Use the WhisperModel function to create the Automatic Speech Recognition (ASR) model by specifying the version as 'large-v2'.\\\\n3. Now, you can use the created ASR model 'large-v2' to transcribe the audio file by calling the transcribe method with the audio file path as the argument.\\\\n4. The transcribe method will return the spoken words as text, along with their start and end times in the audio file.\\\\n5. You can iterate through the segments and print their start and end times along with the corresponding transcribed text.\\\", '<<<code>>>': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(\\\\'large-v2\\\\')\\\\nsegments, info = model.transcribe(\\\\'audio.mp3\\\\')\\\\nfor segment in segments:\\\\n    print(\\\"[%.2fs -> %.2fs] %s\\\" % (segment.start, segment.end, segment.text))', 'api_call': \\\"WhisperModel('large-v2')\\\", 'provider': 'CTranslate2', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': \\\"WhisperModel('large-v2')\\\", 'api_arguments': ['audio.mp3'], 'python_environment_requirements': [['faster_whisper']], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.' </s>\"\n"}
{"text": "<s>[INST]قِم بتحليل الميزات المتعددة في البيانات المدخلة واستخراجها لاستخدامها في النمذجة العميقة والتصنيف.? [/INST]'<<<domain>>>': 'Multimodal Feature Extraction', '<<<api_call>>>': \\\"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the required ASTModel class from the appropriate transformers library/module.\\\\n2. To initialize the Audio Spectrogram Transformer (AST) model, call the from_pretrained method by providing the specified model as input: 'MIT/ast-finetuned-audioset-10-10-0.4593'.\\\\n3. After the AST model is initialized, you can use it to extract features from audio signals by feeding the audio data into the model. The output will be an audio spectrogram representation of the input data, which can be further processed for tasks like recommendation systems.\\\", '<<<code>>>': \\\"from transformers import ASTModel\\\\nast_model = ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\\\", 'api_call': \\\"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'audio-spectrogram-transformer', 'api_call': \\\"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد نص باللغة الطبيعية يصف مشهدًا يصور الحيوانات في البرية باستخدام تقنيات معالجة اللغة. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers library, which include AutoModelForCausalLM, AutoTokenizer, and set_seed. Additionally, import torch for handling tensor computations.\\\\n2. We then load the pre-trained model 'facebook/opt-66b' using the AutoModelForCausalLM class.\\\\n3. To create a tokenizer, we use the from_pretrained method from the AutoTokenizer class.\\\\n4. We set a prompt 'the adventurous journey of a fearless rabbit' to guide the text generation process.\\\\n5. We create input_ids by processing the prompt using the tokenizer and converting it to input tensors.\\\\n6. We use the set_seed function to set a random seed for reproducible results.\\\\n7. The generated text is obtained by calling the generate method from the model and specifying parameters like do_sample, num_return_sequences, and max_length.\\\\n8. After generating the text, we decode it using the tokenizer's batch_decode method to obtain the children's book introduction.\\\", '<<<code>>>': 'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'facebook/opt-66b\\\\', torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/opt-66b\\\\', use_fast=False)\\\\nprompt = \\\"the adventurous journey of a fearless rabbit\\\"\\\\ninput_ids = tokenizer(prompt, return_tensors=\\\\'pt\\\\').input_ids.cuda()\\\\nset_seed(32)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=50)\\\\nintroduction = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/opt-66b', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", 'api_arguments': ['input_ids', 'do_sample', 'num_return_sequences', 'max_length'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-66b, torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-66b, use_fast=False)\\\\nprompt = Hello, I am conscious and\\\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\\\nset_seed(32)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\\\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)', 'performance': {'dataset': 'GPT-3', 'accuracy': 'roughly matched'}, 'description': 'OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. OPT models are trained to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.' </s>\"\n"}
{"text": "<s>[INST]يجب على المبرمج تطوير نظام يقوم بتصنيف البيانات الجدولية باستخدام تقنيات معالجة اللغة الطبيعية بهدف تحسين فهم البيانات واستخراج المعلومات المفيدة. وعليه أن يضمن أن عملية التصنيف تتناسب مع احتياجات وتفضيلات مستخدمي النظام، وتوفر لهم تجربة فعالة وسهلة للوصول إلى البيانات المهمة.? [/INST]{'domain': 'Tabular Tabular Classification', 'api_call': \\\"AutoModel.from_pretrained('harithapliyal/autotrain-tatanic-survival-51030121311')\\\", 'api_provider': 'Hugging Face', 'explanation': '1. Import necessary libraries such as joblib and pandas.', 'code': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face', 'functionality': 'Binary Classification', 'api_name': 'harithapliyal/autotrain-tatanic-survival-51030121311', 'api_call': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_arguments': ['data'], 'python_environment_requirements': ['joblib', 'pandas'], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'harithapliyal/autotrain-data-tatanic-survival', 'accuracy': 0.872}, 'description': 'A tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain. The model predicts whether a passenger survived or not based on features such as age, gender, and passenger class.'}} </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى برنامج للكشف عن النشاط الصوتي في تطبيقنا لمساعدة تحسين تجربة مستخدمي الإعاقة البصرية في تقديم الإجابات على أسئلة الجداول في مجال معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers package such as AutoModelForTableQuestionAnswering, AutoTokenizer, and pipeline.\\\\n2. Load the pretrained TAPAS model 'navteca/tapas-large-finetuned-wtq' using the AutoModelForTableQuestionAnswering.from_pretrained() method.\\\\n3. Load the corresponding tokenizer for the TAPAS model using the AutoTokenizer.from_pretrained() method.\\\\n4. Create a pipeline instance using the loaded model and tokenizer for table-question-answering.\\\\n5. Use the pipeline instance with the given table data and the query 'How many products are imported from Germany?'.\\\\n6. Finally, print the result to get the answer.\\\", '<<<code>>>': \\\"from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\\\\ntapas_model = AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\\ntapas_tokenizer = AutoTokenizer.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\\nnlp = pipeline('table-question-answering', model=tapas_model, tokenizer=tapas_tokenizer)\\\\nresult = nlp({'table': {'Country': ['USA', 'Germany', 'Italy'], 'Products': ['50', '30', '25']}, 'query': 'How many products are imported from Germany?'})\\\\nprint(result)\\\", 'api_call': \\\"AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'navteca/tapas-large-finetuned-wtq', 'api_call': \\\"AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\", 'api_arguments': {'table': 'table_data', 'query': 'query'}, 'python_environment_requirements': [{'transformers': 'AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline'}], 'example_code': \\\"from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\\\\n\\\\n# Load model & tokenizer\\\\ntapas_model = AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\\ntapas_tokenizer = AutoTokenizer.from_pretrained('navteca/tapas-large-finetuned-wtq')\\\\n\\\\n# Get predictions\\\\nnlp = pipeline('table-question-answering', model=tapas_model, tokenizer=tapas_tokenizer)\\\\nresult = nlp({'table': {'Repository': ['Transformers', 'Datasets', 'Tokenizers'], 'Stars': ['36542', '4512', '3934'], 'Contributors': ['651', '77', '34'], 'Programming language': ['Python', 'Python', 'Rust, Python and NodeJS']}, 'query': 'How many stars does the transformers repository have?'})\\\\nprint(result)\\\", 'performance': {'dataset': 'wikisql', 'accuracy': 'Not provided'}, 'description': 'TAPAS large model fine-tuned on WikiTable Questions (WTQ). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table.' </s>\"\n"}
{"text": "<s>[INST]نقوم بإنشاء نموذج طبيعي لمعالجة اللغات الطبيعية من أجل توليد نصوص الى نصوص، ونرغب في توقع راتب الوظيفة استنادًا إلى مجموعات بيانات متنوعة المتاحة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-summarize-news')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the AutoModelWithLMHead and AutoTokenizer classes from the transformers library.\\\\n2. Load the pre-trained model and tokenizer 'mrm8488/t5-base-finetuned-summarize-news'.\\\\n3. Define a function summarize that takes a text and generates its summary of the specified max_length (default is 150 words).\\\\n4. The function tokenizes the input text using the tokenizer and encodes it into input_ids tensor.\\\\n5. The input_ids tensor is then fed to the model to generate summaries with specified parameters like num_beams, max_length, repetition_penalty, etc.\\\\n6. Decode the generated summaries into human-readable text format and return the summary.\\\", '<<<code>>>': 'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'mrm8488/t5-base-finetuned-summarize-news\\\\')\\\\nmodel = AutoModelWithLMHead.from_pretrained(\\\\'mrm8488/t5-base-finetuned-summarize-news\\\\')\\\\n\\\\ndef summarize(text, max_length=150):\\\\n    input_ids = tokenizer.encode(text, return_tensors=\\\\'pt\\\\', add_special_tokens=True)\\\\n    generated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\n    preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\n    return preds[0]\\\\n\\\\nsummary = summarize(\\\"<News Article>\\\", max_length=150)', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-summarize-news')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mrm8488/t5-base-finetuned-summarize-news', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-summarize-news')\\\", 'api_arguments': ['text', 'max_length'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\ndef summarize(text, max_length=150):\\\\n input_ids = tokenizer.encode(text, return_tensors=pt, add_special_tokens=True)\\\\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\nreturn preds[0]', 'performance': {'dataset': 'News Summary', 'accuracy': 'Not provided'}, 'description': \\\"Google's T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017.\\\" </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لتقدير مدى تشابه الجمل في معالجة اللغة الطبيعية للأسئلة والإجابات، باستخدام تقنيات تعلم الآلة مثل الشبكات العصبونية العميقة.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"pipeline('question-answering', model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'), tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'))\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline, AutoModel, AutoTokenizer\\\\nqa_pipeline = pipeline('question-answering', model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'), tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'))\\\\nQA_input = {'question': 'Example question', 'context': 'Example context'}\\\\nanswer = qa_pipeline(QA_input)\\\", 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Question Answering', 'api_name': 'bert-large-uncased-whole-word-masking-squad2', 'api_call': \\\"from transformers import pipeline, AutoModel, AutoTokenizer\\\\nqa_pipeline = pipeline('question-answering', model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'), tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'))\\\\nQA_input = {'question': 'Example question', 'context': 'Example context'}\\\\nanswer = qa_pipeline(QA_input)\\\", 'api_arguments': {'model_name': 'deepset/bert-large-uncased-whole-word-masking-squad2', 'tokenizer': 'deepset/bert-large-uncased-whole-word-masking-squad2'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"QA_input = {\\\\n 'question': 'Why is model conversion important?',\\\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\\\n}\\\\nres = nlp(QA_input)\\\", 'performance': {'dataset': 'squad_v2', 'accuracy': {'Exact Match': 80.885, 'F1': 83.876}}, 'description': 'This is a bert-large model, fine-tuned using the SQuAD2.0 dataset for the task of question answering. It is designed for extractive question answering and supports English language.'}} </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير تقنيات تصوير العمق في مجال رؤية الحاسوب، ونطلب تقدير عمق الأشياء في الصور التي يلتقطها النظام.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the necessary classes MBartForConditionalGeneration and MBart50TokenizerFast from the transformers library.', 'code': 'from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\\\\nmodel = MBartForConditionalGeneration.from_pretrained(\\\\'facebook/mbart-large-50\\\\')\\\\ntokenizer = MBart50TokenizerFast.from_pretrained(\\\\'facebook/mbart-large-50\\\\', src_lang=\\\\'en_XX\\\\', tgt_lang=\\\\'ro_RO\\\\')\\\\ninput_text = \\\"Your father\\\\'s biography in English\\\"\\\\ninput_tokens = tokenizer(input_text, return_tensors=\\\\'pt\\\\')\\\\ngenerated_output = model.generate(**input_tokens)\\\\ntranslated_text = tokenizer.batch_decode(generated_output, skip_special_tokens=True)[0]', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Multilingual Sequence-to-Sequence', 'api_name': 'facebook/mbart-large-50', 'api_call': 'from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\\\\nmodel = MBartForConditionalGeneration.from_pretrained(\\\\'facebook/mbart-large-50\\\\')\\\\ntokenizer = MBart50TokenizerFast.from_pretrained(\\\\'facebook/mbart-large-50\\\\', src_lang=\\\\'en_XX\\\\', tgt_lang=\\\\'ro_RO\\\\')\\\\ninput_text = \\\"Your father\\\\'s biography in English\\\"\\\\ninput_tokens = tokenizer(input_text, return_tensors=\\\\'pt\\\\')\\\\ngenerated_output = model.generate(**input_tokens)\\\\ntranslated_text = tokenizer.batch_decode(generated_output, skip_special_tokens=True)[0]', 'api_arguments': {'src_lang': 'en_XX', 'tgt_lang': 'ro_RO'}, 'python_environment_requirements': {'transformers': 'MBartForConditionalGeneration, MBart50TokenizerFast'}, 'example_code': \\\"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\\\\nmodel = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')\\\\ntokenizer = MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50', src_lang='en_XX', tgt_lang='ro_RO')\\\\nsrc_text = ' UN Chief Says There Is No Military Solution in Syria'\\\\ntgt_text = 'Şeful ONU declară că nu există o soluţie militară în Siria'\\\\nmodel_inputs = tokenizer(src_text, return_tensors='pt')\\\\nwith tokenizer.as_target_tokenizer():\\\\n labels = tokenizer(tgt_text, return_tensors='pt').input_ids\\\\nmodel(**model_inputs, labels=labels)\\\", 'performance': {'dataset': 'Multilingual Denoising Pretraining', 'accuracy': 'Not specified'}, 'description': \\\"mBART-50 is a multilingual Sequence-to-Sequence model pre-trained using the 'Multilingual Denoising Pretraining' objective. It was introduced in Multilingual Translation with Extensible Multilingual Pretraining and Finetuning paper.\\\"}} </s>\"\n"}
{"text": "<s>[INST]أكتب عنوانًا جذابًا لمقالة حول تقنية الإجابة على الأسئلة في معالجة اللغات الطبيعية ودورها في فهم واستجابة النصوص.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the pipeline function from the transformers library provided by Hugging Face to ensure quick and efficient integration of their models.\\\\n2. We create a question-answering pipeline using the pre-trained model 'mrm8488/bert-multi-cased-finetuned-xquadv1'. This model has been fine-tuned on the XQuAD dataset and supports answering in multiple languages.\\\\n3. Given a context and question in any of the supported languages, you can use the question-answering pipeline to generate relevant answers.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\\nresult = qa_pipeline({'context': 'Manuel Romero heeft de laatste tijd hard gewerkt aan de repository hugginface/transformers.', 'question': 'Wie heeft er de laatste tijd hard gewerkt voor hugginface/transformers?'})\\\", 'api_call': \\\"pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mrm8488/bert-multi-cased-finetuned-xquadv1', 'api_call': \\\"pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\", 'api_arguments': {'context': 'string', 'question': 'string'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"qa_pipeline({\\\\n 'context': Manuel Romero has been working hardly in the repository hugginface/transformers lately,\\\\n 'question': Who has been working hard for hugginface/transformers lately?\\\\n})\\\", 'performance': {'dataset': 'XQuAD', 'accuracy': 'Not provided'}, 'description': 'This model is a BERT (base-multilingual-cased) fine-tuned for multilingual Question Answering on 11 different languages using the XQuAD dataset and additional data augmentation techniques.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, in this case, SentenceTransformer and util from sentence_transformers.\\\\n2. Initialize the SentenceTransformer model using the 'sentence-transformers/multi-qa-mpnet-base-cos-v1' model.\\\\n3. Given a specific question (query) and a list of paragraphs or segments (docs), the model can be used to encode the query and docs into high-dimensional representations.\\\\n4. Use the util.dot_score() function to compute the cosine similarity between the query and docs embeddings, and pair the obtained scores with their respective segments.\\\\n5. Sort the pairs in descending order based on the similarity scores, and retrieve the segment with the highest score as the most relevant segment.\\\", '<<<code>>>': \\\"from sentence_transformers import SentenceTransformer, util\\\\nquery = 'Information on company growth...'\\\\ndocs = [\\\\n  'A section about economic policies...',\\\\n  'A section about company growth...',\\\\n  'A section about technology innovations...'\\\\n]\\\\nmodel = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\\\\nquery_emb = model.encode(query)\\\\ndoc_emb = model.encode(docs)\\\\nscores = util.dot_score(query_emb, doc_emb)\\\\ndoc_score_pairs = list(zip(docs, scores))\\\\ndoc_score_pairs.sort(key=lambda x: x[1], reverse=True)\\\\nmost_relevant_segment = doc_score_pairs[0][0]\\\", 'api_call': \\\"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/multi-qa-mpnet-base-cos-v1', 'api_call': \\\"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\\\", 'api_arguments': {'query': 'string', 'docs': 'list of strings'}, 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer, util\\\\nquery = How many people live in London?\\\\ndocs = [Around 9 Million people live in London, London is known for its financial district]\\\\nmodel = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\\\\nquery_emb = model.encode(query)\\\\ndoc_emb = model.encode(docs)\\\\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\\\\ndoc_score_pairs = list(zip(docs, scores))\\\\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\\\\nfor doc, score in doc_score_pairs:\\\\n print(score, doc)\\\", 'performance': {'dataset': '215M (question, answer) pairs from diverse sources', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model that maps sentences and paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources.' </s>\"\n"}
{"text": "<s>[INST]يرجى تطوير برنامج يستخدم تقنيات الرؤية الحاسوبية لتصنيف الصور بدون تدريب مسبق لذوي الإعاقة البصرية.? [/INST]{'domain': 'Computer Vision Zero-Shot Image Classification', 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg')\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\nimage_classification = pipeline('image-classification', model='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg')\\\\nimage_path = 'path/to/image.jpg'\\\\nclass_names = 'dog, cat, bird'\\\\nresult = image_classification(image_path, class_names)\\\\nprint(result)\\\", 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg', 'api_call': \\\"from transformers import pipeline\\\\nimage_classification = pipeline('image-classification', model='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg')\\\\nimage_path = 'path/to/image.jpg'\\\\nclass_names = 'dog, cat, bird'\\\\nresult = image_classification(image_path, class_names)\\\\nprint(result)\\\", 'api_arguments': {'image_path': 'Path to the image', 'class_names': 'Comma-separated list of possible class names'}, 'python_environment_requirements': 'transformers', 'example_code': \\\"from transformers import pipeline\\\\nimage_classification = pipeline('image-classification', model='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg')\\\\nimage_path = 'path/to/image.jpg'\\\\nclass_names = 'dog, cat'\\\\nresult = image_classification(image_path, class_names)\\\\nprint(result)\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '70.8-71.7%'}, 'description': 'A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Base model (convnext_base) as the image tower, and the same text tower as the RN50x4 (depth 12, embed dim 640) model from OpenAI CLIP.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج لتحسين التفاعل بين الإنسان والحاسوب باستخدام تقنيات معالجة اللغات الطبيعية. يجب عليك تدريب النموذج على مجموعة البيانات المعينة واختباره لضمان تحسين جودة التفاعل بشكل صحيح.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. First, we import AutoModelForCausalLM and AutoTokenizer from the transformers library for generating responses and tokenizing text respectively.\\\\n2. We then initialize an instance of the tokenizer and model using the specified pre-trained model, 'microsoft/DialoGPT-small', which is designed for dialogue response generation.\\\\n3. For each conversation turn, we encode the user's input text into token ids, append the conversation history, and generate a response by passing the input ids to the model.\\\\n4. The generated response is decoded back into human-readable text and presented as the chatbot's reply.\\\\n5. This process is iterated for multiple conversation turns, allowing the chatbot to engage in a multi-turn conversation about hospital issues.\\\", '<<<code>>>': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'microsoft/DialoGPT-small\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'microsoft/DialoGPT-small\\\\')\\\\nchat_history_ids = None\\\\n\\\\nfor step in range(5):\\\\n    user_input = input(\\\"User: \\\")\\\\n    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\\\\'pt\\\\')\\\\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\\\\n    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n    reply = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\\\\n    print(f\\\"DialoGPT: {reply}\\\")', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/DialoGPT-small', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')\\\", 'api_arguments': ['bot_input_ids', 'max_length', 'pad_token_id'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-small)\\\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-small)\\\\nfor step in range(5):\\\\n new_user_input_ids = tokenizer.encode(input(>> User:) + tokenizer.eos_token, return_tensors='pt')\\\\n bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\\\\n chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n print(DialoGPT: {}.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\\\", 'performance': {'dataset': 'Reddit discussion thread', 'accuracy': 'Comparable to human response quality under a single-turn conversation Turing test'}, 'description': 'DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multiturn conversations. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.' </s>\"\n"}
{"text": "<s>[INST]نحتاج عميلنا في مجال رؤية الحاسوب وكشف الأجسام إلى تطوير تطبيق لذوي الإعاقة البصرية. يجب علينا توفير برنامج قادر على تحويل النص إلى كلام بشكل فعال لمساعدة مستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8n-blood-cell-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. In order to detect blood cells in microscope images, we can use the 'keremberke/yolov8n-blood-cell-detection' model from the Hugging Face Model Hub. This model is trained on blood cell object detection and based on the YOLOv8 architecture.\\\\n2. Import the necessary classes from the ultralyticsplus library.\\\\n3. Instantiate the YOLO object for blood cell detection using the provided model name.\\\\n4. Configure the model's parameters, such as confidence threshold, Intersection over Union (IoU) threshold, and maximum detections.\\\\n5. Use the model's predict method to detect blood cells in an input image.\\\\n6. Utilize the render_result function to visualize the detected blood cells on the image.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-blood-cell-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nblood_cell_image = 'https://your_microscope_image_url_here.jpg'\\\\nresults = model.predict(blood_cell_image)\\\\nrender = render_result(model=model, image=blood_cell_image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8n-blood-cell-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8n-blood-cell-detection', 'api_call': \\\"YOLO('keremberke/yolov8n-blood-cell-detection')\\\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-blood-cell-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.893}, 'description': 'This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.' </s>\"\n"}
{"text": "<s>[INST]نقدم لكم تطبيقًا مبتكرًا للتعامل مع الملفات الصوتية، يتيح للمستخدمين تحويل الصوت إلى نص والنص إلى صوت بسهولة، مع إمكانية تحسين دقة التعرف على الصوت وفهم المحتوى الصوتي بشكل أوسع وأعمق.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"separator.from_hparams(source='speechbrain/sepformer-whamr-enhancement', savedir='pretrained_models/sepformer-whamr-enhancement')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We'll begin by importing the SepformerSeparation class from the speechbrain pre-trained package, which we'll utilize to separate the desired signals from unwanted noises.\\\\n2. Next, we'll load the pre-trained SepFormer model by invoking the from_hparams method on the SepformerSeparation class, with the model identifier 'speechbrain/sepformer-whamr-enhancement'.\\\\n3. Then, we'll use the separate_file() method of the loaded model to process the input audio file and extract the estimated sources. The result will be an enhanced audio file without the unwanted background noise and reverberation.\\\\n4. Finally, save the enhanced audio file using torchaudio, which can be used for the upgraded conference call system.\\\", '<<<code>>>': \\\"from speechbrain.pretrained import SepformerSeparation as separator\\\\nimport torchaudio\\\\nmodel = separator.from_hparams(source='speechbrain/sepformer-whamr-enhancement', savedir='pretrained_models/sepformer-whamr-enhancement')\\\\nest_sources = model.separate_file(path='input_audio_file.wav')\\\\n# replace 'input_audio_file.wav' with path to your audio file\\\\ntorchaudio.save('enhanced_audio_file.wav', est_sources[:, :, 0].detach().cpu(), 8000)\\\", 'api_call': \\\"separator.from_hparams(source='speechbrain/sepformer-whamr-enhancement', savedir='pretrained_models/sepformer-whamr-enhancement')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'speechbrain/sepformer-whamr-enhancement', 'api_call': \\\"separator.from_hparams(source='speechbrain/sepformer-whamr-enhancement', savedir='pretrained_models/sepformer-whamr-enhancement')\\\", 'api_arguments': {'path': 'Path to the input audio file.'}, 'python_environment_requirements': ['pip install speechbrain'], 'example_code': \\\"from speechbrain.pretrained import SepformerSeparation as separator\\\\nimport torchaudio\\\\nmodel = separator.from_hparams(source=speechbrain/sepformer-whamr-enhancement, savedir='pretrained_models/sepformer-whamr-enhancement')\\\\nest_sources = model.separate_file(path='speechbrain/sepformer-whamr-enhancement/example_whamr.wav')\\\\ntorchaudio.save(enhanced_whamr.wav, est_sources[:, :, 0].detach().cpu(), 8000)\\\", 'performance': {'dataset': 'WHAMR!', 'accuracy': '10.59 dB SI-SNR'}, 'description': 'This repository provides all the necessary tools to perform speech enhancement (denoising + dereverberation) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAMR! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k.' </s>\"\n"}
{"text": "<s>[INST]العميل يطوّر تطبيقًا لذوي الإعاقة البصرية. يجب تطوير برنامج يستطيع تحويل النص إلى كلام تلقائيًا لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-russian')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the SpeechRecognitionModel from the huggingsound package, which is a wrapper around Hugging Face Transformers.\\\\n2. Instantiate the model using the pretrained model 'jonatasgrosman/wav2vec2-large-xlsr-53-russian'. This model is specifically tailored for transcribing Russian speech.\\\\n3. You can then use the model to transcribe audio files containing voice messages in Russian. Pass a list of audio file paths to the transcribe() method to get the transcriptions for each file.\\\", '<<<code>>>': \\\"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-russian')\\\\naudio_paths = ['/path/to/russian_voice_message.mp3', '/path/to/another_russian_voice_message.wav']\\\\n# Replace '/path/to/...' with the actual paths to your audio files\\\\ntranscriptions = model.transcribe(audio_paths)\\\", 'api_call': \\\"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-russian')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'jonatasgrosman/wav2vec2-large-xlsr-53-russian', 'api_call': \\\"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-russian')\\\", 'api_arguments': ['audio_paths'], 'python_environment_requirements': [['huggingsound', 'torch', 'librosa', 'datasets', 'transformers']], 'example_code': \\\"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-russian')\\\\naudio_paths = ['/path/to/file.mp3', '/path/to/another_file.wav']\\\\ntranscriptions = model.transcribe(audio_paths)\\\", 'performance': {'dataset': 'mozilla-foundation/common_voice_6_0', 'accuracy': {'Test WER': 13.3, 'Test CER': 2.88, 'Test WER (+LM)': 9.57, 'Test CER (+LM)': 2.24}}, 'description': 'Fine-tuned XLSR-53 large model for speech recognition in Russian. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Russian using the train and validation splits of Common Voice 6.1 and CSS10.' </s>\"\n"}
{"text": "<s>[INST]عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى برنامج يقوم بتحويل النص إلى كلام وصور لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1', torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the necessary classes from the diffusers library, including StableDiffusionPipeline and DPMSolverMultistepScheduler.\\\\n2. Load the pre-trained model by calling the from_pretrained method of the StableDiffusionPipeline class and providing the model name \\\\'stabilityai/stable-diffusion-2-1\\\\'. Set the torch_dtype to torch.float16 for faster generation with a slight trade-off in image quality.\\\\n3. Set the scheduler for the pipeline using the DPMSolverMultistepScheduler class that comes with the diffusers package.\\\\n4. Create your text prompt describing the required image, i.e., \\\"a futuristic red dress design\\\".\\\\n5. Use the pipeline with the text prompt to generate the image.\\\\n6. Save the generated image to a file for review and inspiration.', '<<<code>>>': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\\\npipe = StableDiffusionPipeline.from_pretrained(\\\\'stabilityai/stable-diffusion-2-1\\\\', torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to(\\\\'cuda\\\\')\\\\nprompt = \\\"a futuristic red dress design\\\"\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(\\\\'futuristic_red_dress.png\\\\')', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1', torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1', torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors']], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2-1\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>\"\n"}
{"text": "<s>[INST]عملاؤنا في مجال تعزيز التعلم يرغبون في تطوير نماذج تعلم آلي لتحسين أداء الروبوتات في البيئات غير المتوقعة. نحن بحاجة إلى تصميم برنامج لتدريب الروبوتات على اتخاذ القرارات المثلى بناءً على التجارب السابقة.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"load_from_hub(repo_id='sb3/ppo-BreakoutNoFrameskip-v4',filename='{MODEL FILENAME}.zip',)\\\", '<<<api_provider>>>': 'Stable-Baselines3', '<<<explanation>>>': '1. First, you need to install the required packages, which are rl_zoo3, stable-baselines3, and stable-baselines3-contrib.\\\\n2. Using the load_from_hub function, we can download the pre-trained PPO agent that has been trained to play the specific game of Breakout without any frames being skipped.\\\\n3. Once the pre-trained agent is loaded, you can run evaluation or enjoy scripts provided by the rl_zoo to evaluate the performance of this trained agent, or visualize the agent playing the game interactively.\\\\n4. This PPO agent has been trained on the BreakoutNoFrameskip-v4 environment and achieves an average score of 398.00 +/- 16.30.', '<<<code>>>': \\\"from rl_zoo3 import load_from_hub\\\\n agent = load_from_hub(repo_id='sb3/ppo-BreakoutNoFrameskip-v4', filename='{MODEL FILENAME}.zip')\\\\n # Replace '{MODEL FILENAME}.zip' with your desired filepath and filename for saving the agent model\\\", 'api_call': \\\"load_from_hub(repo_id='sb3/ppo-BreakoutNoFrameskip-v4',filename='{MODEL FILENAME}.zip',)\\\", 'provider': 'Stable-Baselines3', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ppo-BreakoutNoFrameskip-v4', 'api_call': \\\"load_from_hub(repo_id='sb3/ppo-BreakoutNoFrameskip-v4',filename='{MODEL FILENAME}.zip',)\\\", 'api_arguments': ['algo', 'env', 'orga', 'f'], 'python_environment_requirements': [['rl_zoo3', 'stable-baselines3', 'stable-baselines3-contrib']], 'example_code': ['python -m rl_zoo3.load_from_hub --algo ppo --env BreakoutNoFrameskip-v4 -orga sb3 -f logs/', 'python enjoy.py --algo ppo --env BreakoutNoFrameskip-v4 -f logs/', 'python train.py --algo ppo --env BreakoutNoFrameskip-v4 -f logs/', 'python -m rl_zoo3.push_to_hub --algo ppo --env BreakoutNoFrameskip-v4 -f logs/ -orga sb3'], 'performance': {'dataset': 'BreakoutNoFrameskip-v4', 'accuracy': '398.00 +/- 16.30'}, 'description': 'This is a trained model of a PPO agent playing BreakoutNoFrameskip-v4 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.' </s>\"\n"}
{"text": "<s>[INST]يمكنك استخدام هذه الأداة لتحويل الملفات الصوتية إلى نص بتقنية التعرف الآلي على الكلام. قم بتحميل الملف الصوتي الذي ترغب في تحويله وانتظر بضع ثوانٍ لرؤية النتيجة.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\", 'api_provider': 'PyTorch Transformers', 'explanation': '', 'code': \\\"from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/data2vec-audio-base-960h')\\\\nmodel = Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\\ninput_values = processor(political_interview_audio, return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'PyTorch Transformers', 'functionality': 'Automatic Speech Recognition', 'api_name': 'data2vec-audio-base-960h', 'api_call': \\\"from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/data2vec-audio-base-960h')\\\\nmodel = Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\\ninput_values = processor(political_interview_audio, return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers', 'datasets', 'torch', 'jiwer'], 'example_code': 'from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\n# load model and processor\\\\nprocessor = Wav2Vec2Processor.from_pretrained(facebook/data2vec-audio-base-960h)\\\\nmodel = Data2VecForCTC.from_pretrained(facebook/data2vec-audio-base-960h)\\\\n\\\\n# load dummy dataset and read soundfiles\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\n\\\\n# tokenize\\\\ninput_values = processor(ds[0][audio][array],, return_tensors=pt, padding=longest).input_values # Batch size 1\\\\n\\\\n# retrieve logits\\\\nlogits = model(input_values).logits\\\\n\\\\n# take argmax and decode\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'performance': {'dataset': 'librispeech_asr', 'accuracy': {'clean': 2.77, 'other': 7.08}}, 'description': \\\"Facebook's Data2Vec-Audio-Base-960h model is an Automatic Speech Recognition model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It can be used for transcribing audio files and achieves competitive performance on major benchmarks of speech recognition. The model is based on the Data2Vec framework which uses the same learning method for either speech, NLP, or computer vision.\\\"}} </s>\"\n"}
{"text": "<s>[INST]خلال تدريب التعلم المرئي والسؤال والإجابة متعدد الوسائط، استخدم النمط البصري والنصي لمعالجة البيانات. قم بتصميم نماذج لاستجابة الأسئلة المتعلقة بالصور والفيديوهات بشكل دقيق. قم بتدريب النماذج باستخدام مجموعة متنوعة من البيانات لتحسين أداء النموذج على مختلف الأسئلة والتحديات.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from transformers package, which includes ViltForQuestionAnswering for the visual question answering model.\\\\n2. Load the pre-trained model 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering' using the from_pretrained method of the ViltForQuestionAnswering class.\\\\n3. To use the model, ensure you have an image file and a question related to that image.\\\\n4. With the image processed and the question, you can now use the model to answer the question based on the given image.\\\\n5. Note that the performance of the model may strongly depend on the dataset it was trained on.\\\", '<<<code>>>': \\\"from transformers import ViltForQuestionAnswering\\\\nmodel = ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\\\\nimage_path = 'path/to/image/file'\\\\nquestion = 'your_question'\\\\n# Complete the remaining code to process the image and use the model for question answering.\\\", 'api_call': \\\"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering', 'api_call': \\\"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\\\", 'api_arguments': {'image': 'path/to/image/file', 'question': 'your_question'}, 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random model for Visual Question Answering using the VILT framework.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لاستخراج الملامح من النصوص باستخدام تقنيات معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Feature Extraction', '<<<api_call>>>': \\\"AutoModel.from_pretrained('malteos/scincl')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers library: AutoTokenizer and AutoModel.\\\\n2. To load the model 'malteos/scincl', use the from_pretrained method of AutoModel and AutoTokenizer.\\\\n3. With this trained BERT language model, create a function to generate document-level embeddings for research papers, based on their title and abstract.\\\\n4. These embeddings can then be used as features for the machine learning algorithm to recommend research papers based on their similarity.\\\", '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModel\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\\\n\\\\ndef compute_paper_embeddings(papers):\\\\n    title_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\\\n    inputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors='pt', max_length=512)\\\\n    result = model(**inputs)\\\\n    embeddings = result.last_hidden_state[:, 0, :]\\\\n    return embeddings\\\\n\\\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'}, {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\\\nembeddings = compute_paper_embeddings(papers)\\\", 'api_call': \\\"AutoModel.from_pretrained('malteos/scincl')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'malteos/scincl', 'api_call': \\\"AutoModel.from_pretrained('malteos/scincl')\\\", 'api_arguments': {'tokenizer': \\\"AutoTokenizer.from_pretrained('malteos/scincl')\\\", 'model': \\\"AutoModel.from_pretrained('malteos/scincl')\\\"}, 'python_environment_requirements': [{'transformers': '4.13.0'}], 'example_code': \\\"from transformers import AutoTokenizer, AutoModel\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\\\n\\\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\\\n\\\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\\\n\\\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\\\n\\\\nresult = model(**inputs)\\\\n\\\\nembeddings = result.last_hidden_state[:, 0, :]\\\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>\"\n"}
{"text": "<s>[INST]سيتم التجمع في مختبر الرؤية الحاسوبية حيث سيتعين عليكم تصنيف الصور المقدمة باستخدام نظام الذكاء الاصطناعي. ضعوا خطة للتصنيف الدقيق وتحسين النتائج باستخدام خوارزميات متقدمة وتقنيات تعلم الآلة.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import necessary libraries: ConvNextFeatureExtractor and ConvNextForImageClassification from transformers, torch, and datasets.\\\\n2. Load a dataset containing mobile phone images, such as the 'huggingface/cats-image' dataset used in the example code (replace with a mobile phone images dataset).\\\\n3. Initialize the feature extractor and the ConvNext model for image classification with 'facebook/convnext-base-224' model.\\\\n4. For each image in the dataset, use the feature extractor to convert the image into a tensor, and then forward the tensor through the model to predict the image's category.\\\\n5. Based on the predicted category of the user's favorite images, the recommendation engine will provide personalized discounts for mobile phone products in those categories.\\\", '<<<code>>>': \\\"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset('mobile_phone_images_dataset_name_here')\\\\nimage = dataset['test']['image'][0]\\\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\\\ninputs = feature_extractor(image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/convnext-base-224', 'api_call': \\\"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': [['transformers', 'torch', 'datasets']], 'example_code': \\\"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset('huggingface/cats-image')\\\\nimage = dataset['test']['image'][0]\\\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\\\ninputs = feature_extractor(image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': \\\"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\\\" </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطبيق تقنية الرؤية الحاسوبية على تصنيف الصور بدون تصنيف مسبق، يجب إصدار تعليمات متوافقة مع هذا النطاق. هل يمكنك مساعدتي؟? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary library 'pipeline' from the transformers package provided by Hugging Face.\\\\n2. Use the pipeline function to create an image classification model.\\\\n3. Specify the model 'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft' to be loaded. This model is trained for zero-shot image classification tasks, which makes it perfect for classifying everyday objects in photos.\\\\n4. The classifier takes in the input image and a list of class names. It then ranks the class names based on their likelihood of representing the object present in the image.\\\\n5. Based on the classification results, the app can display the name or information about the object present in the image.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nobject_classifier = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\\\\nimage_path = 'path/to/image.jpg'\\\\nclass_names = ['apple', 'banana', 'car', 'dog', 'building'] # list of possible class names\\\\nclassification_results = object_classifier(image_path, class_names)\\\", 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft', 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\\\", 'api_arguments': {'image_path': 'Path to the image file', 'class_names': 'List of comma-separated class names'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft'); classifier('path/to/image.jpg', ['class1', 'class2'])\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '75.9-76.9%'}, 'description': 'A series of CLIP ConvNeXt-Large models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models achieve between 75.9 and 76.9 top-1 zero-shot accuracy on ImageNet-1k.' </s>\"\n"}
{"text": "<s>[INST]نحن بحاجة لتوليد تعليمة تسأل عن كيفية إنشاء نموذج لتوليد النصوص وفقًا لمواضيع معينة في تكنولوجيا معالجة اللغات الطبيعية. هل يمكنك تقديم إرشادات حول كيفية تطوير نموذج يقوم بتوليد النصوص والنصوص العنصرية بواسطة بيانات تدريبية محددة؟? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the necessary classes from the transformers package. This includes T5Tokenizer and T5ForConditionalGeneration.', 'code': \\\"from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\\\\nmodel = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\\\\ninput_text = 'translate English to German: How old are you?'\\\\ninput_ids = tokenizer(input_text, return_tensors='pt').input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))\\\", 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Language model', 'api_name': 'google/flan-t5-base', 'api_call': \\\"from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\\\\nmodel = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\\\\ninput_text = 'translate English to German: How old are you?'\\\\ninput_ids = tokenizer(input_text, return_tensors='pt').input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))\\\", 'api_arguments': 'input_text', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-base)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-base)\\\\ninput_text = translate English to German: How old are you?\\\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))', 'performance': {'dataset': [{'name': 'MMLU', 'accuracy': '75.2%'}]}, 'description': 'FLAN-T5 is a language model fine-tuned on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering.'}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء برنامج يقوم بتطوير نموذج في مجال رؤية الكمبيوتر لتوليد صور غير مشروطة بشكل تلقائي. يجب أن يكون النموذج قادرًا على إنشاء صور بدقة عالية وبناءً على البيانات المتاحة.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\\nimage = pipeline().images[0]\\\\n# Use and edit image as needed\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'sd-class-pandas-32', 'api_call': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\\nimage = pipeline().images[0]\\\\n# Use and edit image as needed\\\", 'api_arguments': {'pretrained_model': 'schdoel/sd-class-AFHQ-32'}, 'python_environment_requirements': {'package': 'diffusers', 'import': 'from diffusers import DDPMPipeline'}, 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': 'AFHQ', 'accuracy': 'Not provided'}, 'description': 'This model is a diffusion model for unconditional image generation of cute 🦋.'}} </s>\"\n"}
{"text": "<s>[INST]إعداد نموذج فصل الصورة في مجال رؤية الحاسوب? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"YOLO('keremberke/yolov8n-pothole-segmentation')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the YOLO and render_result classes from the ultralyticsplus package.', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pothole-segmentation', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.995, 'mAP@0.5(mask)': 0.995}}, 'description': 'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.'}} </s>\"\n"}
{"text": "<s>[INST]يجب عليك تصميم نموذج لمعالجة اللغة الطبيعية باستخدام تقنيات تعويض الفجوة لمعالجة الجمل غير الكاملة في النصوص.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"AutoModelForMaskedLM.from_pretrained('bert-base-chinese')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' function from the transformers library.\\\\n2. Use the 'pipeline' function to load the model 'bert-base-chinese', which is pre-trained for Chinese text and can be used for masked language modeling tasks.\\\\n3. Create a fill-mask pipeline using the loaded model.\\\\n4. Provide the incomplete English sentence with a missing word masked using the '[MASK]' token. The model will predict the best word to complete the sentence.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nfill_mask = pipeline(\\\\'fill-mask\\\\', model=\\\\'bert-base-chinese\\\\')\\\\nenglish_sentence = \\\"The cat is [MASK] on the sofa.\\\"\\\\nbest_word = fill_mask(english_sentence)[0][\\\\'token_str\\\\']', 'api_call': \\\"AutoModelForMaskedLM.from_pretrained('bert-base-chinese')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-base-chinese', 'api_call': \\\"AutoModelForMaskedLM.from_pretrained('bert-base-chinese')\\\", 'api_arguments': {'pretrained_model_name': 'bert-base-chinese'}, 'python_environment_requirements': [{'transformers': 'from transformers import AutoTokenizer, AutoModelForMaskedLM'}], 'example_code': 'tokenizer = AutoTokenizer.from_pretrained(bert-base-chinese)\\\\nmodel = AutoModelForMaskedLM.from_pretrained(bert-base-chinese)', 'performance': {'dataset': '[More Information Needed]', 'accuracy': '[More Information Needed]'}, 'description': 'This model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper). It can be used for masked language modeling.' </s>\"\n"}
{"text": "<s>[INST]يقوم الباحثون في الاتجاه الجديد للتعلم الآلي بتطوير تقنيات متعددة الأوساط لتطبيق الذكاء الاصطناعي على البيانات الجدولية بدون قيود، ونحتاج إلى القدرة الفعالة على تحليل التغيرات في البيانات الجدولية المولدة.? [/INST]'<<<domain>>>': 'Multimodal Graph Machine Learning', '<<<api_call>>>': \\\"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary classes from the transformers package. Specifically, we need the AutoModel class to load the Graphormer model.\\\\n2. We use the from_pretrained method of the AutoModel class to load the pre-trained model 'clefourrier/graphormer-base-pcqm4mv2'. This Graphormer model has been pretrained on the PCQM4M-LSCv2 dataset and is designed specifically for graph classification tasks.\\\\n3. Process the molecule data to create a graph representation that can be input into the Graphormer model.\\\\n4. The model can be used to classify molecules based on their properties using machine learning.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\\\\n# Process molecule data into a graph representation\\\\n# Then, use the model for classification based on molecular properties\\\", 'api_call': \\\"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'graphormer-base-pcqm4mv2', 'api_call': \\\"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\\\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': ['transformers'], 'example_code': 'See the Graph Classification with Transformers tutorial.', 'performance': {'dataset': 'PCQM4M-LSCv2', 'accuracy': 'Not provided'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير منتج لتحسين أداء استجابة الجداول في مجال معالجة اللغة الطبيعية، ونرغب في استخدام نماذج التعلم الآلي للإجابة على الأسئلة المتعلقة بترجمة معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries from the transformers package, which is 'pipeline' in this case.\\\\n2. Use the 'pipeline' function from the transformers library to create a translation model, and load the pre-trained model 'Helsinki-NLP/opus-mt-en-ar' for English to Arabic translation.\\\\n3. The created translator can then be used to translate any given English text into Arabic.\\\\n4. To achieve this, simply call the translator function with the English text input, and it will return the translated Arabic text.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntranslation = pipeline(\\\\'translation_en_to_ar\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-ar\\\\')\\\\nenglish_text = \\\"We are excited to work with you.\\\"\\\\ntranslated_text = translation(english_text)', 'api_call': \\\"pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-en-ar', 'api_call': \\\"pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\\ntranslated_text = translation('Hello World')\\\", 'performance': {'dataset': 'Tatoeba-test.eng.ara', 'accuracy': {'BLEU': 14.0, 'chr-F': 0.437}}, 'description': \\\"A Hugging Face Transformers model for English to Arabic translation, trained on the Tatoeba dataset. It uses a transformer architecture and requires a sentence initial language token in the form of '>>id<<' (id = valid target language ID).\\\" </s>\"\n"}
{"text": "<s>[INST]يجب تصميم نموذج توليد نصوص يعتمد على معالجة اللغة الطبيعية لتوفير تقدير دقيق لعمق المعلومات المحتواة في النصوص التي يتم توليدها.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\\\\nmodel = AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\\\\n\\\\ndef get_sentence_embeddings(sentences):\\\\n    encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\\\n    with torch.no_grad():\\\\n        model_output = model(**encoded_input)\\\\n    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\\\\n    return sentence_embeddings\\\\n\\\\nsentences = ['list', 'of', 'sentences', 'in', 'Russian']\\\\nsentence_embeddings = get_sentence_embeddings(sentences)\\\", 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \\\"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\\\\nmodel = AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\\\\n\\\\ndef get_sentence_embeddings(sentences):\\\\n    encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\\\n    with torch.no_grad():\\\\n        model_output = model(**encoded_input)\\\\n    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\\\\n    return sentence_embeddings\\\\n\\\\nsentences = ['list', 'of', 'sentences', 'in', 'Russian']\\\\nsentence_embeddings = get_sentence_embeddings(sentences)\\\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \\\"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\n\\\\n# Mean Pooling - Take attention mask into account for correct averaging\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\n\\\\n# Sentences we want sentence embeddings for sentences = ['?']\\\\n\\\\n# Load AutoModel from huggingface model repository\\\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\n\\\\n# Tokenize sentences\\\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\\\n\\\\n# Compute token embeddings\\\\nwith torch.no_grad():\\\\n    model_output = model(**encoded_input)\\\\n\\\\n# Perform pooling. In this case, mean pooling\\\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\\\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.'}} </s>\"\n"}
{"text": "<s>[INST]سيقوم فريق البحث ببناء نموذج للتصنيف الرمزي في مجال معالجة اللغة الطبيعية، ونرغب في استخدام هذا النموذج لتحديد تصنيف الرموز في النصوص.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a Part-of-Speech (POS) tagging model, which assigns a POS tag to each token in a sentence.\\\\n3. We specify the model 'ckiplab/bert-base-chinese-pos', which has been pretrained on Chinese text, and the tokenizer 'bert-base-chinese' to tokenize the input text into individual words.\\\\n4. The created pipeline will be used to apply POS tagging to the given Chinese text.\\\\n5. The output will be a list of tuples, where each tuple contains a word and its corresponding POS tag.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\npos_tagger = pipeline('pos-tagging', model='ckiplab/bert-base-chinese-pos', tokenizer='bert-base-chinese')\\\\ntext = '你好，我是一名程序员。'\\\\npos_tags = pos_tagger(text)\\\", 'api_call': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ckiplab/bert-base-chinese-pos', 'api_call': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'api_arguments': {'tokenizer': \\\"BertTokenizerFast.from_pretrained('bert-base-chinese')\\\"}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import (\\\\n  BertTokenizerFast,\\\\n  AutoModel,\\\\n)\\\\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\\\\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\\\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).' </s>\"\n"}
{"text": "<s>[INST]اصنع نموذجًا جديدًا لتصنيف الصور بدون تسميات باستخدام الرؤية الحاسوبية. قم بتحميل بيانات الصور والتسميات ذات العلامات المقابلة. بعد التدريب، استخدم النموذج لتصنيف الصور التي لم تُرِسَم سابقًا داخل المجال.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We first import the pipeline function from the transformers package.\\\\n2. We then create a pipeline for image classification using the 'laion/CLIP-convnext_base_w-laion2B-s13B-b82K' model. This model has been trained for zero-shot image classification tasks, which is exactly what we need for categorizing the user's profile photo as a cat or a dog.\\\\n3. We then provide the path to the user's uploaded image and a list of possible class names (['cat', 'dog']) as input to the pipeline. The pipeline will perform zero-shot image classification and return the most likely class for the input image.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nclip = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\\\\nresult = clip('/path/to/image.jpg', ['cat', 'dog'])\\\\nprediction = result[0]['label']\\\", 'api_call': \\\"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': \\\"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\\\", 'api_call': \\\"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\\\", 'api_arguments': {'image_path': 'path to the image file', 'labels': 'list of possible class names'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; clip = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion2B-s13B-b82K'); clip('path/to/image.jpg', ['cat', 'dog'])\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '70.8 - 71.7%'}, 'description': 'A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام تصنيف الرموز في مجال معالجة اللغات الطبيعية بإستخدام تقنيات تحليل اللغة الطبيعية، ونحتاج إلى تقدير الرموز المفقودة في الجمل المحللة.? [/INST]{'domain': 'Natural Language Processing Token Classification', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Babelscape/wikineural-multilingual-ner\\\\')\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'Babelscape/wikineural-multilingual-ner\\\\')\\\\nner_pipeline = pipeline(\\\\'ner\\\\', model=model, tokenizer=tokenizer)\\\\nreview = \\\"My name is Wolfgang and I live in Berlin.\\\"\\\\nner_results = ner_pipeline(review)\\\\nprint(ner_results)', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Named Entity Recognition', 'api_name': 'Babelscape/wikineural-multilingual-ner', 'api_call': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Babelscape/wikineural-multilingual-ner\\\\')\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'Babelscape/wikineural-multilingual-ner\\\\')\\\\nner_pipeline = pipeline(\\\\'ner\\\\', model=model, tokenizer=tokenizer)\\\\nreview = \\\"My name is Wolfgang and I live in Berlin.\\\"\\\\nner_results = ner_pipeline(review)\\\\nprint(ner_results)', 'api_arguments': {'model': 'AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)', 'tokenizer': 'AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)'}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline'}, 'example_code': 'tokenizer = AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\\\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\\\nexample = My name is Wolfgang and I live in Berlin\\\\nner_results = nlp(example)\\\\nprint(ner_results)', 'performance': {'dataset': 'Babelscape/wikineural-multilingual-ner', 'accuracy': 'span-based F1-score up to 6 points over previous state-of-the-art systems for data creation'}, 'description': 'A multilingual Named Entity Recognition (NER) model fine-tuned on the WikiNEuRal dataset, supporting 9 languages (de, en, es, fr, it, nl, pl, pt, ru). It is based on the mBERT architecture and trained on all 9 languages jointly. The model can be used with the Hugging Face Transformers pipeline for NER tasks.'}} </s>\"\n"}
{"text": "<s>[INST]إنشاء نمط جديد للصور باستخدام تقنيات الرؤية الحاسوبية بدون شروط. قم بتدريب النموذج على مجموعة بيانات كبيرة لتصميم صور غير مسبوقة.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the diffusers package using pip.\\\\n2. Import the DDPMPipeline class from the diffusers library.\\\\n3. Use the from_pretrained() method with the model identifier 'google/ddpm-bedroom-256' to load a trained image synthesis model, specifically designed for generating images of bedrooms.\\\\n4. Generate an image by calling the loaded model. The image will be a high-quality, unconditional generation based on the model's knowledge of bedroom environments. This can be used as an asset in the virtual reality game environment.\\\\n5. Save the generated image to a file using the save method of the generated image object.\\\", '<<<code>>>': \\\"!pip install diffusers\\\\nfrom diffusers import DDPMPipeline\\\\nmodel_id = 'google/ddpm-bedroom-256'\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\ngenerated_image = ddpm().images[0]\\\\ngenerated_image.save('ddpm_generated_image.png')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/ddpm-bedroom-256', 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\", 'api_arguments': 'None', 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد نموذج يتمكن من توليد صور استنادًا إلى النصوص متعددة الوسائط. هل يمكنك مساعدتي في ذلك؟? [/INST]{'domain': 'Multimodal Text-to-Image', 'api_call': \\\"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\\", 'api_provider': 'Hugging Face', 'explanation': '1. Import the necessary classes from the diffusers package. This includes StableDiffusionInpaintPipeline for the text-to-image model.', 'code': 'from diffusers import StableDiffusionInpaintPipeline\\\\nimport torch\\\\npipe = StableDiffusionInpaintPipeline.from_pretrained(\\\\'stabilityai/stable-diffusion-2-inpainting\\\\', torch_dtype=torch.float16)\\\\nprompt = \\\"happy birthday with a cake and balloons\\\"\\\\nimage = pipe(prompt=prompt).images[0]\\\\nimage.save(\\\\'greeting_card.png\\\\')', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': 'from diffusers import StableDiffusionInpaintPipeline\\\\nimport torch\\\\npipe = StableDiffusionInpaintPipeline.from_pretrained(\\\\'stabilityai/stable-diffusion-2-inpainting\\\\', torch_dtype=torch.float16)\\\\nprompt = \\\"happy birthday with a cake and balloons\\\"\\\\nimage = pipe(prompt=prompt).images[0]\\\\nimage.save(\\\\'greeting_card.png\\\\')', 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \\\"from diffusers import StableDiffusionInpaintPipeline\\\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\\\n\\\\npipe.to(cuda)\\\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\\\nimage.save(./yellow_cat_on_park_bench.png)\\\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير تقنية لتحويل النصوص إلى مقاطع فيديو في مجال النص إلى الفيديو المتعدد الوسائط، وتهدف إلى تحسين جودة الفيديوهات المتخلفة بوضوح ودقة عالية.? [/INST]{'domain': 'Multimodal Text-to-Video', 'api_call': \\\"BaseModel.from_pretrained('duncan93/video')\\\", 'api_provider': 'Hugging Face', 'explanation': '1. First, you need to install the required package, Asteroid, and import the necessary libraries.', 'code': \\\"!pip install Asteroid\\\\nfrom asteroid import BaseModel\\\\nmodel = BaseModel.from_pretrained('duncan93/video')\\\\nframes = model.generate_video_frames(text_description)\\\", 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'duncan93/video', 'api_call': \\\"!pip install Asteroid\\\\nfrom asteroid import BaseModel\\\\nmodel = BaseModel.from_pretrained('duncan93/video')\\\\nframes = model.generate_video_frames(text_description)\\\", 'api_arguments': '', 'python_environment_requirements': 'Asteroid', 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.'}} </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج لتوليد النصوص من النصوص باستخدام معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"pipeline('summarization', model='it5/it5-base-news-summarization')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"pipeline('summarization', model='it5/it5-base-news-summarization')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'it5-base-news-summarization', 'api_call': \\\"pipeline('summarization', model='it5/it5-base-news-summarization')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': 'newsum(Dal 31 maggio è infine partita la piattaforma ITsART, a più di un anno da quando – durante il primo lockdown – il ministro della Cultura Dario Franceschini ne aveva parlato come di «una sorta di Netflix della cultura», pensata per «offrire a tutto il mondo la cultura italiana a pagamento». È presto per dare giudizi definitivi sulla piattaforma, e di certo sarà difficile farlo anche più avanti senza numeri precisi. Al momento, l’unica cosa che si può fare è guardare com’è fatto il sito, contare quanti contenuti ci sono (circa 700 “titoli”, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro varietà. Intanto, una cosa notata da più parti è che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.)', 'performance': {'dataset': 'NewsSum-IT', 'accuracy': {'Rouge1': 0.339, 'Rouge2': 0.16, 'RougeL': 0.263}}, 'description': 'IT5 Base model fine-tuned on news summarization on the Fanpage and Il Post corpora for Italian Language Understanding and Generation.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى تطوير برنامج لتحويل النص إلى كلام بشكل فعال لمستخدمي تطبيق Audio Text-to-Speech الذين يعانون من إعاقة بصرية. يجب أن يكون البرنامج قادرًا على إنشاء تصنيفات بدون استخدام معالجة اللغة الطبيعية.? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TT')\\\", 'api_provider': 'Fairseq', 'explanation': '1. Import the necessary libraries and functions needed, such as hub_utils, load_model_ensemble_and_task_from_hf_hub, CodeHiFiGANVocoder, VocoderHubInterface, etc., from fairseq and torchaudio libraries.', 'code': 'import json\\\\nimport os\\\\nfrom pathlib import Path\\\\nimport IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\n\\\\ncache_dir = os.getenv(\\\\'HUGGINGFACE_HUB_CACHE\\\\')\\\\nlibrary_name = \\\\'fairseq\\\\'\\\\ncache_dir = cache_dir or (Path.home() / \\\\'.cache\\\\' / library_name).as_posix()\\\\ncache_dir = snapshot_download(\\\\'facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS\\\\', cache_dir=cache_dir, library_name=library_name)\\\\n\\\\nx = hub_utils.from_pretrained(cache_dir, \\\\'model.pt\\\\', \\\\'.\\\\', archive_map=CodeHiFiGANVocoder.hub_models(), config_yaml=\\\\'config.json\\\\', fp16=False, is_vocoder=True)\\\\n\\\\nwith open(f\\\"{x[\\\\'args\\\\'][\\\\'data\\\\']}/config.json\\\") as f:\\\\n    vocoder_cfg = json.load(f)\\\\n\\\\nassert len(x[\\\\'args\\\\'][\\\\'model_path\\\\']) == 1, \\\"Too many vocoder models in the input\\\"\\\\nvocoder = CodeHiFiGANVocoder(x[\\\\'args\\\\'][\\\\'model_path\\\\'][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(\\\\'TEXT_INPUT_HERE\\\\')  # Replace \\\\'TEXT_INPUT_HERE\\\\' with your input text\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\nipd.Audio(wav, rate=sr)', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Fairseq', 'functionality': 'Text-to-Speech', 'api_name': 'unit_hifigan_HK_layer12.km2500_frame_TAT-TTS', 'api_call': 'import json\\\\nimport os\\\\nfrom pathlib import Path\\\\nimport IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\n\\\\ncache_dir = os.getenv(\\\\'HUGGINGFACE_HUB_CACHE\\\\')\\\\nlibrary_name = \\\\'fairseq\\\\'\\\\ncache_dir = cache_dir or (Path.home() / \\\\'.cache\\\\' / library_name).as_posix()\\\\ncache_dir = snapshot_download(\\\\'facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS\\\\', cache_dir=cache_dir, library_name=library_name)\\\\n\\\\nx = hub_utils.from_pretrained(cache_dir, \\\\'model.pt\\\\', \\\\'.\\\\', archive_map=CodeHiFiGANVocoder.hub_models(), config_yaml=\\\\'config.json\\\\', fp16=False, is_vocoder=True)\\\\n\\\\nwith open(f\\\"{x[\\\\'args\\\\'][\\\\'data\\\\']}/config.json\\\") as f:\\\\n    vocoder_cfg = json.load(f)\\\\n\\\\nassert len(x[\\\\'args\\\\'][\\\\'model_path\\\\']) == 1, \\\"Too many vocoder models in the input\\\"\\\\nvocoder = CodeHiFiGANVocoder(x[\\\\'args\\\\'][\\\\'model_path\\\\'][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(\\\\'TEXT_INPUT_HERE\\\\')  # Replace \\\\'TEXT_INPUT_HERE\\\\' with your input text\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\nipd.Audio(wav, rate=sr)', 'api_arguments': {'unit': 'Text input for the TTS model'}, 'python_environment_requirements': ['fairseq', 'huggingface_hub', 'torchaudio'], 'example_code': \\\"import json\\\\nimport os\\\\nfrom pathlib import Path\\\\nimport IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\\\\nlibrary_name = fairseq\\\\ncache_dir = (\\\\n cache_dir or (Path.home() / .cache / library_name).as_posix()\\\\n)\\\\ncache_dir = snapshot_download(\\\\n ffacebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS, cache_dir=cache_dir, library_name=library_name\\\\n)\\\\nx = hub_utils.from_pretrained(\\\\n cache_dir,\\\\n model.pt,\\\\n .,\\\\n archive_map=CodeHiFiGANVocoder.hub_models(),\\\\n config_yaml=config.json,\\\\n fp16=False,\\\\n is_vocoder=True,\\\\n)\\\\nwith open(f{x['args']['data']}/config.json) as f:\\\\n vocoder_cfg = json.load(f)\\\\nassert (\\\\n len(x[args][model_path]) == 1\\\\n), Too many vocoder models in the input\\\\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\nipd.Audio(wav, rate=sr)\\\", 'performance': {'dataset': 'TAT-TTS', 'accuracy': 'Not provided'}, 'description': 'Hokkien unit HiFiGAN based vocoder from fairseq. Trained with TAT-TTS data with 4 speakers in Taiwanese Hokkien accent.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتحليل الفيديوهات باستخدام تقنيات رؤية الحواسيب لتصنيفها. هل يمكنك مساعدتي في إنشاء نموذج لتحليل الفيديو؟? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-ssv2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary classes and packages from the transformers library. This includes VideoMAEForVideoClassification for the video classification model and VideoMAEImageProcessor to pre-process video frames.\\\\n2. We load the pre-trained video classification model 'MCG-NJU/videomae-base-finetuned-ssv2', which is a model that has been trained on a large collection of categorized video data.\\\\n3. We capture a video of a person dancing and pre-process the video frames using the VideoMAEImageProcessor.\\\\n4. With the processed video frames, we can use the VideoMAEForVideoClassification model to predict the type of dance style being performed.\\\", '<<<code>>>': 'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport cv2\\\\nimport numpy as np\\\\nvideo_file = \\\\'path_to_video_file.mp4\\\\'\\\\nvideo = cv2.VideoCapture(video_file)\\\\nframes = extract_frames_from_video(video)  # create a function to extract frames\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(\\\\'MCG-NJU/videomae-base-finetuned-ssv2\\\\')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(\\\\'MCG-NJU/videomae-base-finetuned-ssv2\\\\')\\\\ninputs = processor(frames, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-ssv2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'MCG-NJU/videomae-base-finetuned-ssv2', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-ssv2')\\\", 'api_arguments': 'video', 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Something-Something-v2', 'accuracy': {'top-1': 70.6, 'top-5': 92.6}}, 'description': 'VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]يقوم شركتنا بتطوير نماذج للتعرف على الصور باستخدام تقنيات الرؤية الحاسوبية، ونحتاج إلى قدرتها على تصنيف الأشياء في الصور بدقة ودقة عالية.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To achieve this task, we can use the pre-trained 'julien-c/hotdog-not-hotdog' model from Hugging Face Transformers, which is capable of classifying images as hotdog or not hotdog.\\\\n2. We first import the necessary library, specifically the 'pipeline' function from the transformers package.\\\\n3. Then, we create a pipeline for image classification using the pre-trained 'julien-c/hotdog-not-hotdog' model, which can classify images into hotdog or not hotdog.\\\\n4. Users can snap a picture of the food items at the food festival, and the model will categorize the images by whether they contain hotdogs or other types of foods.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nclassifier = pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\\\", 'api_call': \\\"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'julien-c/hotdog-not-hotdog', 'api_call': \\\"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\\\", 'api_arguments': 'image', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': 0.825}, 'description': 'A model that classifies images as hotdog or not hotdog.' </s>\"\n"}
{"text": "<s>[INST]العميل يطوّر تطبيقًا للنص إلى فيديو متعدد القنوات. نحتاج إلى بحرنامج يتحوّل النص إلى فيديو لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Video', '<<<api_call>>>': \\\"pipeline('text-to-video-synthesis', model_dir.as_posix())\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. We first import the necessary classes and modules from the respective packages.\\\\n2. We download the pre-trained model \\\\'damo-vilab/modelscope-damo-text-to-video-synthesis\\\\' from Hugging Face Hub.\\\\n3. We then create a pipeline instance for \\\\'text-to-video-synthesis\\\\' using the local model snapshot, which is suitable for generating a video from a textual description.\\\\n4. We then provide the input description, which is your desired text: \\\"A person enjoying a coffee while reading a book at a cozy coffee shop.\\\"\\\\n5. Finally, the model generates the video file that matches the provided description.', '<<<code>>>': \\\"from huggingface_hub import snapshot_download\\\\nfrom modelscope.pipelines import pipeline\\\\nfrom modelscope.outputs import OutputKeys\\\\nimport pathlib\\\\n\\\\nmodel_dir = pathlib.Path('weights')\\\\nsnapshot_download('damo-vilab/modelscope-damo-text-to-video-synthesis', repo_type='model', local_dir=model_dir)\\\\npipe = pipeline('text-to-video-synthesis', model_dir.as_posix())\\\\n\\\\ntest_text = {\\\\n 'text': 'A person enjoying a coffee while reading a book at a cozy coffee shop.',\\\\n}\\\\noutput_video_path = pipe(test_text)[OutputKeys.OUTPUT_VIDEO]\\\", 'api_call': \\\"pipeline('text-to-video-synthesis', model_dir.as_posix())\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'modelscope-damo-text-to-video-synthesis', 'api_call': \\\"pipeline('text-to-video-synthesis', model_dir.as_posix())\\\", 'api_arguments': {'text': 'A short text description in English'}, 'python_environment_requirements': [['modelscope==1.4.2', 'open_clip_torch', 'pytorch-lightning']], 'example_code': \\\"from huggingface_hub import snapshot_download\\\\nfrom modelscope.pipelines import pipeline\\\\nfrom modelscope.outputs import OutputKeys\\\\nimport pathlib\\\\n\\\\nmodel_dir = pathlib.Path('weights')\\\\nsnapshot_download('damo-vilab/modelscope-damo-text-to-video-synthesis',\\\\n repo_type='model', local_dir=model_dir)\\\\n\\\\npipe = pipeline('text-to-video-synthesis', model_dir.as_posix())\\\\n\\\\ntest_text = {\\\\n 'text': 'A panda eating bamboo on a rock.',\\\\n}\\\\n\\\\noutput_video_path = pipe(test_text,)[OutputKeys.OUTPUT_VIDEO]\\\\nprint('output_video_path:', output_video_path)\\\", 'performance': {'dataset': 'Webvid, ImageNet, LAION5B', 'accuracy': 'Not provided'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير موديل لاكتشاف النشاط الصوتي في الصوتيات، ونريد تدريب النموذج على تمييز بين الصوت النشط والصوت غير النشط في التسجيلات الصوتية المتوفرة لدينا.? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Begin by importing the Model class from the pyannote.audio library.\\\\n2. Load the pretrained model 'popcornell/pyannote-segmentation-chime6-mixer6' using the Model.from_pretrained() function. This model is fine-tuned on data from the CHiME-7 DASR Challenge and is designed for diarization (detecting and distinguishing different speakers) in noisy environments. \\\\n3. Once loaded, this model can be used to detect and distinguish voices in audio samples captured by the AI-based doorbell in noisy situations.\\\", '<<<code>>>': \\\"from pyannote.audio import Model\\\\nvoice_activity_detection_model = Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", 'api_call': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'popcornell/pyannote-segmentation-chime6-mixer6', 'api_call': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['pyannote.audio 2.1.1'], 'example_code': 'from pyannote.audio import Model\\\\nmodel = Model.from_pretrained(popcornell/pyannote-segmentation-chime6-mixer6)', 'performance': {'dataset': 'ami', 'accuracy': 'N/A'}, 'description': 'Pyannote Segmentation model fine-tuned on data from CHiME-7 DASR Challenge. Used to perform diarization in the CHiME-7 DASR diarization baseline.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء تعليمة تفصيلية حول كيفية تطبيق تقنيات معالجة اللغة الطبيعية في الصنف البرمجي Zero-Shot Classification. يجب أن يكون النص المولد دقيقًا وملائمًا للمساعدة في تصنيف النصوص بدقة متناهية دون الحاجة إلى تدريب مسبق على البيانات.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"CrossEncoder('cross-encoder/nli-distilroberta-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include 'CrossEncoder' from 'sentence_transformers'.\\\\n2. Instantiate the 'CrossEncoder' object, specifying the model 'cross-encoder/nli-distilroberta-base'. This model is trained for natural language inference tasks and will generate scores for contradictions, entailments, and neutral relations between sentence pairs.\\\\n3. Provide pairs of sentences, such as news headlines from the tech industry, to the model, which will predict the logical relationship between them.\\\", '<<<code>>>': \\\"from sentence_transformers import CrossEncoder\\\\ncross_encoder = CrossEncoder('cross-encoder/nli-distilroberta-base')\\\\nsentence1 = 'Apple announces new iPhone 13 with advanced features'\\\\nsentence2 = 'Apple unveils its latest smartphone'\\\\nscores = cross_encoder.predict([(sentence1, sentence2)])\\\\nrelationship = ['contradiction', 'entailment', 'neutral'][scores.argmax()]\\\\nprint(relationship)\\\", 'api_call': \\\"CrossEncoder('cross-encoder/nli-distilroberta-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cross-encoder/nli-distilroberta-base', 'api_call': \\\"CrossEncoder('cross-encoder/nli-distilroberta-base')\\\", 'api_arguments': \\\"('A man is eating pizza', 'A man eats something')\\\", 'python_environment_requirements': [['sentence_transformers', 'transformers']], 'example_code': \\\"from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model='cross-encoder/nli-distilroberta-base')\\\\nsent = Apple just announced the newest iPhone X\\\\ncandidate_labels = [technology, sports, politics]\\\\nres = classifier(sent, candidate_labels)\\\\nprint(res)\\\", 'performance': {'dataset': 'SNLI and MultiNLI', 'accuracy': 'See SBERT.net - Pretrained Cross-Encoder for evaluation results'}, 'description': 'This model was trained using SentenceTransformers Cross-Encoder class on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.' </s>\"\n"}
{"text": "<s>[INST]يجب إنشاء نموذج لتوليد النصوص من معالجة اللغة الطبيعية.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"pipeline('text-generation', model='facebook/opt-6.7b')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. Import the necessary libraries, which include 'pipeline' from the transformers package.\\\", 'code': 'from transformers import pipeline\\\\ntext_gen = pipeline(\\\\'text-generation\\\\', model=\\\\'facebook/opt-6.7b\\\\')\\\\nprompt = \\\"The future of technology in education is\\\"\\\\ngenerated_text = text_gen(prompt, max_length=50, num_return_sequences=1)[0][\\\\'generated_text\\\\']', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Transformers', 'functionality': 'Text Generation', 'api_name': 'facebook/opt-6.7b', 'api_call': 'from transformers import pipeline\\\\ntext_gen = pipeline(\\\\'text-generation\\\\', model=\\\\'facebook/opt-6.7b\\\\')\\\\nprompt = \\\"The future of technology in education is\\\"\\\\ngenerated_text = text_gen(prompt, max_length=50, num_return_sequences=1)[0][\\\\'generated_text\\\\']', 'api_arguments': ['torch_dtype'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained('facebook/opt-6.7b', torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained('facebook/opt-6.7b', use_fast=False)\\\\nprompt = Hello, I'm am conscious and\\\\ninput_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\\\\ngenerated_ids = model.generate(input_ids)\\\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)\\\", 'performance': {'dataset': {'BookCorpus': 'unknown', 'CC-Stories': 'unknown', 'The Pile': 'unknown', 'Pushshift.io Reddit': 'unknown', 'CCNewsV2': 'unknown'}, 'accuracy': 'unknown'}, 'description': 'OPT (Open Pre-trained Transformer Language Models) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. It was trained on a large corpus of text, predominantly in English, using a causal language modeling (CLM) objective. The model can be used for prompting for evaluation of downstream tasks, text generation, and fine-tuning on a downstream task using the CLM example.'}} </s>\"\n"}
{"text": "<s>[INST]يجب على المبرمج تطوير نظام يستخدم تقنيات إكتشاف الأجسام في الرؤية الحاسوبية ، عليه أن يضمن أن الصور المفحوصة تلبي الاحتياجات والتفضيلات لمستخدمي التطبيق، وتوفر لهم تجربة استخدام سلسة ومريحة.? [/INST]{'domain': 'Computer Vision Object Detection', 'api_call': \\\"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Start by importing the required libraries: DetrForObjectDetection and DetrImageProcessor from transformers, along with Image from PIL, and requests for fetching the image.', 'code': '', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-50', 'api_call': '', 'api_arguments': {'pretrained_model_name': 'facebook/detr-resnet-50'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\\\nimport torch\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-50)\\\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-50)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '42.0 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى برمجية تستخدم معالجة اللغة الطبيعية لتصنيف الرموز الدلالية. عميلنا يتطوّر تطبيقاً لذوي الإعاقة البصرية. نحتاج إلى برنامج يحول النص إلى كلام لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Start by importing the necessary classes from the transformers package, which include AutoModelForTokenClassification and AutoTokenizer.\\\\n2. Load the pre-trained model 'ismail-lucifer011/autotrain-name_all-904029577' by using the from_pretrained method, and specify the use_auth_token parameter as True for authentication.\\\\n3. Also load the tokenizer associated with this model using the from_pretrained method.\\\\n4. Tokenize the input text document with the tokenizer, setting the return_tensors parameter to 'pt' for PyTorch tensors.\\\\n5. Pass the tokenized inputs to the pre-trained model's forward pass for entity extraction.\\\\n6. Process the model's output to extract and present the entities found in the input text.\\\", '<<<code>>>': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'ismail-lucifer011/autotrain-name_all-904029577\\\\', use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'ismail-lucifer011/autotrain-name_all-904029577\\\\', use_auth_token=True)\\\\ninputs = tokenizer(\\\"I love AutoTrain\\\", return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nentities = process_output(outputs)', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': '904029577', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\\\", 'api_arguments': {'inputs': 'I love AutoTrain'}, 'python_environment_requirements': [{'transformers': 'AutoModelForTokenClassification', 'tokenizer': 'AutoTokenizer'}], 'example_code': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-name_all-904029577, use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-name_all-904029577, use_auth_token=True)\\\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\\\noutputs = model(**inputs)', 'performance': {'dataset': 'ismail-lucifer011/autotrain-data-name_all', 'accuracy': 0.9989316041363876}, 'description': 'This model is trained using AutoTrain for entity extraction. It is based on the DistilBert architecture and has a CO2 Emissions of 0.8375653425894861 grams.' </s>\"\n"}
{"text": "<s>[INST]يحتاج العميل في مجال تقدير العمق في رؤية الحاسوب إلى برنامج يمكنه تقدير العمق بدقة.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We start by importing the necessary package, which is AutoModel from the transformers library provided by Hugging Face.\\\\n2. We use the from_pretrained method of the AutoModel class to load the pre-trained model 'sayakpaul/glpn-nyu-finetuned-diode-230103-091356'. This model has been trained on the diode_subset dataset and is suitable for estimating depth in computer vision tasks.\\\\n3. Once the model is loaded, it can be used to perform depth estimation on images of objects in the room. The model will return a depth map, which can help the robot safely maneuver through the room by identifying objects and distances.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\\\\n\\\\n# Perform depth estimation on an input image\\\\n# depth_map = model(input_image)\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-230103-091356', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers==4.24.0, pytorch==1.12.1+cu116, datasets==2.8.0, tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.436, 'Mae': 0.4251, 'Rmse': 0.6169, 'Abs Rel': 0.45, 'Log Mae': 0.1721, 'Log Rmse': 0.2269, 'Delta1': 0.3828, 'Delta2': 0.6326, 'Delta3': 0.8051}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج أساسي لتصنيف مقاطع الفيديو بدون معاينة باستخدام تقنيات الرؤية الحاسوبية، وابدأ بتحميل مجموعة بيانات شاملة ومضبوطة. قم بتحديد فئات مقاطع الفيديو المتاحة للتصنيف ثم استخدم تقنيات التعلم الآلي اللاحق لتصنيف المقاطع بناءً على الفئات المحددة.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k400')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We need to import the necessary libraries such as transformers, numpy, and torch.\\\\n2. The pre-trained 'facebook/timesformer-hr-finetuned-k400' model should be loaded using the 'TimesformerForVideoClassification.from_pretrained' function. This model is designed for video classification tasks and has been trained on the Kinetics-400 dataset, which contains a wide range of human actions.\\\\n3. Load the gym video and preprocess the video frames.\\\\n4. Pass the video frames to the model, which will classify the type of activity in the video.\\\\n5. Extract the predicted action from the model output.\\\", '<<<code>>>': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo_processor = AutoImageProcessor.from_pretrained(\\\\'facebook/timesformer-hr-finetuned-k400\\\\')\\\\nmodel = TimesformerForVideoClassification.from_pretrained(\\\\'facebook/timesformer-hr-finetuned-k400\\\\')\\\\ninputs = video_processor(images=video_frames, return_tensors=\\\\'pt\\\\')  # Replace \\\\'video_frames\\\\' with your video frames\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k400')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/timesformer-hr-finetuned-k400', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k400')\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': [['transformers', 'numpy', 'torch']], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 448, 448))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-hr-finetuned-k400)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-hr-finetuned-k400)\\\\ninputs = processor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-400', 'accuracy': 'Not specified'}, 'description': 'TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al.' </s>\"\n"}
{"text": "<s>[INST]أنشئ وصفًا متعدد الوسائط لصورة معينة واكتب نصًا يصف المشهد المعروض في الصورة.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries: TrOCRProcessor, VisionEncoderDecoderModel from transformers, and Image from PIL.\\\\n2. Load the image of the handwritten document. You can open the image file using the Image.open() method from the PIL library.\\\\n3. Create a TrOCRProcessor instance by loading the pretrained model 'microsoft/trocr-large-handwritten'.\\\\n4. Create a VisionEncoderDecoderModel instance by loading the pretrained model 'microsoft/trocr-large-handwritten'.\\\\n5. Use the processor to get the pixel values from the image and pass them through the model.\\\\n6. Use the processor.batch_decode() method to convert the output of the model into text.\\\", '<<<code>>>': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\ndef image_to_text(image_path):\\\\n    image = Image.open(image_path).convert('RGB')\\\\n    processor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\\\n    model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\\n    pixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\n    generated_ids = model.generate(pixel_values)\\\\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\\n    return generated_text\\\", 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': [{'packages': ['transformers', 'PIL', 'requests']}], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نظام لمعالجة اللغة الطبيعية لتلخيص النصوص بشكل فعال ودقيق، يجب أن يتضمن النظام قدرة على فهم واستخلاص الأفكار الرئيسية والمعلومات الحيوية من النصوص بدقة عالية وبشكل تلقائي.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a summarization pipeline with the pre-trained model 'google/pegasus-newsroom'. This model is designed for summarization tasks and has been trained on both C4 and HugeNews datasets.\\\\n3. With the created summarization pipeline, we can then input any long article text and transform it into a short article summary suitable for posting on Twitter.\\\\n4. This approach allows users to summarize articles and quickly share valuable information on social media platforms.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsummarization_pipeline = pipeline(\\\\'summarization\\\\', model=\\\\'google/pegasus-newsroom\\\\')\\\\nlong_article = \\\"Your long article text goes here...\\\"\\\\nsummary = summarization_pipeline(long_article)[0][\\\\'summary_text\\\\']', 'api_call': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/pegasus-newsroom', 'api_call': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': 'newsroom', 'accuracy': '45.98/34.20/42.18'}, 'description': 'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks.' </s>\"\n"}
{"text": "<s>[INST]توليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية، باستخدام تقنيات معالجة اللغات الطبيعية. هل تستطيع مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is `pipeline` from transformers.\\\\n2. Use the `pipeline` function from transformers to load the model, which in this case is 'bert-base-cased'.\\\\n3. The loaded model will be used for completing the masked sentence, filling in the word at the position of the mask token `[MASK]`.\\\\n4. Provide the sentence with the `[MASK]` token as input to the model, and the model will predict and return the most likely word to fill in the mask.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'bert-base-cased\\\\')\\\\ninput_text = \\\"I have always wanted to [MASK] a marathon.\\\"\\\\nresult = unmasker(input_text)', 'api_call': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-base-cased', 'api_call': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", 'api_arguments': ['model'], 'python_environment_requirements': [['transformers']], 'example_code': ['from transformers import pipeline', \\\"unmasker = pipeline('fill-mask', model='bert-base-cased')\\\", \\\"unmasker(Hello I'm a [MASK] model.)\\\"], 'performance': {'dataset': 'GLUE', 'accuracy': 79.6}, 'description': \\\"BERT base model (cased) is a pre-trained transformer model on English language using a masked language modeling (MLM) objective. It was introduced in a paper and first released in a repository. This model is case-sensitive, which means it can differentiate between 'english' and 'English'. The model can be used for masked language modeling or next sentence prediction, but it's mainly intended to be fine-tuned on a downstream task.\\\" </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لمعالجة اللغات الطبيعية لمقارنة الجمل، ونريد توليد تنبؤات لدرجة تشابه الجمل بناءً على بيانات متعلقة.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install 'sentence-transformers' package using pip.\\\\n2. Import 'SentenceTransformer' from the 'sentence_transformers' library.\\\\n3. Create an instance of the 'SentenceTransformer' by specifying the model name 'sentence-transformers/all-roberta-large-v1'.\\\\n4. Encode a list of sentences containing customer reviews using the 'encode()' method on the model instance.\\\\n5. The output will be a list of embeddings with each embedding representing the semantic similarity of the corresponding customer review.\\\\n6. You can perform clustering on the embeddings to group similar reviews together.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\ncustomer_reviews = [\\\"This product is excellent!\\\", \\\"I love using this every day.\\\", \\\"Not satisfied with the purchase.\\\"]\\\\n# List of customer reviews\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-roberta-large-v1\\\\')\\\\nembeddings = model.encode(customer_reviews)\\\\n# Perform clustering on the embeddings to group similar reviews.', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/all-roberta-large-v1', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 1024 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نصا توليديا ينشأ باستخدام معالجة اللغة الطبيعية ويبدأ بالعبارة \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('conversational', model='PygmalionAI/pygmalion-350m')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. First, we import transformers pipeline to use as conversation pipeline. \\\\n2. Next, we create an NLP pipeline using the pre-trained \\\"PygmalionAI/pygmalion-350m\\\" Conversational AI model. This model has been fine-tuned to be optimized for dialogue generation, specifically to answer questions in a conversational manner.\\\\n3. Then we can use the pipeline to process input questions from customers and generate appropriate responses for the chatbot to provide.\\\\n4. You can provide some common restaurant-related questions as history/context within the chatbot, which will improve the quality of the generated responses.\\\\n5. Keep in mind the model was fine-tuned with NSFW data, so using it for customer interaction requires thorough monitoring.', '<<<code>>>': 'from transformers import pipeline\\\\nconv_pipeline = pipeline(\\\\'conversational\\\\', model=\\\\'PygmalionAI/pygmalion-350m\\\\')\\\\n\\\\ndef reply_question(question):\\\\n    response = conv_pipeline(question)\\\\n    return response[\\\"generated_text\\\"]\\\\n\\\\n# Example usage:\\\\nquestion = \\\"What are the restaurant\\\\'s opening hours?\\\"\\\\nanswer = reply_question(question)\\\\nprint(answer)', 'api_call': \\\"pipeline('conversational', model='PygmalionAI/pygmalion-350m')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pygmalion-350m', 'api_call': \\\"pipeline('conversational', model='PygmalionAI/pygmalion-350m')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['transformers'], 'example_code': 'N/A', 'performance': {'dataset': 'The Pile', 'accuracy': 'N/A'}, 'description': \\\"This is a proof-of-concept fine-tune of Facebook's OPT-350M model optimized for dialogue, to be used as a stepping stone to higher parameter models. Disclaimer: NSFW data was included in the fine-tuning of this model. Although SFW inputs will usually result in SFW outputs, you are advised to chat at your own risk. This model is not suitable for use by minors.\\\" </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نظام تصنيف الصوت بإستخدام تقنية التعرف التلقائي على الكلام لتصنيف ملفات الصوت إلى فئات معينة بدقة وسرعة عالية. ونحتاج إلى تصميم نموذج يمكنه التعرف على أنواع الصوت بدقة وسرعة.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\", '<<<api_provider>>>': 'SpeechBrain', '<<<explanation>>>': \\\"1. First, install the SpeechBrain library via pip.\\\\n2. Import the foreign_class function from speechbrain.pretrained.interfaces.\\\\n3. Instantiate the classifier by calling the foreign_class function with the provided source repository, custom class name ('CustomEncoderWav2vec2Classifier'), and a 'custom_interface.py' file.\\\\n4. This classifier is pretrained on IEMOCAP training data and utilizes a wav2vec2 model for emotion recognition in speech.\\\\n5. Use the 'classify_file' method of the classifier object to detect emotions in an audio file.\\\\n6. The classifier returns output probabilities, scores, indices, and emotion labels ('text_lab'). You can use these emotion labels to make AI responses.\\\", '<<<code>>>': \\\"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\\nout_prob, score, index, text_lab = classifier.classify_file('path/to/audio/file.wav')\\\\nprint(text_lab)\\\", 'api_call': \\\"foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\", 'provider': 'SpeechBrain', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP', 'api_call': \\\"foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\", 'api_arguments': ['file_path'], 'python_environment_requirements': ['pip install speechbrain'], 'example_code': \\\"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\\nout_prob, score, index, text_lab = classifier.classify_file('speechbrain/emotion-recognition-wav2vec2-IEMOCAP/anger.wav')\\\\nprint(text_lab)\\\", 'performance': {'dataset': 'IEMOCAP', 'accuracy': '78.7%'}, 'description': 'This repository provides all the necessary tools to perform emotion recognition with a fine-tuned wav2vec2 (base) model using SpeechBrain. It is trained on IEMOCAP training data.' </s>\"\n"}
{"text": "<s>[INST]لدينا فريق طلابي يقوم بتصنيف البيانات الصوتية التلقائية كل أسبوعين، وفي هذه الجلسات، يتنافس الطلاب بتصنيف البيانات في مجال التعرف التلقائي على الكلام. زودهم بأداة للعب ضد وكيل التعلم الآلي.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the necessary libraries such as \\\"WhisperProcessor\\\", \\\"WhisperForConditionalGeneration\\\" from the transformers package, and \\\"load_dataset\\\" from the datasets package.\\\\n2. Load the pre-trained Whisper model, \\\\'openai/whisper-tiny.en\\\\', which is capable of transcribing speech.\\\\n3. Create a processor instance from the WhisperProcessor class using the same model name.\\\\n4. Load the audio file from the provided dataset or directly from a file.\\\\n5. Process the audio file using the WhisperProcessor to extract input features.\\\\n6. Use the pretrained Whisper model to generate predicted IDs from the input features.\\\\n7. Decode the predicted IDs back into text using the processor\\\\'s batch_decode function.\\\\n8. Now, you can use the generated transcription in your podcast transcription service.', '<<<code>>>': \\\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-tiny.en')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\\\\nsample = {'audio': audio_data, 'sampling_rate': sampling_rate}\\\\n# replace audio_data with actual audio and sampling_rate with the rate\\\\ninput_features = processor(sample['audio'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\\", 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai/whisper-tiny.en', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\\\", 'api_arguments': {'model_name': 'openai/whisper-tiny.en'}, 'python_environment_requirements': [['transformers', 'datasets', 'torch']], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', 'processor = WhisperProcessor.from_pretrained(openai/whisper-tiny.en)', 'model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-tiny.en)', 'ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)', 'sample = ds[0][audio]', 'input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features', 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)'], 'performance': {'dataset': 'LibriSpeech (clean)', 'accuracy': 8.437}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطبيق تقنيات التعلّم التعزيزي في مجال معالجة اللغة الطبيعية الحوارية لإنشاء نماذج تعليم آلي لتحقيق أداء محسن في مجموعة معينة من المهام بمجال الإجابة على أسئلة من الجداول. هل يمكنك مساعدتي في هذا؟? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': 'from transformers import AutoTokenizer, AutoModelForTableQuestionAnswering, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'google/tapas-mini-finetuned-wtq\\\\')\\\\nmodel = AutoModelForTableQuestionAnswering.from_pretrained(\\\\'google/tapas-mini-finetuned-wtq\\\\')\\\\nnlp = pipeline(\\\\'table-question-answering\\\\', model=model, tokenizer=tokenizer)\\\\ntable_data = # Provide table data as a list of dictionaries\\\\nquestion = \\\"Insert your query related to the table here\\\"\\\\nresult = nlp(question=question, table=table_data)', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-mini-finetuned-wtq', 'api_call': 'from transformers import AutoTokenizer, AutoModelForTableQuestionAnswering, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'google/tapas-mini-finetuned-wtq\\\\')\\\\nmodel = AutoModelForTableQuestionAnswering.from_pretrained(\\\\'google/tapas-mini-finetuned-wtq\\\\')\\\\nnlp = pipeline(\\\\'table-question-answering\\\\', model=model, tokenizer=tokenizer)\\\\ntable_data = # Provide table data as a list of dictionaries\\\\nquestion = \\\"Insert your query related to the table here\\\"\\\\nresult = nlp(question=question, table=table_data)', 'api_arguments': \\\"tokenizer = AutoTokenizer.from_pretrained('google/tapas-mini-finetuned-wtq'); model = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')\\\", 'python_environment_requirements': 'transformers', 'example_code': \\\"from transformers import pipeline\\\\n\\\\nnlp = pipeline('table-question-answering', model='google/tapas-mini-finetuned-wtq', tokenizer='google/tapas-mini-finetuned-wtq')\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.2854}, 'description': 'TAPAS mini model fine-tuned on WikiTable Questions (WTQ). It is pretrained on a large corpus of English data from Wikipedia and can be used for answering questions related to a table.'}} </s>\"\n"}
{"text": "<s>[INST]باستخدام تقنيات رؤية الحاسوب وتقسيم الصور، أنشئ جملة نصية متقدمة تبدأ بـ \\\"باستخدام تقنيات معالجة اللغة الطبيعية، أنشئ سياقًا نصيًا جديدًا من النص الأساسي\\\".? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"YOLO('keremberke/yolov8m-building-segmentation')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage_url = 'https://path.to/your/image.jpg'\\\\nresults = model.predict(image_url)\\\\nrendered_image = render_result(model=model, image=image_url, result=results[0])\\\\nrendered_image.show()\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-building-segmentation', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage_url = 'https://path.to/your/image.jpg'\\\\nresults = model.predict(image_url)\\\\nrendered_image = render_result(model=model, image=image_url, result=results[0])\\\\nrendered_image.show()\\\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.21', 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.623, 'mAP@0.5(mask)': 0.613}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في تقسيم الصور لتمييز الحيوانات بشكل دقيق؟? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary libraries, such as AutoImageProcessor and Mask2FormerForUniversalSegmentation from transformers, Image from PIL, and requests.', 'code': \\\"from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_instance_map = result['segmentation']\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-tiny-coco-instance', 'api_call': \\\"from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_instance_map = result['segmentation']\\\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-tiny-coco-instance'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': \\\"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_instance_map = result['segmentation']\\\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.'}} </s>\"\n"}
{"text": "<s>[INST]يمكنك تجريب نموذج التعبئة الفجوات في مجال تحليل الجداول للتنبؤ بالقيم المستقبلية، ببساطة قم بملء الخلايا الفارغة في الجدول بالبيانات المناسبة وسيقوم النموذج بتنبؤ القيم للخلايا الأخرى.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': 'RandomForestRegressor(max_depth=10, n_estimators=50, random_state=59)', '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': '1. Import the RandomForestRegressor from the scikit-learn library.\\\\n2. Create the RandomForestRegressor model with the provided parameters (max_depth=10, n_estimators=50, random_state=59).\\\\n3. Fit the model on the training dataset which contains historical electricity consumption data.\\\\n4. Use the fitted model to make predictions about future electricity consumption for the next month.', '<<<code>>>': 'from sklearn.ensemble import RandomForestRegressor\\\\nmodel = RandomForestRegressor(max_depth=10, n_estimators=50, random_state=59)\\\\nmodel.fit(X_train, y_train)\\\\npredictions = model.predict(X_test)', 'api_call': 'RandomForestRegressor(max_depth=10, n_estimators=50, random_state=59)', 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rajistics/MAPIE-TS-Electricity', 'api_call': 'RandomForestRegressor(max_depth=10, n_estimators=50, random_state=59)', 'api_arguments': {'bootstrap': 'True', 'ccp_alpha': '0.0', 'criterion': 'squared_error', 'max_depth': '10', 'max_features': '1.0', 'max_leaf_nodes': '', 'max_samples': '', 'min_impurity_decrease': '0.0', 'min_samples_leaf': '1', 'min_samples_split': '2', 'min_weight_fraction_leaf': '0.0', 'n_estimators': '50', 'n_jobs': '', 'oob_score': 'False', 'random_state': '59', 'verbose': '0', 'warm_start': 'False'}, 'python_environment_requirements': [''], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A RandomForestRegressor model for electricity consumption prediction.' </s>\"\n"}
{"text": "<s>[INST]عميلنا يحتاج إلى تطوير نموذج تعلم تعزيزي يمكن استخدامه في روبوتات الذكاء الاصطناعي لتحسين أدائها واتخاذ القرارات الذاتية. نحتاج إلى برنامج قادر على تعزيز تعلم الروبوتات وتحسين سلوكها بناءً على التفاعل مع البيئة المحيطة.? [/INST]'<<<domain>>>': 'Reinforcement Learning Robotics', '<<<api_call>>>': \\\"pipeline('robotics', model='Antheia/Hanna')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a robotics model.\\\\n3. Specify the model 'Antheia/Hanna' to be loaded. This model is trained for reinforcement learning tasks specifically in the domain of robotics.\\\\n4. The created model can be used for various warehouse tasks like picking up objects, stacking, and sorting items. The model can be integrated with robotic hardware through a communication interface to perform the actions based on the refined policy.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nrobotics_model = pipeline('robotics', model='Antheia/Hanna')\\\\n\\\\n# Perform actions like picking up an object, stacking, and sorting items\\\\n# using robotics_model\\\", 'api_call': \\\"pipeline('robotics', model='Antheia/Hanna')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Reinforcement Learning Robotics', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Antheia/Hanna', 'api_call': \\\"pipeline('robotics', model='Antheia/Hanna')\\\", 'api_arguments': 'model', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'openai/webgpt_comparisons', 'accuracy': ''}, 'description': 'Antheia/Hanna is a reinforcement learning model for robotics tasks, trained on the openai/webgpt_comparisons dataset.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نص مسموع يحتوي على الجملة \\\"كان ياما كان في أرض بعيدة\\\" باستخدام تقنية التحويل النصي إلى صوت في مجال معالجة اللغة الطبيعية.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-13b')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We import the AutoModelForCausalLM and AutoTokenizer classes from the transformers library provided by Hugging Face.', 'code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\n\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'facebook/opt-13b\\\\', torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/opt-13b\\\\', use_fast=False)\\\\n\\\\nprompt = \\\"In the distant future,\\\"\\\\ninput_ids = tokenizer(prompt, return_tensors=\\\\'pt\\\\').input_ids.cuda()\\\\ngenerated_ids = model.generate(input_ids)\\\\nopening_sentence = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Generation', 'api_name': 'facebook/opt-13b', 'api_call': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\n\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'facebook/opt-13b\\\\', torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/opt-13b\\\\', use_fast=False)\\\\n\\\\nprompt = \\\"In the distant future,\\\"\\\\ninput_ids = tokenizer(prompt, return_tensors=\\\\'pt\\\\').input_ids.cuda()\\\\ngenerated_ids = model.generate(input_ids)\\\\nopening_sentence = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]', 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-13b, torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-13b, use_fast=False)\\\\nprompt = Hello, I'm am conscious and\\\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\\\ngenerated_ids = model.generate(input_ids)\\\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)\\\", 'performance': {'dataset': 'GPT-3', 'accuracy': 'roughly match the performance and sizes of the GPT-3 class of models'}, 'description': 'OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. The models are trained to match the performance and sizes of the GPT-3 class of models. The primary goal is to enable reproducible and responsible research at scale and to bring more voices to the table in studying the impact of large language models. OPT-13B is a 13-billion-parameter model trained predominantly with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج لتحويل الجمل من اللغة الطبيعية إلى آخرى ولقياس الشبه بينها باستخدام تقنيات معالجة اللغات الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, install the sentence-transformers library using 'pip install -U sentence-transformers'.\\\\n2. Import the SentenceTransformer class from the sentence_transformers library.\\\\n3. Load the pre-trained model 'sentence-transformers/all-MiniLM-L12-v2' using the SentenceTransformer class.\\\\n4. Extract lines of dialogue from movie characters and store them as a list of sentences.\\\\n5. Encode the sentences using the model's 'encode' method, which will generate embeddings for each sentence.\\\\n6. Compare the embeddings using similarity measures like cosine similarity to analyze the relationship between characters based on their lines.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\n\\\\n# Example character lines\\\\ncharacter1_lines = [\\\"We have to do something!\\\", \\\"I think we should split up.\\\"]\\\\ncharacter2_lines = [\\\"We need a plan.\\\", \\\"It might be safer if we split up.\\\"]\\\\n\\\\n# Load the pre-trained model\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-MiniLM-L12-v2\\\\')\\\\n\\\\n# Encode the sentences\\\\ncharacter1_embeddings = model.encode(character1_lines)\\\\ncharacter2_embeddings = model.encode(character2_lines)\\\\n\\\\n# Calculate the similarity between character lines\\\\nsimilarity_matrix = cosine_similarity(character1_embeddings, character2_embeddings)\\\\nprint(similarity_matrix)', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/all-MiniLM-L12-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': '1,170,060,424 training pairs', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في تصنيف الفيديو؟? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the required libraries from the transformers package, including VideoMAEImageProcessor and VideoMAEForPreTraining. We also import numpy and torch packages.\\\\n2. We have a 100-frame video, and we'll sample 16 frames from it to be used as input to the model.\\\\n3. The VideoMAEImageProcessor is used to preprocess the frames and convert them into pixel values.\\\\n4. We then create an instance of the VideoMAEForPreTraining model by loading the pretrained model 'MCG-NJU/videomae-large'.\\\\n5. Next, we calculate the number of patches per frame and the sequence length.\\\\n6. We create a boolean mask for the masked patches and pass it to the model along with the pixel values.\\\\n7. The model will then process the video and provide an output that can be used to detect vegetables and fruit in the video.\\\", '<<<code>>>': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-large')\\\\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-large')\\\\n\\\\npixel_values = processor(video, return_tensors='pt').pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss\\\", 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'videomae-large', 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-large')\\\", 'api_arguments': 'pixel_values, bool_masked_pos', 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-large)\\\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-large)\\\\npixel_values = processor(video, return_tensors=pt).pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss', 'performance': {'dataset': 'Kinetics-400', 'accuracy': 'Not provided'}, 'description': 'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير منتج متعدد الوسائط للإجابة على أسئلة الوثائق في مجال معالجة اللغة الطبيعية، ونحن بحاجة لاستخدام البيانات المتوفرة لتنبؤ بالإجابات المناسبة.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create a translation pipeline for English to Chinese using the pre-trained model 'Helsinki-NLP/opus-mt-en-zh'. This model is trained on the OPUS dataset and is designed to translate English text into Chinese.\\\\n3. Use the translation pipeline to translate the quarterly review text into Chinese.\\\\n4. The translated text can then be shared with the board members as requested.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntranslator = pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\\\\nquarterly_review_chinese = translator(quarterly_review_text)\\\", 'api_call': \\\"pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-en-zh', 'api_call': \\\"pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\\\\ntranslated_text = translation('Hello, world!')\\\", 'performance': {'dataset': 'Tatoeba-test.eng.zho', 'accuracy': {'BLEU': 31.4, 'chr-F': 0.268}}, 'description': \\\"A translation model for English to Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model and trained on the OPUS dataset. The model requires a sentence initial language token in the form of '>>id<<' (id = valid target language ID).\\\" </s>\"\n"}
{"text": "<s>[INST]انشاء سيناريو قصير يبدأ بجملة \\\"كان يُحَكى، أيام طويلة مضت وفي أرض بعيدة\\\"، ويكمله مقدمة قصيرة بحدود 3 جمل تتحدث عن دور الـ Fill-Mask في مجال معالجة اللغة الطبيعية والترجمة.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"DebertaModel.from_pretrained('microsoft/deberta-v2-xlarge')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries. In this case, we will need Python's transformers library.\\\\n2. Load the large version of the DeBERTa model from the transformers library, which is trained on a range of natural language understanding tasks, using the 'microsoft/deberta-v2-xlarge' identifier.\\\\n3. Create a fill-mask pipeline, which uses the loaded DeBERTa v2 model to generate predictions given a context sentence.\\\\n4. Whenever there is a missing word in a sentence, replace it with the [MASK] token. The model will then generate the most appropriate word to fill the blank space based on the context.\\\", '<<<code>>>': 'from transformers import DebertaModel, fill_mask_pipeline\\\\nmodel = DebertaModel.from_pretrained(\\\\'microsoft/deberta-v2-xlarge\\\\')\\\\npipeline = fill_mask_pipeline(model=model)\\\\nmasked_sentence = \\\"The sun shines during the [MASK] and the moon appears at night.\\\"\\\\nfilled_sentence = pipeline(masked_sentence)', 'api_call': \\\"DebertaModel.from_pretrained('microsoft/deberta-v2-xlarge')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/deberta-v2-xlarge', 'api_call': \\\"DebertaModel.from_pretrained('microsoft/deberta-v2-xlarge')\\\", 'api_arguments': 'Mask token: [MASK]', 'python_environment_requirements': ['PyTorch, TensorFlow'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': [{'name': 'SQuAD 1.1', 'accuracy': '95.8/90.8'}, {'name': 'SQuAD 2.0', 'accuracy': '91.4/88.9'}, {'name': 'MNLI-m/mm', 'accuracy': '91.7/91.6'}, {'name': 'SST-2', 'accuracy': '97.5'}, {'name': 'QNLI', 'accuracy': '95.8'}, {'name': 'CoLA', 'accuracy': '71.1'}, {'name': 'RTE', 'accuracy': '93.9'}, {'name': 'MRPC', 'accuracy': '92.0/94.2'}, {'name': 'QQP', 'accuracy': '92.3/89.8'}, {'name': 'STS-B', 'accuracy': '92.9/92.9'}]}, 'description': 'DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data. This is the DeBERTa V2 xlarge model with 24 layers, 1536 hidden size. The total parameters are 900M and it is trained with 160GB raw data.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد صورة من خلال تقنيات الرؤية الحاسوبية بدون شرط. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the 'diffusers' package, which is required for the DiffusionPipeline.\\\\n2. Import the required library 'DiffusionPipeline' from 'diffusers'.\\\\n3. Use 'DiffusionPipeline.from_pretrained()' function to load the 'CompVis/ldm-celebahq-256' model, which is trained on the CelebA-HQ dataset for generating high-quality human face images.\\\\n4. Generate the image using the 'pipeline' object by specifying the number of inference steps. The higher the number of steps, the more accurate the image will be, but the generation process will take longer.\\\\n5. Save the generated image using the 'save()' method of the image object, which will save the image to the local storage.\\\\nNOTE: This process will provide a unique, realistic image of a human face not matching any specific person or set of features.\\\", '<<<code>>>': \\\"!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = 'CompVis/ldm-celebahq-256'\\\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\\\ngenerated_image = pipeline(num_inference_steps=200)\\\\ngenerated_image[0].save('ldm_generated_image.png')\\\", 'api_call': \\\"DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'CompVis/ldm-celebahq-256', 'api_call': \\\"DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')\\\", 'api_arguments': ['model_id'], 'python_environment_requirements': [['diffusers']], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = CompVis/ldm-celebahq-256\\\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = pipeline(num_inference_steps=200)[sample]\\\\nimage[0].save(ldm_generated_image.png)', 'performance': {'dataset': 'CelebA-HQ', 'accuracy': 'N/A'}, 'description': 'Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج تصنيف جدولي لتصنيف البيانات، ونريد التنبؤ بفئة البيانات بناءً على بعض مجموعات البيانات المتوفرة.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. First, we import the necessary libraries, which are 'json', 'joblib', and 'pandas'.\\\\n2. We then use the joblib.load function to load the pre-trained model 'model.joblib'. This model has been trained for multi-class classification tasks, which is exactly what we need for predicting the popularity of movies.\\\\n3. We read the multiple features from a CSV file ('data.csv'), which contains information about various movie-related factors. These features are used by the model to make a prediction.\\\\n4. The predictions are made using the 'predict' function, and the popularity category is inferred from the results.\\\", '<<<code>>>': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Alexei1/imdb', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['json', 'joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'IMDB', 'accuracy': 0.487}, 'description': 'A tabular classification model trained using AutoTrain for sentiment analysis on the IMDB dataset. The model has a CO2 emission of 0.0186 grams and an accuracy of 0.487.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير برنامج يستخدم تقنيات معالجة اللغة الطبيعية لتحويل النص إلى كلام لدعم مستخدمي تطبيق يستهدف أفراد ذوي الإعاقة البصرية.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the transformers library provided by Hugging Face.\\\\n2. Create a model by loading a pretrained BERT model, \\\"neuralmind/bert-base-portuguese-cased\\\", specific for Brazilian Portuguese language.\\\\n3. Create a \\\\'fill-mask\\\\' pipeline, which will be fine-tuned on the downstream task to identify the missing word in a given context.\\\\n4. Run prediction on the given sentence \\\"Tinha uma [MASK] no meio do caminho.\\\" to predict the missing word.', '<<<code>>>': \\\"from transformers import pipeline, AutoModelForPreTraining, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\\\\nmodel = AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\\\\npipe = pipeline('fill-mask', model=model, tokenizer=tokenizer)\\\\nresult = pipe('Tinha uma [MASK] no meio do caminho.')\\\", 'api_call': \\\"AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'neuralmind/bert-base-portuguese-cased', 'api_call': \\\"AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'neuralmind/bert-base-portuguese-cased'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\npipe = pipeline('fill-mask', model=model, tokenizer=tokenizer)\\\\npipe('Tinha uma [MASK] no meio do caminho.')\\\", 'performance': {'dataset': 'brWaC', 'accuracy': 'state-of-the-art'}, 'description': 'BERTimbau Base is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.' </s>\"\n"}
{"text": "<s>[INST]نريد بناء نموذج لتصنيف الصوتيات بناءً على بعض مجموعات البيانات المتوفرة، وذلك لتمكيننا من تحديد تصنيف الصوتيات بشكل فعال ودقيق.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries EncoderClassifier from speechbrain.pretrained and torchaudio.\\\\n2. Initialize the EncoderClassifier model using the trained model provided at source 'TalTechNLP/voxlingua107-epaca-tdnn'. This model is trained to recognize speech in 107 different languages.\\\\n3. Load the audio file using the model's load_audio function.\\\\n4. Use the model's classify_batch function to classify the audio signal into a language.\\\\n5. The results will contain the predicted language of the spoken audio in the file.\\\", '<<<code>>>': \\\"import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\nlanguage_id = EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\\nsignal = language_id.load_audio('<your_audio_file_url>')\\\\nprediction = language_id.classify_batch(signal)\\\\nprint(prediction)\\\", 'api_call': \\\"EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'TalTechNLP/voxlingua107-epaca-tdnn', 'api_call': \\\"EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\", 'api_arguments': ['signal'], 'python_environment_requirements': [['speechbrain', 'torchaudio']], 'example_code': 'import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\nlanguage_id = EncoderClassifier.from_hparams(source=TalTechNLP/voxlingua107-epaca-tdnn, savedir=tmp)\\\\nsignal = language_id.load_audio(https://omniglot.com/soundfiles/udhr/udhr_th.mp3)\\\\nprediction = language_id.classify_batch(signal)\\\\nprint(prediction)', 'performance': {'dataset': 'VoxLingua107', 'accuracy': '93%'}, 'description': 'This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain. The model uses the ECAPA-TDNN architecture that has previously been used for speaker recognition. The model can classify a speech utterance according to the language spoken. It covers 107 different languages.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لتصنيف الرموز في معالجة اللغة الطبيعية باستخدام تقنيات تعلم الآلة مثل الشبكات العصبونية العميقة.? [/INST]{'domain': 'Natural Language Processing Token Classification', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('xlm-roberta-large-finetuned-conll03-english')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the necessary classes from the transformers library, such as AutoModelForTokenClassification and AutoTokenizer.', 'code': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'xlm-roberta-large-finetuned-conll03-english\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'xlm-roberta-large-finetuned-conll03-english\\\\')\\\\nuser_review = \\\"User review here...\\\"\\\\nencoded_input = tokenizer(user_review, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**encoded_input)\\\\n# Process output to identify keywords related to money, location, service, and product quality', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Transformers', 'functionality': 'Token Classification', 'api_name': 'xlm-roberta-large-finetuned-conll03-english', 'api_call': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'xlm-roberta-large-finetuned-conll03-english\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'xlm-roberta-large-finetuned-conll03-english\\\\')\\\\nuser_review = \\\"User review here...\\\"\\\\nencoded_input = tokenizer(user_review, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**encoded_input)\\\\n# Process output to identify keywords related to money, location, service, and product quality', 'api_arguments': {'model': 'AutoModelForTokenClassification.from_pretrained(xlm-roberta-large-finetuned-conll03-english)', 'tokenizer': 'AutoTokenizer.from_pretrained(xlm-roberta-large-finetuned-conll03-english)'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"classifier(Hello I'm Omar and I live in Zürich.)\\\", 'performance': {'dataset': 'conll2003', 'accuracy': 'More information needed'}, 'description': 'The XLM-RoBERTa model is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data. This model is XLM-RoBERTa-large fine-tuned with the conll2003 dataset in English. It can be used for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging.'}} </s>\"\n"}
{"text": "<s>[INST]نقدم إرشادات لتوليد نموذج يمكنه تقدير العمق في مجال \\\"رؤية الحاسوب\\\". هل تحتاج إلى مساعدة في تنفيذ هذه العملية؟? [/INST]{'domain': 'Computer Vision Depth Estimation', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. First, ensure that you have the required Python packages installed: transformers>=4.24.0, pytorch>=1.12.1, and tokenizers>=0.13.2.', 'code': \\\"from transformers import AutoModel\\\\nimport torch\\\\n# Load the pre-trained model\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\\\\n# Prepare input image tensor and pass it through the model\\\\ninput_image = torch.tensor(input_image_data).unsqueeze(0) # Assuming input_image_data is a properly formatted input image tensor\\\\ndepth_map = model(input_image)[0]\\\", 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-082237', 'api_call': \\\"from transformers import AutoModel\\\\nimport torch\\\\n# Load the pre-trained model\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\\\\n# Prepare input image tensor and pass it through the model\\\\ninput_image = torch.tensor(input_image_data).unsqueeze(0) # Assuming input_image_data is a properly formatted input image tensor\\\\ndepth_map = model(input_image)[0]\\\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3421, 'Mae': 0.27, 'Rmse': 0.4042, 'Abs Rel': 0.3279, 'Log Mae': 0.1132, 'Log Rmse': 0.1688, 'Delta1': 0.5839, 'Delta2': 0.8408, 'Delta3': 0.9309}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.'}} </s>\"\n"}
{"text": "<s>[INST]عليك تطوير برنامج في مجال معالجة اللغة الطبيعية وتصنيف النصوص بدون حاجة لتصنيفات سابقة. نحتاج إلى حل يمكنه تحويل النصوص إلى تصنيفات من غير تدريب مسبق لمستخدمي التطبيق.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('Recognai/bert-base-spanish-wwm-cased-xnli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library.\\\\n2. Use the pipeline function to create a zero-shot classification model, specifying the model 'Recognai/bert-base-spanish-wwm-cased-xnli'. This model is fine-tuned on the Spanish portion of the XNLI dataset and can be used for zero-shot classification tasks.\\\\n3. Prepare the news excerpt, candidate labels (such as 'culture', 'society', 'economy', 'health', 'sports'), and hypothesis template ('Este ejemplo es {}').\\\\n4. Use the classifier to determine the most relevant topics in the given news excerpt.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nspanish_classifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'Recognai/bert-base-spanish-wwm-cased-xnli\\\\')\\\\nnews_excerpt = \\\"El autor se perfila, a los 50 años de su muerte, como uno de los grandes de su siglo...\\\"\\\\ncandidate_labels = [\\\\'cultura\\\\', \\\\'sociedad\\\\', \\\\'economia\\\\', \\\\'salud\\\\', \\\\'deportes\\\\']\\\\nhypothesis_template = \\\"Este ejemplo es {}.\\\"\\\\nresult = spanish_classifier(news_excerpt, candidate_labels, hypothesis_template)', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('Recognai/bert-base-spanish-wwm-cased-xnli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Recognai/bert-base-spanish-wwm-cased-xnli', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('Recognai/bert-base-spanish-wwm-cased-xnli')\\\", 'api_arguments': ['sequence', 'candidate_labels', 'hypothesis_template'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=Recognai/bert-base-spanish-wwm-cased-xnli)\\\\nclassifier(\\\\nEl autor se perfila, a los 50 años de su muerte, como uno de los grandes de su siglo,\\\\ncandidate_labels=[cultura, sociedad, economia, salud, deportes],\\\\nhypothesis_template=Este ejemplo es {}. \\\\n)', 'performance': {'dataset': 'XNLI-es', 'accuracy': '79.9%'}, 'description': 'This model is a fine-tuned version of the spanish BERT model with the Spanish portion of the XNLI dataset. You can have a look at the training script for details of the training.' </s>\"\n"}
{"text": "<s>[INST]يتعلق هذا الدليل بتقنية معالجة اللغة الطبيعية NLP في مجال مماثلة الجمل. وهناك حاجة لبناء نموذج يمكنه الاستفادة من البيانات المتاحة لتنبؤ وتحليل الجمل المتشابهة.? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from sentence_transformers import SentenceTransformer\\\\nabstracts = [\\\"Abstract 1...\\\", \\\"Abstract 2...\\\", \\\"Abstract 3...\\\"]\\\\n# Replace with the actual abstract texts\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/bert-base-nli-mean-tokens\\\\')\\\\nembeddings = model.encode(abstracts)\\\\n# Use clustering techniques to group the embeddings based on similarity', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Sentence Embeddings', 'api_name': 'sentence-transformers/bert-base-nli-mean-tokens', 'api_call': 'from sentence_transformers import SentenceTransformer\\\\nabstracts = [\\\"Abstract 1...\\\", \\\"Abstract 2...\\\", \\\"Abstract 3...\\\"]\\\\n# Replace with the actual abstract texts\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/bert-base-nli-mean-tokens\\\\')\\\\nembeddings = model.encode(abstracts)\\\\n# Use clustering techniques to group the embeddings based on similarity', 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}} </s>\"\n"}
{"text": "<s>[INST]يتعين عليك تشغيل التطبيق لتحويل الصوت إلى صوت آخر. يمكنك تحديد التأثيرات الصوتية وضبط مستوى الصوت حسب رغبتك.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and modules such as transformers, numpy, and soundfile.\\\\n2. Load the model 'microsoft/speecht5_vc' using the from_pretrained method of the SpeechT5ForSpeechToSpeech class.\\\\n3. Load the speaker embeddings to a numpy array and convert it to a torch tensor.\\\\n4. Process the input audio data with the loaded model and the provided speaker embeddings.\\\\n5. Use the generated speech output from the model and save it as a new audio file adjusted to the desired voice.\\\", '<<<code>>>': \\\"from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\\\\nimport numpy as np\\\\nimport torch\\\\nimport soundfile as sf\\\\n\\\\nexample_audio_path = 'path_to_audio_file.wav'\\\\n# replace with the path to the input audio file\\\\nsampling_rate = 16000\\\\nexample_speech, _ = sf.read(example_audio_path)\\\\n\\\\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_vc')\\\\nmodel = SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\\\\n\\\\ninputs = processor(audio=example_speech, sampling_rate=sampling_rate, return_tensors='pt')\\\\n\\\\nspeaker_embeddings = np.load('xvector_speaker_embedding.npy')\\\\nspeaker_embeddings = torch.tensor(speaker_embeddings).unsqueeze(0)\\\\n\\\\nspeech = model.generate_speech(inputs['input_values'], speaker_embeddings, vocoder=vocoder)\\\\n\\\\nsf.write('output_speech.wav', speech.numpy(), samplerate=16000)\\\", 'api_call': \\\"SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/speecht5_vc', 'api_call': \\\"SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\", 'api_arguments': {'audio': 'example_speech', 'sampling_rate': 'sampling_rate', 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'datasets', 'numpy', 'torch', 'soundfile']], 'example_code': \\\"from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset('hf-internal-testing/librispeech_asr_demo', 'clean', split='validation')\\\\ndataset = dataset.sort('id')\\\\nsampling_rate = dataset.features['audio'].sampling_rate\\\\nexample_speech = dataset[0]['audio']['array']\\\\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_vc')\\\\nmodel = SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\\\\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\\\\ninputs = processor(audio=example_speech, sampling_rate=sampling_rate, return_tensors='pt')\\\\nimport numpy as np\\\\nimport torch\\\\nspeaker_embeddings = np.load('xvector_speaker_embedding.npy')\\\\nspeaker_embeddings = torch.tensor(speaker_embeddings).unsqueeze(0)\\\\nspeech = model.generate_speech(inputs['input_values'], speaker_embeddings, vocoder=vocoder)\\\\nimport soundfile as sf\\\\nsf.write('speech.wav', speech.numpy(), samplerate=16000)\\\", 'performance': {'dataset': 'CMU ARCTIC', 'accuracy': 'Not specified'}, 'description': 'SpeechT5 model fine-tuned for voice conversion (speech-to-speech) on CMU ARCTIC. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. It is designed to improve the modeling capability for both speech and text. This model can be used for speech conversion tasks.' </s>\"\n"}
{"text": "<s>[INST]يقوم فريقنا بتطوير نموذج متعدد الوسائط يستخدم الصور والنصوص لتحويل الصور إلى نص، ونحن نهدف إلى تحسين أداء النموذج من خلال توظيف تقنيات حديثة في معالجة الصور والنصوص.? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"pipeline('image-to-text', model='microsoft/git-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the pipeline function from the transformers library provided by Hugging Face.', 'code': \\\"from transformers import pipeline\\\\nimport PIL.Image\\\\n\\\\nimage_captioning = pipeline('image-to-text', model='microsoft/git-base')\\\\nimage_path = 'fruit_and_veg_stands.jpg'\\\\nimage = PIL.Image.open(image_path)\\\\ncaption = image_captioning(image)[0]['text']\\\\n\\\\nprint(caption)\\\", 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \\\"from transformers import pipeline\\\\nimport PIL.Image\\\\n\\\\nimage_captioning = pipeline('image-to-text', model='microsoft/git-base')\\\\nimage_path = 'fruit_and_veg_stands.jpg'\\\\nimage = PIL.Image.open(image_path)\\\\ncaption = image_captioning(image)[0]['text']\\\\n\\\\nprint(caption)\\\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \\\"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\\\"}} </s>\"\n"}
{"text": "<s>[INST]سيقوم النظام بتصنيف عمق الصور باستخدام تقنية \\\"خوارزميات التصنيف الصوري بدون استخدام البيانات\\\"، دون الحاجة لتدريب مسبق على مجموعة بيانات محددة. يرجى التأكد من تقديم صورة واضحة وعالية الجودة للنظام للحصول على أفضل نتائج.? [/INST]{'domain': 'Computer Vision Depth Estimation', 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import required libraries, in this case, 'pipeline' function from transformers package.\\\", 'code': \\\"from transformers import pipeline\\\\ndepth_estimator = pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\\\\n# assuming image is already loaded\\\\n# depth_map = depth_estimator(image)\\\", 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-113853', 'api_call': \\\"from transformers import pipeline\\\\ndepth_estimator = pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\\\\n# assuming image is already loaded\\\\n# depth_map = depth_estimator(image)\\\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3384, 'Mae': 0.2739, 'Rmse': 0.3959, 'Abs Rel': 0.323, 'Log Mae': 0.1148, 'Log Rmse': 0.1651, 'Delta1': 0.5576, 'Delta2': 0.8345, 'Delta3': 0.9398}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.'}} </s>\"\n"}
{"text": "<s>[INST]لإنشاء نموذج جديد لتحويل الصوت إلى صوت باستخدام API الصوتية، قم ببرمجة نظام مبتكر للتحويل الصوتي الصوتي. هذا النظام يجب أن يكون قادراً على فحص البيانات الصوتية بشكل فعال وتحليلها بدقة لتحويل النص الصوتي إلى نمط صوتي جديد وفعال. كن مبدعاً واستخدم التكنولوجيا البيولوجية لإنشاء أداة مبتكرة وذكية تقوم بتوليد النص الصوتي بدقة وسلاسة لتلبية احتياجات المستخدمين في مجال تحويل الصوت إلى صوت.? [/INST]{'domain': 'Audio Audio-to-Audio', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk')\\\", 'api_provider': 'Fairseq', 'explanation': '1. Import necessary packages including fairseq, torchaudio, and huggingface_hub.', 'code': '', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Fairseq', 'functionality': 'speech-to-speech-translation', 'api_name': 'xm_transformer_s2ut_en-hk', 'api_call': '', 'api_arguments': {'arg_overrides': {'config_yaml': 'config.yaml', 'task': 'speech_to_text'}, 'cache_dir': 'cache_dir'}, 'python_environment_requirements': ['fairseq', 'huggingface_hub', 'torchaudio'], 'example_code': {'import_modules': ['import json', 'import os', 'from pathlib import Path', 'import IPython.display as ipd', 'from fairseq import hub_utils', 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub', 'from fairseq.models.speech_to_text.hub_interface import S2THubInterface', 'from fairseq.models.text_to_speech import CodeHiFiGANVocoder', 'from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface', 'from huggingface_hub import snapshot_download', 'import torchaudio'], 'load_model': [\\\"cache_dir = os.getenv('HUGGINGFACE_HUB_CACHE')\\\", \\\"models, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk', arg_overrides={'config_yaml': 'config.yaml', 'task': 'speech_to_text'}, cache_dir=cache_dir)\\\", 'model = models[0].cpu()', \\\"cfg['task'].cpu = True\\\"], 'generate_prediction': ['generator = task.build_generator([model], cfg)', \\\"audio, _ = torchaudio.load('/path/to/an/audio/file')\\\", 'sample = S2THubInterface.get_model_input(task, audio)', 'unit = S2THubInterface.get_prediction(task, model, generator, sample)'], 'speech_synthesis': [\\\"library_name = 'fairseq'\\\", \\\"cache_dir = (cache_dir or (Path.home() / '.cache' / library_name).as_posix())\\\", \\\"cache_dir = snapshot_download('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS', cache_dir=cache_dir, library_name=library_name)\\\", \\\"x = hub_utils.from_pretrained(cache_dir, 'model.pt', '.', archive_map=CodeHiFiGANVocoder.hub_models(), config_yaml='config.json', fp16=False, is_vocoder=True)\\\", \\\"with open(f'{x['args']['data']}/config.json') as f:\\\", '  vocoder_cfg = json.load(f)', \\\"assert (len(x['args']['model_path']) == 1), 'Too many vocoder models in the input'\\\", \\\"vocoder = CodeHiFiGANVocoder(x['args']['model_path'][0], vocoder_cfg)\\\", 'tts_model = VocoderHubInterface(vocoder_cfg, vocoder)', 'tts_sample = tts_model.get_model_input(unit)', 'wav, sr = tts_model.get_prediction(tts_sample)', 'ipd.Audio(wav, rate=sr)']}, 'performance': {'dataset': 'MuST-C', 'accuracy': 'Not specified'}, 'description': 'Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain.'}} </s>\"\n"}
{"text": "<s>[INST]عند استخدام تقنية الـ Natural Language Processing، أنشئ قصة قصيرة تبدأ بـ \\\"كان ياما كان في أرض بعيدة\\\" واملأ الفجوة بكلمة مناسبة.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. Import the necessary library, which is \\\\'pipeline\\\\' from transformers.\\\\n2. Use the \\\\'pipeline\\\\' function to load the model \\\\'bert-base-cased\\\\'.\\\\n3. Call the unmasker object with the input sentence containing the MASK token, \\\"Can you structure this article on [MASK] design trends?\\\".\\\\n4. The model uses masked language modeling to fill in the missing word in the given sentence with the most likely prediction.', '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'bert-base-cased\\\\')\\\\nfilled_sentence = unmasker(\\\"Can you structure this article on [MASK] design trends?\\\")', 'api_call': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-base-cased', 'api_call': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", 'api_arguments': ['model'], 'python_environment_requirements': [['transformers']], 'example_code': ['from transformers import pipeline', \\\"unmasker = pipeline('fill-mask', model='bert-base-cased')\\\", \\\"unmasker(Hello I'm a [MASK] model.)\\\"], 'performance': {'dataset': 'GLUE', 'accuracy': 79.6}, 'description': \\\"BERT base model (cased) is a pre-trained transformer model on English language using a masked language modeling (MLM) objective. It was introduced in a paper and first released in a repository. This model is case-sensitive, which means it can differentiate between 'english' and 'English'. The model can be used for masked language modeling or next sentence prediction, but it's mainly intended to be fine-tuned on a downstream task.\\\" </s>\"\n"}
{"text": "<s>[INST]عميلنا يحتاج إلى تطوير نظام تصنيف الصوت. نحن بحاجة إلى برنامج يمكنه تصنيف الصوتيات بدقة وفعالية عالية.? [/INST]{'domain': 'Audio Audio Classification', 'api_call': \\\"foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\", 'api_provider': 'SpeechBrain', 'explanation': \\\"1. Import the necessary library 'foreign_class' from speechbrain.pretrained.interfaces.\\\", 'code': \\\"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\\naudio_file = 'path/to/audio/file'\\\\nout_prob, score, index, emotion = classifier.classify_file(audio_file)\\\\nprint(emotion)\\\", 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'SpeechBrain', 'functionality': 'Emotion Recognition', 'api_name': 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP', 'api_call': \\\"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\\naudio_file = 'path/to/audio/file'\\\\nout_prob, score, index, emotion = classifier.classify_file(audio_file)\\\\nprint(emotion)\\\", 'api_arguments': ['file_path'], 'python_environment_requirements': 'pip install speechbrain', 'example_code': \\\"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\\nout_prob, score, index, text_lab = classifier.classify_file('speechbrain/emotion-recognition-wav2vec2-IEMOCAP/anger.wav')\\\\nprint(text_lab)\\\", 'performance': {'dataset': 'IEMOCAP', 'accuracy': '78.7%'}, 'description': 'This repository provides all the necessary tools to perform emotion recognition with a fine-tuned wav2vec2 (base) model using SpeechBrain. It is trained on IEMOCAP training data.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير منتج في مجال معالجة اللغة الطبيعية لقياس تشابه الجمل، ونريد تنبؤ درجة التشابه بين الجمل بناءً على بيانات متوفرة.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we need to install sentence-transformers and import SentenceTransformer class. \\\\n2. Then, create an instance of the SentenceTransformer with the model 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'. This model is designed to map sentences to 384-dimensional dense vectors, which are suitable for identifying similar sentences or phrases.\\\\n3. Using the model's encode function, we can easily convert a list of user phrases into their corresponding embeddings.\\\\n4. These embeddings can then be used to compare and identify similar phrases in real time, which can be useful for the chatbot.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nuser_phrases = [\\\"I want to buy a shirt\\\", \\\"I\\\\'d like to purchase a blouse\\\"]\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\\\\')\\\\nembeddings = model.encode(user_phrases)', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]نحن شركة متخصصة في تطوير التقنيات المتعددة الوسائط لتحويل النصوص إلى صور ونرغب في تقدير عمق الأشياء في الصور التي تم إنشاؤها، خاصة في مجال معالجة اللغات الطبيعية والترجمة. نطمح إلى إنشاء نظام يمكنه فهم النصوص والصور بشكل متكامل لتسهيل استجابته لأسئلة الوثائق المتعددة.? [/INST]{'domain': 'Multimodal Document Question Answer', 'api_call': \\\"pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\", 'code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\\\nquestion = 'What does the shipping manifest say?'\\\\ndocument = 'path/to/your/image/file.png'\\\\nresult = qa_pipeline(question=question, document=document)\\\\nshipping_manifest_text = result['answer']\\\", 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\\\nquestion = 'What does the shipping manifest say?'\\\\ndocument = 'path/to/your/image/file.png'\\\\nresult = qa_pipeline(question=question, document=document)\\\\nshipping_manifest_text = result['answer']\\\", 'api_arguments': {'model': 'hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa'}, 'python_environment_requirements': {'transformers': '4.27.4', 'pytorch': '2.0.0+cu117', 'datasets': '2.11.0', 'tokenizers': '0.13.2'}, 'example_code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\\\nquestion = 'What is the total amount?'\\\\ndocument = 'path/to/your/image/file.png'\\\\nresult = qa_pipeline(question=question, document=document)\\\\nprint(result)\\\", 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.843}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.'}} </s>\"\n"}
{"text": "<s>[INST]الرجاء تحويل مقطع الفيديو إلى تصنيف فيديو آخر بواسطة التصوير الحاسوبي، مع الحفاظ على دقة التصنيف وجودة الفيديو في مجال معالجة اللغة الطبيعية والتصنيف بدون تدريب.? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"pipeline('zero-shot-classification', model='MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. First, import the required libraries, specifically 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\\\\')\\\\nsequence_to_classify = \\\"Researchers have recently discovered a new strain of the flu virus that poses a potential threat to global health.\\\"\\\\ncandidate_labels = [\\\\'health\\\\', \\\\'business\\\\', \\\\'sports\\\\', \\\\'entertainment\\\\']\\\\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\\\\nprint(output)', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Classification', 'api_name': 'MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli', 'api_call': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\\\\')\\\\nsequence_to_classify = \\\"Researchers have recently discovered a new strain of the flu virus that poses a potential threat to global health.\\\"\\\\ncandidate_labels = [\\\\'health\\\\', \\\\'business\\\\', \\\\'sports\\\\', \\\\'entertainment\\\\']\\\\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\\\\nprint(output)', 'api_arguments': ['sequence_to_classify', 'candidate_labels', 'multi_label'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli)\\\\nsequence_to_classify = Angela Merkel is a politician in Germany and leader of the CDU\\\\ncandidate_labels = [politics, economy, entertainment, environment]\\\\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\\\\nprint(output)', 'performance': {'dataset': [{'name': 'mnli_test_m', 'accuracy': 0.912}, {'name': 'mnli_test_mm', 'accuracy': 0.908}, {'name': 'anli_test', 'accuracy': 0.702}, {'name': 'anli_test_r3', 'accuracy': 0.64}, {'name': 'ling_test', 'accuracy': 0.87}, {'name': 'wanli_test', 'accuracy': 0.77}]}, 'description': 'This model was fine-tuned on the MultiNLI, Fever-NLI, Adversarial-NLI (ANLI), LingNLI and WANLI datasets, which comprise 885 242 NLI hypothesis-premise pairs. This model is the best performing NLI model on the Hugging Face Hub as of 06.06.22 and can be used for zero-shot classification. It significantly outperforms all other large models on the ANLI benchmark.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نموذج تصنيف جدولي بشكل صفري لمعالجة اللغة الطبيعية. هل يمكنك مساعدتي في ذلك؟? [/INST]{'domain': 'Tabular Tabular Classification', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_provider': 'Joblib', 'explanation': '1. First, import the necessary libraries, including json, joblib, and pandas.', 'code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\n\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\n\\\\npredictions = model.predict(data)\\\", 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Joblib', 'functionality': 'Carbon Emissions', 'api_name': 'omarques/autotrain-in-class-test-demo-1659958767', 'api_call': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\n\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\n\\\\npredictions = model.predict(data)\\\", 'api_arguments': ['data'], 'python_environment_requirements': ['json', 'joblib', 'pandas'], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'omarques/autotrain-data-in-class-test-demo', 'accuracy': 0.983}, 'description': 'A model trained for binary classification of carbon emissions using AutoTrain.'}} </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى برمجية تستخدم تقنيات الرؤية الحاسوبية لتقدير العمق الصوتي. عميلنا يطوّر تطبيقاً لتصنيف الملفات الصوتية. نحتاج إلى بشكل خاص برنامج يمكنه تحليل الصور وتقدير العمق الصوتي لمساعدة مستخدمي التطبيق في تصنيف الصوتيات بشكل دقيق وفعال.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"pipeline('audio-classification', model='mazkooleg/0-9up-wavlm-base-plus-ft')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'mazkooleg/0-9up-wavlm-base-plus-ft'.\\\\n3. The loaded model will be used for audio classification, which is a task where the model makes a prediction for a given audio sample. In this case, we want to predict the action based on the user's voice command.\\\\n4. We will provide the voice command as input and the model will classify it, allowing the home automation system to execute the appropriate action.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\naudio_classifier = pipeline('audio-classification', model='mazkooleg/0-9up-wavlm-base-plus-ft')\\\\n# Replace 'audio_file_path.wav' with the path to your audio file\\\\naudio_categories = ['lights_on', 'lights_off', 'increase_temp', 'decrease_temp']\\\\npredictions = audio_classifier('audio_file_path.wav', categories = audio_categories)\\\", 'api_call': \\\"pipeline('audio-classification', model='mazkooleg/0-9up-wavlm-base-plus-ft')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mazkooleg/0-9up-wavlm-base-plus-ft', 'api_call': \\\"pipeline('audio-classification', model='mazkooleg/0-9up-wavlm-base-plus-ft')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers==4.27.3, torch==1.11.0, datasets==2.10.1, tokenizers==0.12.1'], 'example_code': '', 'performance': {'dataset': 'mazkooleg/0-9up_google_speech_commands_augmented_raw', 'accuracy': 0.9973}, 'description': 'This model is a fine-tuned version of microsoft/wavlm-base-plus on the None dataset. It achieves the following results on the evaluation set: Loss: 0.0093, Accuracy: 0.9973.' </s>\"\n"}
{"text": "<s>[INST]يُرجى تحديد الصورة التي ترغب في تقسيمها إلى عناصر مختلفة باستخدام تقنيات رؤية الحاسوب، ثم قم بتنفيذ عملية تقسيم الصورة.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Firstly, you need to import the necessary classes from the transformers and PIL packages (OneFormerForUniversalSegmentation for image segmentation, OneFormerProcessor for pre-processing and post-processing, and Image for opening image files).\\\\n2. Then, you need to create an instance of the OneFormerForUniversalSegmentation model using the from_pretrained function and passing the 'shi-labs/oneformer_ade20k_swin_large' as the pre-trained model name.\\\\n3. Load the image data you need to process (e.g. from a file or stream).\\\\n4. The model then can be used to segment the image into its semantic constituents (trees, cars, buildings, etc.).\\\", '<<<code>>>': \\\"from transformers import OneFormerForUniversalSegmentation\\\\nfrom transformers import OneFormerProcessor\\\\nfrom PIL import Image\\\\nimage = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with path to your image\\\\nprocessor = OneFormerProcessor.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\\\\nsemantic_inputs = processor(images=image, task_inputs=['semantic'], return_tensors='pt')\\\\nsemantic_outputs = model(**semantic_inputs)\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(semantic_outputs, target_sizes=[image.size[::-1]])[0]\\\", 'api_call': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'shi-labs/oneformer_ade20k_swin_large', 'api_call': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\\\", 'api_arguments': ['images', 'task_inputs', 'return_tensors'], 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\\\nsemantic_outputs = model(**semantic_inputs)\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'scene_parse_150', 'accuracy': None}, 'description': 'OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>\"\n"}
{"text": "<s>[INST]تقوم الشركة بتطوير نظام لإنتاج نصوص في مجال معالجة اللغة الطبيعية لتصنيف النصوص بدون تدريب، ونحتاج إلى بناء نموذج يستند إلى البيانات المتوفرة لتحقيق ذلك.? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"CrossEncoder('cross-encoder/nli-deberta-v3-small')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-small')\\\\nsentence_pairs = [('The sky is blue', 'The sky color is blue'), ('A man is walking', 'A male is strolling')]\\\\nmatching_scores = model.predict(sentence_pairs)\\\", 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Classification', 'api_name': 'cross-encoder/nli-deberta-v3-small', 'api_call': \\\"from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-small')\\\\nsentence_pairs = [('The sky is blue', 'The sky color is blue'), ('A man is walking', 'A male is strolling')]\\\\nmatching_scores = model.predict(sentence_pairs)\\\", 'api_arguments': ['sentence1', 'sentence2'], 'python_environment_requirements': ['sentence_transformers', 'transformers'], 'example_code': \\\"from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-small')\\\\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])\\\", 'performance': {'dataset': {'SNLI-test': '91.65', 'MNLI-mismatched': '87.55'}, 'accuracy': {'SNLI-test': '91.65', 'MNLI-mismatched': '87.55'}}, 'description': 'Cross-Encoder for Natural Language Inference based on microsoft/deberta-v3-small, trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام للكشف عن الأجسام بواسطة الكمبيوتر يعتمد على الرؤية الحاسوبية، ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import required libraries 'DetrForObjectDetection' from transformers, 'Image' from PIL, and 'requests'.\\\\n2. Define the URL of the image that you want to detect objects in.\\\\n3. Load the image from the URL using the 'requests' library and the 'PIL.Image.open()' method.\\\\n4. Load the pretrained 'DETR ResNet-101 DC5' model from Facebook Research and enable the model for object detection using 'DetrForObjectDetection.from_pretrained()'.\\\\n5. Create an instance of the 'DetrFeatureExtractor' class, which is responsible for preprocessing the image.\\\\n6. Preprocess the image by calling 'feature_extractor()' on the image.\\\\n7. Pass the preprocessed image to the 'DETR' model to get the logits and bounding box predictions.\\\\n8. Process the output to extract the detected objects and their respective bounding boxes.\\\", '<<<code>>>': \\\"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://example.com/image.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\n\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\", 'api_call': \\\"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/detr-resnet-101-dc5', 'api_call': \\\"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': \\\"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': 'AP 44.9'}, 'description': 'DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.' </s>\"\n"}
{"text": "<s>[INST]قم بتدريب نموذج التصنيف البصري على مجموعة بيانات الصور باستخدام تقنيات الرؤية الحاسوبية.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Begin by importing the necessary libraries for this task, which include 'pipeline' from the transformers package and 'Image' from the PIL package.\\\\n2. Use the 'pipeline' function from transformers to load the pre-trained model, 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k' for image classification tasks.\\\\n3. Open the target image, an animal's image, using the PIL 'Image' class.\\\\n4. Use the loaded model to predict the animal species present in the image.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nfrom PIL import Image\\\\nclassifier = pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\\nimage = Image.open('<path_to_animal_image>')\\\\nprediction = classifier(image)\\\\nprint(prediction)\\\", 'api_call': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'api_call': \\\"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\\\", 'api_arguments': {'model': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'framework': 'pt'}, 'python_environment_requirements': [['transformers', 'torch']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.' </s>\"\n"}
{"text": "<s>[INST]يتوجب عليك تفعيل ترجمة النصوص المكانية لتشغيل نموذج التعلم الآلي لمعالجة اللغات الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"ORTModelForSeq2SeqLM.from_pretrained('optimum/t5-small')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'pipeline', 'AutoTokenizer', and 'ORTModelForSeq2SeqLM' from transformers and optimum.onnxruntime.\\\\n2. Use 'AutoTokenizer' to load the tokenizer for the 'optimum/t5-small' model.\\\\n3. Load the model, 'optimum/t5-small', using 'ORTModelForSeq2SeqLM.from_pretrained()'.\\\\n4. Create a translation pipeline by passing the target translation task, model, and tokenizer as arguments. In this case, the task is 'translation_en_to_fr'.\\\\n5. Use the created translator pipeline to process the English input text and obtain the translated French output.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'optimum/t5-small\\\\')\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(\\\\'optimum/t5-small\\\\')\\\\ntranslator = pipeline(\\\\'translation_en_to_fr\\\\', model=model, tokenizer=tokenizer)\\\\nenglish_text = \\\"My name is Eustache and I have a pet raccoon.\\\"\\\\nresults = translator(english_text)\\\\nprint(results[0][\\\\'translation_text\\\\'])', 'api_call': \\\"ORTModelForSeq2SeqLM.from_pretrained('optimum/t5-small')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'optimum/t5-small', 'api_call': \\\"ORTModelForSeq2SeqLM.from_pretrained('optimum/t5-small')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers', 'optimum.onnxruntime']], 'example_code': 'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\\\nresults = translator(My name is Eustache and I have a pet raccoon)\\\\nprint(results)', 'performance': {'dataset': 'c4', 'accuracy': 'N/A'}, 'description': 'T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.' </s>\"\n"}
{"text": "<s>[INST]يجب على العميل تطوير تطبيق لتحويل النصوص إلى صوت بتقنيات معالجة اللغة الطبيعية. يتطلب ذلك إنشاء برنامج يمكنه تحويل النصوص إلى كلام مسموع بشكل تلقائي.? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur')\\\", 'api_provider': 'Fairseq', 'explanation': '', 'code': \\\"import IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nimport torchaudio\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur')\\\\nmodel = models[0].cpu()\\\\ncfg['task'].cpu = True\\\\n\\\\naudio, _ = torchaudio.load('input_audio_path.flac')\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\\\\nvocoder = load_vocoder_model('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur')\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\n\\\\nipd.Audio(wav, rate=sr)\\\", 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Fairseq', 'functionality': 'Speech-to-speech translation', 'api_name': 'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur', 'api_call': \\\"import IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nimport torchaudio\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur')\\\\nmodel = models[0].cpu()\\\\ncfg['task'].cpu = True\\\\n\\\\naudio, _ = torchaudio.load('input_audio_path.flac')\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\\\\nvocoder = load_vocoder_model('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur')\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\n\\\\nipd.Audio(wav, rate=sr)\\\", 'api_arguments': {'audio': '16000Hz mono channel audio'}, 'python_environment_requirements': ['fairseq', 'hub_utils', 'huggingface_hub', 'IPython.display', 'torchaudio'], 'example_code': ['import json', 'import os', 'from pathlib import Path', 'import IPython.display as ipd', 'from fairseq import hub_utils', 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub', 'from fairseq.models.speech_to_text.hub_interface import S2THubInterface', 'from fairseq.models.text_to_speech import CodeHiFiGANVocoder', 'from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface', 'from huggingface_hub import snapshot_download', 'import torchaudio', 'cache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)', 'models, cfg, task = load_model_ensemble_and_task_from_hf_hub(', 'facebook/xm_transformer_s2ut_800m-es-en-st-asr-bt_h1_2022,', 'arg_overrides={config_yaml: config.yaml, task: speech_to_text},', 'cache_dir=cache_dir,', ')', 'model = models[0].cpu()', 'cfg[task].cpu = True', 'generator = task.build_generator([model], cfg)', '# requires 16000Hz mono channel audio', 'audio, _ = torchaudio.load(/Users/lpw/git/api-inference-community/docker_images/fairseq/tests/samples/sample2.flac)', 'sample = S2THubInterface.get_model_input(task, audio)', 'unit = S2THubInterface.get_prediction(task, model, generator, sample)', 'library_name = fairseq', 'cache_dir = (', ' cache_dir or (Path.home() / .cache / library_name).as_posix()', ')', 'cache_dir = snapshot_download(', ' ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name', ')', 'x = hub_utils.from_pretrained(', ' cache_dir,', ' model.pt,', ' .,', ' archive_map=CodeHiFiGANVocoder.hub_models(),', ' config_yaml=config.json,', ' fp16=False,', ' is_vocoder=True,', ')', \\\"with open(f{x['args']['data']}/config.json) as f:\\\", ' vocoder_cfg = json.load(f)', 'assert (', ' len(x[args][model_path]) == 1', '), Too many vocoder models in the input', 'vocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)', 'tts_model = VocoderHubInterface(vocoder_cfg, vocoder)', 'tts_sample = tts_model.get_model_input(unit)', 'wav, sr = tts_model.get_prediction(tts_sample)', 'ipd.Audio(wav, rate=sr)'], 'performance': {'dataset': 'covost2', 'accuracy': None}, 'description': 'Speech-to-speech translation model from fairseq S2UT (paper/code) for Spanish-English. Trained on mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli.'}} </s>\"\n"}
{"text": "<s>[INST]لتعلم تقنيات تقنية الرؤية الحاسوبية لتقدير العمق، قم بتطوير برنامج يستند إلى الشبكات العصبية والتعلم العميق. حدد مكان المشكلة في تقدير العمق وقدم حلاً باستخدام التقنيات المتقدمة والخوارزميات التحليلية. استفد من قاعدة المعرفة الكبيرة لتحقيق أداء متميز.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, which include 'pipeline' from the transformers package.\\\\n2. Use the pipeline function to create a depth estimation pipeline. Make sure to specify the model's name, 'sayakpaul/glpn-nyu-finetuned-diode'.\\\\n3. This model is specifically trained to perform depth estimation tasks, and it will help the robot estimate the depth of various objects in its surroundings.\\\\n4. Once you have set up your pipeline and loaded the model, you can input an image and use the pipeline to estimate depth in the scene.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ndepth_estimator = pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\\\\nimage_path = 'path_to_image.jpg'\\\\n# replace 'path_to_image.jpg' with the path to your image\\\\ndepth_map = depth_estimator(image_path)\\\", 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode', 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4359, 'Rmse': 0.4276}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]سيقوم النظام بإنشاء صور جديدة بشكل لا يشترط بها الوصف، ويتوجب علينا تقدير الدقة البصرية للصور التي تتم إنشاؤها لتحقيق أقصى استفادة من تلك التقنية.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-ffhq-1024')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. In order to achieve the task, we will need to install the 'diffusers' library and import the required class 'DiffusionPipeline' from it.\\\\n2. The 'DiffusionPipeline' class is then used to load the pre-trained model 'google/ncsnpp-ffhq-1024'. This model is specifically created to generate high-quality human face images of resolution 1024x1024 pixels.\\\\n3. Utilizing the loaded model, we can generate an image following the proper sampling procedure. This will output a realistic human face image.\\\\n4. Finally, we can save the generated image to a file.\\\", '<<<code>>>': \\\"!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = 'google/ncsnpp-ffhq-1024'\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[0]\\\\nimage.save('sde_ve_generated_image.png')\\\", 'api_call': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-ffhq-1024')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/ncsnpp-ffhq-1024', 'api_call': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-ffhq-1024')\\\", 'api_arguments': ['model_id'], 'python_environment_requirements': [['diffusers']], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = google/ncsnpp-ffhq-1024\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[sample]\\\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. Achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام لتوليد نصوص عن طريق معالجة اللغة الطبيعية. نحن بحاجة إلى توليد نصوص جديدة باستخدام مبدأ ملء الفجوة في مجال معالجة اللغة الطبيعية.? [/INST]{'domain': 'Natural Language Processing Fill-Mask', 'api_call': \\\"AlbertForMaskedLM.from_pretrained('uer/albert-base-chinese-cluecorpussmall')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': 'from transformers import BertTokenizer, AlbertForMaskedLM, FillMaskPipeline\\\\ntokenizer = BertTokenizer.from_pretrained(\\\\'uer/albert-base-chinese-cluecorpussmall\\\\')\\\\nmodel = AlbertForMaskedLM.from_pretrained(\\\\'uer/albert-base-chinese-cluecorpussmall\\\\')\\\\nunmasker = FillMaskPipeline(model, tokenizer)\\\\nresult = unmasker(\\\"中国的首都是[MASK]京。\\\")\\\\ncompleted_sentence = result[0][\\\"sequence\\\"]', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Transformers', 'functionality': 'Fill-Mask', 'api_name': 'uer/albert-base-chinese-cluecorpussmall', 'api_call': 'from transformers import BertTokenizer, AlbertForMaskedLM, FillMaskPipeline\\\\ntokenizer = BertTokenizer.from_pretrained(\\\\'uer/albert-base-chinese-cluecorpussmall\\\\')\\\\nmodel = AlbertForMaskedLM.from_pretrained(\\\\'uer/albert-base-chinese-cluecorpussmall\\\\')\\\\nunmasker = FillMaskPipeline(model, tokenizer)\\\\nresult = unmasker(\\\"中国的首都是[MASK]京。\\\")\\\\ncompleted_sentence = result[0][\\\"sequence\\\"]', 'api_arguments': ['model', 'tokenizer'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import BertTokenizer, AlbertForMaskedLM, FillMaskPipeline\\\\ntokenizer = BertTokenizer.from_pretrained(uer/albert-base-chinese-cluecorpussmall)\\\\nmodel = AlbertForMaskedLM.from_pretrained(uer/albert-base-chinese-cluecorpussmall)\\\\nunmasker = FillMaskPipeline(model, tokenizer)\\\\nunmasker(中国的首都是[MASK]京。)', 'performance': {'dataset': 'CLUECorpusSmall', 'accuracy': 'Not provided'}, 'description': 'This is the set of Chinese ALBERT models pre-trained by UER-py on the CLUECorpusSmall dataset. The model can be used for tasks like text generation and feature extraction.'}} </s>\"\n"}
{"text": "<s>[INST]نقدم لعميلنا الذي يعمل في مجال معالجة اللغة الطبيعية والجدولية والإجابة على الأسئلة، تطوير تطبيق يقوم بتحويل النص إلى كلام لمستخدميه لدعم ذوي الإعاقة البصرية.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. First, import the required pipeline function from the transformers library.\\\\n2. Initialize the table-question-answering pipeline by specifying the model as 'google/tapas-medium-finetuned-wtq'. This model is pretrained on WikiTable Questions (WTQ) and can handle table question answering tasks.\\\\n3. Define the table and the question that needs to be answered. The table should be formatted in a way that the model can understand.\\\\n4. Call the table-question-answering pipeline with the table and question input to get the answer.\\\\n5. The returned result will be the model's best guess for the answer based on the given table and question.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntable_qa = pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\\ntable = {...}\\\\nquery = '...'\\\\nanswer = table_qa(table=table, query=query)\\\", 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/tapas-medium-finetuned-wtq', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", 'api_arguments': 'table, query', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntable_qa = pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\\n\\\\n# Define the table and the query\\\\ntable = {...}\\\\nquery = '...'\\\\n\\\\n# Get the answer\\\\nanswer = table_qa(table=table, query=query)\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.4324}, 'description': 'TAPAS medium model fine-tuned on WikiTable Questions (WTQ). This model is pretrained on a large corpus of English data from Wikipedia and is used for answering questions related to a table.' </s>\"\n"}
{"text": "<s>[INST]أنشئ صورة جديدة تحوي على تحويل من صورة إلى صورة باستخدام تقنية الرؤية الحاسوبية.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\\\", '<<<api_provider>>>': 'Diffusers', '<<<explanation>>>': '1. Import the required packages and classes, such as ControlNetModel, StableDiffusionControlNetPipeline, HEDdetector, and UniPCMultistepScheduler, among others.\\\\n2. Load the pre-trained ControlNet model using the from_pretrained method with the checkpoint \\\\'lllyasviel/control_v11p_sd15_scribble\\\\'\\\\n3. Create a processing pipeline using the StableDiffusionControlNetPipeline with the loaded ControlNet model.\\\\n4. Initialize the HEDdetector processor with the pretrained \\\\'lllyasviel/Annotators\\\\'. The processor helps in creating the scribble control images based on the text description.\\\\n5. Provide the input text description, such as \\\"a cozy cabin in the woods\\\".\\\\n6. Generate the scribble control image using the processor.\\\\n7. Finally, use the pre-built pipeline to convert the control image with the provided text description into the final visual image.', '<<<code>>>': 'from controlnet_aux import HEDdetector\\\\nfrom diffusers import (\\\\n    ControlNetModel,\\\\n    StableDiffusionControlNetPipeline,\\\\n    UniPCMultistepScheduler,\\\\n)\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/control_v11p_sd15_scribble\\\\')\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n    \\\\'runwayml/stable-diffusion-v1-5\\\\', controlnet=controlnet)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\nprocessor = HEDdetector.from_pretrained(\\\\'lllyasviel/Annotators\\\\')\\\\nprompt = \\\"a cozy cabin in the woods\\\"\\\\ncontrol_image = processor(prompt, scribble=True)  # Replace with actual image input\\\\nfinal_image = pipe(prompt, num_inference_steps=30, image=control_image).images[0]\\\\nfinal_image.save(\\\\'images/image_out.png\\\\')', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\\\", 'provider': 'Diffusers', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/control_v11p_sd15_scribble', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\\\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_scribble', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0']], 'example_code': \\\"import torch\\\\nimport os\\\\nfrom huggingface_hub import HfApi\\\\nfrom pathlib import Path\\\\nfrom diffusers.utils import load_image\\\\nfrom PIL import Image\\\\nimport numpy as np\\\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\\\nfrom diffusers import (\\\\n ControlNetModel,\\\\n StableDiffusionControlNetPipeline,\\\\n UniPCMultistepScheduler,\\\\n)\\\\ncheckpoint = lllyasviel/control_v11p_sd15_scribble\\\\nimage = load_image(\\\\n https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/images/input.png\\\\n)\\\\nprompt = royal chamber with fancy bed\\\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\\\ncontrol_image = processor(image, scribble=True)\\\\ncontrol_image.save(./images/control.png)\\\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(0)\\\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\nimage.save('images/image_out.png')\\\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتصنيف الصوتيات باستخدام تقنيات الذكاء الصناعي، ونرغب في توقع نوع الصوتيات ومحتواها بناءً على تحليل البيانات المتاحة.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries such as 'datasets' to load the speech dataset, and 'pipeline' from transformers.\\\\n2. Use the 'load_dataset()' function to load a dataset containing speech audio files. In this case, ensure that your speech input is sampled at 16kHz.\\\\n3. Create an audio classification pipeline by specifying 'audio-classification' as the task and 'superb/hubert-large-superb-er' as the model.\\\\n4. The classifier can be used to predict emotions in the audio data by providing the audio file to the classifier.\\\\n5. The classifier generates a list of emotions along with their corresponding probabilities. The emotions with the highest probabilities are returned.\\\", '<<<code>>>': \\\"from datasets import load_dataset\\\\nfrom transformers import pipeline\\\\ndataset = load_dataset('anton-l/superb_demo', 'er', split='session1')\\\\nclassifier = pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\\nlabels = classifier(dataset[0]['file'], top_k=5)\\\", 'api_call': \\\"pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'superb/hubert-large-superb-er', 'api_call': \\\"pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\", 'api_arguments': ['model'], 'python_environment_requirements': [['transformers', 'datasets', 'librosa']], 'example_code': \\\"from datasets import load_dataset\\\\nfrom transformers import pipeline\\\\ndataset = load_dataset('anton-l/superb_demo', 'er', split='session1')\\\\nclassifier = pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\\nlabels = classifier(dataset[0]['file'], top_k=5)\\\", 'performance': {'dataset': 'IEMOCAP', 'accuracy': 0.6762}, 'description': \\\"This is a ported version of S3PRL's Hubert for the SUPERB Emotion Recognition task. The base model is hubert-large-ll60k, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.\\\" </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا ببناء منتج لمعالجة اللغة الطبيعية للإجابة على استفسارات الجداول، ونريد التنبؤ بإجابات الأسئلة بناءً على بعض مجموعات البيانات المتوفرة.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create the table-question-answering pipeline with the \\\\'google/tapas-large-finetuned-wtq\\\\' model, which is specifically designed for answering questions related to tables and spreadsheets.\\\\n3. Apply the qa_pipeline to your table data and provide the query \\\"Total number of SUVs in the inventory\\\".\\\\n4. The model will process the table data and return you the count of SUVs in the inventory.', '<<<code>>>': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\\nquery = 'Total number of SUVs in the inventory'\\\\nresult = qa_pipeline(table=table, query=query)\\\", 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/tapas-large-finetuned-wtq', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", 'api_arguments': {'model': 'google/tapas-large-finetuned-wtq', 'task': 'table-question-answering'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\nqa_pipeline = pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\\n\\\\nresult = qa_pipeline(table=table, query=query)\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.5097}, 'description': 'TAPAS large model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).' </s>\"\n"}
{"text": "<s>[INST]قم بتشغيل تطبيق تصنيف الرموز لمعالجة اللغة الطبيعية واستدعاء وظيفة تصنيف الرموز الآلي من أجل تحليل النصوص وتحديد أنواع الرموز الموجودة في النصوص. حاول تحسين استخراج الرموز وضبط النماذج لزيادة دقة تصنيف الرموز.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-company_all-903429548', use_auth_token=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries: AutoModelForTokenClassification and AutoTokenizer from the transformers module.\\\\n2. Load the pretrained model by passing the model name 'ismail-lucifer011/autotrain-company_all-903429548', and use_auth_token=True in the from_pretrained function.\\\\n3. Load the tokenizer using the same model name.\\\\n4. Tokenize the input text (news article) using the tokenizer and obtain the input tensors.\\\\n5. Pass the input tensors to the model, and it will return the predictions for each token.\\\\n6. Extract the tokens with the highest confidence scores as company names.\\\", '<<<code>>>': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'ismail-lucifer011/autotrain-company_all-903429548\\\\', use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'ismail-lucifer011/autotrain-company_all-903429548\\\\', use_auth_token=True)\\\\nnews_article = \\\"News article text here...\\\"\\\\ninputs = tokenizer(news_article, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-company_all-903429548', use_auth_token=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': '903429548', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-company_all-903429548', use_auth_token=True)\\\", 'api_arguments': {'inputs': 'I love AutoTrain'}, 'python_environment_requirements': [{'transformers': 'AutoModelForTokenClassification, AutoTokenizer'}], 'example_code': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\\\noutputs = model(**inputs)', 'performance': {'dataset': 'ismail-lucifer011/autotrain-data-company_all', 'accuracy': 0.9979930566588805}, 'description': 'A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير تقنيات معالجة الصور من خلال إنشاء تطبيق لتحويل الصور التي تحتوي على نصوص إلى صور بدون نصوص، ونحن بحاجة إلى استخدام هذه التقنيات في مجال الرؤية الحاسوبية Image-to-Image.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the required classes, such as ControlNetModel, StableDiffusionControlNetPipeline, and UniPCMultistepScheduler, from the diffusers, transformers, and accelerate libraries.\\\\n2. Load the pre-trained ControlNet model \\\\'lllyasviel/control_v11p_sd15_seg\\\\'.\\\\n3. Create a new instance of the StableDiffusionControlNetPipeline class, combining the pre-trained ControlNet model and a stable diffusion model \\\\'runwayml/stable-diffusion-v1-5\\\\'.\\\\n4. Set the scheduler and enable model CPU offloading.\\\\n5. Provide the prompt text description (\\\"A cozy living room with a blue sofa, a wooden coffee table, and a large window overlooking a garden\\\") as input, and generate a new image using the pipeline.\\\\n6. Save the generated image as \\\\'image_out.png\\\\'.', '<<<code>>>': 'from diffusers import ControlNetModel, StableDiffusionControlNetPipeline, UniPCMultistepScheduler\\\\nimport torch\\\\n\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/control_v11p_sd15_seg\\\\', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', controlnet=controlnet, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\n\\\\nprompt = \\\"A cozy living room with a blue sofa, a wooden coffee table, and a large window overlooking a garden.\\\"\\\\ngenerator = torch.manual_seed(0)\\\\nimage = pipe(prompt, num_inference_steps=30, generator=generator).images[0]\\\\nimage.save(\\\\'images/image_out.png\\\\')', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/control_v11p_sd15_seg', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg')\\\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_seg'}, 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate']], 'example_code': \\\"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(0)\\\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\nimage.save('images/image_out.png')\\\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نماذج الذكاء الاصطناعي لتوليد صور متعددة الأوضاع من النصوص، ونحتاج إلى تقدير عمق الصور المولدة لضمان دقة وجودة التصاميم.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler), torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the necessary modules and classes from the diffusers library, such as StableDiffusionPipeline and EulerDiscreteScheduler.\\\\n2. Load the pretrained \\\\'stabilityai/stable-diffusion-2\\\\' text-to-image generation model using the from_pretrained method of the StableDiffusionPipeline class.\\\\n3. Load the associated pretrained scheduler for this model using the from_pretrained method of the EulerDiscreteScheduler class.\\\\n4. Use the loaded pipeline to generate an image based on the provided text prompt describing the desired watch design, such as \\\"a gold watch with a black leather strap and a round dial.\\\"\\\\n5. Save the generated image to the desired location and display it on the e-commerce website as the output.', '<<<code>>>': \\\"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = 'stabilityai/stable-diffusion-2'\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder='scheduler')\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to('cuda')\\\\nprompt = 'a gold watch with a black leather strap and a round dial'\\\\nimage = pipe(prompt).images[0]\\\\nimage.save('generated_watch.png')\\\", 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler), torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler), torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors']], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>\"\n"}
{"text": "<s>[INST]يهدف هذا الدليل إلى توليد تسجيل صوتي نصي باستخدام تقنيات تحويل النص إلى كلام، يحتوي على وصف لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في إعداد النص اللازم بناءً على وصف مشابه باللغة العربية؟? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"Tacotron2.from_hparams(source='padmalcom/tts-tacotron2-german')\\\", '<<<api_provider>>>': 'speechbrain', '<<<explanation>>>': \\\"1. Import the necessary functions from speechbrain.pretrained, namely Tacotron2 and HIFIGAN, as well as torchaudio.\\\\n2. Create a Tacotron2 speech synthesis model by loading the pre-trained German model using the source 'padmalcom/tts-tacotron2-german'.\\\\n3. Instantiate a HIFIGAN vocoder model, which will synthesize waveforms from the computed spectrograms. Use the source 'speechbrain/tts-hifigan-ljspeech'.\\\\n4. Encode the German text to be converted to voice using the encode_text method of the Tacotron2 model.\\\\n5. Decode the generated mel spectrograms using the HIFIGAN vocoder to produce the waveforms for the audio.\\\\n6. Use torchaudio to save the resulting waveform as a .wav file for broadcasting in Germany.\\\", '<<<code>>>': 'import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2, HIFIGAN\\\\ntacotron2 = Tacotron2.from_hparams(source=\\\\'padmalcom/tts-tacotron2-german\\\\')\\\\nhifi_gan = HIFIGAN.from_hparams(source=\\\\'speechbrain/tts-hifigan-ljspeech\\\\')\\\\ngerman_text = \\\"Die Sonne schien den ganzen Tag.\\\"\\\\nmel_output, mel_length, alignment = tacotron2.encode_text(german_text)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save(\\\\'example_TTS.wav\\\\', waveforms.squeeze(1),22050)', 'api_call': \\\"Tacotron2.from_hparams(source='padmalcom/tts-tacotron2-german')\\\", 'provider': 'speechbrain', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'padmalcom/tts-tacotron2-german', 'api_call': \\\"Tacotron2.from_hparams(source='padmalcom/tts-tacotron2-german')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['pip install speechbrain']], 'example_code': ['import torchaudio', 'from speechbrain.pretrained import Tacotron2', 'from speechbrain.pretrained import HIFIGAN', 'tacotron2 = Tacotron2.from_hparams(source=padmalcom/tts-tacotron2-german, savedir=tmpdir_tts)', 'hifi_gan = HIFIGAN.from_hparams(source=speechbrain/tts-hifigan-ljspeech, savedir=tmpdir_vocoder)', 'mel_output, mel_length, alignment = tacotron2.encode_text(Die Sonne schien den ganzen Tag.)', 'waveforms = hifi_gan.decode_batch(mel_output)', \\\"torchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\\\"], 'performance': {'dataset': 'custom german dataset', 'accuracy': 'Not provided'}, 'description': 'Text-to-Speech (TTS) with Tacotron2 trained on a custom german dataset with 12 days voice using speechbrain. Trained for 39 epochs (english speechbrain models are trained for 750 epochs) so there is room for improvement and the model is most likely to be updated soon. The hifigan vocoder can fortunately be used language-independently.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نظام يستطيع توليد نصوص باللغة الطبيعية في مجال معالجة اللغة الطبيعية. هل يمكنك مساعدتي في تحقيق ذلك؟? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-3B')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. First, import the BlenderbotForConditionalGeneration class from the transformers library provided by Hugging Face.', 'code': 'from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\\\\nmodel = BlenderbotForConditionalGeneration.from_pretrained(\\\\'facebook/blenderbot-3B\\\\')\\\\ntokenizer = BlenderbotTokenizer.from_pretrained(\\\\'facebook/blenderbot-3B\\\\')\\\\ninput_text = \\\"Tell me about the movie Inception.\\\"\\\\ninputs = tokenizer(input_text, return_tensors=\\\\'pt\\\\')\\\\noutputs = model.generate(**inputs)\\\\ngenerated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Conversational', 'api_name': 'facebook/blenderbot-3B', 'api_call': 'from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\\\\nmodel = BlenderbotForConditionalGeneration.from_pretrained(\\\\'facebook/blenderbot-3B\\\\')\\\\ntokenizer = BlenderbotTokenizer.from_pretrained(\\\\'facebook/blenderbot-3B\\\\')\\\\ninput_text = \\\"Tell me about the movie Inception.\\\"\\\\ninputs = tokenizer(input_text, return_tensors=\\\\'pt\\\\')\\\\noutputs = model.generate(**inputs)\\\\ngenerated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]', 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'Input a message to start chatting with facebook/blenderbot-3B.', 'performance': {'dataset': 'blended_skill_talk', 'accuracy': 'Not provided'}, 'description': 'BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It is trained on the blended_skill_talk dataset and can engage in multi-turn conversations, providing engaging talking points, asking and answering questions, and displaying knowledge, empathy, and personality. The model is available through the Hugging Face Transformers library.'}} </s>\"\n"}
{"text": "<s>[INST]سنقوم بتحويل النص إلى كلام مسموع باستخدام تقنيات تحويل النص إلى كلام المتقدمة. يرجى تقديم النص الذي ترغب في تحويله وتحديد اللهجة والنبرة المطلوبة للصوت.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes and libraries, which includes SpeechT5Processor, SpeechT5ForTextToSpeech and SpeechT5HifiGan for the purpose of text-to-speech conversion.\\\\n2. Load the pre-trained models for processing text and generating speech, using the 'microsoft/speecht5_tts' model.\\\\n3. Specify the text that you want to convert into speech.\\\\n4. Process the text using the SpeechT5Processor and obtain the input_ids.\\\\n5. Generate the speech with the SpeechT5ForTextToSpeech model using the input_ids, speaker_embeddings (optional), and the vocoder.\\\\n6. Save the generated speech in a .wav file.\\\", '<<<code>>>': 'from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\\\\nimport torch\\\\nimport soundfile as sf\\\\nprocessor = SpeechT5Processor.from_pretrained(\\\\'microsoft/speecht5_tts\\\\')\\\\nmodel = SpeechT5ForTextToSpeech.from_pretrained(\\\\'microsoft/speecht5_tts\\\\')\\\\nvocoder = SpeechT5HifiGan.from_pretrained(\\\\'microsoft/speecht5_hifigan\\\\')\\\\nsummary_text = \\\"During our last week\\\\'s meeting, we discussed project updates, milestones, and upcoming deadlines. We reviewed the budget and assigned new tasks to team members.\\\"\\\\ninputs = processor(text=summary_text, return_tensors=\\\\'pt\\\\')\\\\nspeech = model.generate_speech(inputs[\\\\'input_ids\\\\'], vocoder=vocoder)\\\\nsf.write(\\\\'voice_memo_summary.wav\\\\', speech.numpy(), samplerate=16000)', 'api_call': \\\"SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/speecht5_tts', 'api_call': \\\"SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\\\", 'api_arguments': ['text', 'return_tensors', 'input_ids', 'speaker_embeddings', 'vocoder'], 'python_environment_requirements': ['!pip install git+https://github.com/huggingface/transformers sentencepiece datasets'], 'example_code': \\\"from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\nimport soundfile as sf\\\\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_tts')\\\\nmodel = SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\\\\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\\\\ninputs = processor(text='Hello, my dog is cute', return_tensors='pt')\\\\nembeddings_dataset = load_dataset('Matthijs/cmu-arctic-xvectors', split='validation')\\\\nspeaker_embeddings = torch.tensor(embeddings_dataset[7306]['xvector']).unsqueeze(0)\\\\nspeech = model.generate_speech(inputs['input_ids'], speaker_embeddings, vocoder=vocoder)\\\\nsf.write('speech.wav', speech.numpy(), samplerate=16000)\\\", 'performance': {'dataset': 'LibriTTS', 'accuracy': 'Not specified'}, 'description': 'SpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS. It is a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. It can be used for a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد نموذج لربط الصور متعددة الوسائط بالنصوص. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes and functions from Hugging Face's transformers library, as well as the PIL (Python Imaging Library) and requests packages to handle image data.\\\\n2. Use the method 'from_pretrained' of the TrOCR VisionEncoderDecoderModel class to create a model based on the 'microsoft/trocr-large-handwritten' pre-trained model.\\\\n3. Load the image you want to transcribe using the Image class from PIL. This image data will be used as input for the TrOCR model.\\\\n4. Transform the image data to tensors using the processor, and pass the tensors to the TrOCR model to generate the output text transcription.\\\", '<<<code>>>': 'from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\n\\\\nimage = Image.open(\\\\'handwritten_text_image.jpg\\\\').convert(\\\"RGB\\\")\\\\n# replace \\\\'handwritten_text_image.jpg\\\\' with path to your image\\\\nprocessor = TrOCRProcessor.from_pretrained(\\\\'microsoft/trocr-large-handwritten\\\\')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained(\\\\'microsoft/trocr-large-handwritten\\\\')\\\\n\\\\npixel_values = processor(images=image, return_tensors=\\\\'pt\\\\').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': [{'packages': ['transformers', 'PIL', 'requests']}], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نموذج لتصنيف البيانات في مجال تعلم التعزيز، ونحتاج إلى تقييم كفاءة أداء النموذج على مجموعة البيانات المستخدمة.? [/INST]{'domain': 'Reinforcement Learning', 'api_call': \\\"load_from_hub(repo_id='HumanCompatibleAI/ppo-seals-CartPole-v0', filename='{MODEL FILENAME}.zip')\\\", 'api_provider': 'Stable-Baselines3', 'explanation': '', 'code': \\\"from rl_zoo3 import load_from_hub\\\\nimport gym\\\\n\\\\nrepo_id = 'HumanCompatibleAI/ppo-seals-CartPole-v0'\\\\nfilename = 'your_model_filename.zip'\\\\nmodel = load_from_hub(repo_id=repo_id, filename=filename)\\\\n\\\\nenv = gym.make('seals/CartPole-v0')\\\\nobs = env.reset()\\\\n\\\\ndone = False\\\\nwhile not done:\\\\n    action, _states = model.predict(obs)\\\\n    obs, reward, done, info = env.step(action)\\\\n    env.render()\\\\nenv.close()\\\", 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Stable-Baselines3', 'functionality': 'deep-reinforcement-learning', 'api_name': 'ppo-seals-CartPole-v0', 'api_call': \\\"from rl_zoo3 import load_from_hub\\\\nimport gym\\\\n\\\\nrepo_id = 'HumanCompatibleAI/ppo-seals-CartPole-v0'\\\\nfilename = 'your_model_filename.zip'\\\\nmodel = load_from_hub(repo_id=repo_id, filename=filename)\\\\n\\\\nenv = gym.make('seals/CartPole-v0')\\\\nobs = env.reset()\\\\n\\\\ndone = False\\\\nwhile not done:\\\\n    action, _states = model.predict(obs)\\\\n    obs, reward, done, info = env.step(action)\\\\n    env.render()\\\\nenv.close()\\\", 'api_arguments': ['algo', 'env', 'f'], 'python_environment_requirements': ['rl_zoo3', 'stable-baselines3', 'stable-baselines3-contrib'], 'example_code': 'python -m rl_zoo3.load_from_hub --algo ppo --env seals/CartPole-v0 -orga HumanCompatibleAI -f logs/', 'performance': {'dataset': 'seals/CartPole-v0', 'accuracy': '500.00 +/- 0.00'}, 'description': 'This is a trained model of a PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتدريب شبكة عصبونية باستخدام الصور الأساسية والصور المعالجة لتحسين تحديد الأشخاص في الصور.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes, such as Swin2SRForImageSuperResolution for the image super-resolution model and Image to process the image data.\\\\n2. Use the from_pretrained method with the model\\\\\\\\textsuperscript{’}s name 'caidas/swin2sr-classical-sr-x2-64' to load a pretrained Swin2SR model. This model is designed to upscale images and make them look better.\\\\n3. Load the user's online profile image using the Image class from the PIL package.\\\\n4. Feed the image into the model, which will process it and produce an upscaled, higher-quality version of the image.\\\\n5. Save the image and use it to replace the user's original online photo, thereby improving their profile's appearance.\\\", '<<<code>>>': \\\"from transformers import Swin2SRForImageSuperResolution\\\\nfrom PIL import Image\\\\nimage = Image.open('path_to_user_profile_image.jpg')\\\\nmodel = Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'api_call': \\\"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'caidas/swin2SR-classical-sr-x2-64', 'api_call': \\\"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\\\", 'api_arguments': 'image, model, feature_extractor', 'python_environment_requirements': ['transformers'], 'example_code': 'Refer to the documentation.', 'performance': {'dataset': 'arxiv: 2209.11345', 'accuracy': 'Not provided'}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]أنشئ جدول توضح البيانات والمتغيرات الضرورية لتدريب نموذج تحليل تابعي.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\", '<<<api_provider>>>': 'Keras', '<<<explanation>>>': \\\"1. To use the model for detecting anomalies, first, import the necessary libraries: TensorFlow, and Keras.\\\\n2. Load the model using the TFAutoModelForSequenceClassification.from_pretrained() function provided by the Keras library with the model name 'keras-io/timeseries-anomaly-detection'.\\\\n3. The loaded model is a reconstruction convolutional autoencoder, which is suitable for detecting anomalies in timeseries data.\\\\n4. By using the model and the dataset of temperature recordings, you can train the model and identify the anomalies within the dataset.\\\\n5. You will need to split the data into training and validation sets to train and validate the model, and calculate the loss during its training process.\\\", '<<<code>>>': \\\"import tensorflow.compat.v2 as tf\\\\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\\\\nfrom tensorflow.keras.models import TFAutoModelForSequenceClassification\\\\ntemp_data = [series of temperature readings]\\\\nsequence_length = length_of_sequence_for_inputs\\\\n\\\\n# Prepare data for LSTM\\\\ngenerator = TimeseriesGenerator(temp_data, temp_data, length=sequence_length, batch_size=1)\\\\n# Load model\\\\nmodel = TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\\n# Train the model\\\\nmodel.fit(generator, epochs=number_of_epochs)\\\", 'api_call': \\\"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\", 'provider': 'Keras', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keras-io/timeseries-anomaly-detection', 'api_call': \\\"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\", 'api_arguments': {'optimizer': {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'training_precision': 'float32'}, 'python_environment_requirements': [['tensorflow', 'keras']], 'example_code': '', 'performance': {'dataset': 'Numenta Anomaly Benchmark(NAB)', 'accuracy': {'Train Loss': 0.006, 'Validation Loss': 0.008}}, 'description': 'This script demonstrates how you can use a reconstruction convolutional autoencoder model to detect anomalies in timeseries data. We will use the Numenta Anomaly Benchmark(NAB) dataset. It provides artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered, timestamped, single-valued metrics.' </s>\"\n"}
{"text": "<s>[INST]قم بتطبيق تصنيف الفيديو في مجال رؤية الحاسوب على مقطع فيديو معين.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries: AutoModelForVideoClassification from transformers and torch.\\\\n2. Load the pre-trained model 'lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb' using the from_pretrained method of the AutoModelForVideoClassification class.\\\\n3. The model is trained to recognize aggressive behavior in video footage. Use the model to make predictions on the input video frames or sequence of frames.\\\\n4. The model will output classification probabilities for different categories, and the system can use these probabilities to make a decision about whether aggressive behavior was detected or not.\\\", '<<<code>>>': \\\"from transformers import AutoModelForVideoClassification\\\\nimport torch\\\\nmodel = AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb')\\\\ninputs = torch.tensor(video_frames)\\\\noutputs = model(inputs)\\\\npredictions = torch.softmax(outputs.logits, dim=-1)\\\", 'api_call': \\\"AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb', 'api_call': \\\"AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers, torch, tokenizers, datasets'], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': 0.7298}, 'description': 'This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.5482, Accuracy: 0.7298.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لمعالجة اللغة الطبيعية يستخدم مقارنة الجمل لتحليل تشابه الجمل. يمكنك استخدام بيانات تدريب متاحة عامة مثل مجموعة البيانات SNLI، أو يمكنك إنشاء بيانات تدريب خاصة بك. استخدم تقنيات كالتضمين الكلامي والتحويلات العميقة لتحقيق أفضل أداء للنموذج الخاص بك.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('nikcheerla/nooks-amd-detection-realtime')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the SentenceTransformer class from the sentence_transformers library.\\\\n2. Initialize the SentenceTransformer model with the pre-trained model name 'nikcheerla/nooks-amd-detection-realtime'.\\\\n3. Use the encode method of the model to generate embeddings for both the input question and the set of frequently asked questions (FAQs).\\\\n4. Calculate similarity scores between the input question embedding and each FAQ embedding using a similarity metric like cosine similarity.\\\\n5. Based on the similarity scores, identify and return the most similar questions in the FAQ set.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nmodel = SentenceTransformer(\\\\'nikcheerla/nooks-amd-detection-realtime\\\\')\\\\ninput_question = \\\"How do I reset my password?\\\"\\\\nfaq_questions = [...]  # List of frequently asked questions\\\\nembeddings = model.encode([input_question] + faq_questions)\\\\nsimilarity_scores = cosine_similarity(embeddings[0].reshape(1, -1), embeddings[1:])\\\\nmost_similar_question_idx = similarity_scores.argmax()\\\\nmost_similar_question = faq_questions[most_similar_question_idx]', 'api_call': \\\"SentenceTransformer('nikcheerla/nooks-amd-detection-realtime')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nikcheerla/nooks-amd-detection-realtime', 'api_call': \\\"SentenceTransformer('nikcheerla/nooks-amd-detection-realtime')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('{MODEL_NAME}')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]يتعين عليك تنفيذ خطوات التالي لإنشاء نموذج الصور المصنفة بشكل صحيح بالنظر العنصري: الخطوة الأولى، قم بتنزيل وتثبيت بيئة التطوير المناسبة لتصنيف الصور الصفرية باستخدام الرؤية الحاسوبية. الخطوة الثانية، قم بتدريب النموذج باستخدام بيانات التصنيف الصوري الصفرية. الخطوة الثالثة، قم بتقييم أداء النموذج على مجموعة بيانات اختبار غير معروفة سابقا وتقييم فعاليته في التصنيف الصوري للنوع الجديد.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"CLIPModel.from_pretrained('laion/CLIP-ViT-L-14-laion2B-s32B-b82K')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To do zero-shot image classification, we can use the 'laion/CLIP-ViT-L-14-laion2B-s32B-b82K' model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. This model works without specific training for the desired task.\\\\n2. Import the necessary classes from the transformers package. This includes CLIPModel for the image classification model.\\\\n3. Use the from_pretrained method of the CLIPModel class to load the pre-trained model 'laion/CLIP-ViT-L-14-laion2B-s32B-b82K'.\\\\n4. The loaded model can then be used to analyze an image and make zero-shot classification predictions, allowing your app to classify the image and improve its visual search capabilities.\\\", '<<<code>>>': \\\"from transformers import CLIPModel\\\\n\\\\nmodel = CLIPModel.from_pretrained('laion/CLIP-ViT-L-14-laion2B-s32B-b82K')\\\\n\\\\n# Replace 'path/to/image' with the path of your input image\\\\nimage_input = 'path/to/image'\\\\nclassification_output = model(image_input)\\\", 'api_call': \\\"CLIPModel.from_pretrained('laion/CLIP-ViT-L-14-laion2B-s32B-b82K')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-ViT-L-14-laion2B-s32B-b82K', 'api_call': \\\"CLIPModel.from_pretrained('laion/CLIP-ViT-L-14-laion2B-s32B-b82K')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'ImageNet-1k', 'accuracy': 75.3}, 'description': 'A CLIP ViT L/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. Intended for research purposes and exploring zero-shot, arbitrary image classification. Can be used for interdisciplinary studies of the potential impact of such model.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى برنامج في مجال معالجة اللغة الطبيعية لتلخيص النصوص. عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية ونحتاج إلى برنامج يعمل على تحويل النصوص إلى كلام لمستخدمي هذا التطبيق.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('mrm8488/bert2bert_shared-spanish-finetuned-summarization')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries, including AutoModelForSeq2SeqLM and AutoTokenizer from transformers.', 'code': 'from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\\\n\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'mrm8488/bert2bert_shared-spanish-finetuned-summarization\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'mrm8488/bert2bert_shared-spanish-finetuned-summarization\\\\')\\\\n\\\\ndef generate_summary(text):\\\\n    inputs = tokenizer([text], padding=True, truncation=True, max_length=512, return_tensors=\\\\'pt\\\\')\\\\n    input_ids = inputs.input_ids\\\\n    attention_mask = inputs.attention_mask\\\\n    output = model.generate(input_ids, attention_mask=attention_mask)\\\\n    return tokenizer.decode(output[0], skip_special_tokens=True)\\\\n\\\\ntext = \\\"Your Spanish text here...\\\"\\\\nsummary = generate_summary(text)', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Text2Text Generation', 'api_name': 'mrm8488/bert2bert_shared-spanish-finetuned-summarization', 'api_call': 'from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\\\n\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'mrm8488/bert2bert_shared-spanish-finetuned-summarization\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'mrm8488/bert2bert_shared-spanish-finetuned-summarization\\\\')\\\\n\\\\ndef generate_summary(text):\\\\n    inputs = tokenizer([text], padding=True, truncation=True, max_length=512, return_tensors=\\\\'pt\\\\')\\\\n    input_ids = inputs.input_ids\\\\n    attention_mask = inputs.attention_mask\\\\n    output = model.generate(input_ids, attention_mask=attention_mask)\\\\n    return tokenizer.decode(output[0], skip_special_tokens=True)\\\\n\\\\ntext = \\\"Your Spanish text here...\\\"\\\\nsummary = generate_summary(text)', 'api_arguments': ['text'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': 'def generate_summary(text):\\\\n  inputs = tokenizer([text], padding=max_length, truncation=True, max_length=512, return_tensors=pt)\\\\n  input_ids = inputs.input_ids.to(device)\\\\n  attention_mask = inputs.attention_mask.to(device)\\\\n  output = model.generate(input_ids, attention_mask=attention_mask)\\\\n  return tokenizer.decode(output[0], skip_special_tokens=True)\\\\ntext = Your text here...\\\\ngenerate_summary(text)', 'performance': {'dataset': 'mlsum', 'accuracy': {'Rouge1': 26.24, 'Rouge2': 8.9, 'RougeL': 21.01, 'RougeLsum': 21.02}}, 'description': 'Spanish BERT2BERT (BETO) fine-tuned on MLSUM ES for summarization'}} </s>\"\n"}
{"text": "<s>[INST]يتعين علينا تطوير نظام للإجابة على الأسئلة ضمن جداول معالجة اللغة الطبيعية، حيث يقوم بتحليل وتقدير الاستفسارات الموجودة في الجداول بشكل تلقائي ودقيق.? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-sqa')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': 'from transformers import pipeline\\\\ntable_qa_pipeline = pipeline(\\\\'table-question-answering\\\\', model=\\\\'google/tapas-medium-finetuned-sqa\\\\')\\\\ntable = [[\\\"Date\\\", \\\"Product\\\", \\\"Revenue\\\"], [\\\"2021-09-01\\\", \\\"Product A\\\", 1500], [\\\"2021-09-02\\\", \\\"Product B\\\", 2500], [\\\"2021-09-03\\\", \\\"Product C\\\", 1000]]\\\\nquery = \\\"What is the total revenue?\\\"\\\\nresult = table_qa_pipeline(table=table, query=query)', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-medium-finetuned-sqa', 'api_call': 'from transformers import pipeline\\\\ntable_qa_pipeline = pipeline(\\\\'table-question-answering\\\\', model=\\\\'google/tapas-medium-finetuned-sqa\\\\')\\\\ntable = [[\\\"Date\\\", \\\"Product\\\", \\\"Revenue\\\"], [\\\"2021-09-01\\\", \\\"Product A\\\", 1500], [\\\"2021-09-02\\\", \\\"Product B\\\", 2500], [\\\"2021-09-03\\\", \\\"Product C\\\", 1000]]\\\\nquery = \\\"What is the total revenue?\\\"\\\\nresult = table_qa_pipeline(table=table, query=query)', 'api_arguments': 'table, query', 'python_environment_requirements': 'transformers', 'example_code': \\\"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntable_qa_pipeline = pipeline('table-question-answering', model='google/tapas-medium-finetuned-sqa')\\\\n\\\\n# Provide the table and query\\\\nresult = table_qa_pipeline(table=table, query='What is the total revenue?')\\\", 'performance': {'dataset': 'msr_sqa', 'accuracy': 0.6561}, 'description': 'TAPAS medium model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia and uses relative position embeddings. It can be used for answering questions related to a table in a conversational set-up.'}} </s>\"\n"}
{"text": "<s>[INST]يمكنك استخدام تقنيات الرؤية الحاسوبية والكشف عن الكائنات لتحويل النص الموصوف إلى مشهد يصور الحيوانات في البرية. هل تحتاج مساعدة في تنفيذ ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers, PIL, and requests packages. This includes DeformableDetrForObjectDetection for the object detection model and Image for processing image data.\\\\n2. We then use the from_pretrained method of the DeformableDetrForObjectDetection class to load the pre-trained model 'SenseTime/deformable-detr'. This model has been trained for object detection tasks, which is perfect for detecting objects in a security application.\\\\n3. We load the image data from a file or from a URL, or acquire it in real-time from the security camera.\\\\n4. This model can then be used to analyze an image and identify the various objects in it, which can help with security monitoring.\\\", '<<<code>>>': \\\"from transformers import DeformableDetrForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'http://path/to/image'  # Replace with your image URL\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\\n\\\\ninputs = processor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'api_call': \\\"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'deformable-detr', 'api_call': \\\"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': [['transformers', 'torch', 'PIL', 'requests']], 'example_code': \\\"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\\\nimport torch\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\\ninputs = processor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'performance': {'dataset': 'COCO 2017', 'accuracy': 'Not provided'}, 'description': 'Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نماذج لتصنيف النصوص في مجال معالجة اللغات الطبيعية لتصنيف الرموز، ونحتاج إلى بناء نموذج يمكنه تصنيف النصوص بدقة عالية بناءً على محتواها وموضوعاتها المختلفة.? [/INST]{'domain': 'Natural Language Processing Text Classification', 'api_call': \\\"pipeline('text-classification', model='roberta-base-openai-detector')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. Import the necessary libraries, which are 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\ndetector = pipeline(\\\\'text-classification\\\\', model=\\\\'roberta-base-openai-detector\\\\')\\\\ntext = \\\"Hello world! Is this content AI-generated?\\\"\\\\nresult = detector(text)', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Transformers', 'functionality': 'Detect GPT-2 generated text', 'api_name': 'roberta-base-openai-detector', 'api_call': 'from transformers import pipeline\\\\ndetector = pipeline(\\\\'text-classification\\\\', model=\\\\'roberta-base-openai-detector\\\\')\\\\ntext = \\\"Hello world! Is this content AI-generated?\\\"\\\\nresult = detector(text)', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import pipeline\\\\npipe = pipeline(text-classification, model=roberta-base-openai-detector)\\\\nprint(pipe(Hello world! Is this content AI-generated?))', 'performance': {'dataset': 'WebText', 'accuracy': '95%'}, 'description': 'RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model.'}} </s>\"\n"}
{"text": "<s>[INST]تحتاج إلى استخدام واجهة برمجة التطبيقات (API) لتحويل الملف الصوتي إلى نص. يمكنك البحث عن خدمات التعرف التلقائي على الكلام التي تقدم هذه الخدمة واستخدام الوثائق والأمثلة المقدمة لتحقيق هذا الهدف. إذا كنت بحاجة إلى مزيد من المساعدة، فلا تتردد في طرح الأسئلة.? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': \\\"unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')\\\", 'api_provider': 'Fairseq', 'explanation': '', 'code': 'from fairseq import unit\\\\ntts_model = unit.TTS.from_pretrained(\\\\'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10\\\\')\\\\n\\\\n# Example text input for Text-to-Speech model\\\\ntext = \\\"Replace this text with the content of your audiobook in one of the supported languages.\\\"\\\\noutput = tts_model.synthesize(text)\\\\n\\\\nwith open(\\\"output_file.wav\\\", \\\"wb\\\") as f:\\\\n    f.write(output)', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Fairseq', 'functionality': 'Text-to-Speech', 'api_name': 'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10', 'api_call': 'from fairseq import unit\\\\ntts_model = unit.TTS.from_pretrained(\\\\'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10\\\\')\\\\n\\\\n# Example text input for Text-to-Speech model\\\\ntext = \\\"Replace this text with the content of your audiobook in one of the supported languages.\\\"\\\\noutput = tts_model.synthesize(text)\\\\n\\\\nwith open(\\\"output_file.wav\\\", \\\"wb\\\") as f:\\\\n    f.write(output)', 'api_arguments': None, 'python_environment_requirements': 'huggingface_hub, fairseq', 'example_code': None, 'performance': {'dataset': 'covost2', 'accuracy': None}, 'description': 'A text-to-speech model trained on multiple datasets including mtedx, covost2, europarl_st, and voxpopuli. Supports English, Spanish, French, and Italian languages.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتشغيل تسجيل صوتي لجملة قصيرة تتحدث عن كيفية استخدام تقنيات الرؤية الحاسوبية في اكتشاف الكائنات، ثم قم بتطبيق محرك الصوت الآلي لتحويل هذا التسجيل إلى تسجيل صوتي آخر يحتوي على نفس الجملة بصوت مختلف ومشابه للصوت الأصلي. قدم الصوتين للمقارنة.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"Wav2Vec2ForSequenceClassification.from_pretrained('hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers and torchaudio packages, such as Wav2Vec2ForSequenceClassification for the sentiment classification and torchaudio for processing the audio data.\\\\n2. We then use the from_pretrained method of the Wav2Vec2ForSequenceClassification class to load the pre-trained model 'hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD'.\\\\n3. Now, we can process the audio data, which can be acquired from recorded audio calls. You can use torchaudio.load() to load an .wav audio file.\\\\n4. Finally, we can use this model to analyze the sentiment of the audio and classify it.\\\\n5. Keep in mind that this model works for Spanish audio/speech.\\\", '<<<code>>>': \\\"from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Tokenizer\\\\nimport torchaudio\\\\n\\\\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained('hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD')\\\\ntokenizer = Wav2Vec2Tokenizer.from_pretrained('hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD')\\\\n\\\\nfile_name = 'path_to_your_audio_file.wav' # replace with the path to the audio file\\\\naudio_data, _ = torchaudio.load(file_name)\\\\ninput_values = tokenizer(audio_data, return_tensors='pt').input_values\\\\nlogits = model(input_values).logits\\\", 'api_call': \\\"Wav2Vec2ForSequenceClassification.from_pretrained('hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD', 'api_call': \\\"Wav2Vec2ForSequenceClassification.from_pretrained('hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD')\\\", 'api_arguments': {'model_name': 'hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD'}, 'python_environment_requirements': [{'transformers': '4.17.0', 'pytorch': '1.10.0+cu111', 'datasets': '2.0.0', 'tokenizers': '0.11.6'}], 'example_code': '', 'performance': {'dataset': 'MESD', 'accuracy': 0.9308}, 'description': 'This model is a fine-tuned version of facebook/wav2vec2-base on the MESD dataset. It is trained to classify underlying sentiment of Spanish audio/speech.' </s>\"\n"}
{"text": "<s>[INST]نعمل على تطوير نموذج آلي متعدد الوسائط للتعلم الآلي في مجال رؤية الحاسوب لإنشاء الصور بشكل غير مشروط. نريد برنامجاً يستطيع فهم البيانات المعقدة بمختلف الصيغ مثل النصوص والصور والرسوم البيانية.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-cifar10-32')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the necessary class from the diffusers package. This includes DDPMPipeline for the unconditional image generation model.', 'code': '', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'ddpm-cifar10-32', 'api_call': '', 'api_arguments': 'None', 'python_environment_requirements': '!pip install diffusers', 'example_code': 'from diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-cifar10-32\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset. Supports various discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.'}} </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نماذج في مجال معالجة اللغة الطبيعية لقياس تشابه الجملة بين نصوص مختلفة، حيث تقوم هذه النماذج بتحليل البنية اللغوية والدلالية للنصوص لملء الفجوة وتحديد مدى التشابه بينها.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library 'SentenceTransformer' from sentence-transformers.\\\\n2. With the SentenceTransformer, load the model 'sentence-transformers/paraphrase-MiniLM-L6-v2'.\\\\n3. Apply the model on your text to generate sentence embeddings representing semantic information of the sentences in your blog.\\\\n4. Apply clustering or other similarity comparison methods to identify semantically similar sections.\\\\n5. Create an index for your blog based on the similarities to make it more navigable for readers.\\\", '<<<code>>>': \\\"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\\\\nsentences = blog_post.split('\\\\\\\\n')\\\\nembeddings = model.encode(sentences)\\\\n\\\\n# Apply clustering or similarity comparison methods to organize your blog's content into sections\\\\n# Create an index based on the clustered content structure\\\", 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/paraphrase-MiniLM-L6-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': [['sentence-transformers']], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد نموذج تعلم آلي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': 'RandomForestRegressor()', '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': '1. Import the necessary libraries, including RandomForestRegressor and necessary data handling functions from scikit-learn.\\\\n2. Load the California Housing dataset which contains features like median_income, housing_median_age, and total_rooms along with the target variable, median_house_value.\\\\n3. Preprocess the data, splitting it into training and testing sets and scaling the features.\\\\n4. Create a RandomForestRegressor object with suitable hyperparameters.\\\\n5. Fit the model on the training data.\\\\n6. Use the model to make predictions on the testing data.\\\\n7. Evaluate the performance of the model using suitable metrics, such as mean squared error or R2 score.', '<<<code>>>': 'from sklearn.datasets import fetch_california_housing\\\\nfrom sklearn.model_selection import train_test_split\\\\nfrom sklearn.preprocessing import StandardScaler\\\\nfrom sklearn.ensemble import RandomForestRegressor\\\\nfrom sklearn.metrics import mean_squared_error, r2_score\\\\n\\\\ndata = fetch_california_housing()\\\\nX, y = data.data, data.target\\\\n\\\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\\\n\\\\nscaler = StandardScaler()\\\\nX_train_scaled = scaler.fit_transform(X_train)\\\\nX_test_scaled = scaler.transform(X_test)\\\\n\\\\nregressor = RandomForestRegressor(n_estimators=100, random_state=42)\\\\nregressor.fit(X_train_scaled, y_train)\\\\n\\\\ny_pred = regressor.predict(X_test_scaled)\\\\nmse = mean_squared_error(y_test, y_pred)\\\\nr2 = r2_score(y_test, y_pred)\\\\n\\\\nprint(\\\"Mean squared error:\\\", mse)\\\\nprint(\\\"R2 score:\\\", r2)', 'api_call': 'RandomForestRegressor()', 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rajistics/california_housing', 'api_call': 'RandomForestRegressor()', 'api_arguments': {'bootstrap': 'True', 'ccp_alpha': '0.0', 'criterion': 'squared_error', 'max_depth': '', 'max_features': '1.0', 'max_leaf_nodes': '', 'max_samples': '', 'min_impurity_decrease': '0.0', 'min_samples_leaf': '1', 'min_samples_split': '2', 'min_weight_fraction_leaf': '0.0', 'n_estimators': '100', 'n_jobs': '', 'oob_score': 'False', 'random_state': '', 'verbose': '0', 'warm_start': 'False'}, 'python_environment_requirements': ['Scikit-learn'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A RandomForestRegressor model trained on the California Housing dataset for predicting housing prices.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد نموذج لتصنيف اللغة الطبيعية دون تدريب مسبق باستخدام تقنيات معالجة اللغات الطبيعية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. First, we need to import the necessary classes from the transformers package. This includes AutoModelForSequenceClassification, AutoTokenizer, and pipeline for creating a zero-shot classification pipeline.\\\\n2. Initialize the zero-shot classification pipeline using the \\\\'facebook/bart-large-mnli\\\\' model.\\\\n3. Pass the sequence (the article title \\\"one day I will see the world\\\") and a list of candidate labels that the classifier needs to predict from (in this case, \\\\'travel\\\\', \\\\'cooking\\\\', and \\\\'dancing\\\\').\\\\n4. The model will return the predicted category along with its confidence probability.', '<<<code>>>': \\\"from transformers import pipeline\\\\nclassifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\\\\nsequence_to_classify = 'one day I will see the world'\\\\ncandidate_labels = ['travel', 'cooking', 'dancing']\\\\nclassifier(sequence_to_classify, candidate_labels)\\\", 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large-mnli', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\\\", 'api_arguments': {'sequence_to_classify': 'one day I will see the world', 'candidate_labels': \\\"['travel', 'cooking', 'dancing']\\\"}, 'python_environment_requirements': [{'transformers': 'from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline'}], 'example_code': {'with_pipeline': \\\"from transformers import pipeline\\\\nclassifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\\\\nsequence_to_classify = 'one day I will see the world'\\\\ncandidate_labels = ['travel', 'cooking', 'dancing']\\\\nclassifier(sequence_to_classify, candidate_labels)\\\", 'with_manual_pytorch': \\\"from transformers import AutoModelForSequenceClassification, AutoTokenizer\\\\nnli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\\\\ntokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\\\\npremise = sequence\\\\nhypothesis = f'This example is {label}.'\\\\nx = tokenizer.encode(premise, hypothesis, return_tensors='pt', truncation_strategy='only_first')\\\\nlogits = nli_model(x.to(device))[0]\\\\nentail_contradiction_logits = logits[:,[0,2]]\\\\nprobs = entail_contradiction_logits.softmax(dim=1)\\\\nprob_label_is_true = probs[:,1]\\\"}, 'performance': {'dataset': 'multi_nli', 'accuracy': 'Not specified'}, 'description': 'This is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset. The model can be used for zero-shot text classification by posing the sequence to be classified as the NLI premise and constructing a hypothesis from each candidate label. The probabilities for entailment and contradiction are then converted to label probabilities.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لإنشاء توليد نصوص صوتية بدون شروط في مجال تحويل النصوص إلى كلام، ونريد استخدام هذا النموذج لتوليد نصوص صوتية جديدة بشكل تلقائي وغير مشروط.? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'api_provider': 'ESPnet', 'explanation': \\\"1. First, clone the ESPnet repository and switch to the specific commit 'd5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f' that contains the desired functionality.\\\", 'code': 'cd espnet\\\\ngit checkout d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\\\\npip install -e .\\\\ncd egs2/amadeus/tts1\\\\n./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'ESPnet', 'functionality': 'Text-to-Speech', 'api_name': 'mio/amadeus', 'api_call': 'cd espnet\\\\ngit checkout d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\\\\npip install -e .\\\\ncd egs2/amadeus/tts1\\\\n./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'api_arguments': {'model_name': 'mio/amadeus'}, 'python_environment_requirements': {'espnet': 'd5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f', 'transformers': 'latest'}, 'example_code': 'cd espnet\\\\ngit checkout d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\\\\npip install -e .\\\\ncd egs2/amadeus/tts1\\\\n./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'performance': {'dataset': 'amadeus', 'accuracy': 'Not provided'}, 'description': 'This model was trained by mio using amadeus recipe in espnet.'}} </s>\"\n"}
{"text": "<s>[INST]نقترح إنشاء نادي صوتي للتصنيف يجتمع كل شهر لتحليل وتصنيف الصوتيات الجديدة. يمكنك تزويد الأعضاء بأدوات لتصنيف وتقييم النماذج الصوتية المختلفة.? [/INST]'<<<domain>>>': 'Audio Classification', '<<<api_call>>>': \\\"pipeline('audio-classification', model='mazkooleg/0-9up-unispeech-sat-base-ft')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, you need to install the required Python packages, such as transformers, pytorch, datasets, and tokenizers.\\\\n2. Import 'pipeline' from transformers to instantiate the audio-classification pipeline.\\\\n3. Load the pretrained model 'mazkooleg/0-9up-unispeech-sat-base-ft' and create the classification pipeline. This model is designed to recognize digits from 0 to 9.\\\\n4. Pass the audio input to the pipeline, and it will return the predicted class (digit) with the associated confidence score.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nclassifier = pipeline('audio-classification', model='mazkooleg/0-9up-unispeech-sat-base-ft')\\\\n\\\\n# Load and preprocess the audio file\\\\n# ...\\\\n\\\\n# Perform prediction on the preprocessed audio file\\\\nresult = classifier(audio_processed)\\\", 'api_call': \\\"pipeline('audio-classification', model='mazkooleg/0-9up-unispeech-sat-base-ft')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mazkooleg/0-9up-unispeech-sat-base-ft', 'api_call': \\\"pipeline('audio-classification', model='mazkooleg/0-9up-unispeech-sat-base-ft')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers==4.27.1, pytorch==1.11.0, datasets==2.10.1, tokenizers==0.12.1'], 'example_code': '', 'performance': {'dataset': 'mazkooleg/0-9up_google_speech_commands_augmented_raw', 'accuracy': 0.9979}, 'description': 'This model is a fine-tuned version of microsoft/unispeech-sat-base on the None dataset. It achieves the following results on the evaluation set: Loss: 0.0123, Accuracy: 0.9979.' </s>\"\n"}
{"text": "<s>[INST]يمكنك تجريب نموذج تعبئة الفجوات في مجال تصنيف النصوص باستخدام معالجة اللغة الطبيعية عن طريق إدخال عبارة ناقصة مع إشارة التعويض في النص وسيقوم النموذج بملاءمة الفجوة بكلمة مناسبة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. The pipeline function is used to create a text classification model, which helps to classify text into predefined categories such as positive, negative, and neutral.\\\\n3. We load the model 'Seethal/sentiment_analysis_generic_dataset', a pre-trained model designed for sentiment analysis.\\\\n4. The created classifier can be used to analyze Twitter messages or any text, and classify their sentiment as positive, negative, or neutral.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsentiment_analysis = pipeline(\\\\'text-classification\\\\', model=\\\\'Seethal/sentiment_analysis_generic_dataset\\\\')\\\\nresult = sentiment_analysis(\\\"I\\\\'m having a great time at the event!\\\")', 'api_call': \\\"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentiment_analysis_generic_dataset', 'api_call': \\\"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': \\\"sentiment_analysis('I love this product!')\\\", 'performance': {'dataset': 'generic_dataset', 'accuracy': 'Not specified'}, 'description': 'This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text classification.' </s>\"\n"}
{"text": "<s>[INST]نحن مهتمون بإنشاء نموذج لتحويل الصور إلى تصنيفات بواسطة تقنية رؤية الحاسوب، حيث يقوم النموذج بتصنيف المحتوى البصري في الصور. هل لديك الخبرة اللازمة في هذا المجال؟? [/INST]{'domain': 'Computer Vision Image-to-Image', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': 'from PIL import Image\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom controlnet_aux import OpenposeDetector\\\\nfrom diffusers.utils import load_image\\\\n\\\\nopenpose = OpenposeDetector.from_pretrained(\\\\'lllyasviel/ControlNet\\\\')\\\\nimage = load_image(\\\\'patient_exercise_image.jpg\\\\')\\\\n# replace \\\\'patient_exercise_image.jpg\\\\' with path to your image\\\\nimage = openpose(image)\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/sd-controlnet-openpose\\\\', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\nestimated_pose_image = pipe(\\\"exercise description\\\", image, num_inference_steps=20).images[0]\\\\nestimated_pose_image.save(\\\\'patient_pose_estimated.jpg\\\\')', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Human Pose Estimation', 'api_name': 'lllyasviel/sd-controlnet-openpose', 'api_call': 'from PIL import Image\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom controlnet_aux import OpenposeDetector\\\\nfrom diffusers.utils import load_image\\\\n\\\\nopenpose = OpenposeDetector.from_pretrained(\\\\'lllyasviel/ControlNet\\\\')\\\\nimage = load_image(\\\\'patient_exercise_image.jpg\\\\')\\\\n# replace \\\\'patient_exercise_image.jpg\\\\' with path to your image\\\\nimage = openpose(image)\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/sd-controlnet-openpose\\\\', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\nestimated_pose_image = pipe(\\\"exercise description\\\", image, num_inference_steps=20).images[0]\\\\nestimated_pose_image.save(\\\\'patient_pose_estimated.jpg\\\\')', 'api_arguments': {'text': 'chef in the kitchen', 'image': 'image', 'num_inference_steps': 20}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': \\\"from PIL import Image\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom controlnet_aux import OpenposeDetector\\\\nfrom diffusers.utils import load_image\\\\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\\\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\\\\nimage = openpose(image)\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\\\\n)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\\\\nimage.save('images/chef_pose_out.png')\\\", 'performance': {'dataset': '200k pose-image, caption pairs', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.'}} </s>\"\n"}
{"text": "<s>[INST]إنشاء سؤال يطلب من المستخدم إيجاد إجابة على السؤال من خلال صورة ونص قصير.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"pipeline('document-question-answering', model='donut-base-finetuned-docvqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required 'pipeline' function from the transformers library.\\\\n2. Initialize the 'document-question-answering' pipeline using the model 'naver-clova-ix/donut-base-finetuned-docvqa' which is trained for this purpose.\\\\n3. Load an image containing the table and ask a question related to the statistics in the table.\\\\n4. The 'pipeline' function creates a model that can identify text from the image, intelligently analyze the content, and generate a relevant response to the question provided.\\\\n5. Finally, the model will answer the question based on its understanding of the image's content.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ndoc_qa = pipeline('document-question-answering', model='naver-clova-ix/donut-base-finetuned-docvqa')\\\\nimage_path = 'path_to_image'\\\\nquestion = 'your_question'\\\\nanswer = doc_qa({'image': image_path, 'question': question})\\\\nprint(answer)\\\", 'api_call': \\\"pipeline('document-question-answering', model='donut-base-finetuned-docvqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'naver-clova-ix/donut-base-finetuned-docvqa', 'api_call': \\\"pipeline('document-question-answering', model='donut-base-finetuned-docvqa')\\\", 'api_arguments': {'image': 'path_to_image', 'question': 'your_question'}, 'python_environment_requirements': ['Transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ndoc_qa = pipeline('document-question-answering', model='naver-clova-ix/donut-base-finetuned-docvqa')\\\\n\\\\n# Load an image and ask a question\\\\nimage_path = 'path_to_image'\\\\nquestion = 'your_question'\\\\n\\\\n# Get the answer\\\\nanswer = doc_qa({'image': image_path, 'question': question})\\\\nprint(answer)\\\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository. Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>\"\n"}
{"text": "<s>[INST]قموا بإعداد كلمة جديدة لاختبار جودة النص الصوتي. انطلقوا في تحويل النص إلى كلام باستخدام تقنية تحويل النص إلى كلام. قموا بتسجيل النص الصوتي الذي تم إنشاؤه ومقارنته مع النص الأصلي. قوموا بتحليل الفروق وتحسين جودة التحويل بالبحث عن حلول إضافية.? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': \\\"Tacotron2.from_hparams(source='speechbrain/tts-tacotron2-ljspeech')\\\", 'api_provider': 'SpeechBrain', 'explanation': '1. We first import the necessary libraries and classes. This includes Tacotron2 from SpeechBrain for the text-to-speech model and torchaudio for audio processing.', 'code': 'import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2, HIFIGAN\\\\n\\\\ntext = \\\"Introducing our new product, the XYZ Widget, designed to make your life easier!\\\"\\\\ntacotron2 = Tacotron2.from_hparams(source=\\\\'speechbrain/tts-tacotron2-ljspeech\\\\')\\\\nhifi_gan = HIFIGAN.from_hparams(source=\\\\'speechbrain/tts-hifigan-ljspeech\\\\')\\\\n\\\\nmel_output, _, _ = tacotron2.encode_text(text)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save(\\\\'product_information_TTS.wav\\\\', waveforms.squeeze(1), 22050)', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'SpeechBrain', 'functionality': 'Text-to-Speech', 'api_name': 'speechbrain/tts-tacotron2-ljspeech', 'api_call': 'import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2, HIFIGAN\\\\n\\\\ntext = \\\"Introducing our new product, the XYZ Widget, designed to make your life easier!\\\"\\\\ntacotron2 = Tacotron2.from_hparams(source=\\\\'speechbrain/tts-tacotron2-ljspeech\\\\')\\\\nhifi_gan = HIFIGAN.from_hparams(source=\\\\'speechbrain/tts-hifigan-ljspeech\\\\')\\\\n\\\\nmel_output, _, _ = tacotron2.encode_text(text)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save(\\\\'product_information_TTS.wav\\\\', waveforms.squeeze(1), 22050)', 'api_arguments': ['text'], 'python_environment_requirements': ['speechbrain'], 'example_code': ['import torchaudio', 'from speechbrain.pretrained import Tacotron2', 'from speechbrain.pretrained import HIFIGAN', 'tacotron2 = Tacotron2.from_hparams(source=speechbrain/tts-tacotron2-ljspeech, savedir=tmpdir_tts)', 'hifi_gan = HIFIGAN.from_hparams(source=speechbrain/tts-hifigan-ljspeech, savedir=tmpdir_vocoder)', 'mel_output, mel_length, alignment = tacotron2.encode_text(Mary had a little lamb)', 'waveforms = hifi_gan.decode_batch(mel_output)', \\\"torchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\\\"], 'performance': {'dataset': 'LJSpeech', 'accuracy': 'Not specified'}, 'description': 'This repository provides all the necessary tools for Text-to-Speech (TTS) with SpeechBrain using a Tacotron2 pretrained on LJSpeech. The pre-trained model takes in input a short text and produces a spectrogram in output. One can get the final waveform by applying a vocoder (e.g., HiFIGAN) on top of the generated spectrogram.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتحميل مجموعة بيانات الصوت لتصنيف الصوت على الكمبيوتر، ثم قم بتدريب نموذج التعلم العميق على هذه البيانات الصوتية لتصنيفها تلقائياً. بعد ذلك، قم بتقييم أداء النموذج وقارنه بنماذج مختلفة لتحديد أفضل نموذج لتصنيف الصوت.? [/INST]{'domain': 'Audio Audio Classification', 'api_call': \\\"Wav2Vec2ForSpeechClassification.from_pretrained('harshit345/xlsr-wav2vec-speech-emotion-recognition')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary libraries, which include transformers and torchaudio.', 'code': 'from transformers import Wav2Vec2ForSpeechClassification, Wav2Vec2Processor\\\\nimport torchaudio\\\\nimport torch\\\\n\\\\ntokenizer = Wav2Vec2Processor.from_pretrained(\\\"harshit345/xlsr-wav2vec-speech-emotion-recognition\\\")\\\\nmodel = Wav2Vec2ForSpeechClassification.from_pretrained(\\\"harshit345/xlsr-wav2vec-speech-emotion-recognition\\\")\\\\n\\\\ndef load_audio_file(file_path, sampling_rate=16000):\\\\n    speech, _ = torchaudio.load(file_path)\\\\n    return speech\\\\n\\\\ndef get_emotion_prediction(file_path, sampling_rate=16000):\\\\n    speech = load_audio_file(file_path, sampling_rate)\\\\n    input_values = tokenizer(speech, return_tensors=\\\"pt\\\", padding=\\\"longest\\\").input_values\\\\n    audio_logits = model(input_values).logits\\\\n    predicted_emotion_id = torch.argmax(audio_logits, dim=-1)\\\\n    return tokenizer.decode(predicted_emotion_id)\\\\n\\\\nemotion_prediction = get_emotion_prediction(customer_support_call_file_path)', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Speech Emotion Recognition', 'api_name': 'harshit345/xlsr-wav2vec-speech-emotion-recognition', 'api_call': 'from transformers import Wav2Vec2ForSpeechClassification, Wav2Vec2Processor\\\\nimport torchaudio\\\\nimport torch\\\\n\\\\ntokenizer = Wav2Vec2Processor.from_pretrained(\\\"harshit345/xlsr-wav2vec-speech-emotion-recognition\\\")\\\\nmodel = Wav2Vec2ForSpeechClassification.from_pretrained(\\\"harshit345/xlsr-wav2vec-speech-emotion-recognition\\\")\\\\n\\\\ndef load_audio_file(file_path, sampling_rate=16000):\\\\n    speech, _ = torchaudio.load(file_path)\\\\n    return speech\\\\n\\\\ndef get_emotion_prediction(file_path, sampling_rate=16000):\\\\n    speech = load_audio_file(file_path, sampling_rate)\\\\n    input_values = tokenizer(speech, return_tensors=\\\"pt\\\", padding=\\\"longest\\\").input_values\\\\n    audio_logits = model(input_values).logits\\\\n    predicted_emotion_id = torch.argmax(audio_logits, dim=-1)\\\\n    return tokenizer.decode(predicted_emotion_id)\\\\n\\\\nemotion_prediction = get_emotion_prediction(customer_support_call_file_path)', 'api_arguments': {'model_name_or_path': 'harshit345/xlsr-wav2vec-speech-emotion-recognition'}, 'python_environment_requirements': ['pip install git+https://github.com/huggingface/datasets.git', 'pip install git+https://github.com/huggingface/transformers.git', 'pip install torchaudio', 'pip install librosa'], 'example_code': \\\"path = '/data/jtes_v1.1/wav/f01/ang/f01_ang_01.wav'\\\\noutputs = predict(path, sampling_rate)\\\", 'performance': {'dataset': 'JTES v1.1', 'accuracy': {'anger': 0.82, 'disgust': 0.85, 'fear': 0.78, 'happiness': 0.84, 'sadness': 0.86, 'overall': 0.806}}, 'description': 'This model is trained on the JTES v1.1 dataset for speech emotion recognition. It uses the Wav2Vec2 architecture for audio classification and can recognize emotions like anger, disgust, fear, happiness, and sadness.'}} </s>\"\n"}
{"text": "<s>[INST]على سبيل المثال، ابدأ بتهيئة واجهة الواجهة البرمجية لإدارة استجوابات المستخدم. ثم قم بإنشاء نموذج للتعرف على الكلام الطبيعي وتحويل النص إلى فهم قابل للمعالجة. بعد ذلك، قم بدمج نموذج التعلم العميق في واجهة برمجة التطبيقات لديك. اختبر الأداء باستخدام أدوات تقييم الجودة وقم بتحسين التحديثات والتعلم المستقبلي لتعزيز القدرات التفاعلية للنظام.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelWithLMHead.from_pretrained('output-small')\\\", '<<<api_provider>>>': 'PyTorch Transformers', '<<<explanation>>>': '1. Import the required libraries: AutoTokenizer and AutoModelWithLMHead from transformers.\\\\n2. Prepare the tokenizer by using AutoTokenizer.from_pretrained(\\\\'microsoft/DialoGPT-small\\\\').\\\\n3. Load the pretrained model \\\\'output-small\\\\' using AutoModelWithLMHead.from_pretrained(\\\\'output-small\\\\'). This model is trained on the dialogues of a character, Joshua, from the game \\\"The World Ends With You\\\".\\\\n4. Define a loop in which user input will be taken and the model will generate a response. Tokenize the user input and combine it with previous conversation history (if any).\\\\n5. Use the model.generate() function to generate a response from the model up to a maximum length of 500 tokens. Fine-tune the generated response by adjusting parameters like temperature, top_k, top_p, and no_repeat_ngram_size.\\\\n6. Decode the generated response back to text and display it.', '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModelWithLMHead\\\\nimport torch\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\\\\nmodel = AutoModelWithLMHead.from_pretrained('output-small')\\\\n\\\\nfor step in range(100):\\\\n    new_user_input_ids = tokenizer.encode(input('>> User:') + tokenizer.eos_token, return_tensors='pt')\\\\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\\\\n    chat_history_ids = model.generate(bot_input_ids, max_length=500, pad_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=3, do_sample=True, top_k=100, top_p=0.7, temperature=0.8)\\\\n    print('AI: {}'.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\\\", 'api_call': \\\"AutoModelWithLMHead.from_pretrained('output-small')\\\", 'provider': 'PyTorch Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'satvikag/chatbot', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('output-small')\\\", 'api_arguments': {'tokenizer': \\\"AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\\\", 'model': \\\"AutoModelWithLMHead.from_pretrained('output-small')\\\"}, 'python_environment_requirements': [['torch', 'transformers']], 'example_code': \\\"for step in range(100):\\\\n  new_user_input_ids = tokenizer.encode(input('&gt;&gt; User:') + tokenizer.eos_token, return_tensors='pt')\\\\n  bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step &gt; 0 else new_user_input_ids\\\\n  chat_history_ids = model.generate(bot_input_ids, max_length=500, pad_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=3, do_sample=True, top_k=100, top_p=0.7, temperature = 0.8)\\\\n  print('AI: {}'.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\\\", 'performance': {'dataset': 'Kaggle game script dataset', 'accuracy': 'Not provided'}, 'description': 'DialoGPT Trained on the Speech of a Game Character, Joshua from The World Ends With You.' </s>\"\n"}
{"text": "<s>[INST]نقترح إنشاء نادي صوتي-صوري متعدد الوسائط يجتمع كل شهر لتحليل وتصنيف الصوتيات والصور الجديدة. يمكنك تزويد الأعضاء بأدوات لتصنيف وتقييم النماذج الصوتية والبصرية المختلفة.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-anime-1.0', torch_dtype=torch.float16)(prompt, negative_prompt=negative_prompt)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the StableDiffusionPipeline from the diffusers library and torch.\\\\n2. Load the Dreamlike Anime 1.0 model using the from_pretrained method of StableDiffusionPipeline, specifying the torch_dtype as 'torch.float16'.\\\\n3. Create a positive text prompt that describes the desired high-quality anime artwork.\\\\n4. Optionally, create a negative text prompt to guide the model away from specific unwanted attributes.\\\\n5. Use the loaded pipeline with the provided text prompts to generate an anime image.\\\\n6. Save the generated anime image to a desired location.\\\", '<<<code>>>': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\npipe = StableDiffusionPipeline.from_pretrained(\\\\'dreamlike-art/dreamlike-anime-1.0\\\\', torch_dtype=torch.float16)\\\\nprompt = \\\"anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, blue eyes, skirt\\\"\\\\nnegative_prompt = \\\"simple background, duplicate, retro style, low quality, lowres, watermark, signature, text, jpeg artifacts, blurry\\\"\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(\\\\'./result.jpg\\\\')', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-anime-1.0', torch_dtype=torch.float16)(prompt, negative_prompt=negative_prompt)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dreamlike-art/dreamlike-anime-1.0', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-anime-1.0', torch_dtype=torch.float16)(prompt, negative_prompt=negative_prompt)\\\", 'api_arguments': ['prompt', 'negative_prompt'], 'python_environment_requirements': [['diffusers', 'torch']], 'example_code': \\\"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = 'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': \\\"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\\\" </s>\"\n"}
{"text": "<s>[INST]أنشئ وصفًا مُرئيًا لصورة مُتعددة الوسائط تُستند إلى الصورة.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the necessary libraries: requests, PIL (Python Imaging Library), BlipProcessor, and Blip2ForConditionalGeneration from transformers.\\\\n2. Instantiate a BlipProcessor and Blip2ForConditionalGeneration model by using the pretrained model \\\\'Salesforce/blip2-flan-t5-xxl\\\\'.\\\\n3. Load the image by providing a URL or accessing the image file on disk and convert the image into an appropriate format.\\\\n4. Ask the model a question about the image, such as \\\"How many dogs are in the picture?\\\".\\\\n5. Process the image and question using the BlipProcessor instance to generate input tensors for the model.\\\\n6. Generate an output by passing the input tensors to the model, and then decode the output to produce a text description of the image.\\\\n7. The text description can be used to answer the quiz question.', '<<<code>>>': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\\\nprocessor = BlipProcessor.from_pretrained('Salesforce/blip2-flan-t5-xxl')\\\\nmodel = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\\\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\\\nquestion = 'How many dogs are in the picture?'\\\\ninputs = processor(raw_image, question, return_tensors='pt')\\\\nout = model.generate(**inputs)\\\\ntext_description = processor.decode(out[0], skip_special_tokens=True)\\\", 'api_call': \\\"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \\\"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\\\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': [['requests', 'PIL', 'transformers']], 'example_code': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\\\nquestion = how many dogs are in the picture?\\\\ninputs = processor(raw_image, question, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. First, we will import the necessary libraries that are needed to work with the model. That includes json, joblib, and pandas.\\\\n2. Next, we will load the pre-trained model using joblib.load(). This model is stored in a file named 'model.joblib'. The model is trained to predict carbon emissions from given input data.\\\\n3. Now, we will load our data using pandas' read_csv() function and filter it based on the required features mentioned in the config file.\\\\n4. We will then pass that filtered data into the model using model.predict(data), and obtain the predictions.\\\\n5. These predictions will provide us with the carbon emissions that can be used to propose solutions for reducing CO2 emissions.\\\", '<<<code>>>': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Robertooo/autotrain-hmaet-2037366891', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['json', 'joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'Robertooo/autotrain-data-hmaet', 'accuracy': {'Loss': 0.067, 'R2': 0.486, 'MSE': 0.005, 'MAE': 0.055, 'RMSLE': 0.036}}, 'description': 'A tabular regression model trained with AutoTrain to predict carbon emissions.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير برنامج يستخدم تقنيات معالجة اللغة الطبيعية لتصنيف النصوص بدون حاجة إلى تصنيف مسبق. هذا البرنامج يجب أن يكون قادراً على التعرف على الفئات الخاصة بذوي الإعاقة البصرية بناءً على النص.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('sileod/deberta-v3-base-tasksource-nli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries like AutoModelForSequenceClassification from transformers.\\\\n2. Load the DeBERTa model for sequence classification by using the from_pretrained method with the model name 'sileod/deberta-v3-base-tasksource-nli'. This model has been fine-tuned on 520 tasks using multi-task learning and has strong zero-shot validation performance on many tasks.\\\\n3. This model can be used for zero-shot NLI (Natural Language Inference) pipeline tasks, similar to bart-mnli but with better performance.\\\\n4. With the loaded model, you can perform zero-shot classification tasks and natural language inference.\\\", '<<<code>>>': \\\"from transformers import AutoModelForSequenceClassification\\\\nmodel = AutoModelForSequenceClassification.from_pretrained('sileod/deberta-v3-base-tasksource-nli')\\\", 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('sileod/deberta-v3-base-tasksource-nli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sileod/deberta-v3-base-tasksource-nli', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('sileod/deberta-v3-base-tasksource-nli')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': ['glue', 'piqa', 'sciq'], 'accuracy': '70% on WNLI'}, 'description': 'DeBERTa-v3-base fine-tuned with multi-task learning on 520 tasks of the tasksource collection. This checkpoint has strong zero-shot validation performance on many tasks, and can be used for zero-shot NLI pipeline (similar to bart-mnli but better).' </s>\"\n"}
{"text": "<s>[INST]يحتاج عميلنا في مجال التعددية الوسيلة لتحويل الصور إلى نص إلى برنامج يمكنه تحويل الجداول إلى سؤال وجواب.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model for Optical Character Recognition (OCR), which in this case is 'kha-white/manga-ocr-base'.\\\\n3. This loaded model was specifically designed to work with Japanese text and has been trained primarily for Japanese manga image-to-text tasks.\\\\n4. After loading the OCR model, use the pipeline to provide the manga image and the model will return the recognized text from the manga image.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nocr_pipeline = pipeline(\\\\'ocr\\\\', model=\\\\'kha-white/manga-ocr-base\\\\')\\\\nimage_path = \\\"path/to/your/manga/image.jpg\\\"\\\\nwith open(image_path, \\\\'rb\\\\') as image_file:\\\\n    image = image_file.read()\\\\nrecognized_text = ocr_pipeline(image)', 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'api_arguments': 'image', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نموذج لتوليد النصوص متعددة الوسائط بشكل تلقائي للصور باستخدام معالجة اللغة الطبيعية، ونحتاج إلى تقدير الجودة والدقة في النصوص التي يتم إنشاؤها.? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\\\nprocessor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\\\\nmodel = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\\\\nraw_image = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with the path to your image\\\\ninputs = processor(raw_image, 'a photography of', return_tensors='pt')\\\\noutputs = model.generate(**inputs)\\\\ncaption = processor.decode(outputs[0], skip_special_tokens=True)\\\\nprint(caption)\\\", 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \\\"from PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\\\nprocessor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\\\\nmodel = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\\\\nraw_image = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with the path to your image\\\\ninputs = processor(raw_image, 'a photography of', return_tensors='pt')\\\\noutputs = model.generate(**inputs)\\\\ncaption = processor.decode(outputs[0], skip_special_tokens=True)\\\\nprint(caption)\\\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\\\ntext = a photography of\\\\ninputs = processor(raw_image, text, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).'}} </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى توليد ملخص نصي لمشهد يصور مقاطع فيديو للحيوانات في البرية بواسطة تقنيات التصوير الحاسوبي والتصنيف. هل يمكنك مساعدتي في ذلك؟? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-ssv2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary classes from the transformers library, which are VideoMAEImageProcessor and VideoMAEForVideoClassification.', 'code': 'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(\\\\'MCG-NJU/videomae-base-finetuned-ssv2\\\\')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(\\\\'MCG-NJU/videomae-base-finetuned-ssv2\\\\')\\\\ninputs = processor(video, return_tensors=\\\\'pt\\\\')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'MCG-NJU/videomae-base-finetuned-ssv2', 'api_call': 'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(\\\\'MCG-NJU/videomae-base-finetuned-ssv2\\\\')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(\\\\'MCG-NJU/videomae-base-finetuned-ssv2\\\\')\\\\ninputs = processor(video, return_tensors=\\\\'pt\\\\')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_arguments': 'video', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Something-Something-v2', 'accuracy': {'top-1': 70.6, 'top-5': 92.6}}, 'description': 'VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]إذا كنت بحاجة إلى تحويل الصوت إلى صوت بلغة عربية، يمكنك استخدام واجهة برمجة التطبيقات المخصصة لتحويل الصوت إلى صوت. يمكنك تزويد الصوت المطلوب لتحويله، وستحصل على الإخراج الصوتي المناسب تلقائيًا.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"Asteroid('JorisCos/ConvTasNet_Libri2Mix_sepclean_16k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first install the necessary Python packet: Asteroid.\\\\n2. We import the Asteroid class and instantiate it with the 'JorisCos/ConvTasNet_Libri2Mix_sepclean_16k' model provided by Hugging Face Transformers. This model has been trained to separate clean speech from noisy audio, which is exactly what we need for removing background noise from podcasts.\\\\n3. With the instantiated model, we can process any audio file, separate the clean speech, and save the cleaned audio result as a new file.\\\", '<<<code>>>': \\\"from asteroid import Asteroid\\\\nmodel = Asteroid('JorisCos/ConvTasNet_Libri2Mix_sepclean_16k')\\\\ncleaned_audio = model.denoise(audio_file_path)\\\\n# Replace 'audio_file_path' with the path to your audio file\\\\ncleaned_audio.export('denoised_podcast.wav', format='wav')\\\\n# Replace 'denoised_podcast.wav' with the desired output file name\\\", 'api_call': \\\"Asteroid('JorisCos/ConvTasNet_Libri2Mix_sepclean_16k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ConvTasNet_Libri2Mix_sepclean_16k', 'api_call': \\\"Asteroid('JorisCos/ConvTasNet_Libri2Mix_sepclean_16k')\\\", 'api_arguments': [], 'python_environment_requirements': [['asteroid']], 'example_code': '', 'performance': {'dataset': 'Libri2Mix', 'accuracy': {'si_sdr': 15.243671356901526, 'si_sdr_imp': 15.243034178473609, 'sdr': 15.668108919568112, 'sdr_imp': 15.578229918028036, 'sir': 25.295100756629957, 'sir_imp': 25.205219921301754, 'sar': 16.307682590197313, 'sar_imp': -51.64989963759405, 'stoi': 0.9394951175291422, 'stoi_imp': 0.22640192740016568}}, 'description': 'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.' </s>\"\n"}
{"text": "<s>[INST]يمكنك استخدام تقنيات معالجة اللغة الطبيعية لتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل تحتاج مساعدة في هذا المجال؟? [/INST]{'domain': 'Natural Language Processing Conversational', 'api_call': \\\"pipeline('text-generation', model='pygmalion-6b')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import pipeline\\\\n\\\\nchatbot = pipeline(\\\\'text-generation\\\\', model=\\\\'pygmalion-6b\\\\')\\\\ninput_prompt = \\\"You: Hey, do you know about any recent breakthroughs in artificial intelligence?\\\"\\\\n\\\\nresponse_text = chatbot(input_prompt)\\\\nresponse = response_text[0][\\\\'generated_text\\\\']', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Generation', 'api_name': 'pygmalion-6b', 'api_call': 'from transformers import pipeline\\\\n\\\\nchatbot = pipeline(\\\\'text-generation\\\\', model=\\\\'pygmalion-6b\\\\')\\\\ninput_prompt = \\\"You: Hey, do you know about any recent breakthroughs in artificial intelligence?\\\"\\\\n\\\\nresponse_text = chatbot(input_prompt)\\\\nresponse = response_text[0][\\\\'generated_text\\\\']', 'api_arguments': ['input_prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \\\"[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\\\\n<START>\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\\\", 'performance': {'dataset': '56MB of dialogue data', 'accuracy': 'Not specified'}, 'description': \\\"Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B. The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model was initialized from the uft-6b ConvoGPT model and fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed.\\\"}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام لمعالجة اللغة الطبيعية لقياس التشابه بين الجمل، ونحتاج إلى تحديد مدى تشابه بين الجمل النصية.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the SentenceTransformer class from the sentence_transformers package.\\\\n2. Instantiate the SentenceTransformer class with 'sentence-transformers/nli-mpnet-base-v2', which is a pre-trained model that maps texts to dense vector embeddings.\\\\n3. Given a list of research paper abstracts as strings, encode them using the encode method of the SentenceTransformer instance. This will return a list of embeddings for each abstract.\\\\n4. Compute similarity between abstract embeddings using cosine similarity, and identify the most similar research papers based on their similarity scores.\\\", '<<<code>>>': \\\"from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nmodel = SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\\n\\\\nabstracts = [\\\\n    'Abstract of paper 1...',\\\\n    'Abstract of paper 2...',\\\\n    'Abstract of paper 3...',\\\\n    # Add more abstracts\\\\n]\\\\n\\\\nembeddings = model.encode(abstracts)\\\\nsimilarity_matrix = cosine_similarity(embeddings)\\\", 'api_call': \\\"SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/nli-mpnet-base-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لتحسين تقنية تقسيم الصور في مجال رؤية الحاسوب.? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"YOLO('keremberke/yolov8s-building-segmentation')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the necessary YOLO class and render_result function from the ultralyticsplus package.', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-building-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://example.com/satellite-image.jpg'\\\\n# replace the URL with the actual satellite image URL\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-building-segmentation', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-building-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://example.com/satellite-image.jpg'\\\\n# replace the URL with the actual satellite image URL\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_arguments': ['conf', 'iou', 'agnostic_nms', 'max_det', 'image'], 'python_environment_requirements': ['ultralyticsplus==0.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8s-building-segmentation')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'print(results[0].masks)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.661, 'mAP@0.5(mask)': 0.651}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.'}} </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير تقنية التصنيف البصري للصور في شركتنا، ونحتاج إلى إنشاء وظيفة براتب يحتوي على بكالوريوس متاحة للبيانات لتحسين الأداء.? [/INST]{'domain': 'Computer Vision Image Classification', 'api_call': \\\"timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import necessary libraries: timm module for creating and using the model, PIL.Image module for processing image files, and urllib.request to download images from the internet.', 'code': 'from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimage_url = \\\"https://example.com/path/to/image.jpg\\\"\\\\nimg = Image.open(urlopen(image_url))\\\\nmodel = timm.create_model(\\\\'convnext_base.fb_in1k\\\\', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification, Feature Map Extraction, Image Embeddings', 'api_name': 'convnext_base.fb_in1k', 'api_call': 'from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimage_url = \\\"https://example.com/path/to/image.jpg\\\"\\\\nimg = Image.open(urlopen(image_url))\\\\nmodel = timm.create_model(\\\\'convnext_base.fb_in1k\\\\', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))', 'api_arguments': {'pretrained': 'True', 'features_only': 'True', 'num_classes': '0'}, 'python_environment_requirements': ['timm'], 'example_code': ['from urllib.request import urlopen', 'from PIL import Image', 'import timm', \\\"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\\", \\\"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\", 'model = model.eval()', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'output = model(transforms(img).unsqueeze(0))'], 'performance': {'dataset': 'imagenet-1k', 'accuracy': '83.82%'}, 'description': 'A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير منتج في مجال معالجة اللغة الطبيعية لتصنيف النصوص، حيث نهدف إلى تنبؤ رتب الوظائف بناء على تحليل بيانات النصوص المتاحة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers library, which are AutoTokenizer and AutoModelForSequenceClassification.\\\\n2. Load the pre-trained model using the from_pretrained method with model name 'cross-encoder/ms-marco-TinyBERT-L-2-v2'. This model was trained on the MS Marco Passage Ranking task, which makes it suitable for information retrieval tasks such as matching user queries with book descriptions.\\\\n3. Instantiate the tokenizer using the AutoTokenizer.from_pretrained method with the same model name, which will be used to tokenize the input texts (queries and book descriptions).\\\\n4. Tokenize the user queries and corresponding book descriptions using the tokenizer.encode function with appropriate padding, truncation, and return_tensors options.\\\\n5. Pass the tokenized inputs to the model for SequenceClassification and obtain ranking scores for each query-description pair.\\\\n6. Sort the book descriptions based on their ranking scores to find the best matching books for the given user query.\\\", '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nimport torch\\\\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\\\\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\\\\nqueries = ['First user query', 'Second user query']\\\\nbook_descriptions = ['First book description', 'Second book description']\\\\nfeatures = tokenizer(queries, book_descriptions, padding=True, truncation=True, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    scores = model(**features).logits\\\", 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cross-encoder/ms-marco-TinyBERT-L-2-v2', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\\\", 'api_arguments': {'tokenizer': \\\"tokenizer = AutoTokenizer.from_pretrained('model_name')\\\", 'features': \\\"features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors='pt')\\\"}, 'python_environment_requirements': [['transformers', 'torch']], 'example_code': {'import': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification', 'model': \\\"model = AutoModelForSequenceClassification.from_pretrained('model_name')\\\", 'tokenizer': \\\"tokenizer = AutoTokenizer.from_pretrained('model_name')\\\", 'features': \\\"features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors='pt')\\\", 'scores': 'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)'}, 'performance': {'dataset': 'TREC Deep Learning 2019', 'accuracy': '69.84 (NDCG@10)'}, 'description': 'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتحويل الصور إلى نصوص متعددة الوسائط، ونشجع على توفير بيانات وسائط متعددة لتحسين جودة الإخراج في مجال تصنيف الصور بدون تصنيف مسبق باستخدام الرؤية الحاسوبية.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"CLIPModel.from_pretrained('openai/clip-vit-large-patch14').\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. For Zero-Shot Image Classification, you can use the CLIP model. We first import the necessary classes from the transformers package. This includes CLIPModel and CLIPConfig.\\\\n2. We then use the from_pretrained method of the CLIPModel class to load the pre-trained model 'openai/clip-vit-large-patch14'.\\\\n3. This model can be used to analyze an image and identify the best-matching labels for it without requiring labeled data.\\\\n4. You must provide your own image, and the model should be able to classify the image based on the information it has learned during its pre-training.\\\", '<<<code>>>': \\\"from transformers import CLIPModel, CLIPConfig, CLIPProcessor\\\\nconfig = CLIPConfig.from_pretrained('openai/clip-vit-large-patch14')\\\\nmodel = CLIPModel.from_pretrained('openai/clip-vit-large-patch14', config=config)\\\\nimage_path = 'path_to_image.jpg' # replace this with the actual path to the image\\\\ntokenizer = AutoTokenizer.from_pretrained('openai/clip-vit-large-patch14')\\\\nprocessor = CLIPProcessor.from_pretrained('openai/clip-vit-large-patch14')\\\\ninputs = processor(images=image_path, text=labels, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'api_call': \\\"CLIPModel.from_pretrained('openai/clip-vit-large-patch14').\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai/clip-vit-large-patch14-336', 'api_call': \\\"CLIPModel.from_pretrained('openai/clip-vit-large-patch14').\\\", 'api_arguments': 'image_path, tokenizer, model', 'python_environment_requirements': ['Transformers 4.21.3, TensorFlow 2.8.2, Tokenizers 0.12.1'], 'example_code': 'N/A', 'performance': {'dataset': 'unknown', 'accuracy': 'N/A'}, 'description': 'This model was trained from scratch on an unknown dataset.' </s>\"\n"}
{"text": "<s>[INST]استخدم تقنيات معالجة اللغة الطبيعية لإنشاء قصة قصيرة تبدأ بالنص: \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"LEDForConditionalGeneration.from_pretrained('MingZhong/DialogLED-base-16384')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the LEDForConditionalGeneration class from the transformers library provided by Hugging Face.\\\\n2. Load the pre-trained DialogLED model with the \\\\'MingZhong/DialogLED-base-16384\\\\' model ID. DialogLED is designed for long dialogue understanding and summarization.\\\\n3. Use the loaded model to generate a summary of the provided long conversation, which discusses the book \\\"Anna Karenina\\\" by Leo Tolstoy.\\\\n4. You can then use the generated summary to get an overview of the main points discussed in the conversation.', '<<<code>>>': 'from transformers import LEDForConditionalGeneration, LEDTokenizer\\\\nmodel = LEDForConditionalGeneration.from_pretrained(\\\\'MingZhong/DialogLED-base-16384\\\\')\\\\ntokenizer = LEDTokenizer.from_pretrained(\\\\'MingZhong/DialogLED-base-16384\\\\')\\\\ninput_text = \\\"Anna Karenina is a novel by the Russian author Leo Tolstoy, first published in book form in 1878...\\\"\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=\\\\'pt\\\\')\\\\nsummary_ids = model.generate(input_ids, num_beams=4, length_penalty=2.0, min_length=30, max_length=150)\\\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)', 'api_call': \\\"LEDForConditionalGeneration.from_pretrained('MingZhong/DialogLED-base-16384')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DialogLED-base-16384', 'api_call': \\\"LEDForConditionalGeneration.from_pretrained('MingZhong/DialogLED-base-16384')\\\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'arxiv', 'accuracy': '2109.02492'}, 'description': 'DialogLED is a pre-trained model for long dialogue understanding and summarization. It builds on the Longformer-Encoder-Decoder (LED) architecture and uses window-based denoising as the pre-training task on a large amount of long dialogue data for further training. Here is a base version of DialogLED, the input length is limited to 16,384 in the pre-training phase.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير النظام للكشف عن نشاط الصوت في ملفات الصوت باستخدام تقنيات تحليل الصوت، ونريد توجيه النظام لتحسين دقة الكشف عن نشاط الصوت.? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"Pipeline.from_pretrained('philschmid/pyannote-speaker-diarization-endpoint')\\\", '<<<api_provider>>>': 'pyannote.audio', '<<<explanation>>>': \\\"1. First, import the Pipeline class from the pyannote.audio library.\\\\n2. Then, use the from_pretrained method to load the pre-trained model 'philschmid/pyannote-speaker-diarization-endpoint'. This model provides a speaker diarization pipeline that detects and separates speakers in audio recordings.\\\\n3. Apply the diarization pipeline to the meeting recordings. This will yield a speaker diarization result for each given audio file or stream.\\\\n4. Optionally, you can write speaker diarization results to an RTTM (Rich Transcription Time Marked) file for storage and analysis.\\\", '<<<code>>>': \\\"from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained('philschmid/pyannote-speaker-diarization-endpoint')\\\\ndiarization = pipeline('audio_file.wav')\\\\n# replace 'audio_file.wav' with path to your audio file\\\\nwith open('output_file.rttm', 'w') as rttm:\\\\n    diarization.write_rttm(rttm)\\\", 'api_call': \\\"Pipeline.from_pretrained('philschmid/pyannote-speaker-diarization-endpoint')\\\", 'provider': 'pyannote.audio', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'philschmid/pyannote-speaker-diarization-endpoint', 'api_call': \\\"Pipeline.from_pretrained('philschmid/pyannote-speaker-diarization-endpoint')\\\", 'api_arguments': ['num_speakers', 'min_speakers', 'max_speakers', 'segmentation_onset'], 'python_environment_requirements': ['pyannote.audio 2.0'], 'example_code': ['from pyannote.audio import Pipeline', 'pipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)', 'diarization = pipeline(audio.wav)', 'with open(audio.rttm, w) as rttm:', '  diarization.write_rttm(rttm)'], 'performance': {'dataset': [{'name': 'AISHELL-4', 'accuracy': {'DER%': 14.61, 'FA%': 3.31, 'Miss%': 4.35, 'Conf%': 6.95}}, {'name': 'AMI Mix-Headset only_words', 'accuracy': {'DER%': 18.21, 'FA%': 3.28, 'Miss%': 11.07, 'Conf%': 3.87}}, {'name': 'AMI Array1-01 only_words', 'accuracy': {'DER%': 29.0, 'FA%': 2.71, 'Miss%': 21.61, 'Conf%': 4.68}}, {'name': 'CALLHOME Part2', 'accuracy': {'DER%': 30.24, 'FA%': 3.71, 'Miss%': 16.86, 'Conf%': 9.66}}, {'name': 'DIHARD 3 Full', 'accuracy': {'DER%': 20.99, 'FA%': 4.25, 'Miss%': 10.74, 'Conf%': 6.0}}, {'name': 'REPERE Phase 2', 'accuracy': {'DER%': 12.62, 'FA%': 1.55, 'Miss%': 3.3, 'Conf%': 7.76}}, {'name': 'VoxConverse v0.0.2', 'accuracy': {'DER%': 12.76, 'FA%': 3.45, 'Miss%': 3.85, 'Conf%': 5.46}}]}, 'description': 'A speaker diarization pipeline that uses pyannote.audio to perform voice activity detection, speaker change detection, and overlapped speech detection. It can handle fully automatic processing with no manual intervention and can be fine-tuned with various hyperparameters.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا ببناء وتطوير نموذج للتعلم التعزيزي، الذي يهدف إلى تحقيق أداء ممتاز في مجال الوثائق متعددة الوسائط من خلال التفاعل مع البيئة، ونحتاج إلى تنبؤ بالسلوك واتخاذ القرارات الصحيحة بناءً على البيانات المرسلة إلى النموذج.? [/INST]{'domain': 'Multimodal Document Question Answer', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer\\\\n\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained(\\\\'hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering\\\\')\\\\n\\\\nquestion = \\\"What is the contract termination date?\\\"\\\\ncontext = \\\"This is a fictional legal document text containing the termination date: December 31, 2022.\\\"\\\\n\\\\ninputs = tokenizer(question, context, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**inputs)\\\\nanswer_start = output.start_logits.argmax().item()\\\\nanswer_end = output.end_logits.argmax().item()\\\\nanswer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs.input_ids[0][answer_start:answer_end+1]))', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMForQuestionAnswering', 'api_call': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer\\\\n\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained(\\\\'hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering\\\\')\\\\n\\\\nquestion = \\\"What is the contract termination date?\\\"\\\\ncontext = \\\"This is a fictional legal document text containing the termination date: December 31, 2022.\\\"\\\\n\\\\ninputs = tokenizer(question, context, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**inputs)\\\\nanswer_start = output.start_logits.argmax().item()\\\\nanswer_end = output.end_logits.argmax().item()\\\\nanswer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs.input_ids[0][answer_start:answer_end+1]))', 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLM model for question answering. This model is not pretrained and serves as an example for the LayoutLM architecture.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نموذج في اللغة العربية يمكنه ملء الفجوات في النصوص للمساعدة في معالجة اللغة الطبيعية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-char')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required libraries from the transformers package, including AutoModelForMaskedLM and AutoTokenizer.\\\\n2. Load the pre-trained BERT model for the Japanese language with character-level tokenization using the 'cl-tohoku/bert-base-japanese-char' model identifier.\\\\n3. Initialize the tokenizer to work with the Japanese language using the 'cl-tohoku/bert-base-japanese-char' tokenizer.\\\\n4. Replace the missing word(s) in your text with the [MASK] token.\\\\n5. Tokenize and convert the input text with the missing word(s) into a format that the model can understand.\\\\n6. Pass the processed input text to the pre-trained model for prediction.\\\\n7. Finally, predict the most suitable word for the [MASK] token in the input sentence.\\\", '<<<code>>>': 'from transformers import AutoModelForMaskedLM, AutoTokenizer\\\\nmodel = AutoModelForMaskedLM.from_pretrained(\\\\'cl-tohoku/bert-base-japanese-char\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'cl-tohoku/bert-base-japanese-char\\\\')\\\\ntext_with_mask = \\\"その後、彼はパーティーに[MASK]きました。\\\"\\\\ninput = tokenizer(text_with_mask, return_tensors=\\\"pt\\\")\\\\noutput = model(**input)\\\\npredictions = output.logits.argmax(-1)\\\\npredicted_words = tokenizer.batch_decode(predictions)', 'api_call': \\\"AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-char')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cl-tohoku/bert-base-japanese-char', 'api_call': \\\"AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-char')\\\", 'api_arguments': 'text', 'python_environment_requirements': [['transformers']], 'example_code': \\\"fill_mask('[MASK]')\\\", 'performance': {'dataset': 'wikipedia', 'accuracy': 'N/A'}, 'description': 'This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by character-level tokenization.' </s>\"\n"}
{"text": "<s>[INST]املأ الفراغ باستخدام تقنيات معالجة اللغة الطبيعية: \\\"كان ياما كان في أرض بعيدة، أظهر علماء معالجة اللغة الطبيعية أن الـ [MASK] يمكنه أن يكون مفيدًا في تحليل النصوص.\\\"? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the pretrained BERT model 'cl-tohoku/bert-base-japanese-whole-word-masking'.\\\\n3. The loaded model is trained to fill in masked tokens in a Japanese text, and can be used to provide suggestions for completing sentences or phrases.\\\\n4. Enter the text with a [MASK] token where the suggestion is needed, and the model will generate suggestions for the masked token.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nfill_mask = pipeline('fill-mask', model='cl-tohoku/bert-base-japanese-whole-word-masking')\\\\nsentence_with_mask = '今日の天気は[MASK]です。'\\\\nsuggestions = fill_mask(sentence_with_mask)\\\", 'api_call': \\\"AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cl-tohoku/bert-base-japanese-whole-word-masking', 'api_call': \\\"AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\\\", 'api_arguments': {'model': 'cl-tohoku/bert-base-japanese-whole-word-masking'}, 'python_environment_requirements': [{'transformers': 'latest'}], 'example_code': \\\"fill_mask('[MASK]')\\\", 'performance': {'dataset': 'Japanese Wikipedia', 'accuracy': 'Not provided'}, 'description': 'This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization. Additionally, the model is trained with the whole word masking enabled for the masked language modeling (MLM) objective.' </s>\"\n"}
{"text": "<s>[INST]أنشئ سؤالًا قصيرًا حول الكشف عن الكائنات في الصورة التي ستُعرض، واستخدم اللغة العربية.? [/INST]{'domain': 'Computer Vision Object Detection', 'api_call': \\\"YOLO('keremberke/yolov8m-blood-cell-detection')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries YOLO and render_result from ultralyticsplus.', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-blood-cell-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nresults = model.predict(image_path)\\\\nrender = render_result(model=model, image=image_path, result=results[0])\\\\nrender.show()\\\", 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-blood-cell-detection', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-blood-cell-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nresults = model.predict(image_path)\\\\nrender = render_result(model=model, image=image_path, result=results[0])\\\\nrender.show()\\\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8m-blood-cell-detection')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.927}, 'description': 'A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج لتوقع القيم الرقمية باستخدام بيانات جدولية. استخدم تقنيات معالجة اللغة الطبيعية الحديثة لتحسين أداء النموذج والتنبؤ بالبيانات بدقة عالية.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': 'RandomForestRegressor()', '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': '1. We first import the RandomForestRegressor class from the scikit-learn library.\\\\n2. We then create an instance of the RandomForestRegressor class with the specified parameters for housing price prediction.\\\\n3. After preparing a dataset with features of houses in California, we split the data into train and test sets, and fit the RandomForestRegressor model to the training data.\\\\n4. Once the model is trained, we can use it to make predictions on the test set or any new data to estimate house prices in California.', '<<<code>>>': 'from sklearn.ensemble import RandomForestRegressor\\\\nfrom sklearn.model_selection import train_test_split\\\\nfrom sklearn.metrics import mean_squared_error\\\\n\\\\n# Load and preprocess the dataset\\\\nX, y = process_data(dataset)\\\\n\\\\n# Split the dataset into train and test sets\\\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\\\n\\\\n# Create a RandomForestRegressor model\\\\nmodel = RandomForestRegressor()\\\\n\\\\n# Fit the model to the training data\\\\nmodel.fit(X_train, y_train)\\\\n\\\\n# Make predictions on the test set\\\\ny_pred = model.predict(X_test)\\\\n\\\\n# Calculate the mean squared error of the predictions\\\\nmse = mean_squared_error(y_test, y_pred)', 'api_call': 'RandomForestRegressor()', 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rajistics/california_housing', 'api_call': 'RandomForestRegressor()', 'api_arguments': {'bootstrap': 'True', 'ccp_alpha': '0.0', 'criterion': 'squared_error', 'max_depth': '', 'max_features': '1.0', 'max_leaf_nodes': '', 'max_samples': '', 'min_impurity_decrease': '0.0', 'min_samples_leaf': '1', 'min_samples_split': '2', 'min_weight_fraction_leaf': '0.0', 'n_estimators': '100', 'n_jobs': '', 'oob_score': 'False', 'random_state': '', 'verbose': '0', 'warm_start': 'False'}, 'python_environment_requirements': ['Scikit-learn'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A RandomForestRegressor model trained on the California Housing dataset for predicting housing prices.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام تصنيف الصوتيات، ونحتاج إلى إعداد بيانات تدريب متوافقة لتحسين أداء نموذج التصنيف الصوتي.? [/INST]'<<<domain>>>': 'Audio Classification', '<<<api_call>>>': \\\"HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries: HubertForSpeechClassification, Wav2Vec2FeatureExtractor, AutoConfig, torch, numpy, and pydub.\\\\n2. Load the pretrained model 'Rajaram1996/Hubert_emotion' using HubertForSpeechClassification.from_pretrained() and feature extractor using Wav2Vec2FeatureExtractor.from_pretrained('facebook/hubert-base-ls960').\\\\n3. Write a function called speech_file_to_array() to convert the audio file to a numpy array.\\\\n4. Use the function predict_emotion_hubert() that takes an audio file path as input, preprocesses the audio, and returns the top two emotions expressed in the audio segment along with the probability scores.\\\\n5. Call the function with the audio file path.\\\", '<<<code>>>': \\\"from audio_models import HubertForSpeechClassification\\\\nfrom transformers import Wav2Vec2FeatureExtractor, AutoConfig\\\\nimport torch.nn.functional as F\\\\nimport torch\\\\nimport numpy as np\\\\nfrom pydub import AudioSegment\\\\n\\\\n# Load the pretrained model and feature extractor\\\\nmodel = HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')\\\\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/hubert-base-ls960')\\\\n\\\\naudio_file = '/path/to/podcast_audio.wav'\\\\n\\\\nresult = predict_emotion_hubert(audio_file)\\\\nprint(result)\\\", 'api_call': \\\"HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Rajaram1996/Hubert_emotion', 'api_call': \\\"HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')\\\", 'api_arguments': {'audio_file': 'string'}, 'python_environment_requirements': [['audio_models', 'transformers', 'torch', 'numpy', 'pydub']], 'example_code': \\\"def predict_emotion_hubert(audio_file):\\\\n from audio_models import HubertForSpeechClassification\\\\n from transformers import Wav2Vec2FeatureExtractor, AutoConfig\\\\n import torch.nn.functional as F\\\\n import torch\\\\n import numpy as np\\\\n from pydub import AudioSegment\\\\nmodel = HubertForSpeechClassification.from_pretrained(Rajaram1996/Hubert_emotion)\\\\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(facebook/hubert-base-ls960)\\\\nsampling_rate=16000\\\\nconfig = AutoConfig.from_pretrained(Rajaram1996/Hubert_emotion)\\\\ndef speech_file_to_array(path, sampling_rate):\\\\n sound = AudioSegment.from_file(path)\\\\n sound = sound.set_frame_rate(sampling_rate)\\\\n sound_array = np.array(sound.get_array_of_samples())\\\\n return sound_array\\\\nsound_array = speech_file_to_array(audio_file, sampling_rate)\\\\ninputs = feature_extractor(sound_array, sampling_rate=sampling_rate, return_tensors=pt, padding=True)\\\\ninputs = {key: inputs[key].to(cpu).float() for key in inputs}\\\\nwith torch.no_grad():\\\\n logits = model(**inputs).logits\\\\nscores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\\\\noutputs = [{\\\\n emo: config.id2label[i],\\\\n score: round(score * 100, 1)}\\\\n for i, score in enumerate(scores)\\\\n]\\\\nreturn [row for row in sorted(outputs, key=lambda x:x[score], reverse=True) if row['score'] != '0.0%'][:2]\\\\nresult = predict_emotion_hubert(male-crying.mp3)\\\\nresult\\\", 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'A pretrained model for predicting emotion in local audio files using Hubert.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='bert-large-cased')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We import the 'pipeline' function of the transformers library.\\\\n2. We create an instance of the pipeline using the 'fill-mask' task and the BERT large cased model.\\\\n3. We pass the input text with a masked token [MASK] to the pipeline. The model will complete the missing word(s) in the given text based on its knowledge and training.\\\\n4. The completed text will be returned as output.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'bert-large-cased\\\\')\\\\ninput_text = \\\"All work and no play makes [MASK] a dull boy.\\\"\\\\noutput_text = unmasker(input_text)', 'api_call': \\\"pipeline('fill-mask', model='bert-large-cased')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-large-cased', 'api_call': \\\"pipeline('fill-mask', model='bert-large-cased')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"unmasker(Hello I'm a [MASK] model.)\\\", 'performance': {'dataset': {'SQUAD 1.1': {'F1': 91.5, 'EM': 84.8}, 'Multi NLI': {'accuracy': 86.09}}}, 'description': 'BERT large model (cased) pretrained on English language using a masked language modeling (MLM) objective. It has 24 layers, 1024 hidden dimensions, 16 attention heads, and 336M parameters.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتحويل النصوص إلى كلام مسجل في مجال تحويل الصوت إلى نص. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. The first step is to create the desired children's story text input.\\\\n2. Upon obtaining the desired input text, we use the ESPnet2 TTS model pre-trained on the 'amadeus' dataset.\\\\n3. To accomplish this, we utilize the 'run.sh' script provided by the 'kazusam/kt' API.\\\\n4. Include relevant flags to skip_data_prep and skip_train and specify the model to be downloaded, mio/amadeus.\\\\n5. The pre-trained model is then utilized to generate audio speech from the input text, resulting in a children's bedtime audiobook.\\\", '<<<code>>>': '# First, write the children\\\\'s story text input as a string\\\\ninput_text = \\\"Once upon a time in a magical forest...\\\"\\\\n\\\\n# Next, convert the input text to audio using the ESPnet2 TTS model\\\\n# Note: You need to have the ESPnet2 library and run.sh script properly set up\\\\n./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'api_call': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kazusam/kt', 'api_call': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'api_arguments': ['input_text'], 'python_environment_requirements': [['Hugging Face', 'ESPnet']], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': 'amadeus', 'accuracy': 'Not provided'}, 'description': 'An ESPnet2 TTS model trained by mio using amadeus recipe in espnet.' </s>\"\n"}
{"text": "<s>[INST]لتحسين مهارات البرمجة في مجال معالجة اللغة الطبيعية لإنتاج النصوص، أقترح تشكيل فريق طلابي يلتقون بانتظام لمناقشة التحديات وإيجاد الحلول. يجب عليك توفير أفضل الأدوات والموارد للطلاب لبناء نماذج نصية وتطبيقها على وكلاء التعلم الآلي.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"pipeline('text2text-generation', model='google/t5-v1_1-base')\\\", 'api_provider': 'Transformers', 'explanation': '1. To translate the text, we use the text-to-text generation pipeline from the transformers library provided by Hugging Face.', 'code': \\\"from transformers import pipeline\\\\nt5 = pipeline('text2text-generation', model='google/t5-v1_1-base')\\\\ntranslated_text = t5('translate English to German: The highly experienced team at our company is dedicated to transforming the world of innovation. Our products and services bridge the gap between cutting-edge technology and practical applications, providing solutions for industries such as healthcare, robotics, and renewable energy. By partnering with us, you can unlock unlimited potential for growth and success.')\\\", 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Text Generation', 'api_name': 'google/t5-v1_1-base', 'api_call': \\\"from transformers import pipeline\\\\nt5 = pipeline('text2text-generation', model='google/t5-v1_1-base')\\\\ntranslated_text = t5('translate English to German: The highly experienced team at our company is dedicated to transforming the world of innovation. Our products and services bridge the gap between cutting-edge technology and practical applications, providing solutions for industries such as healthcare, robotics, and renewable energy. By partnering with us, you can unlock unlimited potential for growth and success.')\\\", 'api_arguments': {'model': 'google/t5-v1_1-base'}, 'python_environment_requirements': {'transformers': '>=4.0.0'}, 'example_code': \\\"from transformers import pipeline\\\\n\\\\nt5 = pipeline('text2text-generation', model='google/t5-v1_1-base')\\\\nt5('translate English to French: Hugging Face is a great company')\\\", 'performance': {'dataset': 'c4', 'accuracy': 'Not provided'}, 'description': \\\"Google's T5 Version 1.1 is a state-of-the-art text-to-text transformer model that achieves high performance on various NLP tasks such as summarization, question answering, and text classification. It is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on downstream tasks.\\\"}} </s>\"\n"}
{"text": "<s>[INST]نريد تطوير برنامج في مجال معالجة اللغة الطبيعية يقوم بمقارنة الجمل لمساعدة المستخدمين على تحسين فهم النصوص. يُسهم ذلك في تحسين تجربة المستخدم وفهم النصوص بشكل أفضل.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we need to install and import the sentence-transformers library.\\\\n2. Next, we can use the provided model 'sentence-transformers/all-MiniLM-L12-v2' to create a SentenceTransformer instance.\\\\n3. Using this model, we can convert the product reviews into dense vector representations (embeddings) by calling the encode method.\\\\n4. With these embeddings, we can perform clustering or semantic search to group similar product reviews together and analyze patterns, common features, or complaints.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-MiniLM-L12-v2\\\\')\\\\nreviews = [\\\"This product is great!\\\", \\\"Amazing quality!\\\", \\\"I am not satisfied with the purchase.\\\"]\\\\nembeddings = model.encode(reviews)\\\\n# Apply clustering or semantic search methods like K-means, DBSCAN, or Cosine Similarity on embeddings', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/all-MiniLM-L12-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': '1,170,060,424 training pairs', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]قم بتطبيق تقنيات معالجة اللغات الطبيعية لتحويل النص المكتوب إلى ملف صوتي. هل يمكنك مساعدتي في هذا المجال؟? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoTokenizer.from_pretrained('Salesforce/codegen-350M-multi')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes AutoTokenizer and AutoModelForCausalLM from the transformers package provided by Hugging Face.\\\\n2. Use the from_pretrained() method to load the pre-trained tokenizer and model with the identifier 'Salesforce/codegen-350M-multi'. This model is trained for generating executable code snippets in multiple programming languages, including Python.\\\\n3. When making a code generation request, tokenize the input text using the loaded tokenizer to obtain input IDs. Pass these input IDs into the generate method of the loaded model to generate the output code snippet.\\\\n4. Finally, use the tokenizer's decode method to convert the generated output into a human-readable format.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Salesforce/codegen-350M-multi\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'Salesforce/codegen-350M-multi\\\\')\\\\n\\\\n# Example of generating a code snippet:\\\\nprompt = \\\"Create a Python function that outputs \\\\'Hello, World!\\\\'\\\"\\\\ninput_ids = tokenizer(prompt, return_tensors=\\\"pt\\\").input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\ncode_snippet = tokenizer.decode(generated_ids[0], skip_special_tokens=True)', 'api_call': \\\"AutoTokenizer.from_pretrained('Salesforce/codegen-350M-multi')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Salesforce/codegen-350M-multi', 'api_call': \\\"AutoTokenizer.from_pretrained('Salesforce/codegen-350M-multi')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval and MTPB', 'accuracy': 'Refer to the paper for accuracy details'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'pardeepSF/layoutlm-vqa' that uses the LayoutLM architecture for document question answering.\\\\n3. The loaded model will be used for answering questions from the given document.\\\\n4. Provide the question and the context (i.e., the document) as input to the model, and it will return an answer based on the provided information.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nquestion_answering = pipeline(\\\\'question-answering\\\\', model=\\\\'pardeepSF/layoutlm-vqa\\\\')\\\\nquestion = \\\"What year was the company founded?\\\"\\\\ncontext = \\\"The company was founded in 2005 and has been providing high-quality products and services ever since.\\\"\\\\nresult = question_answering(question=question, context=context)\\\\nprint(result[\\\\'answer\\\\'])', 'api_call': \\\"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'layoutlm-vqa', 'api_call': \\\"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\\\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering using the LayoutLM architecture.' </s>\"\n"}
{"text": "<s>[INST]يجب تطبيق تقنيات تقسيم الصورة بواسطة الحاسوب على الصور الواردة، حيث يجب تحديد الحدود بين الأشياء في الصور وتقدير عمقها بدقة، لذا يجب أن تكون النتائج دقيقة وموثوقة لضمان فعالية التطبيقات المستقبلية.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the required classes from the transformers and PIL packages for performing image segmentation. This includes SegformerForSemanticSegmentation for the model and Image for processing image data.\\\\n2. We then use the from_pretrained method of the SegformerForSemanticSegmentation class to load the pre-trained model 'nvidia/segformer-b5-finetuned-ade-640-640'. This fine-tuned model has been trained to carry out segmentation tasks, which is exactly what we need to identify various components in an environment.\\\\n3. We load the image that we want to segment from a file or a URL. Alternatively, it can also be acquired in real-time from a camera on the site.\\\\n4. We use this model to analyze the given image, and it segments it into different regions based on the identified landscape and building components.\\\", '<<<code>>>': \\\"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\\nimage = Image.open('image_path.jpg')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\", 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nvidia/segformer-b5-finetuned-ade-640-640', 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20K', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نص توجيهي لتوليد ملخص نصي لنص يصف الحيوانات في البرية باستخدام تقنيات معالجة اللغة الطبيعية. هل تستطيع تقديم المساعدة في هذا الأمر؟? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"AutoModelWithLMHead.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. First, import the required libraries from transformers library provided by Hugging Face, including AutoTokenizer and AutoModelWithLMHead.\\\\n2. The AutoModelWithLMHead.from_pretrained() function is used to load the pretrained model 'SEBIS/code_trans_t5_base_code_documentation_generation_python'.\\\\n3. The model is designed to generate code documentation based on tokenized Python code.\\\\n4. Also, load the tokenizer specific to the model using AutoTokenizer.from_pretrained().\\\\n5. Finally, use the SummarizationPipeline with the loaded model and tokenizer to transform the code snippet into a brief description.\\\", '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline\\\\npipeline = SummarizationPipeline(\\\\n    model=AutoModelWithLMHead.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python'),\\\\n    tokenizer=AutoTokenizer.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python', skip_special_tokens=True),\\\\n    device=0\\\\n)\\\\ncode_snippet = 'def e(message, exit_code=None): print_log(message, YELLOW, BOLD) if exit_code is not None: sys.exit(exit_code)'\\\\ndescription = pipeline([code_snippet])\\\", 'api_call': \\\"AutoModelWithLMHead.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'code_trans_t5_base_code_documentation_generation_python', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python')\\\", 'api_arguments': ['tokenized_code'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline\\\\npipeline = SummarizationPipeline(\\\\n model=AutoModelWithLMHead.from_pretrained(SEBIS/code_trans_t5_base_code_documentation_generation_python),\\\\n tokenizer=AutoTokenizer.from_pretrained(SEBIS/code_trans_t5_base_code_documentation_generation_python, skip_special_tokens=True),\\\\n device=0\\\\n)\\\\ntokenized_code = def e ( message , exit_code = None ) : print_log ( message , YELLOW , BOLD ) if exit_code is not None : sys . exit ( exit_code )\\\\npipeline([tokenized_code])', 'performance': {'dataset': 'CodeSearchNet Corpus python dataset', 'accuracy': '20.26 BLEU score'}, 'description': 'This CodeTrans model is based on the t5-base model and is trained on tokenized python code functions. It can be used to generate descriptions for python functions or be fine-tuned on other python code tasks. The model works best with tokenized python functions but can also be used on unparsed and untokenized python code.' </s>\"\n"}
{"text": "<s>[INST]سوف تعمل الشركة على تطوير نظام لتعرف الكلام التلقائي للوثائق الصوتية، وتحتاج إلى تصميم خوارزميات تستطيع الإجابة على أسئلة متعلقة بمضامين هذه الوثائق.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"pipeline.from_pretrained('pyannote/overlapped-speech-detection', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", '<<<api_provider>>>': 'pyannote.audio', '<<<explanation>>>': \\\"1. Import the Pipeline class from the pyannote.audio library.\\\\n2. Instantiate the pipeline using the from_pretrained method with the 'pyannote/overlapped-speech-detection' model. Make sure to replace 'ACCESS_TOKEN_GOES_HERE' with your actual access token.\\\\n3. Use the created pipeline instance to process the conference call audio file (e.g., 'audio.wav').\\\\n4. Iterate through the output.get_timeline().support() to find intervals where two or more speakers are active simultaneously.\\\\n5. With these intervals, you can identify points in the conference call where multiple people are speaking at the same time.\\\", '<<<code>>>': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(\\\\'pyannote/overlapped-speech-detection\\\\', use_auth_token=\\\\'ACCESS_TOKEN_GOES_HERE\\\\')\\\\noutput = pipeline(\\\\'audio.wav\\\\')\\\\nfor speech in output.get_timeline().support():\\\\n    start = speech.start\\\\n    end = speech.end\\\\n    print(f\\\"Overlapping speech detected between {start} seconds and {end} seconds.\\\")', 'api_call': \\\"pipeline.from_pretrained('pyannote/overlapped-speech-detection', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", 'provider': 'pyannote.audio', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pyannote/overlapped-speech-detection', 'api_call': \\\"pipeline.from_pretrained('pyannote/overlapped-speech-detection', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", 'api_arguments': ['audio.wav'], 'python_environment_requirements': [['pyannote.audio 2.1']], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...', 'performance': {'dataset': 'ami', 'accuracy': None}, 'description': 'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.' </s>\"\n"}
{"text": "<s>[INST]نحن بحاجة إلى تطوير نظام لتصنيف الفيديوهات باستخدام تقنيات الرؤية الحاسوبية. يتعين على الفريق تطوير برنامج يستطيع تحليل المشاهد وتصنيفها تلقائياً.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries from transformers, namely VideoMAEImageProcessor and VideoMAEForVideoClassification.\\\\n2. Load the pre-trained video classification model 'MCG-NJU/videomae-large-finetuned-kinetics' using the VideoMAEForVideoClassification.from_pretrained() function.\\\\n3. Load the processor for this pre-trained model using VideoMAEImageProcessor.from_pretrained() method with the same model name.\\\\n4. Use the processor to convert the input video into the required format (return_tensors='pt').\\\\n5. Pass the processed input to the video classification model to obtain the predicted output.\\\\n6. Extract the predicted class index from the logits and use the id2label dictionary to obtain the corresponding action label.\\\", '<<<code>>>': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')\\\\ninputs = processor(video, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\\\", 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'MCG-NJU/videomae-large-finetuned-kinetics', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')\\\", 'api_arguments': ['video'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-large-finetuned-kinetics)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-large-finetuned-kinetics)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-400', 'accuracy': {'top-1': 84.7, 'top-5': 96.5}}, 'description': 'VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]العميل يقوم بتطوير تطبيق في مجال اللغة الطبيعية لترجمة النصوص. يُرجى توفير برنامج التعرف التلقائي على الكلام لتحويل النص إلى كلام لهذا التطبيق.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. Import the 'pipeline' function from transformers.\\\", 'code': 'from transformers import pipeline\\\\ntranslation = pipeline(\\\\'translation_it_to_en\\\\', model=\\\\'Helsinki-NLP/opus-mt-it-en\\\\')\\\\ntext = \\\"Dolce al limone dato forma e porzione, su ciascuno dei quali è presente una sua immagine di un principe di giocatori di calcio. Richiedi il tuo preferito e riceve una risposta personalizzata. Inoltre contiene tutti gli ingredienti necessari per l\\\\'ultima festa di compleanno che ti farà sentire sempre speciale.\\\"\\\\ntranslated_text = translation(text)[0][\\\\'translation_text\\\\']', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Transformers', 'functionality': 'Translation', 'api_name': 'Helsinki-NLP/opus-mt-it-en', 'api_call': 'from transformers import pipeline\\\\ntranslation = pipeline(\\\\'translation_it_to_en\\\\', model=\\\\'Helsinki-NLP/opus-mt-it-en\\\\')\\\\ntext = \\\"Dolce al limone dato forma e porzione, su ciascuno dei quali è presente una sua immagine di un principe di giocatori di calcio. Richiedi il tuo preferito e riceve una risposta personalizzata. Inoltre contiene tutti gli ingredienti necessari per l\\\\'ultima festa di compleanno che ti farà sentire sempre speciale.\\\"\\\\ntranslated_text = translation(text)[0][\\\\'translation_text\\\\']', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')('Ciao mondo!')\\\", 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': {'newssyscomb2009.it.en': 35.3, 'newstest2009.it.en': 34.0, 'Tatoeba.it.en': 70.9}, 'chr-F': {'newssyscomb2009.it.en': 0.6, 'newstest2009.it.en': 0.594, 'Tatoeba.it.en': 0.808}}}, 'description': 'A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتطبيق تقنية التعلم التعزيزي على مشكلة تجريبية لتطوير نموذج يتعلم تحقيق المكافأة عبر القيام بإجراءات محددة.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary class from the transformers package. This includes AutoModel for the reinforcement learning model.\\\\n2. We then use the from_pretrained method of the AutoModel class to load the pre-trained model 'edbeeching/decision-transformer-gym-walker2d-expert'. This model has been trained for controlling a bipedal robot in the Gym Walker2d environment and can be a suitable choice for controlling a bipedal robot efficiently.\\\\n3. By utilizing the pre-trained model, we can now use it to make decisions and control the robot.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\\\", 'api_call': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'edbeeching/decision-transformer-gym-walker2d-expert', 'api_call': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\\\", 'api_arguments': {'mean': [1.2384834, 0.19578537, -0.10475016, -0.18579608, 0.23003316, 0.022800924, -0.37383768, 0.337791, 3.925096, -0.0047428459, 0.025267061, -0.0039287535, -0.01736751, -0.48212224, 0.00035432147, -0.0037124525, 0.0026285544], 'std': [0.06664903, 0.16980624, 0.17309439, 0.21843709, 0.74599105, 0.02410989, 0.3729872, 0.6226182, 0.9708009, 0.72936815, 1.504065, 2.495893, 3.511518, 5.3656907, 0.79503316, 4.317483, 6.1784487]}, 'python_environment_requirements': [['transformers']], 'example_code': 'See our Blog Post, Colab notebook or Example Script for usage.', 'performance': {'dataset': 'Gym Walker2d environment', 'accuracy': 'Not provided'}, 'description': 'Decision Transformer model trained on expert trajectories sampled from the Gym Walker2d environment.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد ملف صوتي من تسجيل صوتي آخر. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk')\\\", '<<<api_provider>>>': 'Fairseq', '<<<explanation>>>': \\\"1. Import the necessary modules, including fairseq, torchaudio, and huggingface_hub.\\\\n2. Load the xm_transformer_s2ut_en-hk model from Hugging Face hub using the load_model_ensemble_and_task_from_hf_hub function.\\\\n3. Load an audio file containing the English speech using torchaudio.load().\\\\n4. Prepare the task with the provided args_overrides (config_yaml, task) and input the English audio file's waveform.\\\\n5. Get the prediction from the S2THubInterface, which will contain translated Hokkien speech.\\\\n6. Use the speech synthesis code provided in the example_code to convert the Hokkien text into speech waveform and sampling rate.\\\\n7. Use IPython.display to play the generated audio.\\\", '<<<code>>>': \\\"import os\\\\nfrom pathlib import Path\\\\nimport torchaudio\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\n\\\\ncache_dir = os.getenv('HUGGINGFACE_HUB_CACHE')\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk', arg_overrides={'config_yaml': 'config.yaml', 'task': 'speech_to_text'}, cache_dir=cache_dir)\\\\nmodel = models[0].cpu()\\\\ncfg['task'].cpu = True\\\\n\\\\naudio, _ = torchaudio.load('/path/to/an/audio/file')\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\nunit = S2THubInterface.get_prediction(task, model, task.build_generator([model], cfg), sample)\\\\n\\\\ncache_dir = snapshot_download('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS', cache_dir=cache_dir, library_name='fairseq')\\\\nx = hub_utils.from_pretrained(cache_dir, 'model.pt', '.', archive_map=CodeHiFiGANVocoder.hub_models(), config_yaml='config.json', fp16=False, is_vocoder=True)\\\\nvocoder = CodeHiFiGANVocoder(x['args']['model_path'][0], x['args'])\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\n\\\\nfrom IPython.display import Audio\\\\nAudio(wav, rate=sr)\\\", 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk')\\\", 'provider': 'Fairseq', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'xm_transformer_s2ut_en-hk', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk')\\\", 'api_arguments': {'arg_overrides': {'config_yaml': 'config.yaml', 'task': 'speech_to_text'}, 'cache_dir': 'cache_dir'}, 'python_environment_requirements': [['fairseq', 'huggingface_hub', 'torchaudio']], 'example_code': {'import_modules': ['import json', 'import os', 'from pathlib import Path', 'import IPython.display as ipd', 'from fairseq import hub_utils', 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub', 'from fairseq.models.speech_to_text.hub_interface import S2THubInterface', 'from fairseq.models.text_to_speech import CodeHiFiGANVocoder', 'from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface', 'from huggingface_hub import snapshot_download', 'import torchaudio'], 'load_model': [\\\"cache_dir = os.getenv('HUGGINGFACE_HUB_CACHE')\\\", \\\"models, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk', arg_overrides={'config_yaml': 'config.yaml', 'task': 'speech_to_text'}, cache_dir=cache_dir)\\\", 'model = models[0].cpu()', \\\"cfg['task'].cpu = True\\\"], 'generate_prediction': ['generator = task.build_generator([model], cfg)', \\\"audio, _ = torchaudio.load('/path/to/an/audio/file')\\\", 'sample = S2THubInterface.get_model_input(task, audio)', 'unit = S2THubInterface.get_prediction(task, model, generator, sample)'], 'speech_synthesis': [\\\"library_name = 'fairseq'\\\", \\\"cache_dir = (cache_dir or (Path.home() / '.cache' / library_name).as_posix())\\\", \\\"cache_dir = snapshot_download('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS', cache_dir=cache_dir, library_name=library_name)\\\", \\\"x = hub_utils.from_pretrained(cache_dir, 'model.pt', '.', archive_map=CodeHiFiGANVocoder.hub_models(), config_yaml='config.json', fp16=False, is_vocoder=True)\\\", \\\"with open(f'{x['args']['data']}/config.json') as f:\\\", '  vocoder_cfg = json.load(f)', \\\"assert (len(x['args']['model_path']) == 1), 'Too many vocoder models in the input'\\\", \\\"vocoder = CodeHiFiGANVocoder(x['args']['model_path'][0], vocoder_cfg)\\\", 'tts_model = VocoderHubInterface(vocoder_cfg, vocoder)', 'tts_sample = tts_model.get_model_input(unit)', 'wav, sr = tts_model.get_prediction(tts_sample)', 'ipd.Audio(wav, rate=sr)']}, 'performance': {'dataset': 'MuST-C', 'accuracy': 'Not specified'}, 'description': 'Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذجًا لتصنيف النصوص الصفري باستخدام معالجة اللغة الطبيعية للرموز.? [/INST]{'domain': 'Natural Language Processing Token Classification', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('vblagoje/bert-english-uncased-finetuned-pos')\\\", 'api_provider': 'Transformers', 'explanation': '1. We import the necessary modules from the transformers library, specifically the AutoModelForTokenClassification class.', 'code': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'vblagoje/bert-english-uncased-finetuned-pos\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'vblagoje/bert-english-uncased-finetuned-pos\\\\')\\\\ntext = \\\"Once upon a time, in a land far away, there lived a brave little girl who loved exploring the forest with her friends.\\\"\\\\ninputs = tokenizer(text, return_tensors=\\\"pt\\\")\\\\noutputs = model(**inputs)\\\\npredictions = outputs[0]\\\\npredicted_parts_of_speech = predictions.argmax(-1)', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Transformers', 'functionality': 'Token Classification', 'api_name': 'vblagoje/bert-english-uncased-finetuned-pos', 'api_call': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'vblagoje/bert-english-uncased-finetuned-pos\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'vblagoje/bert-english-uncased-finetuned-pos\\\\')\\\\ntext = \\\"Once upon a time, in a land far away, there lived a brave little girl who loved exploring the forest with her friends.\\\"\\\\ninputs = tokenizer(text, return_tensors=\\\"pt\\\")\\\\noutputs = model(**inputs)\\\\npredictions = outputs[0]\\\\npredicted_parts_of_speech = predictions.argmax(-1)', 'api_arguments': 'model', 'python_environment_requirements': ['transformers'], 'example_code': \\\"nlp('Hello world!')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A BERT model fine-tuned for Part-of-Speech (POS) tagging in English text.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير حلول في مجال معالجة اللغة الطبيعية، تتضمن دمج وتحليل المعلومات من النصوص لخلاصة موجزة ومفيدة. نحن بحاجة لخوارزميات تلخيص تستند إلى الذكاء الاصطناعي لمساعدتنا على فهم المحتوى واستخلاص المعلومات الرئيسية من النصوص المتنوعة.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers package. This includes BarthezModel for text summarization.\\\\n2. Use the from_pretrained method of the BarthezModel class to load the pre-trained model 'moussaKam/barthez-orangesum-abstract'.\\\\n3. This model has been trained for French text summarization tasks, which is exactly what we need for summarizing a large French article.\\\\n4. Provide the text of the French article as input to the BarthezModel.\\\\n5. The model will return a summarized version of the input text.\\\", '<<<code>>>': \\\"from transformers import BarthezTokenizer, BarthezModel\\\\ntokenizer = BarthezTokenizer.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\\nmodel = BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\\ninputs = tokenizer(article_text, return_tensors='pt', truncation=True, max_length=512)\\\\noutputs = model.generate(**inputs)\\\\nsummary = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\", 'api_call': \\\"BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'moussaKam/barthez-orangesum-abstract', 'api_call': \\\"BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'orangeSum', 'accuracy': ''}, 'description': 'Barthez model finetuned on orangeSum for abstract generation in French language' </s>\"\n"}
{"text": "<s>[INST]لتدريب النموذج في تحليل بيانات جدولية لتنبؤ بالقيم المستقبلية، قم بتقسيم بيانات التدريب إلى مجموعتين: مجموعة تحتوي على البيانات التي ستقوم النموذج بتدريبها، ومجموعة أخرى تحوي على البيانات التي سيتم استخدامها لاختبار أداء النموذج.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. Import necessary libraries, such as joblib, pandas, and json.\\\\n2. Load the pre-trained model 'model.joblib' using joblib.load.\\\\n3. Read the JSON config file 'config.json' to get the input features.\\\\n4. Load the given dataset 'data.csv' as a Pandas dataframe.\\\\n5. Pre-process the dataframe.\\\\n6. Use the loaded model to make predictions for the carbon emissions of each manufacturing process.\\\", '<<<code>>>': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pcoloc/autotrain-mikrotik-7-7-1860563588', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['json', 'joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'pcoloc/autotrain-data-mikrotik-7-7', 'accuracy': {'Loss': 48.213, 'R2': 0.654, 'MSE': 2324.518, 'MAE': 32.634, 'RMSLE': 0.586}}, 'description': 'A tabular regression model trained with AutoTrain to predict carbon emissions based on input features.' </s>\"\n"}
{"text": "<s>[INST]قم بتشغيل النموذج اللغوي لمعالجة اللغة الطبيعية لإنشاء نص التعليمات اللازم لإنتاج النص المستهدف، ثم قم بتوجيه النموذج لخلق محادثة تفاعلية بين الطلاب لتحليل تحقق الإيضاح و تقييم طرق التوجيه تجاه مسألة التصنيف في توجيه الدوائر لتنشيط الردود.? [/INST]{'domain': 'Tabular Tabular Regression', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_provider': 'Joblib', 'explanation': '', 'code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('industry_data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Joblib', 'functionality': 'Carbon Emissions', 'api_name': 'pcoloc/autotrain-dragino-7-7-max_300m-1861063640', 'api_call': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('industry_data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_arguments': ['model.joblib', 'config.json', 'data.csv'], 'python_environment_requirements': ['json', 'joblib', 'pandas'], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'pcoloc/autotrain-data-dragino-7-7-max_300m', 'accuracy': {'Loss': 50.918, 'R2': 0.304, 'MSE': 2592.667, 'MAE': 39.693, 'RMSLE': 0.429}}, 'description': 'A tabular regression model for predicting carbon emissions using the pcoloc/autotrain-dragino-7-7-max_300m-1861063640 dataset. Trained with AutoTrain.'}} </s>\"\n"}
{"text": "<s>[INST]أُنشئ جدولًا تابعيًا لتحقيق إستدلال تابعي في تصنيف الصوتيات.? [/INST]{'domain': 'Audio Audio Classification', 'api_call': \\\"foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\", 'api_provider': 'SpeechBrain', 'explanation': \\\"1. Import the 'foreign_class' from the speechbrain.pretrained.interfaces package.\\\", 'code': '', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'SpeechBrain', 'functionality': 'Emotion Recognition', 'api_name': 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP', 'api_call': '', 'api_arguments': ['file_path'], 'python_environment_requirements': 'pip install speechbrain', 'example_code': \\\"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\\nout_prob, score, index, text_lab = classifier.classify_file('speechbrain/emotion-recognition-wav2vec2-IEMOCAP/anger.wav')\\\\nprint(text_lab)\\\", 'performance': {'dataset': 'IEMOCAP', 'accuracy': '78.7%'}, 'description': 'This repository provides all the necessary tools to perform emotion recognition with a fine-tuned wav2vec2 (base) model using SpeechBrain. It is trained on IEMOCAP training data.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطوير نموذج لتصنيف الصور بدون مشاهدة عبر الرؤية الحاسوبية. هل يمكنك مساعدتي في إنشاء نموذج يمكنه تصنيف الصور التي لم يتم تدريبه عليها؟? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"CLIPModel.from_pretrained('openai/clip-vit-large-patch14').\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the Transformers package. This includes the CLIPModel class for the zero-shot image classification model and the corresponding tokenizer.\\\\n2. We use the from_pretrained method to load the pre-trained model 'openai/clip-vit-large-patch14'. This model has been designed to perform zero-shot classification tasks and is suitable for assessing artworks.\\\\n3. We can load an image of an artwork and tokenize it using the provided tokenizer.\\\\n4. This model can then be used to analyze the image and classify the artwork into various categories.\\\\n\\\\nNote that you need to update the Transformers version to 4.21.3 and Tokenizers to 0.12.1.\\\", '<<<code>>>': 'from transformers import CLIPModel, CLIPProcessor\\\\nfrom PIL import Image\\\\nimage = Image.open(\\\\'artwork_image.jpg\\\\')\\\\n# replace \\\\'artwork_image.jpg\\\\' with the path to your image\\\\nprocessor = CLIPProcessor.from_pretrained(\\\\'openai/clip-vit-large-patch14\\\\')\\\\nmodel = CLIPModel.from_pretrained(\\\\'openai/clip-vit-large-patch14\\\\')\\\\ninputs = processor(images=image, return_tensors=\\\"pt\\\")\\\\noutputs = model(**inputs)', 'api_call': \\\"CLIPModel.from_pretrained('openai/clip-vit-large-patch14').\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai/clip-vit-large-patch14-336', 'api_call': \\\"CLIPModel.from_pretrained('openai/clip-vit-large-patch14').\\\", 'api_arguments': 'image_path, tokenizer, model', 'python_environment_requirements': ['Transformers 4.21.3, TensorFlow 2.8.2, Tokenizers 0.12.1'], 'example_code': 'N/A', 'performance': {'dataset': 'unknown', 'accuracy': 'N/A'}, 'description': 'This model was trained from scratch on an unknown dataset.' </s>\"\n"}
{"text": "<s>[INST]نحن بحاجة إلى تطوير برنامج يستخدم استخراج الميزات متعددة الوسائط، لتحويل النص إلى صورة لمستخدمي التطبيق الخاص بذوي الإعاقة البصرية.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. First, import the pipeline method from the transformers library provided by Hugging Face.\\\\n2. Then create a text-to-image model using the provided model name \\\\'22h/vintedois-diffusion-v0-1\\\\'. This model is trained to generate images based on given textual prompts.\\\\n3. Pass the text description \\\"landscape with a lake and mountains in the background\\\" to the model along with other model parameters like CFG Scale, Scheduler, Steps, and Seed.\\\\n4. The generated image created in response to this prompt can be used as the digital painting that the client requested.', '<<<code>>>': \\\"from transformers import pipeline\\\\ntext2img = pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\\\\nimage = text2img('landscape with a lake and mountains in the background', CFG_Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\\\", 'api_call': \\\"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \\\"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\\\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\\\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نموذج لتصنيف البيانات الجدولية باستخدام خوارزميات التعلم العميق. هل يمكنك مساعدتي في هذا المجال؟? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"load('path_to_folder/sklearn_model.joblib')\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': \\\"1. Install the required packages, including scikit-learn and joblib.\\\\n2. Use the joblib.load function to load the pre-trained scikit-learn model 'julien-c/skops-digits' by passing the model file path as an argument.\\\\n3. Process your dataset containing user behavior to create a model input.\\\\n4. Call predict on the loaded model with the input data to get predictions for mobile plan preferences.\\\\n5. The output will be an array of predicted mobile plan preferences based on the user behavior input.\\\", '<<<code>>>': \\\"import joblib\\\\nfrom sklearn.preprocessing import StandardScaler\\\\nmodel = joblib.load('/path_to_folder/sklearn_model.joblib')\\\\n\\\\n# Assuming you have a dataset `user_behavior_data`\\\\nscaler = StandardScaler()\\\\nscaled_input = scaler.fit_transform(user_behavior_data)\\\\nplan_predictions = model.predict(scaled_input)\\\", 'api_call': \\\"load('path_to_folder/sklearn_model.joblib')\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'julien-c/skops-digits', 'api_call': \\\"load('path_to_folder/sklearn_model.joblib')\\\", 'api_arguments': {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': 'False', 'epsilon': 1e-08, 'hidden_layer_sizes': '(100,)', 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 200, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': 'True', 'power_t': 0.5, 'random_state': None, 'shuffle': 'True', 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 'False', 'warm_start': 'False'}, 'python_environment_requirements': ['scikit-learn, joblib'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': None, 'accuracy': None}, 'description': 'A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج لمعالجة اللغة الطبيعية للإجابة عن الأسئلة باستخدام البيانات المتاحة.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Instantiate a question-answering pipeline using the 'mrm8488/bert-multi-cased-finetuned-xquadv1' model and tokenizer. This model is specifically designed for multilingual question answering tasks and can answer historical questions, like the one about Charlemagne's birth.\\\\n3. You can then use the instantiated pipeline to ask the question and provide any relevant context that might be needed for the question. In this case, the model will need a passage of text containing information about Charlemagne's birth.\\\\n4. The model will then return the answer extracted from the given context.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\'question-answering\\\\', model=\\\\'mrm8488/bert-multi-cased-finetuned-xquadv1\\\\', tokenizer=\\\\'mrm8488/bert-multi-cased-finetuned-xquadv1\\\\')\\\\ncontext = \\\\'Charlemagne, also known as Charles the Great, was king of the Franks and ruler of the Holy Roman Empire. He was born on April 2, 742, and became king in 768.\\\\'\\\\nquestion = \\\"When was Charlemagne born?\\\"\\\\nresult = qa_pipeline({\\\\'context\\\\': context, \\\\'question\\\\': question})\\\\nanswer = result[\\\\'answer\\\\']', 'api_call': \\\"pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mrm8488/bert-multi-cased-finetuned-xquadv1', 'api_call': \\\"pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\", 'api_arguments': {'context': 'string', 'question': 'string'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"qa_pipeline({\\\\n 'context': Manuel Romero has been working hardly in the repository hugginface/transformers lately,\\\\n 'question': Who has been working hard for hugginface/transformers lately?\\\\n})\\\", 'performance': {'dataset': 'XQuAD', 'accuracy': 'Not provided'}, 'description': 'This model is a BERT (base-multilingual-cased) fine-tuned for multilingual Question Answering on 11 different languages using the XQuAD dataset and additional data augmentation techniques.' </s>\"\n"}
{"text": "<s>[INST]نقوم بإنشاء نموذج تصنيف جدولي بواسطة شركتنا لتصنيف بيانات جدولية وتحليلها، بهدف تحقيق تنبؤ دقيق بالفئات المختلفة في البيانات.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'sklearn_model.joblib')))\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': '1. Import the necessary libraries including hf_hub_url, cached_download from huggingface_hub, joblib, and pandas.\\\\n2. Load the Scikit-learn model from Hugging Face Hub using the cached_download and joblib.load functions.\\\\n3. Load the CSV file containing the wine data using pandas, and separate the input features (X) and target labels (Y).\\\\n4. Use the model to predict the wine quality for specific samples from the input features.', '<<<code>>>': \\\"from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nREPO_ID = 'julien-c/wine-quality'\\\\nFILENAME = 'sklearn_model.joblib'\\\\nmodel = joblib.load(cached_download(hf_hub_url(REPO_ID, FILENAME)))\\\\ndata_file = cached_download(hf_hub_url(REPO_ID, 'winequality-red.csv'))\\\\nwinedf = pd.read_csv(data_file, sep=';')\\\\nX = winedf.drop(['quality'], axis=1)\\\\nY = winedf['quality']\\\\nsample_input = X[:3]\\\\nlabels = model.predict(sample_input)\\\", 'api_call': \\\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'sklearn_model.joblib')))\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'osanseviero/wine-quality', 'api_call': \\\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'sklearn_model.joblib')))\\\", 'api_arguments': 'X', 'python_environment_requirements': [['huggingface_hub', 'joblib', 'pandas']], 'example_code': 'from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nREPO_ID = julien-c/wine-quality\\\\nFILENAME = sklearn_model.joblib\\\\nmodel = joblib.load(cached_download(\\\\n hf_hub_url(REPO_ID, FILENAME)\\\\n))\\\\ndata_file = cached_download(\\\\n hf_hub_url(REPO_ID, winequality-red.csv)\\\\n)\\\\nwinedf = pd.read_csv(data_file, sep=;)\\\\nX = winedf.drop([quality], axis=1)\\\\nY = winedf[quality]\\\\nprint(X[:3])\\\\nlabels = model.predict(X[:3])', 'performance': {'dataset': 'winequality-red.csv', 'accuracy': 0.6616635397123202}, 'description': 'A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج تصنيف الرموز في مجال معالجة اللغة الطبيعية. نحتاج إلى نظام يتمكن من تحديد تصنيفات الرموز المختلفة في النص، مثل تحويل النص إلى كلام لمستخدمي التطبيقات القائمة على هذا النموذج في مجال Natural Language Processing Fill-Mask.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a fill-mask model.\\\\n3. Specify the model 'camembert-base' to be used for predictions. This model is pretrained on French text data and can be used to complete sentences in French.\\\\n4. Set the tokenizer also to 'camembert-base', which is the tokenizer specifically designed for the CamemBERT model to process French text.\\\\n5. The created model can be used to complete sentences in French by filling in the missing words (<mask>).\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ncamembert_fill_mask = pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')\\\\nresults = camembert_fill_mask('Le camembert est <mask> :)')\\\", 'api_call': \\\"pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'camembert-base', 'api_call': \\\"pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')\\\", 'api_arguments': ['model', 'tokenizer'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline; camembert_fill_mask = pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base'); results = camembert_fill_mask('Le camembert est <mask> :)')\\\", 'performance': {'dataset': 'oscar', 'accuracy': 'N/A'}, 'description': 'CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. It is available on Hugging Face in 6 different versions with varying number of parameters, amount of pretraining data, and pretraining data source domains. It can be used for Fill-Mask tasks.' </s>\"\n"}
{"text": "<s>[INST]يمكنك الآن استخدام أداة توليد النصوص لمعالجة اللغات الطبيعية في مجال توليد النصوص. قم بإدخال النص الأصلي واستخدام خوارزميات تعلم الآلة لتوليد نص جديد يمكن تعبئة الفراغات الجدولية بها.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create a text-generation pipeline with the \\\\'EleutherAI/gpt-neo-2.7B\\\\' model by calling the pipeline function with the proper arguments. This model is a powerful GPT-3 style language model capable of generating coherent and diverse text based on the given input prompt.\\\\n3. Provide the input prompt \\\"Delivering Happiness\\\" to the text-generation pipeline, and set the options like do_sample=True for a more creative output and min_length to control the minimum length of the generated text.\\\\n4. The generated text will be a catchy slogan based on the original input slogan that can be used by the marketing department for their campaign.', '<<<code>>>': 'from transformers import pipeline\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'EleutherAI/gpt-neo-2.7B\\\\')\\\\ngenerated_text = generator(\\\"Delivering Happiness\\\", do_sample=True, min_length=50)\\\\ncatchy_slogan = generated_text[0][\\\\'generated_text\\\\']', 'api_call': \\\"pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'EleutherAI/gpt-neo-2.7B', 'api_call': \\\"pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\\\", 'api_arguments': ['text', 'do_sample', 'min_length'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\n generator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\\\\n generator(EleutherAI has, do_sample=True, min_length=50)\\\", 'performance': {'dataset': 'the_pile', 'accuracy': {'Lambada_Acc': '62.22%', 'Winogrande': '56.50%', 'Hellaswag': '42.73%'}}, 'description': \\\"GPT-Neo 2.7B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. It was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model. This model is best suited for generating texts from a prompt and can be used directly with a pipeline for text generation.\\\" </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في تقسيم الصورة لتحسين التحديد والتعرف على الحيوانات؟? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'MaskFormerFeatureExtractor', 'MaskFormerForInstanceSegmentation', 'Image', and 'requests'.\\\\n2. Use the 'from_pretrained' method to load the pre-trained model 'facebook/maskformer-swin-base-coco' for instance segmentation (specifically designed to segment objects in images).\\\\n3. Open the input image, which is the image of the kitchen table with cookwares, using the 'Image' library.\\\\n4. Use the 'feature_extractor' method to pre-process the input image and prepare it to be fed into the model.\\\\n5. Finally, feed the pre-processed image into the model and obtain the segmented kitchen objects using 'outputs' and 'post_process_panoptic_segmentation' method.\\\", '<<<code>>>': 'from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(\\\\'facebook/maskformer-swin-base-coco\\\\')\\\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(\\\\'facebook/maskformer-swin-base-coco\\\\')\\\\nimage_path = \\\"kitchen_table_image.jpg\\\"\\\\nimage = Image.open(image_path)\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\n\\\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result[\\\\'segmentation\\\\']', 'api_call': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/maskformer-swin-base-coco', 'api_call': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\", 'api_arguments': ['image'], 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': \\\"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\\n\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\n\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\n\\\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result['segmentation']\\\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]يمكنك تجهيز مكتبة التعرف على النشاط الصوتي لوكالة التعلم الآلي عبر تحديث المتطلبات وتضمين الوحدة اللازمة للكشف عن الأصوات واستخراج المعلومات الصوتية المهمة.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We import the necessary functionality from the transformers library provided by Hugging Face. This includes WhisperProcessor and WhisperForConditionalGeneration classes for speech recognition.', 'code': \\\"from transformers import WhisperForConditionalGeneration, WhisperProcessor\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\\ninput_features = processor(conference_call_audio, sampling_rate=audio_sampling_rate, return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\\", 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Automatic Speech Recognition and Speech Translation', 'api_name': 'openai/whisper-large-v2', 'api_call': \\\"from transformers import WhisperForConditionalGeneration, WhisperProcessor\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\\ninput_features = processor(conference_call_audio, sampling_rate=audio_sampling_rate, return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\\", 'api_arguments': {'forced_decoder_ids': \\\"WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')\\\"}, 'python_environment_requirements': ['transformers', 'datasets'], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', \\\"processor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\\\", \\\"model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", 'model.config.forced_decoder_ids = None', \\\"ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')\\\", \\\"sample = ds[0]['audio']\\\", \\\"input_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\\\", 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)'], 'performance': {'dataset': 'LibriSpeech test-clean', 'accuracy': 3.0003583080317573}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning.'}} </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نموذج لتحويل الوثائق المتعددة الأوجه إلى أسئلة وأجوبة، باستخدام تقنيات رؤية الحاسوب، ونرغب في إنشاء توجيهات لتحسين أداء النموذج وجودة الوثائق المتولدة.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'layoutlm-invoices', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\\\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers'], 'example_code': \\\"nlp(question='What is the total amount?', context='your_invoice_text')\\\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>\"\n"}
{"text": "<s>[INST]يحتاج الباحث إلى تطوير نموذج لتصنيف النصوص بدون تدريب في مجال معالجة اللغة الطبيعية. قام العميل بتطوير تطبيق لذوي الإعاقة البصرية ونحن بحاجة إلى برنامج يقوم بتحويل النص إلى كلام لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('cointegrated/rubert-base-cased-nli-threeway')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Start by importing necessary libraries like torch and transformers.\\\\n2. Load the tokenizer and model using 'cointegrated/rubert-base-cased-nli-threeway' checkpoint.\\\\n3. Ensure the model is loaded onto the appropriate device (GPU or CPU).\\\\n4. Take two input sentences (text1 and text2) and tokenize them using the tokenizer.\\\\n5. Run the model with the tokenized inputs and obtain logits.\\\\n6. Apply softmax function to the logits to get probabilities for the three categories: entailment, contradiction, and neutral.\\\\n7. Return the classification result and corresponding probabilities.\\\", '<<<code>>>': \\\"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nmodel_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\\\\nif torch.cuda.is_available():\\\\n    model.cuda()\\\\n\\\\ndef classify_relationship(text1, text2):\\\\n    with torch.inference_mode():\\\\n        out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\\\\n        proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\\\\n    return {v: proba[k] for k, v in model.config.id2label.items()}\\\", 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('cointegrated/rubert-base-cased-nli-threeway')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cointegrated/rubert-base-cased-nli-threeway', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('cointegrated/rubert-base-cased-nli-threeway')\\\", 'api_arguments': ['text1', 'text2'], 'python_environment_requirements': [['transformers', 'sentencepiece']], 'example_code': \\\"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nmodel_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\\\\nif torch.cuda.is_available():\\\\n model.cuda()\\\\ntext1 = '.'\\\\ntext2 = '.'\\\\nwith torch.inference_mode():\\\\n out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\\\\n proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\\\\nprint({v: proba[k] for k, v in model.config.id2label.items()})\\\", 'performance': {'dataset': ['JOCI', 'MNLI', 'MPE', 'SICK', 'SNLI', 'ANLI', 'NLI-style FEVER', 'IMPPRES'], 'accuracy': {'ROC AUC': {'entailment': 0.91, 'contradiction': 0.71, 'neutral': 0.79}}}, 'description': 'This is the DeepPavlov/rubert-base-cased fine-tuned to predict the logical relationship between two short texts: entailment, contradiction, or neutral.' </s>\"\n"}
{"text": "<s>[INST]قم بتدريب نموذج شبكة عصبونية لربط الصور بالنص باستخدام البيانات المدمجة في مجال تحويل الصور إلى نص لرؤية الكمبيوتر.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import necessary classes from the transformers library. This includes VisionEncoderDecoderModel for the image captioning model, ViTFeatureExtractor for vision inputs, AutoTokenizer for processing text, and other required packages like torch and PIL.\\\\n2. We then use the from_pretrained method to load the pre-trained model 'facebook/mmt-en-de'. This model has been trained for image captioning tasks, which is exactly what we need for generating captions for product images in our online store.\\\\n3. We create a function `predict_step` that takes a list of image file paths as input and outputs captions for each image.\\\\n4. Load the images, extract features, tokenize the inputs, and pass them to the model to generate captions.\\\\n5. The generated captions can then be used as a basis for online store product descriptions.\\\", '<<<code>>>': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\\\\nfrom PIL import Image\\\\nimport torch\\\\n\\\\nmodel = VisionEncoderDecoderModel.from_pretrained(\\\\'facebook/mmt-en-de\\\\')\\\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(\\\\'facebook/vit-base-224-pt-16\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/mmt-en-de\\\\')\\\\n\\\\ndef predict_step(images: list):\\\\n    captions = []\\\\n    for image_path in images:\\\\n        image = Image.open(image_path)\\\\n        inputs = feature_extractor(images=image, return_tensors=\\\"pt\\\")\\\\n        input_ids = torch.ones((1, 1), dtype=torch.long)\\\\n        gen_tokens = model.generate(input_ids=input_ids, vision_features=inputs[\\\\'pixel_values\\\\']).image[0]\\\\n        captions.append(tokenizer.decode(gen_tokens))\\\\n     \\\\n    return captions', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\\\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': [{'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}], 'example_code': \\\"predict_step(['Image URL.jpg'])\\\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في تصنيف الصوت؟? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. First, import the necessary libraries like \\\\'AutoModelForAudioClassification\\\\' and \\\\'AudioConfig\\\\' from the transformers package. \\\\n2. Load the pre-trained \\\\'MIT/ast-finetuned-speech-commands-v2\\\\' model, which is designed for classifying commands heard in audio files.\\\\n3. Create a function \\\\'audio_classifier\\\\' to process the audio input and use the model to predict the command present in the audio.\\\\n4. Given the audio input, the program should classify the command as one of the options: \\\"up\\\", \\\"down\\\", \\\"left\\\", \\\"right\\\", \\\"stop\\\", \\\"go\\\".', '<<<code>>>': \\\"from transformers import AutoModelForAudioClassification, AudioConfig\\\\nmodel = AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\\naudio_config = AudioConfig.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\\n\\\\ndef audio_classifier(audio_file_path):\\\\n    audio = Audio.open(audio_file_path, audio_config)\\\\n    logits = model(audio.input_values.to(audio.device), attention_mask=audio.attention_mask.to(audio.device)).logits\\\\n    predicted_id = torch.argmax(logits, dim=-1)\\\\n    return model.config.id2label[int(predicted_id)]\\\\n\\\\nresult = audio_classifier('path/to/audio/file.wav')\\\", 'api_call': \\\"AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ast-finetuned-speech-commands-v2', 'api_call': \\\"AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\\\", 'api_arguments': 'audio file', 'python_environment_requirements': ['transformers library'], 'example_code': \\\"result = audio_classifier('path/to/audio/file.wav')\\\", 'performance': {'dataset': 'Speech Commands v2', 'accuracy': '98.120'}, 'description': 'Audio Spectrogram Transformer (AST) model fine-tuned on Speech Commands v2. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لإنشاء تعرف تلقائي للكلام دون شروط في مجال تقنيات التعرف على الصوت، ونريد استخدام هذا النموذج لتوليد تعرف تلقائي جديد بشكل تلقائي وغير مشروط.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-dutch')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-dutch')\\\\naudio_paths = ['/path/to/call1.mp3', '/path/to/call2.wav']\\\\n# Replace '/path/to/call1.mp3' and '/path/to/call2.wav' with your audio files\\\\ntranscriptions = model.transcribe(audio_paths)\\\", 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Speech Recognition', 'api_name': 'jonatasgrosman/wav2vec2-large-xlsr-53-dutch', 'api_call': \\\"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-dutch')\\\\naudio_paths = ['/path/to/call1.mp3', '/path/to/call2.wav']\\\\n# Replace '/path/to/call1.mp3' and '/path/to/call2.wav' with your audio files\\\\ntranscriptions = model.transcribe(audio_paths)\\\", 'api_arguments': ['audio_paths'], 'python_environment_requirements': ['huggingsound', 'torch', 'librosa', 'datasets', 'transformers'], 'example_code': \\\"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-dutch')\\\\naudio_paths = ['/path/to/file.mp3', '/path/to/another_file.wav']\\\\ntranscriptions = model.transcribe(audio_paths)\\\", 'performance': {'dataset': 'Common Voice nl', 'accuracy': {'Test WER': 15.72, 'Test CER': 5.35, 'Test WER (+LM)': 12.84, 'Test CER (+LM)': 4.64}}, 'description': 'Fine-tuned XLSR-53 large model for speech recognition in Dutch. Fine-tuned on Dutch using the train and validation splits of Common Voice 6.1 and CSS10.'}} </s>\"\n"}
{"text": "<s>[INST]تعمل شركتنا على تطوير نظم تمييز الكلام الآلي في مجال معالجة الكلام الطبيعي تلقائيًا باستخدام تقنيات معالجة الكلام المتقدمة، ونحتاج إلى تصنيف دقيق للنصوص الصوتية بناءً على المحتوى والمعلومات التي تحتوي عليها.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers and datasets packages. This includes WhisperProcessor for processing audio data and WhisperForConditionalGeneration for the transcription model.\\\\n2. We then use the from_pretrained method of the WhisperForConditionalGeneration class to load the pre-trained model 'openai/whisper-small'. This model has been trained for automating speech recognition tasks, which is what we need for transcribing voice meetings.\\\\n3. Load the audio file from the meeting and process it using the WhisperProcessor.\\\\n4. This model can then be used to generate a transcription of the meeting, which can be attached to the meeting minutes.\\\", '<<<code>>>': \\\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-small')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\\\\naudio_data = 'path_to_audio_file.wav'  # replace with the path to your audio file\\\\ninput_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\\\nprint(transcription)\\\", 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai/whisper-small', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\\\", 'api_arguments': {'language': 'english', 'task': 'transcribe'}, 'python_environment_requirements': [{'transformers': 'latest', 'datasets': 'latest'}], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', 'processor = WhisperProcessor.from_pretrained(openai/whisper-small)', 'model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-small)', 'model.config.forced_decoder_ids = None', 'ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)', 'sample = ds[0][audio]', 'input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features', 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)', 'print(transcription)'], 'performance': {'dataset': 'LibriSpeech (clean) test set', 'accuracy': '3.432 WER'}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and supports transcription and translation in various languages.' </s>\"\n"}
{"text": "<s>[INST]أنشئ جملة تبدأ بـ \\\"التصنيف النصي الآلي في توليد النصوص يعتمد على تقنيات معالجة اللغات الطبيعية.\\\" باستخدام تقنيات معالجة اللغات الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"HappyTextToText('T5', 'vennify/t5-base-grammar-correction')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, install the required happytransformer library using 'pip install happytransformer'.\\\\n2. Import the HappyTextToText class and TTSettings from the happytransformer library.\\\\n3. Initialize the HappyTextToText model using the 'T5' tokenizer and the 'vennify/t5-base-grammar-correction' model.\\\\n4. Create a TTSettings object to set the number of beams and minimum length required for the output.\\\\n5. Call the generate_text() method on the HappyTextToText model to generate grammatically correct text using the input text with incorrect grammar.\\\\n6. Print the corrected text after revising it with the model.\\\", '<<<code>>>': 'from happytransformer import HappyTextToText, TTSettings\\\\nhappy_tt = HappyTextToText(\\\\'T5\\\\', \\\\'vennify/t5-base-grammar-correction\\\\')\\\\nargs = TTSettings(num_beams=5, min_length=1)\\\\ninput_text = \\\"Grammar: This sentences has has bads grammar.\\\"\\\\nresult = happy_tt.generate_text(input_text, args=args)\\\\nprint(result.text)', 'api_call': \\\"HappyTextToText('T5', 'vennify/t5-base-grammar-correction')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'vennify/t5-base-grammar-correction', 'api_call': \\\"HappyTextToText('T5', 'vennify/t5-base-grammar-correction')\\\", 'api_arguments': {'num_beams': 5, 'min_length': 1}, 'python_environment_requirements': [{'package': 'happytransformer', 'installation': 'pip install happytransformer'}], 'example_code': 'from happytransformer import HappyTextToText, TTSettings\\\\nhappy_tt = HappyTextToText(T5, vennify/t5-base-grammar-correction)\\\\nargs = TTSettings(num_beams=5, min_length=1)\\\\nresult = happy_tt.generate_text(grammar: This sentences has has bads grammar., args=args)\\\\nprint(result.text)', 'performance': {'dataset': 'jfleg', 'accuracy': 'Not provided'}, 'description': 'This model generates a revised version of inputted text with the goal of containing fewer grammatical errors. It was trained with Happy Transformer using a dataset called JFLEG.' </s>\"\n"}
{"text": "<s>[INST]نبحث عن مطوّر يعمل على تطبيق تصنيف الصور بتقنيات الرؤية الحاسوبية. يجب أن يكون البرنامج قادرًا على تحديد المميزات والفئات في الصور وتقديم تصنيف دقيق. يرجى اعتماد تقنيات متقدمة في هذا المجال وضمان التحسين المستمر للأداء.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import AutoImageProcessor and AutoModelForImageClassification classes from the transformers library.\\\\n2. Load the image data of the product.\\\\n3. Use the from_pretrained method of the AutoImageProcessor and AutoModelForImageClassification classes to load the pre-trained model 'google/mobilenet_v1_0.75_192'. This model is trained for image classification tasks and can be used to identify products from their images.\\\\n4. Process the image and pass it through the classification model.\\\\n5. Obtain the predicted class index and use the model's config to map it to a specific label.\\\\n6. The identified product label can be used for other tasks, such as product catalog management and recommendation systems.\\\", '<<<code>>>': \\\"from transformers import AutoImageProcessor, AutoModelForImageClassification\\\\nfrom PIL import Image\\\\n\\\\nimage = Image.open('product_image.jpg')\\\\n# replace 'product_image.jpg' with your image file path\\\\npreprocessor = AutoImageProcessor.from_pretrained('google/mobilenet_v1_0.75_192')\\\\nmodel = AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\\\\ninputs = preprocessor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nproduct_class = model.config.id2label[predicted_class_idx]\\\", 'api_call': \\\"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/mobilenet_v1_0.75_192', 'api_call': \\\"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'google/mobilenet_v1_0.75_192'}, 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\\\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\\\\ninputs = preprocessor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.' </s>\"\n"}
{"text": "<s>[INST]قم بتصنيف الصور باستخدام الرؤية الحاسوبية بدون تدريب، ابدأ بتصنيف صورة معينة وبوضوح محدد.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a zero-shot image classification model.\\\\n3. Specify the model 'laion/CLIP-ViT-g-14-laion2B-s34B-b88K' to be loaded. This model is based on OpenCLIP and is capable of classifying images into various categories without requiring training data for those categories.\\\\n4. Provide the path to the image file and a list of class names that the classifier should consider. The model will classify the image and assign a probability to each class.\\\\n5. The created classifier can be used to categorize the images in your website into their appropriate sections.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\\\\nclass_names = ['Animals', 'Vehicles', 'Landscape', 'Technology', 'People']\\\\nclassified_image = image_classifier('path/to/image/file', class_names=class_names)\\\", 'api_call': \\\"pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-ViT-g-14-laion2B-s34B-b88K', 'api_call': \\\"pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\\\", 'api_arguments': {'image': 'path/to/image/file', 'class_names': 'list_of_class_names'}, 'python_environment_requirements': [{'huggingface_hub': '0.0.17', 'transformers': '4.11.3', 'torch': '1.9.0', 'torchvision': '0.10.0'}], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.' </s>\"\n"}
{"text": "<s>[INST]يقوم العميل بتطوير تطبيق لتصنيف النصوص في معالجة اللغة الطبيعية. نحن بحاجة إلى برنامج قادر على تحليل النصوص وتقديمها بطريقة مرتبة لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"pipeline(sentiment-analysis, model='cardiffnlp/twitter-xlm-roberta-base-sentiment')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function with the 'sentiment-analysis' parameter and specify the model 'cardiffnlp/twitter-xlm-roberta-base-sentiment'. This model is pre-trained for sentiment analysis and supports multiple languages, allowing you to classify sentiments without the need for translation first.\\\\n3. Provide a tweet to the pipeline, and the model will return the sentiment classification (positive, negative, or neutral).\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nsentiment_task = pipeline('sentiment-analysis', model='cardiffnlp/twitter-xlm-roberta-base-sentiment')\\\\ntweet = 'Mi piace il tuo sorriso!'\\\\nresult = sentiment_task(tweet)\\\", 'api_call': \\\"pipeline(sentiment-analysis, model='cardiffnlp/twitter-xlm-roberta-base-sentiment')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cardiffnlp/twitter-xlm-roberta-base-sentiment', 'api_call': \\\"pipeline(sentiment-analysis, model='cardiffnlp/twitter-xlm-roberta-base-sentiment')\\\", 'api_arguments': ['model_path'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\nmodel_path = cardiffnlp/twitter-xlm-roberta-base-sentiment\\\\nsentiment_task = pipeline(sentiment-analysis, model=model_path, tokenizer=model_path)\\\\nsentiment_task(T'estimo!)\\\", 'performance': {'dataset': 'Twitter', 'accuracy': 'Not provided'}, 'description': 'This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).' </s>\"\n"}
{"text": "<s>[INST]لإنشاء نموذج تحويل الصورة إلى صورة في مجال رؤية الحواسيب، يجب على الباحثين تطوير شبكة عصبونية تدمج الصورة الأصلية مع الصورة المستهدفة للتدريب. ينصح بإضافة آلية تعلم تقوم بتنظيم بيانات التدريب بشكل ملائم لتحقيق أداء عالي للنموذج. يمكن تحسين النتائج عن طريق تعزيز التعرف على الأنماط والميزات البصرية المهمة.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary libraries and classes, including ControlNetModel from the diffusers package.\\\\n2. Load the 'lllyasviel/sd-controlnet-seg' model using the from_pretrained method of the ControlNetModel class. This model is trained for image segmentation tasks, which is suitable for segmenting satellite images of cities.\\\\n3. Apply the model to a given satellite image using the pipe function and save the segmented image to a file.\\\\n4. The segmented image can then be analyzed by urban planners to study different aspects of city growth and development.\\\", '<<<code>>>': \\\"from diffusers import ControlNetModel\\\\nfrom utils import pipe\\\\n\\\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\\nsatellite_image = 'path_to_satellite_image'\\\\n\\\\nsegmented_image = pipe(satellite_image, controlnet, num_inference_steps=20).images[0]\\\\nsegmented_image.save('segmented_city.png')\\\", 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/sd-controlnet-seg', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", 'api_arguments': ['torch_dtype'], 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate']], 'example_code': \\\"image = pipe(house, image, num_inference_steps=20).images[0]\\\\nimage.save('./images/house_seg_out.png')\\\", 'performance': {'dataset': 'ADE20K', 'accuracy': 'Trained on 164K segmentation-image, caption pairs'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.' </s>\"\n"}
{"text": "<s>[INST]لإجراء محادثة مع نموذج التعلم الآلي لمعالجة اللغة الطبيعية، اتبع الخطوات التالية: ١. ابدأ الجلسة بالتحية اللائقة. ٢. طرح السؤال بوضوح وإيضاح. ٣. قم بالاستماع للاستجابة بانتباه ودقة. ٤. تحقق من فهم الرد والتأكد من دقته. ٥. في حالة عدم الفهم، اطلب توضيح أو اعادة صياغة السؤال بشكل مختلف. ٦. استخدم أدوات معالجة اللغة الطبيعية والتعلم الآلي لتحسين تفاعلاتك مع نموذج التعلم الآلي.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"pipeline('conversational', model='hyunwoongko/blenderbot-9B')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a conversational model.\\\\n3. Specify the model 'hyunwoongko/blenderbot-9B' to be loaded. This model has been trained on a large dataset containing multi-turn conversations and excels at engaging users in natural conversations.\\\\n4. The created chatbot can be used to engage users in natural conversations by providing input text to the model and generating appropriate responses.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nchatbot = pipeline(\\\\'conversational\\\\', model=\\\\'hyunwoongko/blenderbot-9B\\\\')\\\\nconversation = \\\"What\\\\'s your favorite movie?\\\"\\\\nresponse = chatbot(conversation)\\\\nprint(response[\\\\'generated_text\\\\'])', 'api_call': \\\"pipeline('conversational', model='hyunwoongko/blenderbot-9B')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hyunwoongko/blenderbot-9B', 'api_call': \\\"pipeline('conversational', model='hyunwoongko/blenderbot-9B')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': 'Input a message to start chatting with hyunwoongko/blenderbot-9B.', 'performance': {'dataset': 'blended_skill_talk', 'accuracy': 'Not provided'}, 'description': 'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.' </s>\"\n"}
{"text": "<s>[INST]عليك بتطوير نظام لتصنيف الصور باستخدام تقنية رؤية الحاسوب. المطلوب هو تصميم خوارزمية قادرة على تصنيف الصور بدقة واعتمادًا على البيانات المتاحة.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. We first import the necessary classes from the transformers, torch, and datasets packages. This includes AutoImageProcessor for image processing, ResNetForImageClassification for the image classification model, and load_dataset for loading your cat image dataset.\\\\n2. We then use the \\\"from_pretrained\\\" method of the ResNetForImageClassification class to load the pre-trained model \\\\'microsoft/resnet-50\\\\'. This model has been trained for image classification tasks and can be fine-tuned on specific classification problems, such as cat breed classification.\\\\n3. We load the cat image dataset using the load_dataset function from the datasets package.\\\\n4. We process the image data and create input tensors for the model using the AutoImageProcessor class.\\\\n5. The model is then used to predict the cat breed for a given image, with the label output by the model corresponding to the predicted breed.', '<<<code>>>': \\\"from transformers import AutoImageProcessor, ResNetForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\n\\\\ndataset = load_dataset('your_cat_dataset')\\\\n# replace 'your_cat_dataset' with the name of your cat dataset\\\\nimage = dataset['test']['image'][0]\\\\nprocessor = AutoImageProcessor.from_pretrained('microsoft/resnet-50')\\\\nmodel = ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\\\\ninputs = processor(image, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\n    predicted_label = logits.argmax(-1).item()\\\\n    print(model.config.id2label[predicted_label])\\\", 'api_call': \\\"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/resnet-50', 'api_call': \\\"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\\\", 'api_arguments': {'from_pretrained': 'microsoft/resnet-50'}, 'python_environment_requirements': [{'transformers': 'AutoImageProcessor, ResNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}], 'example_code': 'from transformers import AutoImageProcessor, ResNetForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset(huggingface/cats-image)\\\\nimage = dataset[test][image][0]\\\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/resnet-50)\\\\nmodel = ResNetForImageClassification.from_pretrained(microsoft/resnet-50)\\\\ninputs = processor(image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': '~0.5% top1'}, 'description': 'ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback.' </s>\"\n"}
{"text": "<s>[INST]يُرجى تطوير نموذج لتقسيم الصور في مجال \\\"رؤية الحاسوب\\\"، يتمكن هذا النموذج من تقسيم الصور إلى أقسام واعتبار كل قسم على أنه كائن مستقل. يجب أن يكون النموذج قادرًا على تحليل الصور الواردة وتقسيمها لتمييز الكائنات المختلفة في الصور. هل يمكنك تطوير هذا النموذج؟? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries, which are YOLO and render_result from ultralyticsplus.', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\n\\\\npcb_image = 'path_to_your_pcb_image'\\\\nresults = model.predict(pcb_image)\\\\n\\\\nidentified_defects = results[0].boxes\\\\nrendered_image = render_result(model=model, image=pcb_image, result=results[0])\\\\nrendered_image.show()\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pcb-defect-segmentation', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\n\\\\npcb_image = 'path_to_your_pcb_image'\\\\nresults = model.predict(pcb_image)\\\\n\\\\nidentified_defects = results[0].boxes\\\\nrendered_image = render_result(model=model, image=pcb_image, result=results[0])\\\\nrendered_image.show()\\\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\n\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.515, 'mAP@0.5(mask)': 0.491}}, 'description': 'YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم الشركة بتطوير نظام جديد لتصنيف البيانات الجدولية، وهو يتطلب بناء نموذج يستند إلى البيانات المتوفرة لتحقيق هذا الهدف.? [/INST]{'domain': 'Tabular Tabular Classification', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary libraries, which are 'joblib' and 'pandas'.\\\", 'code': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\ndata = pd.read_csv('data.csv')\\\\n# replace 'data.csv' with path to your data file\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Tabular Classification', 'api_name': 'datadmg/autotrain-test-news-44534112235', 'api_call': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\ndata = pd.read_csv('data.csv')\\\\n# replace 'data.csv' with path to your data file\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_arguments': ['data'], 'python_environment_requirements': ['joblib', 'pandas'], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'datadmg/autotrain-data-test-news', 'accuracy': 0.333}, 'description': 'This model is trained for Multi-class Classification on CO2 Emissions dataset. It uses the Hugging Face Transformers framework and is based on the extra_trees algorithm. The model is trained with AutoTrain and has a tabular classification functionality.'}} </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"CrossEncoder('cross-encoder/nli-deberta-v3-base')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. To use this API, you need to import the 'CrossEncoder' class from the 'sentence_transformers' package.\\\", 'code': 'from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder(\\\\'cross-encoder/nli-deberta-v3-base\\\\')\\\\nsentence_pair = (\\\"When an ice skater moves their arms closer to the body, the spin becomes faster.\\\", \\\"This phenomenon proves the conservation of angular momentum.\\\")\\\\nscores = model.predict([sentence_pair])', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Transformers', 'functionality': 'Cross-Encoder for Natural Language Inference', 'api_name': 'cross-encoder/nli-deberta-v3-base', 'api_call': 'from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder(\\\\'cross-encoder/nli-deberta-v3-base\\\\')\\\\nsentence_pair = (\\\"When an ice skater moves their arms closer to the body, the spin becomes faster.\\\", \\\"This phenomenon proves the conservation of angular momentum.\\\")\\\\nscores = model.predict([sentence_pair])', 'api_arguments': ['sentence_pairs'], 'python_environment_requirements': ['sentence_transformers', 'transformers'], 'example_code': \\\"from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-base')\\\\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])\\\", 'performance': {'dataset': {'SNLI-test': '92.38', 'MNLI mismatched set': '90.04'}}, 'description': 'This model is based on microsoft/deberta-v3-base and was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.'}} </s>\"\n"}
{"text": "<s>[INST]يتم تطوير تطبيق يستخدم تقنيات معالجة اللغة الطبيعية التفاعلية. يحتاج البرنامج إلى القدرة على فهم وتحليل اللغة الطبيعية بشكل دقيق وفعّال لتحسين تفاعل المستخدم وتوجيهه.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('facebook/blenderbot-90M')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import necessary libraries, including AutoModelForCausalLM and AutoTokenizer from the transformers package.\\\\n2. Load the pre-trained model 'facebook/blenderbot-90M' using AutoModelForCausalLM.from_pretrained method, which is optimized for multi-turn dialogue tasks.\\\\n3. Load the corresponding tokenizer using AutoTokenizer.from_pretrained method.\\\\n4. To make the AI respond to an input message, tokenize this input message and append an end-of-string marker to it.\\\\n5. The generated tokens are passed to the model, which will output a sequence of tokens representing its response.\\\\n6. Finally, decode the response tokens into a string and display the AI's response.\\\", '<<<code>>>': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained('facebook/blenderbot-90M')\\\\nmodel = AutoModelForCausalLM.from_pretrained('facebook/blenderbot-90M')\\\\n\\\\ninput_message = 'What is your favorite color?'\\\\ntokenized_input = tokenizer.encode(input_message + tokenizer.eos_token, return_tensors='pt')\\\\n\\\\noutput = model.generate(tokenized_input, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n\\\\nresponse = tokenizer.decode(output[:, tokenized_input.shape[-1]:][0], skip_special_tokens=True)\\\\nprint(response)\\\", 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/blenderbot-90M')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/blenderbot-90M', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/blenderbot-90M')\\\", 'api_arguments': {'input_message': 'str'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained('facebook/blenderbot-90M')\\\\nmodel = AutoModelForCausalLM.from_pretrained('facebook/blenderbot-90M')\\\\n\\\\n# Chat with the model\\\\ninput_message = 'What is your favorite color?'\\\\ntokenized_input = tokenizer.encode(input_message + tokenizer.eos_token, return_tensors='pt')\\\\n\\\\noutput = model.generate(tokenized_input, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n\\\\nresponse = tokenizer.decode(output[:, tokenized_input.shape[-1]:][0], skip_special_tokens=True)\\\\nprint(response)\\\", 'performance': {'dataset': 'blended_skill_talk', 'accuracy': 'Not provided'}, 'description': 'BlenderBot-90M is a conversational AI model developed by Facebook AI. It is trained on the Blended Skill Talk dataset and aims to provide engaging and human-like responses in a multi-turn dialogue setting. The model is deprecated, and it is recommended to use the identical model https://huggingface.co/facebook/blenderbot_small-90M instead.' </s>\"\n"}
{"text": "<s>[INST]قم بتطبيق تعلم التعزيز على روبوت لتعليمه كيفية التنقل داخل بيئة مجهولة.? [/INST]'<<<domain>>>': 'Reinforcement Learning Robotics', '<<<api_call>>>': \\\"pipeline('robotics', model='Antheia/Hanna')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a reinforcement learning model for robotics.\\\\n3. Specify the model 'Antheia/Hanna' to be loaded. This model is trained on the openai/webgpt_comparisons dataset for various robotics tasks.\\\\n4. The created model can be used as the decision-making system for the robotic arm. By providing it with the current context of the environment (e.g., object positions on the conveyor belt), the model can help determine the appropriate action (where to grab objects) based on its reinforcement learning training.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nrobotics_pipeline = pipeline('robotics', model='Antheia/Hanna')\\\\nrobotic_arm_decisions = robotics_pipeline(context)\\\", 'api_call': \\\"pipeline('robotics', model='Antheia/Hanna')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Reinforcement Learning Robotics', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Antheia/Hanna', 'api_call': \\\"pipeline('robotics', model='Antheia/Hanna')\\\", 'api_arguments': 'model', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'openai/webgpt_comparisons', 'accuracy': ''}, 'description': 'Antheia/Hanna is a reinforcement learning model for robotics tasks, trained on the openai/webgpt_comparisons dataset.' </s>\"\n"}
{"text": "<s>[INST]نحن بحاجة إلى برنامج في مجال تحويل النص إلى صور يستطيع التصنيف بدون تدريب مسبق. هذا البرنامج سيساعد العميل في تطوير تطبيقه لفئة ذوي الإعاقة البصرية عبر تحويل النص إلى صور لمستخدمي التطبيق.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting', revision=fp16, torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the StableDiffusionInpaintPipeline model from the diffusers library.\\\\n2. Instantiate the model by loading it from Hugging Face\\\\'s model hub using the from_pretrained() method.\\\\n3. Provide the given text prompt \\\"A yellow cat sitting on a park bench\\\" to the model.\\\\n4. Use the model to generate a photo-realistic image based on the provided text prompt, without the need for mask_image in this case since we are not inpainting existing images.\\\\n5. Save the generated image to a file, such as \\\"yellow_cat_on_park_bench.png\\\".', '<<<code>>>': 'from diffusers import StableDiffusionInpaintPipeline\\\\npipe = StableDiffusionInpaintPipeline.from_pretrained(\\\\'runwayml/stable-diffusion-inpainting\\\\', revision=\\\\'fp16\\\\', torch_dtype=torch.float16)\\\\nprompt = \\\"A yellow cat sitting on a park bench\\\"\\\\ngenerated_image = pipe(prompt=prompt).images[0]\\\\ngenerated_image.save(\\\"yellow_cat_on_park_bench.png\\\")', 'api_call': \\\"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting', revision=fp16, torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \\\"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting', revision=fp16, torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': [{'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}], 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطبيق تقنيات الرؤية الحاسوبية لتحديد وتتبع الحيوانات في البرية. هل يمكنك مساعدتي في توليد فيديو من وصف نصي لمشهد يصور الحيوانات؟? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8m-forklift-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the ultralyticsplus package. The YOLO and render_result classes are required for object detection using the YOLOv8 framework.\\\\n2. Load the pre-trained model 'keremberke/yolov8m-forklift-detection' using the YOLO class. This model has been specifically trained for detecting forklifts and people, which is suitable for our warehouse safety system.\\\\n3. Adjust the model parameters such as confidence threshold, IOU threshold, and maximum detections to better fit our use case.\\\\n4. We can then input the image (URL or local path) containing the dangerous area and run the model to detect forklifts and people in that image.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-forklift-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://sample_url_here/image_name'  # replace with URL or local path to your image\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8m-forklift-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8m-forklift-detection', 'api_call': \\\"YOLO('keremberke/yolov8m-forklift-detection')\\\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8m-forklift-detection')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'forklift-object-detection', 'accuracy': 0.846}, 'description': 'A YOLOv8 model for detecting forklifts and persons in images.' </s>\"\n"}
{"text": "<s>[INST]يطوّر العميل تطبيقًا خاصًا بذوي الإعاقة البصرية. نحتاج إلى برنامج لتحويل النص إلى كلام لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"AutoModelForTTS.from_pretrained('lakahaga/novel_reading_tts')\\\", '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers package. This includes AutoModelForTTS for the text-to-speech model.\\\\n2. We then load the pre-trained model 'lakahaga/novel_reading_tts' that is designed specifically for Korean text-to-speech tasks.\\\\n3. We prepare the input text from our Korean novel.\\\\n4. The model is then used to generate the audiobook version of the novel content, and it can be saved as an audio file for distribution to consumers.\\\", '<<<code>>>': 'from transformers import AutoModelForTTS, AutoTokenizer\\\\nmodel = AutoModelForTTS.from_pretrained(\\\\'lakahaga/novel_reading_tts\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'lakahaga/novel_reading_tts\\\\')\\\\n\\\\ntext = \\\"이것은 새로 출판된 한국어 소설의 텍스트입니다.\\\"  # replace with the novel text\\\\ninputs = tokenizer(text, return_tensors=\\\\'pt\\\\')\\\\ngenerated_audio = model.generate(**inputs)', 'api_call': \\\"AutoModelForTTS.from_pretrained('lakahaga/novel_reading_tts')\\\", 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lakahaga/novel_reading_tts', 'api_call': \\\"AutoModelForTTS.from_pretrained('lakahaga/novel_reading_tts')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"inputs = processor(text, return_tensors='pt'); generated_audio = model.generate(**inputs);\\\", 'performance': {'dataset': 'novelspeech', 'accuracy': None}, 'description': 'This model was trained by lakahaga using novelspeech recipe in espnet. It is designed for Korean text-to-speech tasks.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج لتصنيف الرموز في معالجة اللغة الطبيعية دون مسبق التدريب من خلال الرؤية الحاسوبية.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"pipeline('token-classification', model='kredor/punctuate-all')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' function from the transformers library.\\\\n2. Load the pre-trained 'kredor/punctuate-all' model, which is designed for punctuation prediction in different languages, using the 'pipeline' function.\\\\n3. Use the loaded model to predict the punctuation for the given text about a historical event.\\\", '<<<code>>>': 'from transformers import pipeline\\\\npunctuate_pipeline = pipeline(\\\\'token-classification\\\\', model=\\\\'kredor/punctuate-all\\\\')\\\\ntext = \\\"On April 16 1912 the Titanic sank in the early hours of the morning after striking an iceberg on its maiden voyage from Southampton England to New York City the disaster resulted in the loss of over 1500 lives and remains one of the deadliest peacetime maritime disasters in history\\\"\\\\npunctuated_text = punctuate_pipeline(text)', 'api_call': \\\"pipeline('token-classification', model='kredor/punctuate-all')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kredor/punctuate-all', 'api_call': \\\"pipeline('token-classification', model='kredor/punctuate-all')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'multilingual', 'accuracy': 0.98}, 'description': 'A finetuned xlm-roberta-base model for punctuation prediction on twelve languages: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian.' </s>\"\n"}
{"text": "<s>[INST]أنشئ صورة متعددة الوسائط مستوحاة من النص التالي: \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the \\\\'pipeline\\\\' function from the \\\\'transformers\\\\' library provided by Hugging Face.\\\\n2. The \\\\'pipeline\\\\' function is used to create a text-to-image model converting a given text prompt into a corresponding visual content.\\\\n3. We use \\\\'22h/vintedois-diffusion-v0-1\\\\' as the model to load for this task.\\\\n4. Call the created pipeline function with the narrative \\\"a beautiful mountain scenery during the sunset\\\" and additional parameters such as \\\\'CFG Scale\\\\', \\\\'Scheduler\\\\', \\\\'Steps\\\\', and \\\\'Seed\\\\' to generate the image.', '<<<code>>>': \\\"from transformers import pipeline\\\\ntext2img = pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\\\\ngenerated_image = text2img('a beautiful mountain scenery during the sunset', CFG_Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\\\", 'api_call': \\\"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \\\"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\\\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\\\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>\"\n"}
{"text": "<s>[INST]يجب علينا تطوير نظام للتحويل من النص إلى كلام مسموع يعتمد على الصوتيات، ويهدف إلى تحويل النصوص إلى كلام مفهوم. يتطلب ذلك دقة في تحليل النصوص وتحويلها إلى كلام مسموع.? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'api_provider': 'ESPnet', 'explanation': '', 'code': '', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'ESPnet', 'functionality': 'Text-to-Speech', 'api_name': 'kazusam/kt', 'api_call': '', 'api_arguments': ['input_text'], 'python_environment_requirements': ['Hugging Face', 'ESPnet'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': 'amadeus', 'accuracy': 'Not provided'}, 'description': 'An ESPnet2 TTS model trained by mio using amadeus recipe in espnet.'}} </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى برنامج قادر على معالجة اللغة الطبيعية وتوليد النصوص، حيث يقوم بفهم النصوص والرسائل المكتوبة والمقروءة بواسطة الإنسان، ثم يقوم بتحويل هذه المعلومات إلى أسئلة قابلة للفهم والإجابة عليها من قبل المستخدمين في تطبيقنا متعدد الوسائط لتسهيل التفاعل مع مستخدميه ذوي الإعاقة البصرية.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"AutoModelWithLMHead.from_pretrained('flexudy/t5-base-multi-sentence-doctor')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. First, you will need to import AutoTokenizer and AutoModelWithLMHead classes from the transformers library.\\\\n2. Initialize both the tokenizer and the model by calling the from_pretrained() method, providing \\\\'flexudy/t5-base-multi-sentence-doctor\\\\' as the pre-trained model name.\\\\n3. Prepare the input text with the text that needs repair using the format \\\"repair_sentence: {input_sentence} context: {additional_context} </s>\\\", where input_sentence is the target text and additional_context can provide extra information to help the model with the correction.\\\\n4. Encode the input text using the tokenizer, to generate the input_ids for the model.\\\\n5. Use the generate() function on the model to create the corrected-output sentence.\\\\n6. Decode the output sentence(s) using tokenizer.decode() method with parameters to skip_special_tokens and clean_up_tokenization_spaces.\\\\n7. The final decoded sentence will contain the corrected version of the input text.', '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'flexudy/t5-base-multi-sentence-doctor\\\\')\\\\nmodel = AutoModelWithLMHead.from_pretrained(\\\\'flexudy/t5-base-multi-sentence-doctor\\\\')\\\\ninput_text = \\\"repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\"\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=\\\\'pt\\\\')\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('flexudy/t5-base-multi-sentence-doctor')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('flexudy/t5-base-multi-sentence-doctor')\\\", 'api_arguments': ['input_text'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء صورة متعددة الوسائط من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في تقسيم الصورة إلى مناطق تمثل كل نوع من الحيوانات؟? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries: Image from PIL, AutoImageProcessor from transformers, and Mask2FormerForUniversalSegmentation from transformers.', 'code': \\\"from PIL import Image\\\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\nimport requests\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-cityscapes-semantic', 'api_call': \\\"from PIL import Image\\\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\nimport requests\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\n\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-large-cityscapes-semantic'}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \\\"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.'}} </s>\"\n"}
{"text": "<s>[INST]لتحسين أداء نموذج الرؤية الحاسوبية الخاص بك في تحويل الصور إلى صور، قم بتدريب شبكة عصبونية عميقة باستخدام مجموعة البيانات المناسبة واستخدم خوارزميات مثل GANs. قم بتقديم الصور الأصلية والنتائج المتوقعة لتقويم فعالية النموذج. ويجب أخيراً إجراء اختبارات استدعائية لضمان جودة الصور الناتجة.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We first import necessary libraries and classes, including PIL's Image library, and the ControlNetModel class and various other tools from the diffusers package.\\\\n2. We will then be using the MLSDdetector to detect straight lines in the input image of the building.\\\\n3. The ControlNet model will be employed after loading it using the from_pretrained method. This model is designed to control diffusion models by adding extra conditions.\\\\n4. The image will be processed by the model and straight lines will be detected and highlighted.\\\", '<<<code>>>': \\\"from PIL import Image\\\\nfrom diffusers import ControlNetModel, StableDiffusionControlNetPipeline, load_image\\\\nfrom controlnet_aux import MLSDdetector\\\\nimport torch\\\\n\\\\nmlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\\\nimage = load_image('image_path.png')\\\\n# replace 'image_path.png' with path to your image\\\\nimage = mlsd(image)\\\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\n\\\\nimage = pipe(room, image, num_inference_steps=20).images[0]\\\\nimage.save('images/building_mlsd_out.png')\\\", 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/sd-controlnet-mlsd', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\\\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': [{'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}], 'example_code': {'import': ['from PIL import Image', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'import torch', 'from controlnet_aux import MLSDdetector', 'from diffusers.utils import load_image'], 'setup': [\\\"mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)', 'image = mlsd(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)'], 'execution': ['pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', 'image = pipe(room, image, num_inference_steps=20).images[0]', \\\"image.save('images/room_mlsd_out.png')\\\"]}, 'performance': {'dataset': '600k edge-image, caption pairs generated from Places2', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتصميم نماذج بيانات جدولية، ونحتاج إلى تصنيف البيانات وتحديد فئاتها بدقة لتحسين أداء نماذج التعلم الآلي.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. In this case, we'll use the 'joblib' library to load a pre-trained machine learning model. This model has been trained on a dataset of plants, so it can classify plant species based on their characteristics.\\\\n2. We import 'joblib', 'pandas', and 'json' libraries for this task. We use 'joblib' to load the pre-trained model, 'pandas' to process tabular data, and 'json' to read the configuration file.\\\\n3. We load the pre-trained model using 'joblib.load' and load the configuration file to know which features are needed.\\\\n4. Given plant characteristics like sepal and petal dimensions, we can predict the species using the 'predict' method of the loaded model.\\\", '<<<code>>>': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.DataFrame({'characteristic_1': [value_1], 'characteristic_2': [value_2], 'characteristic_3': [value_3], 'characteristic_4': [value_4]})\\\\n# Input characteristics of the plant\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'abhishek/autotrain-iris-xgboost', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'scikit-learn/iris', 'accuracy': 0.8666666666666667}, 'description': 'A tabular classification model trained on the Iris dataset using XGBoost and AutoTrain. The model is capable of multi-class classification and has an accuracy of 86.67%.' </s>\"\n"}
{"text": "<s>[INST]سوف تعمل الشركة على تطوير نظام يتعامل مع اللغة الطبيعية لترجمة النصوص، وتحتاج إلى تصميم خوارزميات تستطيع الإجابة على أسئلة متعلقة بترجمة المضمون من لغة إلى أخرى.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. First, import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a machine translation model by specifying the model 'Helsinki-NLP/opus-mt-es-en'. This model is designed to translate Spanish text to English.\\\\n3. Provide the Spanish email body as input to the model and get back an English translation of the email content.\\\\n4. Read the translated email content to understand the message from the client.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntranslation_model = pipeline(\\\\'translation_es_to_en\\\\', model=\\\\'Helsinki-NLP/opus-mt-es-en\\\\')\\\\nspanish_email = \\\"Estimado/a, me gustaría obtener más información sobre sus servicios y precios. Además, podría facilitarnos un folleto con los detalles de sus productos? Agradezco su atención. Saludos cordiales.\\\"\\\\nenglish_translation = translation_model(spanish_email)[0][\\\\'translation_text\\\\']', 'api_call': \\\"pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-es-en', 'api_call': \\\"pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')('Hola, ¿cómo estás?')\\\", 'performance': {'dataset': [{'name': 'newssyscomb2009-spaeng.spa.eng', 'accuracy': {'BLEU': 30.6, 'chr-F': 0.57}}, {'name': 'news-test2008-spaeng.spa.eng', 'accuracy': {'BLEU': 27.9, 'chr-F': 0.553}}, {'name': 'newstest2009-spaeng.spa.eng', 'accuracy': {'BLEU': 30.4, 'chr-F': 0.572}}, {'name': 'newstest2010-spaeng.spa.eng', 'accuracy': {'BLEU': 36.1, 'chr-F': 0.614}}, {'name': 'newstest2011-spaeng.spa.eng', 'accuracy': {'BLEU': 34.2, 'chr-F': 0.599}}, {'name': 'newstest2012-spaeng.spa.eng', 'accuracy': {'BLEU': 37.9, 'chr-F': 0.624}}, {'name': 'newstest2013-spaeng.spa.eng', 'accuracy': {'BLEU': 35.3, 'chr-F': 0.609}}, {'name': 'Tatoeba-test.spa.eng', 'accuracy': {'BLEU': 59.6, 'chr-F': 0.739}}]}, 'description': 'Helsinki-NLP/opus-mt-es-en is a machine translation model trained to translate from Spanish to English using the Hugging Face Transformers library. The model is based on the Marian framework and was trained on the OPUS dataset.' </s>\"\n"}
{"text": "<s>[INST]قم بتنشيط عملية التعلم الآلي من خلال إجراء عمليات تباين لاستكشاف مجموعة البيانات الصوتية وضبط النماذج لتحقيق أفضل أداء في توقع نصوص التحويل التلقائي للكلام. يمكنك القيام بذلك بإعداد طاولة البيانات بشكل جيد واستخدام خوارزميات التحسين المتاحة لك في أداة التحليل الصوتي المخصصة.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': 'pipeline(\\\"automatic-speech-recognition\\\", model=\\\"openai/whisper-tiny\\\")', 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the required libraries, such as 'pipeline' from the transformers package.\\\", 'code': 'from transformers import pipeline\\\\nasr_model = pipeline(\\\"automatic-speech-recognition\\\", model=\\\"openai/whisper-tiny\\\")\\\\naudio_file = \\\"path_to_advertisement_audio_file\\\"\\\\ntranscription = asr_model(audio_file)', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Transcription and Translation', 'api_name': 'openai/whisper-tiny', 'api_call': 'from transformers import pipeline\\\\nasr_model = pipeline(\\\"automatic-speech-recognition\\\", model=\\\"openai/whisper-tiny\\\")\\\\naudio_file = \\\"path_to_advertisement_audio_file\\\"\\\\ntranscription = asr_model(audio_file)', 'api_arguments': {'forced_decoder_ids': \\\"WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')\\\"}, 'python_environment_requirements': ['transformers', 'datasets'], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', \\\"processor = WhisperProcessor.from_pretrained('openai/whisper-tiny')\\\", \\\"model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny')\\\", 'model.config.forced_decoder_ids = None', \\\"ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')\\\", \\\"sample = ds[0]['audio']\\\", \\\"input_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\\\", 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)'], 'performance': {'dataset': 'LibriSpeech (clean)', 'accuracy': 7.54}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model that can be used for transcription and translation tasks.'}} </s>\"\n"}
{"text": "<s>[INST]عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية في مجال معالجة اللغةالطبيعية والإجابة على الأسئلة. نحن بحاجة إلى برنامج يقوم بتحويل النص إلى كلام لمستخدمي هذا التطبيق ليستفيدوا منه في مجال الإجابة على أسئلة المستندات متعددة الوسائط.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/minilm-uncased-squad2')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers package. This includes AutoModelForQuestionAnswering, AutoTokenizer for handling tokenization, and the pipeline function.\\\\n2. We then use the from_pretrained method of the AutoModelForQuestionAnswering class to load the pre-trained model 'deepset/minilm-uncased-squad2'. This model has been trained for question answering tasks, which is exactly what we need for automatic FAQ answering.\\\\n3. We create a question-answering pipeline with the model and tokenizer. A sample input containing the user question and context from the FAQ page is provided to the pipeline.\\\\n4. The output from the pipeline will be the answer to the user's question.\\\", '<<<code>>>': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel_name = \\\\'deepset/minilm-uncased-squad2\\\\'\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=model_name, tokenizer=model_name)\\\\nQA_input = {\\\\n \\\\'question\\\\': \\\"What is the purpose of user\\\\'s freedom?\\\",\\\\n \\\\'context\\\\': \\\"The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\\\"\\\\n}\\\\nres = nlp(QA_input)', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/minilm-uncased-squad2')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'deepset/minilm-uncased-squad2', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/minilm-uncased-squad2')\\\", 'api_arguments': {'model_name': 'deepset/minilm-uncased-squad2'}, 'python_environment_requirements': [['transformers']], 'example_code': {'a': {'code': \\\"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel_name = deepset/minilm-uncased-squad2\\\\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\\\\nQA_input = {\\\\n 'question': 'Why is model conversion important?',\\\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\\\n}\\\\nres = nlp(QA_input)\\\"}, 'b': {'code': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer\\\\nmodel_name = deepset/minilm-uncased-squad2\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)'}}, 'performance': {'dataset': 'squad_v2', 'accuracy': {'exact': 76.13071675229513, 'f1': 79.49786500219953}}, 'description': 'MiniLM-L12-H384-uncased is a language model fine-tuned for extractive question answering on the SQuAD 2.0 dataset. It is based on the microsoft/MiniLM-L12-H384-uncased model and can be used with the Hugging Face Transformers library.' </s>\"\n"}
{"text": "<s>[INST]قم بتنزيل موديل توليد الصور غير المشروط من مجموعة بيانات Computer Vision. بعد ذلك، قم بتشغيل النموذج لإنشاء صور جديدة بدون شروط. يمكنك تخزين الصور الناتجة في مجلد الإخراج. استمتع بإنشاء صور مبتكرة وفريدة بكل سهولة.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the DDPMPipeline class from the diffusers package, which is required for unconditional image generation using the pre-trained model.', 'code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\\nimage = pipeline().images[0]\\\\nimage.save('butterfly_image.png')\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'ocariz/butterfly_200', 'api_call': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\\nimage = pipeline().images[0]\\\\nimage.save('butterfly_image.png')\\\", 'api_arguments': '', 'python_environment_requirements': 'diffusers', 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is a diffusion model for unconditional image generation of cute butterflies trained for 200 epochs.'}} </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى تطوير برنامج في معالجة اللغات الطبيعية لقياس التشابه بين الجمل. يجب أن يكون البرنامج قادراً على تحليل الجمل وتقديمها بشكل منسق لتسهيل فهم النصوص لمستخدمي التطبيق.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries from the transformers package provided by Hugging Face, such as AutoModel and AutoTokenizer.\\\\n2. Using AutoModel.from_pretrained, load the 'princeton-nlp/unsup-simcse-roberta-base' pre-trained model. This model is designed for unsupervised sentence embedding based on the SimCSE method, which helps measure the similarity between sentences.\\\\n3. For each document, tokenize its sentences using the AutoTokenizer, and then use the loaded model to convert them into embeddings.\\\\n4. When comparing two documents, calculate the similarity between their embeddings by a cosine similarity measurement.\\\\n5. The calculated similarity score can be used to determine the similarity between any two documents.\\\", '<<<code>>>': \\\"from transformers import AutoModel, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\\\nmodel = AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\\\n\\\\n# Convert documents to embeddings\\\\ndoc1_embeddings = model(**tokenizer(doc1, return_tensors='pt')).last_hidden_state\\\\ndoc2_embeddings = model(**tokenizer(doc2, return_tensors='pt')).last_hidden_state\\\\n\\\\n# Calculate cosine similarity between embeddings\\\\ncosine_similarity = doc1_embeddings.cpu().detach().numpy() @ doc2_embeddings.cpu().detach().numpy().T\\\\n\\\\n# Analyze the similarity score between documents\\\\nsimilarity_score = cosine_similarity[0][0]\\\", 'api_call': \\\"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \\\"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\\\", 'api_arguments': None, 'python_environment_requirements': [['transformers']], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نموذج جدولي للتنبؤ باستخدام تقنية التعلم الآلي. نحتاج إلى برنامج يمكنه تحويل البيانات الجدولية إلى تنبؤات تلقائية لمساعدة المستخدمين في فهم البيانات واتخاذ القرارات بفعالية.? [/INST]{'domain': 'Tabular Tabular Regression', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_provider': 'Joblib', 'explanation': '', 'code': 'import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\\\'model.joblib\\\\')\\\\nsquare_footage_data = pd.DataFrame([{\\\"sqft\\\": 1500}, {\\\"sqft\\\": 2000}, {\\\"sqft\\\": 2500}], columns=[\\\"sqft\\\"])\\\\npredictions = model.predict(square_footage_data)', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Joblib', 'functionality': 'Single Column Regression', 'api_name': 'jwan2021/autotrain-us-housing-prices-1771761513', 'api_call': 'import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\\\'model.joblib\\\\')\\\\nsquare_footage_data = pd.DataFrame([{\\\"sqft\\\": 1500}, {\\\"sqft\\\": 2000}, {\\\"sqft\\\": 2500}], columns=[\\\"sqft\\\"])\\\\npredictions = model.predict(square_footage_data)', 'api_arguments': ['data'], 'python_environment_requirements': ['joblib', 'pandas'], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'jwan2021/autotrain-data-us-housing-prices', 'accuracy': {'Loss': 100581.032, 'R2': 0.922, 'MSE': 10116543945.03, 'MAE': 81586.656, 'RMSLE': 0.101}}, 'description': 'A single column regression model for predicting US housing prices, trained with AutoTrain and using the Joblib framework.'}} </s>\"\n"}
{"text": "<s>[INST]يتعين علينا الآن إنشاء نماذج جديدة تماما من خلال تدريب نموذج تصنيف النصوص لمعالجة اللغة الطبيعية دون قيود. توجه إلى واجهة البرنامج وقم بتوصيل النموذج بوكيل التعلم الآلي لبدء إنشاء تصنيف النصوص الرائع.? [/INST]{'domain': 'Natural Language Processing Text Classification', 'api_call': \\\"pipeline('text-classification', model='prithivida/parrot_adequacy_model')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. We import the necessary 'pipeline' function from the transformers library provided by Hugging Face.\\\", 'code': 'from transformers import pipeline\\\\nadequacy_classifier = pipeline(\\\\'text-classification\\\\', model=\\\\'prithivida/parrot_adequacy_model\\\\')\\\\nparaphrases = [\\\"paraphrase 1\\\", \\\"paraphrase 2\\\", \\\"paraphrase 3\\\"]\\\\nadequacy_scores = [adequacy_classifier(p)[\\\\'label\\\\'] for p in paraphrases]\\\\nbest_paraphrase = paraphrases[adequacy_scores.index(\\\"LABEL_1\\\")]', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'prithivida/parrot_adequacy_model', 'api_call': 'from transformers import pipeline\\\\nadequacy_classifier = pipeline(\\\\'text-classification\\\\', model=\\\\'prithivida/parrot_adequacy_model\\\\')\\\\nparaphrases = [\\\"paraphrase 1\\\", \\\"paraphrase 2\\\", \\\"paraphrase 3\\\"]\\\\nadequacy_scores = [adequacy_classifier(p)[\\\\'label\\\\'] for p in paraphrases]\\\\nbest_paraphrase = paraphrases[adequacy_scores.index(\\\"LABEL_1\\\")]', 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Parrot is a paraphrase-based utterance augmentation framework purpose-built to accelerate training NLU models. This model is an ancillary model for Parrot paraphraser.'}} </s>\"\n"}
{"text": "<s>[INST]عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى برنامج يقوم بتحويل النص إلى كلام لمستخدمي هذا التطبيق بمجال تحويل النص إلى صوت.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': './run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori', '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. Install the necessary libraries and tools, which include 'espnet2' and 'huggingface_hub'.\\\\n2. Use the provided shell script 'run.sh' to download and load the pretrained text-to-speech model 'mio/tokiwa_midori' from Hugging Face Hub.\\\\n3. Set the configuration options to skip data preparation and training, as you only want to load the pretrained model for inference.\\\\n4. Input the educational text into the Text2Speech module of the loaded model, and it will generate the speech audio file for the narration.\\\\n5. Save the generated audio file for use in the e-learning platform.\\\", '<<<code>>>': \\\"# Install the necessary libraries and tools\\\\n!pip install espnet2 huggingface_hub\\\\n\\\\n# Load the pretrained TTS model\\\\n!./run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori\\\\n\\\\n# Convert educational text to speech\\\\neducational_text = 'Educational text goes here...'\\\\ntts = Text2Speech(config, model_path)\\\\nwav, sr = tts.synthesize(educational_text)\\\\nwavfile.write('output_narration.wav', sr, wav.numpy())\\\", 'api_call': './run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori', 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mio/tokiwa_midori', 'api_call': './run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori', 'api_arguments': ['espnet', 'mio/tokiwa_midori', 'Text2Speech', 'config', 'model_path'], 'python_environment_requirements': [['espnet2', 'huggingface_hub']], 'example_code': './run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori', 'performance': {'dataset': 'amadeus', 'accuracy': 'Not provided'}, 'description': 'This model was trained by mio using amadeus recipe in espnet.' </s>\"\n"}
{"text": "<s>[INST]يوفر لنا عدة مكتبات البيانات مجموعة واسعة من البيانات الجدولية لاستكشاف تطبيقات التصنيف الجدولي. استخدم هذه البيانات لإنشاء نماذج تصنيف قائمة على إدخالات متعددة وقم بتقديمها لنادي طلابي ليُلعِب ضد وكيل التعلم الآلي.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': 'TF_Decision_Trees(input_features, target)', '<<<api_provider>>>': 'Keras', '<<<explanation>>>': \\\"1. Start by gathering the required dataset, which includes employee demographics and their respective salaries. Ensure the data is cleaned and preprocessed before it is used.\\\\n2. We will use TensorFlow's Gradient Boosted Trees model for binary classification of the structured data that comprises employee demographics.\\\\n3. Input features will be considered, such as age, education, work experience, and occupation. Our target is to predict whether an employee makes over 50,000 USD per year.\\\\n4. We will employ a custom binary target encoder as a Keras preprocessing layer to encode the categorical features according to their target value co-occurrences. Then, we will use the encoded features to develop a decision forests model for salary prediction.\\\", '<<<code>>>': 'import tensorflow as tf\\\\nfrom TF_Decision_Trees import TF_Decision_Trees\\\\nimport pandas as pd\\\\n\\\\n# Load and preprocess data\\\\ndata = pd.read_csv(\\\"employee_demographics.csv\\\")\\\\ninput_features = data[[\\\\'age\\\\', \\\\'education\\\\', \\\\'work_experience\\\\', \\\\'occupation\\\\']] \\\\ntarget = (data[\\\\'salary\\\\'] > 50000).astype(int)\\\\n\\\\n# Build and train the Decision Trees model\\\\nmodel = TF_Decision_Trees(input_features, target)\\\\nhistory = model.fit(input_features, target, epochs=100, batch_size=32, validation_split=0.2)', 'api_call': 'TF_Decision_Trees(input_features, target)', 'provider': 'Keras', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'TF_Decision_Trees', 'api_call': 'TF_Decision_Trees(input_features, target)', 'api_arguments': ['input_features', 'target'], 'python_environment_requirements': [['tensorflow >= 7.0']], 'example_code': 'https://github.com/tdubon/TF-GB-Forest/blob/c0cf4c7e3e29d819b996cfe4eecc1f2728115e52/TFDecisionTrees_Final.ipynb', 'performance': {'dataset': 'Census-Income Data Set', 'accuracy': 96.57}, 'description': \\\"Use TensorFlow's Gradient Boosted Trees model in binary classification of structured data. Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is trained on the US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables. The task is to determine whether a person makes over 50k a year.\\\" </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطبيق تقنيات التعلم التعزيزي في مجال إنشاء نماذج تعلم آلي للأتمتة. هل يمكنك مساعدتي في تحقيق ذلك؟? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"load_from_hub(repo_id='sb3/ppo-CartPole-v1',filename='{MODEL FILENAME}.zip',)\\\", '<<<api_provider>>>': 'Stable-Baselines3', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"load_from_hub(repo_id='sb3/ppo-CartPole-v1',filename='{MODEL FILENAME}.zip',)\\\", 'provider': 'Stable-Baselines3', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sb3/ppo-CartPole-v1', 'api_call': \\\"load_from_hub(repo_id='sb3/ppo-CartPole-v1',filename='{MODEL FILENAME}.zip',)\\\", 'api_arguments': ['algo', 'env', 'f'], 'python_environment_requirements': [['rl_zoo3', 'stable-baselines3', 'stable-baselines3-contrib']], 'example_code': 'python -m rl_zoo3.load_from_hub --algo ppo --env CartPole-v1 -orga sb3 -f logs/', 'performance': {'dataset': 'CartPole-v1', 'accuracy': '500.00 +/- 0.00'}, 'description': 'This is a trained model of a PPO agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.' </s>\"\n"}
{"text": "<s>[INST]سيتم تنفيذ نموذج توليد النص إلى نص باستخدام تقنيات معالجة اللغة الطبيعية في مشهد معين. يتطلب ذلك فهم النص المدخل وإنشاء نص خرجي معاد توليده.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5Model.from_pretrained('t5-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, T5Tokenizer and T5Model, from the transformers package.\\\\n2. Load the pre-trained T5 model and tokenizer using the 't5-base' name.\\\\n3. Tokenize the input sentence using the T5 tokenizer, including a prefix such as 'translate <LANGUAGE>:' before the sentence for the desired target language (e.g., 'translate Spanish:').\\\\n4. Use the pre-trained T5 model to generate the translated text based on the tokenized input.\\\\n5. Decode the output tokens from the model to construct the translated sentence in the desired language.\\\", '<<<code>>>': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\n\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'t5-base\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'t5-base\\\\')\\\\ninput_text = \\\"translate Spanish: The advancements in artificial intelligence have revolutionized the way we live.\\\"\\\\ninput_ids = tokenizer(input_text, return_tensors=\\\\'pt\\\\').input_ids\\\\ntranslated_tokens = model.generate(input_ids)\\\\ntranslated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)', 'api_call': \\\"T5Model.from_pretrained('t5-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 't5-base', 'api_call': \\\"T5Model.from_pretrained('t5-base')\\\", 'api_arguments': ['input_ids', 'decoder_input_ids'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import T5Tokenizer, T5Model\\\\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\\\\nmodel = T5Model.from_pretrained('t5-base')\\\\ninput_ids = tokenizer('Studies have been shown that owning a dog is good for you', return_tensors='pt').input_ids\\\\ndecoder_input_ids = tokenizer('Studies show that', return_tensors='pt').input_ids\\\\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\\\\nlast_hidden_states = outputs.last_hidden_state\\\", 'performance': {'dataset': 'c4', 'accuracy': 'See research paper, Table 14'}, 'description': 'T5-Base is a Text-To-Text Transfer Transformer (T5) model with 220 million parameters. It is designed to perform various NLP tasks, including machine translation, document summarization, question answering, and text classification. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be used with the Transformers library.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد ملخص نصي لمحتوى مكتوب باستخدام تقنيات معالجة اللغة الطبيعية للمشهد المحدد أعلاه. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary classes from the transformers library and custom tokenization module. This includes PegasusForConditionalGeneration for the text summarization model and PegasusTokenizer for processing Chinese text.\\\\n2. We then use the from_pretrained method to load the pre-trained model 'IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese'. This model has been fine-tuned on multiple Chinese text summarization datasets and is ideal for generating a summary of a Chinese article.\\\\n3. We load the text in the variable 'text' and tokenize it using the PegasusTokenizer.\\\\n4. The model processes the tokenized text and generates a summary, which can be obtained by decoding the tokens.\\\", '<<<code>>>': \\\"from transformers import PegasusForConditionalGeneration\\\\nfrom tokenizers_pegasus import PegasusTokenizer\\\\nmodel = PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntokenizer = PegasusTokenizer.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntext = '你的中文文章内容'\\\\ninputs = tokenizer(text, max_length=1024, return_tensors='pt')\\\\nsummary_ids = model.generate(inputs['input_ids'])\\\\nsummary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\\\", 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Randeng-Pegasus-238M-Summary-Chinese', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\", 'api_arguments': {'text': 'string', 'max_length': 'integer'}, 'python_environment_requirements': [['transformers', 'tokenizers_pegasus.py', 'data_utils.py']], 'example_code': \\\"from transformers import PegasusForConditionalGeneration\\\\nfrom tokenizers_pegasus import PegasusTokenizer\\\\nmodel = PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntokenizer = PegasusTokenizer.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntext = '1'\\\\ninputs = tokenizer(text, max_length=1024, return_tensors='pt')\\\\nsummary_ids = model.generate(inputs['input_ids'])\\\\ntokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\\\", 'performance': {'dataset': 'LCSTS', 'accuracy': {'rouge-1': 43.46, 'rouge-2': 29.59, 'rouge-L': 39.76}}, 'description': 'Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus. It is fine-tuned on 7 Chinese text summarization datasets including education, new2016zh, nlpcc, shence, sohu, thucnews, and weibo. The model can be used to generate summaries for Chinese text inputs.' </s>\"\n"}
{"text": "<s>[INST]قم بتحليل السؤال الآتي والإجابة عليه باستخدام تقنيات معالجة اللغة الطبيعية في مجال الاستفسار والإجابة: \\\"ما هو تأثير تطبيق تقنيات معالجة اللغة الطبيعية في تحسين فهم النصوص وإجابة الأسئلة ذات الصلة؟\\\"? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'), tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'))\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required libraries: pipeline from transformers, AutoModel, and AutoTokenizer.\\\\n2. Load the model by calling the pipeline function with 'question-answering' as task and providing the pretrained model 'deepset/bert-large-uncased-whole-word-masking-squad2'.\\\\n3. After loading the model and tokenizer, you can use them to answer questions related to any given context. In our case, the chatbot will answer questions about a specific text.\\\\n4. Prepare the input by providing the question and context where the answer will be extracted.\\\\n5. Call the loaded pipeline with the prepared input and it will return the answer to the question based on the context.\\\", '<<<code>>>': \\\"from transformers import AutoModel, AutoTokenizer, pipeline\\\\nqa_pipeline = pipeline(\\\\n    'question-answering',\\\\n    model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'),\\\\n    tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2')\\\\n)\\\\ninput_qa = {\\\\n    'question': 'What is the purpose of the chatbot?',\\\\n    'context': 'The chatbot is designed to answer questions based on a specific text, helping users find information quickly and easily.'\\\\n}\\\\nresponse = qa_pipeline(input_qa)\\\", 'api_call': \\\"pipeline('question-answering', model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'), tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'))\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-large-uncased-whole-word-masking-squad2', 'api_call': \\\"pipeline('question-answering', model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'), tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'))\\\", 'api_arguments': {'model_name': 'deepset/bert-large-uncased-whole-word-masking-squad2', 'tokenizer': 'deepset/bert-large-uncased-whole-word-masking-squad2'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"QA_input = {\\\\n 'question': 'Why is model conversion important?',\\\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\\\n}\\\\nres = nlp(QA_input)\\\", 'performance': {'dataset': 'squad_v2', 'accuracy': {'Exact Match': 80.885, 'F1': 83.876}}, 'description': 'This is a bert-large model, fine-tuned using the SQuAD2.0 dataset for the task of question answering. It is designed for extractive question answering and supports English language.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى تطوير برنامج يحول النص إلى صورة متعددة القنوات لتصنيف الرمز الطبيعي لمستخدمي تطبيقنا الجديد في مجال معالجة اللغة الطبيعية.? [/INST]{'domain': 'Natural Language Processing Token Classification', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': 'from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\\\\n\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'d4data/biomedical-ner-all\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'d4data/biomedical-ner-all\\\\')\\\\nbiomedical_ner = pipeline(\\\\'ner\\\\', model=model, tokenizer=tokenizer)\\\\nanalyzed_report = biomedical_ner(\\\"The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.\\\")', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Transformers', 'functionality': 'Named Entity Recognition', 'api_name': 'd4data/biomedical-ner-all', 'api_call': 'from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\\\\n\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'d4data/biomedical-ner-all\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'d4data/biomedical-ner-all\\\\')\\\\nbiomedical_ner = pipeline(\\\\'ner\\\\', model=model, tokenizer=tokenizer)\\\\nanalyzed_report = biomedical_ner(\\\"The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.\\\")', 'api_arguments': {'model': 'AutoModelForTokenClassification.from_pretrained(d4data/biomedical-ner-all)', 'tokenizer': 'AutoTokenizer.from_pretrained(d4data/biomedical-ner-all)', 'aggregation_strategy': 'simple'}, 'python_environment_requirements': {'transformers': 'from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification'}, 'example_code': 'pipe(The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.)', 'performance': {'dataset': 'Maccrobat', 'accuracy': 'Not provided'}, 'description': 'An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased.'}} </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي لكشف النشاط الصوتي.? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"Pipeline.from_pretrained('pyannote/speaker-diarization@2.1',use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", '<<<api_provider>>>': 'pyannote.audio', '<<<explanation>>>': \\\"1. Import the Pipeline function from the pyannote.audio library.\\\\n2. Use the Pipeline's from_pretrained method to load the pre-trained model 'pyannote/speaker-diarization@2.1'. You will need to provide your access token as an argument.\\\\n3. The loaded speaker diarization model can process an audio file of the court proceeding.\\\\n4. The model outputs the speaker information, and the output can be saved in RTTM format. This will provide timestamps indicating who is speaking during different segments of the audio.\\\", '<<<code>>>': \\\"from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained('pyannote/speaker-diarization@2.1', use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\\ndiarization = pipeline('court_proceeding_audio.wav')\\\\nwith open('court_proceeding_audio.rttm', 'w') as rttm:\\\\n    diarization.write_rttm(rttm)\\\", 'api_call': \\\"Pipeline.from_pretrained('pyannote/speaker-diarization@2.1',use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", 'provider': 'pyannote.audio', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'johnislarry/cloned-pyannote-speaker-diarization-endpoint', 'api_call': \\\"Pipeline.from_pretrained('pyannote/speaker-diarization@2.1',use_auth_token='ACCESS_TOKEN_GOES_HERE')\\\", 'api_arguments': ['num_speakers', 'min_speakers', 'max_speakers', 'segmentation_onset'], 'python_environment_requirements': ['pyannote.audio 2.0'], 'example_code': {'load_pipeline': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)', 'apply_pipeline': 'diarization = pipeline(audio.wav)', 'save_output': 'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)'}, 'performance': {'dataset': [{'name': 'AISHELL-4', 'accuracy': {'DER%': 14.61, 'FA%': 3.31, 'Miss%': 4.35, 'Conf%': 6.95}}, {'name': 'AMI Mix-Headset only_words', 'accuracy': {'DER%': 18.21, 'FA%': 3.28, 'Miss%': 11.07, 'Conf%': 3.87}}, {'name': 'AMI Array1-01 only_words', 'accuracy': {'DER%': 29.0, 'FA%': 2.71, 'Miss%': 21.61, 'Conf%': 4.68}}, {'name': 'CALLHOME Part2', 'accuracy': {'DER%': 30.24, 'FA%': 3.71, 'Miss%': 16.86, 'Conf%': 9.66}}, {'name': 'DIHARD 3 Full', 'accuracy': {'DER%': 20.99, 'FA%': 4.25, 'Miss%': 10.74, 'Conf%': 6.0}}, {'name': 'REPERE Phase 2', 'accuracy': {'DER%': 12.62, 'FA%': 1.55, 'Miss%': 3.3, 'Conf%': 7.76}}, {'name': 'VoxConverse v0.0.2', 'accuracy': {'DER%': 12.76, 'FA%': 3.45, 'Miss%': 3.85, 'Conf%': 5.46}}]}, 'description': 'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نموذج لتصنيف الرموز في معالجة اللغة الطبيعية، ونهدف إلى تحديد وتصنيف أنواع الكلمات والتعابير في النصوص اللغوية.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-job_all-903929564', use_auth_token=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers package. This includes AutoModelForTokenClassification for the entity extraction model and AutoTokenizer for processing text data.\\\\n2. We then use the from_pretrained method of the AutoModelForTokenClassification class to load the pre-trained model 'ismail-lucifer011/autotrain-job_all-903929564'. This model has been trained for entity extraction tasks, which is exactly what we need to identify transaction details from customers' messages.\\\\n3. The loaded model will be used for token classification, specifically for entity extraction in this case. The input text will be tokenized and processed by the model to identify and classify entities within the text.\\\", '<<<code>>>': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(\\\\'ismail-lucifer011/autotrain-job_all-903929564\\\\', use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'ismail-lucifer011/autotrain-job_all-903929564\\\\', use_auth_token=True)\\\\n# replace the msg variable with client\\\\'s message\\\\nmsg = \\\"I transferred $500 to John\\\\'s account. Transaction ID is A123456.\\\"\\\\ninputs = tokenizer(msg, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nentities = process_entities(outputs)', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-job_all-903929564', use_auth_token=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': '903929564', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-job_all-903929564', use_auth_token=True)\\\", 'api_arguments': {'inputs': 'I love AutoTrain'}, 'python_environment_requirements': [{'transformers': 'AutoModelForTokenClassification, AutoTokenizer'}], 'example_code': 'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-job_all-903929564, use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-job_all-903929564, use_auth_token=True)\\\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\\\noutputs = model(**inputs)', 'performance': {'dataset': 'ismail-lucifer011/autotrain-data-job_all', 'accuracy': 0.9989412009896035}, 'description': 'A Token Classification model trained using AutoTrain for Entity Extraction. The model is based on distilbert and achieves high accuracy, precision, recall, and F1 score.' </s>\"\n"}
{"text": "<s>[INST]يجب تسجيل قصة قصيرة بالصوت تبدأ بالجملة \\\"كان ياما كان في أرض بعيدة\\\" لتحسين نظام التعرف التلقائي على الكلام في مجال تصنيف الفيديو بتقنية الرؤية الحاسوبية.? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries and classes from the transformers package, numpy, and torch. This includes VideoMAEImageProcessor and VideoMAEForPreTraining.', 'code': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\npixel_values = processor(video, return_tensors='pt').pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss\\\", 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'videomae-base-short-ssv2', 'api_call': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\npixel_values = processor(video, return_tensors='pt').pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss\\\", 'api_arguments': ['video', 'return_tensors'], 'python_environment_requirements': ['transformers', 'numpy', 'torch'], 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\\\npixel_values = processor(video, return_tensors=pt).pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss', 'performance': {'dataset': 'Something-Something-v2', 'accuracy': 'N/A'}, 'description': 'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.'}} </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى برنامج في مجال معالجة اللغة الطبيعية لتطوير تطبيق يقوم بتحويل النص إلى كلام لمستخدمي ذوي الإعاقة البصرية.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Start by installing the necessary library sentence-transformers.\\\\n2. Import the SentenceTransformer package from sentence_transformers.\\\\n3. Initialize the SentenceTransformer model using 'sentence-transformers/paraphrase-distilroberta-base-v2' which is trained to compute semantic similarity between sentences.\\\\n4. Encode each forum title or description using the model to get their embedding vectors.\\\\n5. Perform clustering on the embeddings to group related forums together (e.g., using K-means, DBSCAN or other clustering algorithms).\\\\n6. Assign the forums to their respective clusters based on similarity.\\\", '<<<code>>>': \\\"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\\nforum_sentences = [...] # Forum titles or descriptions\\\\nembeddings = model.encode(forum_sentences)\\\\n\\\\n# Perform clustering on embeddings and assign forums to clusters (e.g., using K-means, DBSCAN, or other clustering algorithms)\\\", 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/paraphrase-distilroberta-base-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتدريب نموذج تحسين للجداول بشكل تلقائي من بيانات تدريبية زجاجية. هل يمكنك مساعدتي في تحقيق هذا؟? [/INST]{'domain': 'Tabular Tabular Regression', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\ndata = pd.read_csv('data.csv')\\\\n# replace 'data.csv' with path to your input data\\\\npredictions = model.predict(data)\\\", 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face', 'functionality': 'Carbon Emissions', 'api_name': '45473113800', 'api_call': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\ndata = pd.read_csv('data.csv')\\\\n# replace 'data.csv' with path to your input data\\\\npredictions = model.predict(data)\\\", 'api_arguments': ['data'], 'python_environment_requirements': ['joblib', 'pandas'], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'samvelkoch/autotrain-data-prknsn-2', 'accuracy': {'Loss': 5.079, 'R2': 0.109, 'MSE': 25.795, 'MAE': 3.78, 'RMSLE': 0.849}}, 'description': 'A tabular regression model trained with AutoTrain for predicting carbon emissions.'}} </s>\"\n"}
{"text": "<s>[INST]استخدم الواجهة البرمجية للتعرف التلقائي على الكلام لتحويل النص التالي إلى كلام: \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\", '<<<api_provider>>>': 'PyTorch Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes and functions from the transformers and datasets libraries.\\\\n2. Load the Data2VecForCTC model pretrained on the 'facebook/data2vec-audio-base-960h' dataset, which supports the transcription of audio files and delivers competitive performance on major speech recognition benchmarks.\\\\n3. Load the Wav2Vec2Processor to convert the raw audio input into the form required by the model.\\\\n4. Transcribe the audio from the presentation by first processing it using the processor and then running it through the model to retrieve logits. Decode the logits to obtain the corresponding transcriptions.\\\", '<<<code>>>': \\\"from transformers import Data2VecForCTC, Wav2Vec2Processor\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/data2vec-audio-base-960h')\\\\nmodel = Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\\n\\\\ninput_values = processor(audio_data, return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_call': \\\"Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\", 'provider': 'PyTorch Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'data2vec-audio-base-960h', 'api_call': \\\"Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers', 'datasets', 'torch', 'jiwer']], 'example_code': 'from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\n# load model and processor\\\\nprocessor = Wav2Vec2Processor.from_pretrained(facebook/data2vec-audio-base-960h)\\\\nmodel = Data2VecForCTC.from_pretrained(facebook/data2vec-audio-base-960h)\\\\n\\\\n# load dummy dataset and read soundfiles\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\n\\\\n# tokenize\\\\ninput_values = processor(ds[0][audio][array],, return_tensors=pt, padding=longest).input_values # Batch size 1\\\\n\\\\n# retrieve logits\\\\nlogits = model(input_values).logits\\\\n\\\\n# take argmax and decode\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'performance': {'dataset': 'librispeech_asr', 'accuracy': {'clean': 2.77, 'other': 7.08}}, 'description': \\\"Facebook's Data2Vec-Audio-Base-960h model is an Automatic Speech Recognition model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It can be used for transcribing audio files and achieves competitive performance on major benchmarks of speech recognition. The model is based on the Data2Vec framework which uses the same learning method for either speech, NLP, or computer vision.\\\" </s>\"\n"}
{"text": "<s>[INST]تحتاج شركتنا إلى تطوير نموذج لتصنيف الصوتيات بدقة، ونحتاج إلى تقدير مضمون الصوت والتعرف على الأصوات بدقة عالية.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the encoder_classifier and torchaudio libraries.\\\\n2. Load the pretrained model 'TalTechNLP/voxlingua107-epaca-tdnn' using the from_hparams() method from the EncoderClassifier class.\\\\n3. The loaded model can then be used to load audio files and process them accordingly.\\\\n4. The model can classify a speech utterance according to the language spoken and covers 107 different languages.\\\", '<<<code>>>': \\\"from speechbrain.pretrained import EncoderClassifier\\\\nimport torchaudio\\\\n\\\\nlanguage_id = EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\\nsignal, _ = torchaudio.load(audio_file_path)\\\\nprediction = language_id.classify_batch(signal)\\\\n\\\\nprint(prediction)\\\", 'api_call': \\\"EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'TalTechNLP/voxlingua107-epaca-tdnn', 'api_call': \\\"EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\", 'api_arguments': ['signal'], 'python_environment_requirements': [['speechbrain', 'torchaudio']], 'example_code': 'import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\nlanguage_id = EncoderClassifier.from_hparams(source=TalTechNLP/voxlingua107-epaca-tdnn, savedir=tmp)\\\\nsignal = language_id.load_audio(https://omniglot.com/soundfiles/udhr/udhr_th.mp3)\\\\nprediction = language_id.classify_batch(signal)\\\\nprint(prediction)', 'performance': {'dataset': 'VoxLingua107', 'accuracy': '93%'}, 'description': 'This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain. The model uses the ECAPA-TDNN architecture that has previously been used for speaker recognition. The model can classify a speech utterance according to the language spoken. It covers 107 different languages.' </s>\"\n"}
{"text": "<s>[INST]يرجى إنشاء جملة قصيرة في مجال معالجة اللغة الطبيعية تحتوي على فجوة يجب ملئها بكلمة مناسبة تبدأ بـ \\\"كان ياما كان\\\" وذلك لتحسين نماذج ملئ الفجوات.? [/INST]{'domain': 'Natural Language Processing Fill-Mask', 'api_call': \\\"AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary libraries and classes. For this task, we need AutoModelForMaskedLM and AutoTokenizer from the transformers library.', 'code': 'from transformers import AutoModelForMaskedLM, AutoTokenizer\\\\nmodel = AutoModelForMaskedLM.from_pretrained(\\\\'cl-tohoku/bert-base-japanese-whole-word-masking\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'cl-tohoku/bert-base-japanese-whole-word-masking\\\\')\\\\nsentence_with_blank = \\\"この空間には_空白_の部屋があります.\\\" # Fill the blank with the appropriate word\\\\nsentence_with_mask = sentence_with_blank.replace(\\\"_空白_\\\", \\\"[MASK]\\\")\\\\ninputs = tokenizer(sentence_with_mask, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\npredictions = outputs.logits.argmax(dim=-1)\\\\npredicted_word = tokenizer.decode(predictions[0]).replace(\\\" \\\",\\\"\\\")\\\\nfilled_sentence = sentence_with_blank.replace(\\\"_空白_\\\", predicted_word)', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Fill-Mask', 'api_name': 'cl-tohoku/bert-base-japanese-whole-word-masking', 'api_call': 'from transformers import AutoModelForMaskedLM, AutoTokenizer\\\\nmodel = AutoModelForMaskedLM.from_pretrained(\\\\'cl-tohoku/bert-base-japanese-whole-word-masking\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'cl-tohoku/bert-base-japanese-whole-word-masking\\\\')\\\\nsentence_with_blank = \\\"この空間には_空白_の部屋があります.\\\" # Fill the blank with the appropriate word\\\\nsentence_with_mask = sentence_with_blank.replace(\\\"_空白_\\\", \\\"[MASK]\\\")\\\\ninputs = tokenizer(sentence_with_mask, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\npredictions = outputs.logits.argmax(dim=-1)\\\\npredicted_word = tokenizer.decode(predictions[0]).replace(\\\" \\\",\\\"\\\")\\\\nfilled_sentence = sentence_with_blank.replace(\\\"_空白_\\\", predicted_word)', 'api_arguments': {'model': 'cl-tohoku/bert-base-japanese-whole-word-masking'}, 'python_environment_requirements': {'transformers': 'latest'}, 'example_code': \\\"fill_mask('[MASK]')\\\", 'performance': {'dataset': 'Japanese Wikipedia', 'accuracy': 'Not provided'}, 'description': 'This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization. Additionally, the model is trained with the whole word masking enabled for the masked language modeling (MLM) objective.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام لتصنيف النصوص بمجال معالجة اللغة الطبيعية، ونرغب في تنبؤ تصنيف الوثائق النصية بناءً على بيانات التدريب المتاحة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. To achieve this, we import the 'pipeline' function from the transformers library provided by Hugging Face.\\\\n2. We use the 'pipeline' function to create a text-classification model that can predict sentiments and emotions from the text.\\\\n3. We specify the model 'joeddav/distilbert-base-uncased-go-emotions-student' for loading. This model is distilled from the zero-shot classification pipeline on the GoEmotions dataset, which consists of text data annotated with various emotions.\\\\n4. The created model can be used to classify the text into various emotions like happiness, sadness, anger, etc.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nnlp = pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')\\\\nresult = nlp('I am so happy today!')\\\", 'api_call': \\\"pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'joeddav/distilbert-base-uncased-go-emotions-student', 'api_call': \\\"pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')\\\", 'api_arguments': 'text', 'python_environment_requirements': [['transformers', 'torch', 'tensorflow']], 'example_code': \\\"from transformers import pipeline\\\\nnlp = pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')\\\\nresult = nlp('I am so happy today!')\\\", 'performance': {'dataset': 'go_emotions'}, 'description': 'This model is distilled from the zero-shot classification pipeline on the unlabeled GoEmotions dataset. It is primarily intended as a demo of how an expensive NLI-based zero-shot model can be distilled to a more efficient student, allowing a classifier to be trained with only unlabeled data.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا في مجال تصنيف البيانات الجدولية بتطوير نماذج لتصنيف البيانات بمساعدة الذكاء الاصطناعي، ونحن بحاجة إلى تقدير الجودة والدقة للتصنيف الناتج.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary libraries, including joblib, pandas and open the config.json file to get the features that the trained model is expecting.\\\\n2. Load the 'model.joblib' binary classification model. This model has been trained on the IMDb sentiment analysis dataset, so it's capable of predicting sentiment (positive or negative) for movie reviews.\\\\n3. Read movie reviews using pandas, from a data.csv file, and preprocess data according to the config.json file.\\\\n4. Transform column names to match expected format in the model and predict sentiment scores using the loaded model.\\\\n5. The predictions will return a binary sentiment result (positive or negative) for each movie review in the dataset.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nimport json\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\n\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\n\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'desertdev/autotrain-imdb-sentiment-analysis-44994113085', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'desertdev/autotrain-data-imdb-sentiment-analysis', 'accuracy': 0.565}, 'description': 'A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء ملف صوتي قصير يحتوي على تصنيف صوتي لعملية تصنيف الرموز في مجال معالجة اللغة الطبيعية.? [/INST]{'domain': 'Natural Language Processing Token Classification', 'api_call': \\\"AutoModel.from_pretrained('ckiplab/bert-base-chinese-ws')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': 'from transformers import BertTokenizerFast, AutoModel\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\\\'bert-base-chinese\\\\')\\\\nmodel = AutoModel.from_pretrained(\\\\'ckiplab/bert-base-chinese-ws\\\\')\\\\nchinese_text = \\\"这是一篇中文文章。\\\"\\\\ntokenized_input = tokenizer(chinese_text, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**tokenized_input)', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Transformers', 'functionality': 'Token Classification', 'api_name': 'ckiplab/bert-base-chinese-ws', 'api_call': 'from transformers import BertTokenizerFast, AutoModel\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\\\'bert-base-chinese\\\\')\\\\nmodel = AutoModel.from_pretrained(\\\\'ckiplab/bert-base-chinese-ws\\\\')\\\\nchinese_text = \\\"这是一篇中文文章。\\\"\\\\ntokenized_input = tokenizer(chinese_text, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**tokenized_input)', 'api_arguments': {'pretrained_model': 'ckiplab/bert-base-chinese-ws'}, 'python_environment_requirements': {'transformers': 'BertTokenizerFast, AutoModel'}, 'example_code': \\\"from transformers import (\\\\n BertTokenizerFast,\\\\n AutoModel,\\\\n)\\\\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\\\\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-ws')\\\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).'}} </s>\"\n"}
{"text": "<s>[INST]توجيه الصوت إلى نص: قم بتحويل النص إلى كلام باستخدام تقنية تحويل النص إلى كلام. تأكد من توفير إعدادات ضبط جودة الصوت واللكنة بشكل صحيح.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-es-css10')\\\", '<<<api_provider>>>': 'Fairseq', '<<<explanation>>>': \\\"1. Import the necessary modules from fairseq and IPython.display.\\\\n2. Use the load_model_ensemble_and_task_from_hf_hub function to load the 'facebook/tts_transformer-es-css10' model. This is a Spanish male voice text-to-speech model trained on the CSS10 dataset.\\\\n3. Set up the TTSHubInterface with the model configuration and task data configuration.\\\\n4. Create a task-specific generator using the task object and model configuration.\\\\n5. In order to generate speech from text, simply pass the text to the TTSHubInterface methods and receive a waveform and sampling rate as the output.\\\\n6. Use IPython.display to play the generated audio.\\\", '<<<code>>>': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\'facebook/tts_transformer-es-css10\\\\', arg_overrides={\\\\'vocoder\\\\': \\\\'hifigan\\\\', \\\\'fp16\\\\': False})\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\ntext = \\\"Hola, esta es una prueba.\\\"\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-es-css10')\\\", 'provider': 'Fairseq', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/tts_transformer-es-css10', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-es-css10')\\\", 'api_arguments': {'arg_overrides': {'vocoder': 'hifigan', 'fp16': False}}, 'python_environment_requirements': [['fairseq', 'IPython']], 'example_code': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n facebook/tts_transformer-es-css10,\\\\n arg_overrides={vocoder: hifigan, fp16: False}\\\\n)\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\ntext = Hola, esta es una prueba.\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)', 'performance': {'dataset': 'CSS10', 'accuracy': None}, 'description': 'Transformer text-to-speech model from fairseq S^2. Spanish single-speaker male voice trained on CSS10.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد ترجمة نصية لمشهد يصور الحيوانات في البرية في مجال معالجة اللغة الطبيعية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. We import the necessary classes from the transformers package. This includes AutoModelForSeq2SeqLM for the translation model and AutoTokenizer for tokenizing inputs.\\\\n2. We first load the Helsinki-NLP/opus-mt-en-de model by using the from_pretrained method of the AutoModelForSeq2SeqLM class. This model has been trained on a large corpus of text and can translate English text to German.\\\\n3. We create an instance of the AutoTokenizer, specifying the same Hugging Face model.\\\\n4. We use the tokenizer to convert the input English text into a format suitable for the model, and then use the model to generate the translated German text.\\\\n5. Finally, we tokenize and reconstruct the translated German text to make it human-readable.', '<<<code>>>': 'from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-de\\\\')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-de\\\\')\\\\ninput_text = \\\"response in English\\\"\\\\ninput_tokenized = tokenizer.prepare_seq2seq_batch(src_texts=[input_text], return_tensors=\\\\'pt\\\\')\\\\noutput_tokenized = model.generate(**input_tokenized)\\\\ntranslated_text = tokenizer.batch_decode(output_tokenized, skip_special_tokens=True)[0]', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-en-de', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'Helsinki-NLP/opus-mt-en-de'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'performance': {'dataset': 'newstest2018-ende.en.de', 'accuracy': {'BLEU': 45.2, 'chr-F': 0.69}}, 'description': 'The Helsinki-NLP/opus-mt-en-de model is a translation model developed by the Language Technology Research Group at the University of Helsinki. It translates English text to German using the Hugging Face Transformers library. The model is trained on the OPUS dataset and has a BLEU score of 45.2 on the newstest2018-ende.en.de dataset.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتحديد نشاط الصوت في التسجيلات الصوتية. هل يمكنك مساعدتي في توليد تعليمات لتنفيذ هذه العملية باستخدام تقنيات معالجة الإشارات الصوتية؟? [/INST]{'domain': 'Audio Voice Activity Detection', 'api_call': \\\"pipeline('voice-activity-detection', model='Eklavya/ZFF_VAD')\\\", 'api_provider': 'Hugging Face', 'explanation': \\\"1. First, you will need to import the 'pipeline' function from the transformers package provided by Hugging Face.\\\", 'code': \\\"from transformers import pipeline\\\\nvad = pipeline('voice-activity-detection', model='Eklavya/ZFF_VAD')\\\\nspeech_segments = vad(audio_file_path)\\\", 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face', 'functionality': 'Voice Activity Detection', 'api_name': 'Eklavya/ZFF_VAD', 'api_call': \\\"from transformers import pipeline\\\\nvad = pipeline('voice-activity-detection', model='Eklavya/ZFF_VAD')\\\\nspeech_segments = vad(audio_file_path)\\\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers, torch', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A Voice Activity Detection model by Eklavya, using the Hugging Face framework.'}} </s>\"\n"}
{"text": "<s>[INST]نحن بحاجة إلى برنامج لتحويل النص إلى كلام لمستخدمي تطبيق التطوير الخاص بعميلنا الذي يعمل في مجال معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_en_to_es', model='Helsinki-NLP/opus-mt-en-es')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library.\\\\n2. Use the pipeline function to create a translation model taking 'translation_en_to_es' as the task and the model 'Helsinki-NLP/opus-mt-en-es' for the translation.\\\\n3. The created translation model can be used to translate your website's text from English to Spanish using the translation function of the pipeline.\\\\n4. The translated text can then be incorporated into your website for a bilingual experience.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_en_to_es\\\\', model=\\\\'Helsinki-NLP/opus-mt-en-es\\\\')\\\\nwebsite_text = \\\"Welcome to our travel company! We offer amazing packages and experiences.\\\"\\\\ntranslated_text = translator(website_text)', 'api_call': \\\"pipeline('translation_en_to_es', model='Helsinki-NLP/opus-mt-en-es')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-en-es', 'api_call': \\\"pipeline('translation_en_to_es', model='Helsinki-NLP/opus-mt-en-es')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_es', model='Helsinki-NLP/opus-mt-en-es')\\\\ntranslated_text = translation('Hello, how are you?')\\\", 'performance': {'dataset': 'Tatoeba-test.eng.spa', 'accuracy': 54.9}, 'description': 'This model is a translation model from English to Spanish using the Hugging Face Transformers library. It is based on the Marian framework and trained on the OPUS dataset. The model achieves a BLEU score of 54.9 on the Tatoeba test set.' </s>\"\n"}
{"text": "<s>[INST]أكتب جملة تستخدم توليد النصوص في معالجة اللغة الطبيعية لتوليد عبارة توجيهية باللغة العربية.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='gpt2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class and function from the transformers package. This includes the pipeline function for creating a text-generation model.\\\\n2. We then create a text-generation model using the pipeline function and specifying the model as 'gpt2'.\\\\n3. We can now use the generated model to create ten sentences about climate change and its impact on the environment.\\\", '<<<code>>>': 'from transformers import pipeline, set_seed\\\\n\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'gpt2\\\\')\\\\nset_seed(42)\\\\ngenerated_text = generator(\\\\n    \\\"Climate change affects the environment because\\\",\\\\n    max_length=50,\\\\n    num_return_sequences=10,\\\\n)\\\\n\\\\n# Extract sentences from the generated text\\\\nsentences = [result[\\\\'generated_text\\\\'] for result in generated_text]\\\\nfor i, sentence in enumerate(sentences, start=1):\\\\n    print(f\\\"{i}. {sentence}\\\")', 'api_call': \\\"pipeline('text-generation', model='gpt2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'gpt2', 'api_call': \\\"pipeline('text-generation', model='gpt2')\\\", 'api_arguments': ['text', 'max_length', 'num_return_sequences'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline, set_seed\\\\ngenerator = pipeline('text-generation', model='gpt2')\\\\nset_seed(42)\\\\ngenerator(Hello, I'm a language model,, max_length=30, num_return_sequences=5)\\\", 'performance': {'dataset': {'LAMBADA': {'accuracy': '35.13'}, 'CBT-CN': {'accuracy': '45.99'}, 'CBT-NE': {'accuracy': '87.65'}, 'WikiText2': {'accuracy': '83.4'}, 'PTB': {'accuracy': '29.41'}, 'enwiki8': {'accuracy': '65.85'}, 'text8': {'accuracy': '1.16'}, 'WikiText103': {'accuracy': '1.17'}, '1BW': {'accuracy': '37.50'}}}, 'description': 'GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.' </s>\"\n"}
{"text": "<s>[INST]يجب عليك أولاً فهم علم الرؤية الحاسوبية وكيفية استخدامه في توليد صور واقعية بدون شروط للحيوانات في البرية. بعد ذلك، قم بتطوير نموذج Image-to-Image من أجل تحقيق هذا الهدف. يمكنك تطبيق تقنيات معالجة اللغة الطبيعية والتصنيف الصفري لتحسين جودة ودقة الصور المولدة.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"!pip install diffusers\\\\nfrom diffusers import DDPMPipeline\\\\nddpm = DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\\ngenerated_bedroom_image = ddpm().images[0]\\\\ngenerated_bedroom_image.save('virtual_bedroom_image.png')\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-bedroom-256', 'api_call': \\\"!pip install diffusers\\\\nfrom diffusers import DDPMPipeline\\\\nddpm = DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\\\\ngenerated_bedroom_image = ddpm().images[0]\\\\ngenerated_bedroom_image.save('virtual_bedroom_image.png')\\\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج عصري لمعالجة اللغة الطبيعية في مجال النسخ إلى النص، ونرغب في توليد تعليمات مناسبة بناءً على النصوص الداخلة في تصنيف الصفر للمعالجة اللغوية الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"CrossEncoder('cross-encoder/nli-deberta-v3-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the CrossEncoder class from the sentence_transformers package.\\\\n2. Create a model instance using: model = CrossEncoder('cross-encoder/nli-deberta-v3-base'), loading the mentioned model, which is trained on natural language inference tasks using the SNLI and MultiNLI datasets.\\\\n3. To infer the relationship between two sentences, provide the pair of sentences as input to the predict() method of the model in the form of a tuple.\\\\n4. The model will return an array of scores corresponding to three labels: contradiction, entailment, and neutral. The highest score indicates the relationship between the two sentences.\\\", '<<<code>>>': 'from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder(\\\\'cross-encoder/nli-deberta-v3-base\\\\')\\\\nsentence_pair = (\\\"A man is eating pizza\\\", \\\"A man eats something\\\")\\\\nscores = model.predict([sentence_pair])', 'api_call': \\\"CrossEncoder('cross-encoder/nli-deberta-v3-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cross-encoder/nli-deberta-v3-base', 'api_call': \\\"CrossEncoder('cross-encoder/nli-deberta-v3-base')\\\", 'api_arguments': ['sentence_pairs'], 'python_environment_requirements': [['sentence_transformers', 'transformers']], 'example_code': \\\"from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-base')\\\\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])\\\", 'performance': {'dataset': {'SNLI-test': '92.38', 'MNLI mismatched set': '90.04'}}, 'description': 'This model is based on microsoft/deberta-v3-base and was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نماذج لتصنيف النصوص باستخدام تقنيات معالجة اللغة الطبيعية، ونحتاج إلى تحديد مدى تشابه الجمل بدقة عالية.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, install the sentence-transformers library using 'pip install -U sentence-transformers'.\\\\n2. Import the SentenceTransformer class from the sentence_transformers library.\\\\n3. Load the pre-trained model 'sentence-transformers/all-MiniLM-L12-v2' using the SentenceTransformer class.\\\\n4. Extract lines of dialogue from movie characters and store them as a list of sentences.\\\\n5. Encode the sentences using the model's 'encode' method, which will generate embeddings for each sentence.\\\\n6. Compare the embeddings using similarity measures like cosine similarity to analyze the relationship between characters based on their lines.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\n\\\\n# Example character lines\\\\ncharacter1_lines = [\\\"We have to do something!\\\", \\\"I think we should split up.\\\"]\\\\ncharacter2_lines = [\\\"We need a plan.\\\", \\\"It might be safer if we split up.\\\"]\\\\n\\\\n# Load the pre-trained model\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-MiniLM-L12-v2\\\\')\\\\n\\\\n# Encode the sentences\\\\ncharacter1_embeddings = model.encode(character1_lines)\\\\ncharacter2_embeddings = model.encode(character2_lines)\\\\n\\\\n# Calculate the similarity between character lines\\\\nsimilarity_matrix = cosine_similarity(character1_embeddings, character2_embeddings)\\\\nprint(similarity_matrix)', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/all-MiniLM-L12-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': '1,170,060,424 training pairs', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بالتطوير في مجال تقنية الرؤية الحاسوبية، حيث نهدف لتقدير العمق في الصور والفيديوهات، ونريد بناء نموذج يستطيع تقدير الأبعاد الثلاثية بناءً على المدخلات المرئية المختلفة التي نملكها.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required AutoModel class from the transformers package provided by Hugging Face.\\\\n2. Use the from_pretrained method of the AutoModel class to load the pre-trained model 'sayakpaul/glpn-nyu-finetuned-diode-221116-110652'. This model is specifically fine-tuned for depth estimation tasks, making it ideal for estimating the depth of objects in an image of a room.\\\\n3. Pass an input image into the model for inference. The model will return the depth estimation map corresponding to the input image. This map can be useful in understanding the depth and distance between objects in the room.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nimport torch\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\\\\ninput_image = your_preprocessed_image_tensor  # Preprocess the input image to match the model's input format\\\\nimage_depth_map = model(torch.unsqueeze(input_image, 0)).squeeze()  # Add a batch dimension to the image tensor and remove it from the output\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221116-110652', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4018, 'Mae': 0.3272, 'Rmse': 0.4546, 'Abs Rel': 0.3934, 'Log Mae': 0.138, 'Log Rmse': 0.1907, 'Delta1': 0.4598, 'Delta2': 0.7659, 'Delta3': 0.9082}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>\"\n"}
{"text": "<s>[INST]لتنفيذ كشف نشاط الصوت في ملفات الصوت، يمكنك استخدام تقنيات معالجة الإشارات الصوتية. ابدأ بتحليل الصوت باستخدام تقنيات معرفة وتقييم مدى تواجد النشاط الصوتي. يمكنك استخدام خوارزميات التعلم الآلي لتحسين دقة كشف النشاط وتحديد فترات الصمت بدقة أعلى. كما يمكنك تطبيق تقنيات تصنيف النمط الصوتي لزيادة كفاءة كشف النشاط بشكل أفضل.? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the Model class from the pyannote.audio library.\\\\n2. Use the Model.from_pretrained() method to load the pre-trained model 'popcornell/pyannote-segmentation-chime6-mixer6'. This model is fine-tuned for voice activity detection and can distinguish between noise and human voice in audio recordings.\\\\n3. The loaded model can then be used to analyze a meeting recording file to detect segments with human voice and separate them from the noise.\\\\n4. Using this information, it's possible to create an application that filters out noise and only focuses on the valuable content of the human voice.\\\", '<<<code>>>': \\\"from pyannote.audio import Model\\\\nvoice_detection_model = Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\\nsegmentation_result = voice_detection_model.apply_to(audio_file_path)\\\", 'api_call': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'popcornell/pyannote-segmentation-chime6-mixer6', 'api_call': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['pyannote.audio 2.1.1'], 'example_code': 'from pyannote.audio import Model\\\\nmodel = Model.from_pretrained(popcornell/pyannote-segmentation-chime6-mixer6)', 'performance': {'dataset': 'ami', 'accuracy': 'N/A'}, 'description': 'Pyannote Segmentation model fine-tuned on data from CHiME-7 DASR Challenge. Used to perform diarization in the CHiME-7 DASR diarization baseline.' </s>\"\n"}
{"text": "<s>[INST]أنشئ صورة متعددة الوسائط تصور قصة قصيرة تبدأ بجملة \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required 'pipeline' function from the transformers library.\\\\n2. Initialize the model and tokenizer using the 'pipeline' function, specifying the model and tokenizer with the identifier 'seungwon12/layoutlmv2-base-uncased_finetuned_docvqa' and the task type as 'question-answering'.\\\\n3. Once initialized, you can provide the context (a document in text form) and the question to the model, and it will return the answer to the question.\\\\n4. Make sure that the document text and questions are appropriately formatted and in plain text to achieve better results.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\'question-answering\\\\', model=\\\\'seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\\\\', tokenizer=\\\\'seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\\\\')\\\\ndocument = \\\"\\\"\\\"Put your document text here.\\\"\\\"\\\"\\\\nquestion = \\\"Your question here\\\"\\\\nanswer = qa_pipeline({\\\\'context\\\\': document, \\\\'question\\\\': question})\\\\nprint(answer)', 'api_call': \\\"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \\\"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'A document question answering model finetuned on the DocVQA dataset using LayoutLMv2-base-uncased.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير خوارزميات متطورة في تجهيز اللغة الطبيعية. نريد معرفة كيفية استخدام نماذج الحشو في تحسين جودة المعالجة اللغوية.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Initialize the pipeline with 'fill-mask', 'camembert-base' model, and 'camembert-base' tokenizer. These specifications are designed to work with French text and the model focuses on filling in the missing words in the given sentences.\\\\n3. Pass the sentence containing the masked word '<mask>' as input to the model, and it will return the completed sentence with the most suitable word in place of the mask.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ncamembert_fill_mask = pipeline(\\\\'fill-mask\\\\', model=\\\\'camembert-base\\\\', tokenizer=\\\\'camembert-base\\\\')\\\\nresults = camembert_fill_mask(\\\"Le camembert est <mask> :)\\\")', 'api_call': \\\"pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'camembert-base', 'api_call': \\\"pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')\\\", 'api_arguments': ['model', 'tokenizer'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline; camembert_fill_mask = pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base'); results = camembert_fill_mask('Le camembert est <mask> :)')\\\", 'performance': {'dataset': 'oscar', 'accuracy': 'N/A'}, 'description': 'CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. It is available on Hugging Face in 6 different versions with varying number of parameters, amount of pretraining data, and pretraining data source domains. It can be used for Fill-Mask tasks.' </s>\"\n"}
{"text": "<s>[INST]لتحسين مهارات البرمجة في مجال معالجة اللغة الطبيعية لتصنيف النصوص، أقترح تكوين نادٍ طلابي يجتمع بانتظام لمناقشة التحديات وحلها. يمكنك توفير أفضل الأدوات والموارد للطلاب ليقوموا ببناء نماذج لغوية وتطبيقها على وكلاء تعلم آلي.? [/INST]{'domain': 'Natural Language Processing Text Classification', 'api_call': \\\"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We import the pipeline function from the transformers library by Hugging Face.', 'code': '', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'sentiment_analysis_generic_dataset', 'api_call': '', 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': \\\"sentiment_analysis('I love this product!')\\\", 'performance': {'dataset': 'generic_dataset', 'accuracy': 'Not specified'}, 'description': 'This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text classification.'}} </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي لتطوير وتحسين تصنيف الفيديو الخاص بتقنية الرؤية الحاسوبية.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoClassificationPipeline(model='hf-tiny-model-private/tiny-random-VideoMAEForVideoClassification')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary components from the 'transformers' library, which includes 'VideoClassificationPipeline'.\\\\n2. Instantiate a video classification pipeline using the 'hf-tiny-model-private/tiny-random-VideoMAEForVideoClassification' model. This is a small model designed for video classification tasks.\\\\n3. Use the created pipeline to classify short meeting videos into three categories: 'productive', 'unproductive', or 'neutral'.\\\", '<<<code>>>': \\\"from transformers import VideoClassificationPipeline\\\\nvideo_classifier = VideoClassificationPipeline(model='hf-tiny-model-private/tiny-random-VideoMAEForVideoClassification')\\\\nclassification_result = video_classifier(video_input, candidate_labels=['productive', 'unproductive', 'neutral'])\\\", 'api_call': \\\"VideoClassificationPipeline(model='hf-tiny-model-private/tiny-random-VideoMAEForVideoClassification')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tiny-random-VideoMAEForVideoClassification', 'api_call': \\\"VideoClassificationPipeline(model='hf-tiny-model-private/tiny-random-VideoMAEForVideoClassification')\\\", 'api_arguments': 'model', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random VideoMAE model for video classification.' </s>\"\n"}
{"text": "<s>[INST]يمكنك استخدام واجهة برمجة التطبيقات (API) في مجال معالجة اللغة الطبيعية (Natural Language Processing) لتحليل النصوص الطبيعية وملئ الفجوات (Fill-Mask). قم بتوفير النص الذي يحتوي على الفجوات التي ترغب في ملئها، وسيقوم النظام بتحليلها وإعادة صياغتها بملئ الفجوات.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is `pipeline` from transformers.\\\\n2. Use the `pipeline` function from transformers to load the model, which in this case is 'bert-base-cased'.\\\\n3. The loaded model will be used for completing the masked sentence, filling in the word at the position of the mask token `[MASK]`.\\\\n4. Provide the sentence with the `[MASK]` token as input to the model, and the model will predict and return the most likely word to fill in the mask.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'bert-base-cased\\\\')\\\\ninput_text = \\\"I have always wanted to [MASK] a marathon.\\\"\\\\nresult = unmasker(input_text)', 'api_call': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-base-cased', 'api_call': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", 'api_arguments': ['model'], 'python_environment_requirements': [['transformers']], 'example_code': ['from transformers import pipeline', \\\"unmasker = pipeline('fill-mask', model='bert-base-cased')\\\", \\\"unmasker(Hello I'm a [MASK] model.)\\\"], 'performance': {'dataset': 'GLUE', 'accuracy': 79.6}, 'description': \\\"BERT base model (cased) is a pre-trained transformer model on English language using a masked language modeling (MLM) objective. It was introduced in a paper and first released in a repository. This model is case-sensitive, which means it can differentiate between 'english' and 'English'. The model can be used for masked language modeling or next sentence prediction, but it's mainly intended to be fine-tuned on a downstream task.\\\" </s>\"\n"}
{"text": "<s>[INST]يرجى تحديد الصورة التي ترغب في تصنيفها بدون استخدام النماذج المدربة مسبقًا، ثم انتقل إلى عملية التصنيف باستخدام تقنية Classification Zero-Shot من حواسب الرؤية الحاسوبية.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the pipeline function from the transformers library.\\\\n2. Create a pipeline object for zero-shot image classification by specifying the model 'Bingsu/clip-vit-base-patch32-ko'. This is a pre-trained model designed to recognize objects in images without prior knowledge of the specific classes.\\\\n3. Provide the image URL and a list of candidate labels (e.g. names of Korean foods or drinks) to the pipeline. The candidate_labels parameter should contain an array of strings representing the potential food or drink categories.\\\\n4. Run the pipeline, and the model will return the most likely category for the image based on the provided candidate labels.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nurl = 'https://example.com/korean_food_image.jpg'\\\\n# replace with a valid image URL containing a Korean food or drink\\\\ncandidate_labels = ['kimchi', 'bulgogi', 'bibimbap', 'soju', 'makgeolli']\\\\npipe = pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\\\\nresult = pipe(images=url, candidate_labels=candidate_labels)\\\", 'api_call': \\\"pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'clip-vit-base-patch32-ko', 'api_call': \\\"pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\\\", 'api_arguments': {'images': 'url', 'candidate_labels': 'Array of strings', 'hypothesis_template': 'String'}, 'python_environment_requirements': [['transformers', 'torch', 'PIL', 'requests']], 'example_code': \\\"from transformers import pipeline\\\\nrepo = 'Bingsu/clip-vit-base-patch32-ko'\\\\npipe = pipeline('zero-shot-image-classification', model=repo)\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nresult = pipe(images=url, candidate_labels=[], hypothesis_template='{}')\\\\nresult\\\", 'performance': {'dataset': 'AIHUB', 'accuracy': 'Not provided'}, 'description': 'Korean CLIP model trained by Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. It is a zero-shot image classification model that can be used to classify images without any training data.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير برنامج يقوم بإنشاء نصوص طبيعية باللغة العربية. يجب أن يكون البرنامج قادرًا على تحويل النصوص المدخلة إلى نصوص مكتوبة بطريقة تجعلها بينتاغاممية وسلسة. عليك أن تضمن أن النصوص المُنتَجة تحتوي على نطق صحيح معتمد على تقنيات معالجة اللغة الطبيعية. يمكن استخدام هذا البرنامج في مجال الجيل التلقائي للنصوص وذلك لتحقيق أهداف تطوير تطبيقات تقنية مساعدة لذوي الإعاقة البصرية.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"XLNetModel.from_pretrained('xlnet-base-cased')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the XLNetTokenizer and XLNetModel classes from the transformers library.\\\\n2. Load the pre-trained 'xlnet-base-cased' model, which is capable of generating coherent text given a context.\\\\n3. Tokenize the text from the news article about floods using the tokenizer's from_pretrained method.\\\\n4. Use the model to generate a summary of the input text by passing the tokenized input to the model.\\\\n5. Format and output the generated summary.\\\", '<<<code>>>': 'from transformers import XLNetTokenizer, XLNetModel\\\\narticle_text = \\\"News article about floods here...\\\"\\\\ntokenizer = XLNetTokenizer.from_pretrained(\\\\'xlnet-base-cased\\\\')\\\\nmodel = XLNetModel.from_pretrained(\\\\'xlnet-base-cased\\\\')\\\\ninputs = tokenizer(article_text, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nlast_hidden_states = outputs.last_hidden_state\\\\n# Post-processing to obtain the summary from the last_hidden_state', 'api_call': \\\"XLNetModel.from_pretrained('xlnet-base-cased')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'xlnet-base-cased', 'api_call': \\\"XLNetModel.from_pretrained('xlnet-base-cased')\\\", 'api_arguments': {'pretrained_model_name': 'xlnet-base-cased'}, 'python_environment_requirements': [{'library': 'transformers', 'version': '4.0.0+'}], 'example_code': \\\"from transformers import XLNetTokenizer, XLNetModel\\\\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\\\\nmodel = XLNetModel.from_pretrained('xlnet-base-cased')\\\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlast_hidden_states = outputs.last_hidden_state\\\", 'performance': {'dataset': 'bookcorpus, wikipedia', 'accuracy': 'state-of-the-art (SOTA) results on various downstream language tasks'}, 'description': 'XLNet model pre-trained on English language. It was introduced in the paper XLNet: Generalized Autoregressive Pretraining for Language Understanding by Yang et al. and first released in this repository. XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتصنيف الصور غير المرئية في الرؤية الحاسوبية دون استخدام بيانات التدريب، تحتاج إلى بناء نموذج يمكنه تحقيق ذلك بدقة وفعالية عالية.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create an image classification model and specify the model to be used: 'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup'.\\\\n3. This model is trained for zero-shot image classification and can be used to classify images without any prior knowledge of the exact category.\\\\n4. To categorize the mushrooms in the given image, provide the image file path and the categories (edible, poisonous) to the classifier.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')\\\\nmushroom_classification = image_classifier(image_path, class_names='edible, poisonous')\\\", 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup', 'api_call': \\\"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')\\\", 'api_arguments': 'image_path, class_names', 'python_environment_requirements': ['transformers'], 'example_code': \\\"results = model(image_path, class_names='cat, dog, bird')\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '76.9'}, 'description': 'A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Large model (convnext_large) as the image tower, a MLP (fc - gelu - drop - fc) head in vision tower instead of the single projection of other CLIP models, and a text tower with same width but 4 layers more depth than ViT-L / RN50x16 models (depth 16, embed dim 768).' </s>\"\n"}
{"text": "<s>[INST]يُعَدُّ التَّصْنِيفُ الصُّفْرِي لِمِعَالَجَةِ اللُّغَةِ الطَّبِيعِيَّةِ مِن مَجَالِ تَطْوِيرِ تَقْنِيَّاتِ الْتَعَلُّمِ الذَّكِيِّ حَيَوِيًا. نَقُومُ بِتَطْبِيقِ نُمُذَجِنَا عَلَى الْبَيَانَاتِ مِن مَجَالَاتٍ مُتَنَوِّعَةٍ دُونَ الْحَاجَةِ إِلَى تَصْنِيفِ مُسَبَّقٍ أَو تَدْرِيبٍ.? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"YOLO('keremberke/yolov8s-pothole-segmentation')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary libraries from ultralyticsplus, which are YOLO and render_result.', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'URL or local path to the image'\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pothole-segmentation', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'URL or local path to the image'\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.928, 'mAP@0.5(mask)': 0.928}}, 'description': 'A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.'}} </s>\"\n"}
{"text": "<s>[INST]يجب تصميم نموذج لتصنيف الرموز الخاص بمعالجة اللغة الطبيعية، وذلك من خلال تحديد تصنيف كل رمز ضمن النص المدخل بشكل صحيح. عليك توضيح معايير التحديد المستخدمة وضبط النموذج لتكون النتائج دقيقة وموثوقة.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"SequenceTagger.load('flair/ner-english-ontonotes-large')\\\", '<<<api_provider>>>': 'Flair', '<<<explanation>>>': \\\"1. Import the SequenceTagger model and Sentence class from the Flair library.\\\\n2. To extract named entities such as names, dates, and other relevant information, load the Flair pre-trained model, 'flair/ner-english-ontonotes-large'.\\\\n3. Create a Sentence object containing a user message.\\\\n4. Use the SequenceTagger.predict() method on the message to predict named entities.\\\\n5. The predicted entities can be accessed by iterating over the named entity spans in the Sentence object.\\\", '<<<code>>>': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\ntagger = SequenceTagger.load('flair/ner-english-ontonotes-large')\\\\nmessage = Sentence('I need support for my order number 12345 shipped on May 5th, 2021.')\\\\n\\\\ntagger.predict(message)\\\\n\\\\nentities = {}\\\\nfor entity in message.get_spans('ner'):\\\\n    entities[entity.tag] = entity.text\\\\n\\\\nprint(entities)\\\", 'api_call': \\\"SequenceTagger.load('flair/ner-english-ontonotes-large')\\\", 'provider': 'Flair', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'flair/ner-english-ontonotes-large', 'api_call': \\\"SequenceTagger.load('flair/ner-english-ontonotes-large')\\\", 'api_arguments': ['sentence'], 'python_environment_requirements': [['flair']], 'example_code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\n# load tagger\\\\ntagger = SequenceTagger.load('flair/ner-english-ontonotes-large')\\\\n\\\\n# make example sentence\\\\nsentence = Sentence('On September 1st George won 1 dollar while watching Game of Thrones.')\\\\n\\\\n# predict NER tags\\\\ntagger.predict(sentence)\\\\n\\\\n# print sentence\\\\nprint(sentence)\\\\n\\\\n# print predicted NER spans\\\\nprint('The following NER tags are found:')\\\\n\\\\n# iterate over entities and print\\\\nfor entity in sentence.get_spans('ner'):\\\\n    print(entity)\\\", 'performance': {'dataset': 'Ontonotes', 'accuracy': 90.93}, 'description': 'English NER in Flair (Ontonotes large model). This is the large 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on document-level XLM-R embeddings and FLERT.' </s>\"\n"}
{"text": "<s>[INST]املأ الفراغ في النص التالي باستخدام Natural Language Processing: \\\"كان ياما كان في أرض بعيدة، تمر عصافير الى السماء بهدوء وجمال\\\". اختر الكلمة المناسبة لاستكمال النص.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Firstly, import 'pipeline' from the transformers package.\\\\n2. Instantiate the masked language model by calling the 'pipeline' function with the 'fill-mask' task and the model name 'bert-base-cased'.\\\\n3. Pass a sentence containing a masked token [MASK] to the model as input, which will predict and return the most likely word to fill the gap.\\\\n4. This model can be used to create statements with missing words filled in based on the context.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nfill_mask = pipeline(\\\\'fill-mask\\\\', model=\\\\'bert-base-cased\\\\')\\\\ninput_sentence = \\\"The media company specializes in creating [MASK] content.\\\"\\\\nfilled_statement = fill_mask(input_sentence)', 'api_call': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-base-cased', 'api_call': \\\"pipeline('fill-mask', model='bert-base-cased')\\\", 'api_arguments': ['model'], 'python_environment_requirements': [['transformers']], 'example_code': ['from transformers import pipeline', \\\"unmasker = pipeline('fill-mask', model='bert-base-cased')\\\", \\\"unmasker(Hello I'm a [MASK] model.)\\\"], 'performance': {'dataset': 'GLUE', 'accuracy': 79.6}, 'description': \\\"BERT base model (cased) is a pre-trained transformer model on English language using a masked language modeling (MLM) objective. It was introduced in a paper and first released in a repository. This model is case-sensitive, which means it can differentiate between 'english' and 'English'. The model can be used for masked language modeling or next sentence prediction, but it's mainly intended to be fine-tuned on a downstream task.\\\" </s>\"\n"}
{"text": "<s>[INST]العميل يطوّر تطبيقًا لمعالجة اللغة الطبيعية وإعداد الخلاصات. نحن بحاجة إلى برنامج يقوم بتحويل النصوص إلى خلاصات لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required libraries - BartForConditionalGeneration, and BartTokenizer from transformers.\\\\n2. Load the pre-trained 'sshleifer/distilbart-cnn-6-6' model using BartForConditionalGeneration.from_pretrained() method. The loaded model is a distilled version of BART which is fine-tuned for text summarization.\\\\n3. Load the tokenizer using the BartTokenizer.from_pretrained() method with the same model name.\\\\n4. Use the tokenizer to encode the given article text and then use the model to generate a summary.\\\\n5. Decode the generated summary using the tokenizer's decode() method.\\\", '<<<code>>>': 'from transformers import BartForConditionalGeneration, BartTokenizer\\\\nmodel = BartForConditionalGeneration.from_pretrained(\\\\'sshleifer/distilbart-cnn-6-6\\\\')\\\\ntokenizer = BartTokenizer.from_pretrained(\\\\'sshleifer/distilbart-cnn-6-6\\\\')\\\\ninputs = tokenizer.encode(\\\"Your long news article here...\\\", return_tensors=\\\\'pt\\\\', max_length=1024, truncation=True)\\\\nsummary_ids = model.generate(inputs, num_beams=4, max_length=100, early_stopping=True)\\\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sshleifer/distilbart-cnn-6-6', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': {'cnn_dailymail': {'Rouge 2': 20.17, 'Rouge-L': 29.7}, 'xsum': {'Rouge 2': 20.92, 'Rouge-L': 35.73}}}, 'description': 'DistilBART model for text summarization, trained on the CNN/Daily Mail and XSum datasets. It is a smaller and faster version of BART, suitable for summarizing English text.' </s>\"\n"}
{"text": "<s>[INST]نحن نطلب من عميلنا تطوير نموذج تصنيف الفيديوهات في مجال رؤية الحاسوب باستخدام تقنيات معالجة الصور والفيديوهات الحديثة. الهدف هو بناء نظام يستطيع تصنيف الفيديوهات بناءً على محتواها وفهم محتواها بشكل أعمق. عليك إنشاء برنامج يستطيع التعرف على مضمون الفيديو بدقة وفهم محتواه.? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries: VideoMAEFeatureExtractor and VideoMAEForPreTraining from the transformers package.', 'code': \\\"from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(16, 3, 224, 224))  # Replace with a list of frames from the actual video\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\n\\\\npixel_values = feature_extractor(video, return_tensors='pt').pixel_values\\\\n\\\\nnum_frames = 16\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss\\\\n\\\\n# Perform classification and intrusion detection by adding a linear layer on top of the pre-trained encoder\\\", 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'videomae-base-ssv2', 'api_call': \\\"from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(16, 3, 224, 224))  # Replace with a list of frames from the actual video\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\n\\\\npixel_values = feature_extractor(video, return_tensors='pt').pixel_values\\\\n\\\\nnum_frames = 16\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss\\\\n\\\\n# Perform classification and intrusion detection by adding a linear layer on top of the pre-trained encoder\\\", 'api_arguments': 'video', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\\\npixel_values = feature_extractor(video, return_tensors=pt).pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss', 'performance': {'dataset': 'Something-Something-v2', 'accuracy': ''}, 'description': 'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتدريب نموذج التصنيف الجدولي على تصنيف البيانات الجدولية. اختر مجموعة بيانات شاملة ومتنوعة لتحقيق أداء عالي للنموذج.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': '1. Import the required libraries joblib and pandas. Joblib is used to load the pre-trained KNN model, and pandas is used to process the input data.\\\\n2. Load the pre-trained KNN model for multi-class classification of Iris flower species using joblib.load().\\\\n3. Read the input data, which contains sepal and petal lengths and widths, into a pandas DataFrame. Ensure the column names match the expected format.\\\\n4. Use the loaded KNN model to predict the species of the Iris flowers in the input data.', '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\ndata = pd.DataFrame({'feat_sepal_length': [5.1, 6.3],\\\\n                     'feat_sepal_width': [3.5, 2.8],\\\\n                     'feat_petal_length': [1.4, 5.1],\\\\n                     'feat_petal_width': [0.2, 1.5]})\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'abhishek/autotrain-iris-knn', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'scikit-learn/iris', 'accuracy': 0.9}, 'description': 'A K-Nearest Neighbors (KNN) model trained on the Iris dataset for multi-class classification. The model is trained using AutoTrain and has an accuracy of 0.9.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتوليد النصوص بشكل طبيعي في مجال معالجة اللغة الطبيعية، ونحن بحاجة لإعداد جهاز لتوليد النصوص بشكل تلقائي.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, in this case, transformers and pipeline from the Hugging Face library.\\\\n2. Set up the conversational pipeline using the 'ingen51/DialoGPT-medium-GPT4' model. This model has been specifically trained for handling conversational tasks.\\\\n3. Pass the customer questions as input to the conversational_pipeline.\\\\n4. The model generates human-like responses to the customer questions that can be used for managing customer support inquiries.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nconversational_pipeline = pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\\\\nresponse = conversational_pipeline('Hello, how do I change my password?')\\\", 'api_call': \\\"pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ingen51/DialoGPT-medium-GPT4', 'api_call': \\\"pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\\\", 'api_arguments': ['message'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"conversational_pipeline('Hello, how are you?')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A GPT-4 model for generating conversational responses in a dialogue setting.' </s>\"\n"}
{"text": "<s>[INST]أنشئ قصة قصيرة تبدأ بـ \\\"كان ياما كان في أرض بعيدة\\\" تتحدث عن عملية تقسيم الصور في الرؤية الحاسوبية.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers and PIL libraries. This includes SegformerForSemanticSegmentation for the semantic segmentation model, SegformerImageProcessor for processing the image, and Image for handling image data.\\\\n2. Load the required pre-trained model 'nvidia/segformer-b0-finetuned-ade-512-512', which can identify components and segment them based on their labels.\\\\n3. Create an instance of SegformerImageProcessor and SegformerForSemanticSegmentation with this pre-trained model.\\\\n4. Load the image from a file or a provided URL, and then pass the image to SegformerImageProcessor to prepare the input for the model.\\\\n5. Finally, forward the processed image through the model to obtain segmentation results.\\\", '<<<code>>>': \\\"from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nprocessor = SegformerImageProcessor.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\", 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nvidia/segformer-b0-finetuned-ade-512-512', 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\\\", 'api_arguments': {'images': 'Image', 'return_tensors': 'pt'}, 'python_environment_requirements': [{'transformers': 'SegformerImageProcessor, SegformerForSemanticSegmentation', 'PIL': 'Image', 'requests': 'requests'}], 'example_code': 'from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nprocessor = SegformerImageProcessor.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]يقوم فريق العمل بتطوير نموذج للكشف عن الكائنات في مجال رؤية الحاسوب، من خلال تحليل الصور وتجسيد الكائنات وتحديد مواقعها، باستخدام تقنيات معالجة الصور والاعتماد على البيانات المتاحة لتدريب النموذج على تنفيذ هذه المهمة.? [/INST]{'domain': 'Computer Vision Object Detection', 'api_call': \\\"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\\\nimport torch\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'your_image_url_here'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\\n\\\\ninputs = processor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\n\\\\n# Extract object detection results from 'outputs'\\\", 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'deformable-detr', 'api_call': \\\"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\\\nimport torch\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'your_image_url_here'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\\n\\\\ninputs = processor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\n\\\\n# Extract object detection results from 'outputs'\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': \\\"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\\\nimport torch\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\\\ninputs = processor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'performance': {'dataset': 'COCO 2017', 'accuracy': 'Not provided'}, 'description': 'Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]المطورون بحاجة إلى تصميم برنامج لتصنيف النصوص بدون استخدام اللغة الطبيعية. ينصح ببناء نظام يقوم بتحديد الفترات في التسجيل الصوتي التي تحتوي على كلام لتحسين تجربة المستخدمين.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('typeform/mobilebert-uncased-mnli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes AutoTokenizer and AutoModelForSequenceClassification from transformers library by Hugging Face.\\\\n2. Initialize the tokenizer and the model by calling the from_pretrained method with the pretrained model 'typeform/mobilebert-uncased-mnli'. This is a sequence classification model specifically fine-tuned for the task of zero-shot classification.\\\\n3. Use the tokenizer to tokenize the input text (social media post) and create the input tensors required by the model.\\\\n4. Make predictions using the model by feeding it the input tensors.\\\\n5. Obtain the highest-scoring category label for the input text from the model's output, which can be one of the three categories: positive, negative, or neutral.\\\", '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\ntokenizer = AutoTokenizer.from_pretrained('typeform/mobilebert-uncased-mnli')\\\\nmodel = AutoModelForSequenceClassification.from_pretrained('typeform/mobilebert-uncased-mnli')\\\\ninput_texts = [post]  # Replace `post` with the actual post content\\\\ninputs = tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True)\\\\noutputs = model(**inputs)\\\\ncategory_ids = outputs.logits.argmax(dim=-1)\\\\ncategory_labels = ['positive', 'negative', 'neutral']\\\\ncategory = category_labels[category_ids[0].item()]\\\", 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('typeform/mobilebert-uncased-mnli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'typeform/mobilebert-uncased-mnli', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('typeform/mobilebert-uncased-mnli')\\\", 'api_arguments': {'pretrained_model': 'typeform/mobilebert-uncased-mnli'}, 'python_environment_requirements': [{'transformers': 'AutoTokenizer, AutoModelForSequenceClassification'}], 'example_code': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\ntokenizer = AutoTokenizer.from_pretrained(typeform/mobilebert-uncased-mnli)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(typeform/mobilebert-uncased-mnli)', 'performance': {'dataset': 'multi_nli', 'accuracy': 'More information needed'}, 'description': 'This model is the Multi-Genre Natural Language Inference (MNLI) fine-turned version of the uncased MobileBERT model. It can be used for the task of zero-shot classification.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام استشعار للكشف عن النشاط الصوتي، ونحتاج إلى تحديد متى يحدث النشاط الصوتي في البيانات الصوتية.? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To detect and monitor speakers in an audio recording, we need to use a Voice Activity Detection model.\\\\n2. Import the necessary libraries, which are 'Model' from pyannote.audio.\\\\n3. Load the pretrained 'popcornell/pyannote-segmentation-chime6-mixer6' model using the 'Model.from_pretrained' method.\\\\n4. Apply the model on the audio recording to segment it and identify the speakers.\\\\n5. The model will return timestamps and speaker labels, which can be used to assess individual speaker contributions to the meeting.\\\", '<<<code>>>': \\\"from pyannote.audio import Model\\\\nmodel = Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\\n# Then apply the model on the audio file to perform voice activity detection\\\", 'api_call': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'popcornell/pyannote-segmentation-chime6-mixer6', 'api_call': \\\"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\\\", 'api_arguments': 'N/A', 'python_environment_requirements': ['pyannote.audio 2.1.1'], 'example_code': 'from pyannote.audio import Model\\\\nmodel = Model.from_pretrained(popcornell/pyannote-segmentation-chime6-mixer6)', 'performance': {'dataset': 'ami', 'accuracy': 'N/A'}, 'description': 'Pyannote Segmentation model fine-tuned on data from CHiME-7 DASR Challenge. Used to perform diarization in the CHiME-7 DASR diarization baseline.' </s>\"\n"}
{"text": "<s>[INST]يمكنك بناء نموذج لتوقع قيم رقمية باستخدام الجداول باستخدام واجهة برمجة التطبيقات الخاصة لنمذجة التابعات الجدولية. ضع في اعتبارك تحليل وتجهيز البيانات المطلوبة قبل تدريب النموذج.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\", '<<<api_provider>>>': 'Keras', '<<<explanation>>>': \\\"1. We first import the necessary libraries such as tensorflow and keras.\\\\n2. We use the from_pretrained method of the TFAutoModelForSequenceClassification class to load the pre-trained model 'keras-io/timeseries-anomaly-detection'. This model has been designed for anomaly detection tasks in timeseries data.\\\\n3. As we're working with HVAC units data, we preprocess the timeseries data to make it compatible with the model.\\\\n4. The anomaly detection model will be used to identify anomalous behavior in the HVAC system by analyzing the timeseries data from the HVAC units.\\\\n5. After processing the data and detecting anomalies, the building management team can take necessary actions to fix the problem and prevent further complications.\\\", '<<<code>>>': \\\"import tensorflow as tf\\\\nfrom transformers import TFAutoModelForSequenceClassification\\\\n\\\\nmodel = TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\\n\\\\n# Preprocess the timeseries data from the HVAC units\\\\n# ...\\\\n\\\\n# Feed the preprocessed data to the model to detect anomalies\\\\n# anomaly_results = model(...)\\\", 'api_call': \\\"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\", 'provider': 'Keras', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keras-io/timeseries-anomaly-detection', 'api_call': \\\"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\", 'api_arguments': {'optimizer': {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'training_precision': 'float32'}, 'python_environment_requirements': [['tensorflow', 'keras']], 'example_code': '', 'performance': {'dataset': 'Numenta Anomaly Benchmark(NAB)', 'accuracy': {'Train Loss': 0.006, 'Validation Loss': 0.008}}, 'description': 'This script demonstrates how you can use a reconstruction convolutional autoencoder model to detect anomalies in timeseries data. We will use the Numenta Anomaly Benchmark(NAB) dataset. It provides artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered, timestamped, single-valued metrics.' </s>\"\n"}
{"text": "<s>[INST]تقوم الشركة بتطوير منتج في مجال معالجة اللغة الطبيعية والإجابة على الأسئلة، ونرغب في استخدام البيانات لبناء نموذج قادر على الاستجابة للأسئلة بشكل دقيق وفعال.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the 'pipeline' function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a question-answering model, specifying the model 'sultan/BioM-ELECTRA-Large-SQuAD2'.\\\\n3. This model is trained on the BioASQ dataset and fine-tuned on SQuAD2.0, making it suitable for understanding complex biological information.\\\\n4. The created model can be used to answer questions from students within a given context (text passage) covering biological topics.\\\\n5. To use the model, simply provide the context (text) and the question, and the model will return the most likely answer.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\'question-answering\\\\', model=\\\\'sultan/BioM-ELECTRA-Large-SQuAD2\\\\')\\\\ncontext = \\\"Biological context here...\\\"\\\\nquestion = \\\"Student question here...\\\"\\\\nresult = qa_pipeline({\\\\'context\\\\': context, \\\\'question\\\\': question})', 'api_call': \\\"pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sultan/BioM-ELECTRA-Large-SQuAD2', 'api_call': \\\"pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\", 'api_arguments': None, 'python_environment_requirements': [['transformers', 'sentencepiece']], 'example_code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\\nresult = qa_pipeline({'context': 'your_context', 'question': 'your_question'})\\\", 'performance': {'dataset': 'SQuAD2.0 Dev', 'accuracy': {'exact': 84.33420365535248, 'f1': 87.49354241889522}}, 'description': 'BioM-ELECTRA-Large-SQuAD2 is a fine-tuned version of BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts, on the SQuAD2.0 dataset. Fine-tuning the biomedical language model on the SQuAD dataset helps improve the score on the BioASQ challenge. This model is suitable for working with BioASQ or biomedical QA tasks.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج ترجمة آلي يعتمد على معالجة اللغة الطبيعية للتعامل مع مجموعة متنوعة من اللغات بدقة وفعالية عالية.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import necessary libraries, including the translation_pipeline from the transformers package.\\\\n2. Create a 'translation_pipeline' object, specifying the specific translation model you want to use, 'Helsinki-NLP/opus-mt-sv-en'.\\\\n3. Use the 'translation_pipeline' function to translate the email text from Swedish to English.\\\\n4. Then use the translated text to derive the appropriate response or perform further analysis.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_sv_to_en\\\\', model=\\\\'Helsinki-NLP/opus-mt-sv-en\\\\')\\\\nswedish_text = \\\"Swedish email text here...\\\"\\\\nenglish_translation = translator(swedish_text)\\\\nenglish_text = english_translation[0][\\\\'translation_text\\\\']\\\\n\\\\n# Process the English text and generate a response', 'api_call': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-sv-en', 'api_call': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", 'api_arguments': ['inputs'], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'Tatoeba.sv.en', 'accuracy': 'BLEU: 64.5, chr-F: 0.763'}, 'description': 'A Swedish to English translation model trained on the OPUS dataset using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece.' </s>\"\n"}
{"text": "<s>[INST]تشمل أعمال شركتنا تطوير نظام متعدد الوسائط لتحويل النصوص إلى صور، حيث نرغب في تصنيف الصور المنتجة بناءً على البيانات التي تم جمعها.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0', torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. We import the necessary libraries, including Torch and StableDiffusionPipeline from the diffusers package.\\\\n2. We load the pretrained \\\\'andite/anything-v4.0\\\\' model, specifying it should use float16 data type for improved speed and memory efficiency.\\\\n3. The model is sent to GPU (if available) for faster processing.\\\\n4. To generate an image, we provide a description, such as \\\"a warrior with a sword and shield\\\", as the input prompt for the model.\\\\n5. The model generates the image, which can then be saved as a file or further processed as needed.', '<<<code>>>': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\n\\\\nmodel_id = \\\\'andite/anything-v4.0\\\\'\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(\\\\'cuda\\\\')  # If GPU is available\\\\nprompt = \\\"a warrior with a sword and shield\\\"\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(\\\"warrior_with_sword_and_shield.png\\\")', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0', torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'andite/anything-v4.0', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0', torch_dtype=torch.float16)\\\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': [{'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}], 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتصميم مركبات ذاتية القيادة، ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا لتصنيف الصور في مجال الرؤية الحاسوبية.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary libraries and classes, including BeitImageProcessor and BeitForImageClassification from the transformers package, and the Image class from the PIL package.\\\\n2. We then use the from_pretrained method of the BeitForImageClassification class to load the pre-trained model 'microsoft/beit-base-patch16-224-pt22k-ft22k'. This model has been trained for image classification tasks and can identify objects in a variety of categories.\\\\n3. We load the image data from an uploaded file or it can be obtained directly from your online store.\\\\n4. We process the image using the BeitImageProcessor before passing it to the BeitForImageClassification model.\\\\n5. The model can then analyze the image and classify it into one of the pre-defined categories.\\\", '<<<code>>>': 'from transformers import BeitImageProcessor, BeitForImageClassification\\\\nfrom PIL import Image\\\\nimage = Image.open(\\\\'product_image_path.jpg\\\\')\\\\n# replace \\\\'product_image_path.jpg\\\\' with the path to your product image\\\\nprocessor = BeitImageProcessor.from_pretrained(\\\\'microsoft/beit-base-patch16-224-pt22k-ft22k\\\\')\\\\nmodel = BeitForImageClassification.from_pretrained(\\\\'microsoft/beit-base-patch16-224-pt22k-ft22k\\\\')\\\\ninputs = processor(images=image, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(f\\\"Predicted class: {model.config.id2label[predicted_class_idx]}\\\")', 'api_call': \\\"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/beit-base-patch16-224-pt22k-ft22k', 'api_call': \\\"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import BeitImageProcessor, BeitForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\\\ninputs = processor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\\", 'performance': {'dataset': 'ImageNet-22k', 'accuracy': 'Not specified'}, 'description': 'BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام يمكنه تحويل الصور المتعددة الوسائط إلى نص، ونحتاج إلى تنبؤ بنوعية النص الناتج باستخدام بيانات الصور المتوفرة.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes and functions from the transformers package. This includes the 'pipeline' function for setting up the model and 'AutoTokenizer' for tokenizing the input images.\\\\n2. We then use the 'pipeline' function from transformers to load the 'microsoft/git-large-r-textcaps' model. This model is specifically designed for generating text descriptions from images, which is what we need to automatically label products on an e-commerce website.\\\\n3. To use this model, we first need to tokenize the input image using the 'AutoTokenizer' class from transformers.\\\\n4. The model can then be used to generate a text description for the input image, which can be used as a label for the product.\\\", '<<<code>>>': \\\"from transformers import pipeline, AutoTokenizer\\\\ncaption_generator = pipeline('text-generation', model='microsoft/git-large-r-textcaps')\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/git-large-r-textcaps')\\\\nimage_path = 'path/to/image.jpg'  # Replace with the path to your product image\\\\ninputs = tokenizer(image_path)  # Tokenize the input image\\\\noutputs = caption_generator(inputs)  # Generate a text description for the image\\\\nlabel = outputs[0]['generated_text']\\\", 'api_call': \\\"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'git-large-r-textcaps', 'api_call': \\\"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\\\", 'api_arguments': 'image', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'TextCaps', 'accuracy': ''}, 'description': \\\"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\\\" </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج استرجاع للإجابة باستخدام معالجة اللغة الطبيعية للإجاية على الأسئلة في وثيقة متعددة الوسائط.? [/INST]{'domain': 'Multimodal Document Question Answer', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary classes from the transformers library, which includes the AutoModelForDocumentQuestionAnswering class for document question answering tasks.', 'code': 'from transformers import AutoModelForDocumentQuestionAnswering\\\\noursales_model = AutoModelForDocumentQuestionAnswering.from_pretrained(\\\\'impira/layoutlm-invoices\\\\')\\\\n\\\\ndef find_total_sales(receipts):\\\\n    total_sales = 0\\\\n    for receipt in receipts:\\\\n        result = oursales_model(question=\\\"What is the total amount?\\\", context=receipt)\\\\n        total_sales += float(result[\\\\'answer\\\\'])\\\\n    return total_sales', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-invoices', 'api_call': 'from transformers import AutoModelForDocumentQuestionAnswering\\\\noursales_model = AutoModelForDocumentQuestionAnswering.from_pretrained(\\\\'impira/layoutlm-invoices\\\\')\\\\n\\\\ndef find_total_sales(receipts):\\\\n    total_sales = 0\\\\n    for receipt in receipts:\\\\n        result = oursales_model(question=\\\"What is the total amount?\\\", context=receipt)\\\\n        total_sales += float(result[\\\\'answer\\\\'])\\\\n    return total_sales', 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \\\"nlp(question='What is the total amount?', context='your_invoice_text')\\\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.'}} </s>\"\n"}
{"text": "<s>[INST]يهدف هذا المشروع إلى تطوير نموذج يستطيع تصنيف النصوص بدقة عالية دون الحاجة إلى تصنيفات مسبقة. نريد تصميم نموذج يمكنه تحليل النصوص بطريقة ذاتية وتحديد فئة كل نص استنادًا إلى المعلومات المتاحة بدون الحاجة إلى إشراف بشري. يتطلب ذلك فهمًا عميقًا للغة الطبيعية والقدرة على التعامل مع النصوص بلغات متعددة.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the 'pipeline' function from the transformers library.\\\\n2. Initialize the classification model by loading the 'svalabs/gbert-large-zeroshot-nli' model using the 'pipeline' function. This model is designed for zero-shot classification tasks in German language.\\\\n3. Define the input sentence and the labels to classify the sentence, in this case, ['Computer', 'Handy', 'Tablet', 'dringend', 'nicht dringend'].\\\\n4. Run the text classification by providing the input sentence and labels.\\\\n5. The result will be a category that best describes the context of the input sentence.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsequence = \\\"Ich habe ein Problem mit meinem Iphone das so schnell wie möglich gelöst werden muss\\\"\\\\nlabels = [\\\\'Computer\\\\', \\\\'Handy\\\\', \\\\'Tablet\\\\', \\\\'dringend\\\\', \\\\'nicht dringend\\\\']\\\\nhypothesis_template = \\\"In diesem Satz geht es um das Thema {}.\\\"\\\\nzershot_pipeline = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'svalabs/gbert-large-zeroshot-nli\\\\')\\\\npredictions = zershot_pipeline(sequence, labels, hypothesis_template=hypothesis_template)\\\\ncategory = predictions[\\\\'labels\\\\'][0]', 'api_call': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'svalabs/gbert-large-zeroshot-nli', 'api_call': \\\"pipeline('zero-shot-classification', model='svalabs/gbert-large-zeroshot-nli')\\\", 'api_arguments': ['sequence', 'labels', 'hypothesis_template'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\nzershot_pipeline = pipeline(zero-shot-classification, model=svalabs/gbert-large-zeroshot-nli)\\\\nsequence = Ich habe ein Problem mit meinem Iphone das so schnell wie möglich gelöst werden muss\\\\nlabels = [Computer, Handy, Tablet, dringend, nicht dringend]\\\\nhypothesis_template = In diesem Satz geht es um das Thema {}. ', 'performance': {'dataset': 'XNLI TEST-Set', 'accuracy': '85.6%'}, 'description': 'A German zeroshot classification model based on the German BERT large model from deepset.ai and finetuned for natural language inference using machine-translated nli sentence pairs from mnli, anli, and snli datasets.' </s>\"\n"}
{"text": "<s>[INST]بتصميم نظم التصور الحاسوبي لتصنيف الصور، نحتاج إلى فهم عمق الأشياء في الصور التي نتحلو بتصويرها لتحقيق تصنيف دقيق وفعال.? [/INST]{'domain': 'Computer Vision Image Classification', 'api_call': \\\"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimport torch\\\\nimg = Image.open(urlopen('https://image_url_here'))\\\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))\\\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\\\", 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'convnextv2_huge.fcmae_ft_in1k', 'api_call': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimport torch\\\\nimg = Image.open(urlopen('https://image_url_here'))\\\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))\\\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\\\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': ['timm'], 'example_code': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimg = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))\\\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\\\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 86.256}, 'description': 'A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k.'}} </s>\"\n"}
{"text": "<s>[INST]يمكنك القيام بورشة عمل حول معالجة اللغة الطبيعية وتلخيص النصوص. يوصى بتوفير وصول لقاعدة بيانات نصية ضخمة لاستخدامها كمصدر للبيانات عند تطوير نموذج التلخيص.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('csebuetnlp/mT5_multilingual_XLSum')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('csebuetnlp/mT5_multilingual_XLSum')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'csebuetnlp/mT5_multilingual_XLSum', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('csebuetnlp/mT5_multilingual_XLSum')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers==4.11.0.dev0']], 'example_code': \\\"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub('\\\\\\\\s+', ' ', re.sub('\\\\\\\\n+', ' ', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\\\", 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 36.5, 'ROUGE-2': 13.934, 'ROUGE-L': 28.988, 'ROUGE-LSUM': 28.996, 'loss': 2.067, 'gen_len': 26.973}}, 'description': 'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نموذج تصنيف الفيديوهات بمستخدم تقنيات رؤية الحاسوب. هل يمكنك مساعدتي في تصنيف مقاطع الفيديو لمجموعة مشاهد تحتوي على حيوانات برية؟? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224)) # replace with actual video frames\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\\nmodel = TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\\ninputs = processor(images=video, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n    predicted_class_idx = logits.argmax(-1).item()\\\\n    print('Predicted class:', model.config.id2label[predicted_class_idx])\\\", 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'facebook/timesformer-base-finetuned-k600', 'api_call': \\\"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224)) # replace with actual video frames\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\\nmodel = TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\\ninputs = processor(images=video, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n    predicted_class_idx = logits.argmax(-1).item()\\\\n    print('Predicted class:', model.config.id2label[predicted_class_idx])\\\", 'api_arguments': ['images'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k600)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k600)\\\\ninputs = processor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-600', 'accuracy': None}, 'description': 'TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج لمعالجة اللغة الطبيعية للإجابة على الأسئلة، واستخدمه لإجابة على أسئلة قصيرة.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/tinyroberta-squad2')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/tinyroberta-squad2')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'deepset/tinyroberta-squad2', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/tinyroberta-squad2')\\\", 'api_arguments': {'model_name_or_path': 'deepset/tinyroberta-squad2', 'question': 'Why is model conversion important?', 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel_name = deepset/tinyroberta-squad2\\\\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\\\\nQA_input = {\\\\n 'question': 'Why is model conversion important?',\\\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\\\n}\\\\nres = nlp(QA_input)\\\", 'performance': {'dataset': 'squad_v2', 'accuracy': {'exact': 78.69114798281817, 'f1': 81.9198998536977}}, 'description': 'This is the distilled version of the deepset/roberta-base-squad2 model. This model has a comparable prediction quality and runs at twice the speed of the base model.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير تقنية معالجة اللغة الطبيعية للجدول للإجابة على الأسئلة، ونريد التنبؤ بالإجابات بناءً على بيانات الجدول والنصوص المتاحة.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary classes from the transformers package. This includes TapasTokenizer to tokenize the input text and TapasForQuestionAnswering for the question-answering model.\\\\n2. We then use the from_pretrained method of the TapasForQuestionAnswering class to load the pre-trained model 'google/tapas-small-finetuned-wikisql-supervised'.\\\\n3. Utilize this model to analyze the text input (renters' questions) and the table representing the car selection, and provide answers to the queries.\\\", '<<<code>>>': 'from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\ntokenizer = TapasTokenizer.from_pretrained(\\\\'google/tapas-small-finetuned-wikisql-supervised\\\\')\\\\nmodel = TapasForQuestionAnswering.from_pretrained(\\\\'google/tapas-small-finetuned-wikisql-supervised\\\\')\\\\nquestions = [\\\"Which car has the lowest rental price?\\\", \\\"What is the price of the SUV?\\\"]\\\\n# replace with renters\\\\' real questions\\\\ninputs = tokenizer(questions, tables=table_data, return_tensors=\\\"pt\\\")\\\\noutputs = model(**inputs)', 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/tapas-small-finetuned-wikisql-supervised', 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\\\", 'api_arguments': \\\"model = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\\\", 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\ntokenizer = TapasTokenizer.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\\\\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\\\", 'performance': {'dataset': 'wikisql', 'accuracy': 'Not specified'}, 'description': 'TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. This model is fine-tuned on WikiSQL and can be used for answering questions related to a table.' </s>\"\n"}
{"text": "<s>[INST]يجب على طلاب استخراج ميزات متعددة الوسائط تصنيف النصوص المتقدمين أداة للتعلم الآلي في كل أسبوع لتحسين الأداء? [/INST]{'domain': 'Multimodal Feature Extraction', 'api_call': \\\"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import necessary libraries, including 'AutoTokenizer' and 'AutoModel' from the transformers package.\\\", 'code': 'import torch\\\\nfrom transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/dragon-plus-query-encoder\\\\')\\\\nquery_encoder = AutoModel.from_pretrained(\\\\'facebook/dragon-plus-query-encoder\\\\')\\\\ncontext_encoder = AutoModel.from_pretrained(\\\\'facebook/dragon-plus-context-encoder\\\\')\\\\nquery = \\\\'Where was Marie Curie born?\\\\'\\\\ncontexts = [\\\\n  \\\"Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\\\",\\\\n  \\\"Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.\\\"\\\\n]\\\\nquery_input = tokenizer(query, return_tensors=\\\\'pt\\\\')\\\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors=\\\\'pt\\\\')\\\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\\\nscores = torch.matmul(query_emb, ctx_emb.T).squeeze()\\\\nmost_relevant_answer = contexts[torch.argmax(scores).item()]', 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': 'import torch\\\\nfrom transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/dragon-plus-query-encoder\\\\')\\\\nquery_encoder = AutoModel.from_pretrained(\\\\'facebook/dragon-plus-query-encoder\\\\')\\\\ncontext_encoder = AutoModel.from_pretrained(\\\\'facebook/dragon-plus-context-encoder\\\\')\\\\nquery = \\\\'Where was Marie Curie born?\\\\'\\\\ncontexts = [\\\\n  \\\"Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\\\",\\\\n  \\\"Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.\\\"\\\\n]\\\\nquery_input = tokenizer(query, return_tensors=\\\\'pt\\\\')\\\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors=\\\\'pt\\\\')\\\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\\\nscores = torch.matmul(query_emb, ctx_emb.T).squeeze()\\\\nmost_relevant_answer = contexts[torch.argmax(scores).item()]', 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \\\"import torch\\\\nfrom transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\\\nquery = 'Where was Marie Curie born?'\\\\ncontexts = [\\\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\\\n]\\\\nquery_input = tokenizer(query, return_tensors='pt')\\\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\\\nscore1 = query_emb @ ctx_emb[0]\\\\nscore2 = query_emb @ ctx_emb[1]\\\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج لتحويل النص إلى صوت في مجال التحويل الصوتي للنص.? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/fastspeech2-en-200_speaker-cv4', arg_overrides={'vocoder': 'hifigan', 'fp16': False})\\\", 'api_provider': 'Fairseq', 'explanation': '1. We first import the required libraries and functions from fairseq, the checkpoint_utils module, and the IPython library for audio playback.', 'code': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\'facebook/fastspeech2-en-200_speaker-cv4\\\\', arg_overrides={\\\\'vocoder\\\\': \\\\'hifigan\\\\', \\\\'fp16\\\\': False})\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(models[0], cfg)\\\\n\\\\ntext = \\\"Hello, this is a news article.\\\"\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, models[0], generator, sample)\\\\n\\\\nipd.Audio(wav, rate=rate)', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Fairseq', 'functionality': 'Text-to-Speech', 'api_name': 'fastspeech2-en-male1', 'api_call': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\'facebook/fastspeech2-en-200_speaker-cv4\\\\', arg_overrides={\\\\'vocoder\\\\': \\\\'hifigan\\\\', \\\\'fp16\\\\': False})\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(models[0], cfg)\\\\n\\\\ntext = \\\"Hello, this is a news article.\\\"\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, models[0], generator, sample)\\\\n\\\\nipd.Audio(wav, rate=rate)', 'api_arguments': ['text'], 'python_environment_requirements': ['fairseq', 'IPython'], 'example_code': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n facebook/fastspeech2-en-200_speaker-cv4,\\\\n arg_overrides={vocoder: hifigan, fp16: False}\\\\n)\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\ntext = Hello, this is a test run.\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)', 'performance': {'dataset': 'common_voice', 'accuracy': None}, 'description': 'FastSpeech 2 text-to-speech model from fairseq S^2. English, 200 male/female voices, trained on Common Voice v4.'}} </s>\"\n"}
{"text": "<s>[INST]يمكنك استخدام تقنية الرؤية الحاسوبية لتصنيف الصور التي تصور الحيوانات في البرية. هل ترغب في توليد فيديو من وصف نصي لهذا المشهد؟? [/INST]'<<<domain>>>': 'Multimodal Feature Extraction', '<<<api_call>>>': \\\"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary libraries to create a feature extraction model. We will use the 'AutoModel' from the transformers library.\\\\n2. We load the 'DeepPavlov/rubert-base-cased' model using the 'from_pretrained()' method. This model has been trained on a large corpus of Russian text composed of Wikipedia and news data.\\\\n3. The loaded model can then be used to analyze and extract features from the input text.\\\\n4. With the feature extraction, you can study the grammatical structures, relations between sentences, and other language-related patterns within the text.\\\", '<<<code>>>': 'from transformers import AutoModel, AutoTokenizer\\\\n\\\\nrubert_model = AutoModel.from_pretrained(\\\\'DeepPavlov/rubert-base-cased\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'DeepPavlov/rubert-base-cased\\\\')\\\\n\\\\ninput_text = \\\"Russian text goes here...\\\"\\\\nencoded_input = tokenizer.encode(input_text, return_tensors=\\\\'pt\\\\')\\\\nfeature_extracted = rubert_model(encoded_input)\\\\n\\\\n# Now analyze the extracted features to reveal patterns and insights within the text', 'api_call': \\\"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \\\"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>\"\n"}
{"text": "<s>[INST]يرجى إنشاء نموذج تصنيف الصور لمعالجة الرؤية الحاسوبية استنادًا إلى البيانات المقدمة. يرجى التأكد من تدريب النموذج على تصنيف الصور بدقة عالية. هل بإمكانك القيام بذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include AutoFeatureExtractor from transformers, Image from PIL, and requests.\\\\n2. Load the image from the provided URL using the Image library and the requests module.\\\\n3. Use the AutoFeatureExtractor to obtain features of the image.\\\\n4. Load the SwinForImageClassification model using the specific pretrained model 'microsoft/swin-tiny-patch4-window7-224'.\\\\n5. Pass the features extracted from the image to the Swin Transformer model.\\\\n6. The output 'logits' from the model are used to get the index of the predicted class.\\\\n7. Retrieve the name of the object in the image using the 'id2label' dictionary provided in the model's configuration.\\\", '<<<code>>>': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = \\\"http://images.cocodataset.org/val2017/000000039769.jpg\\\"\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\\\\'microsoft/swin-tiny-patch4-window7-224\\\\')\\\\nmodel = SwinForImageClassification.from_pretrained(\\\\'microsoft/swin-tiny-patch4-window7-224\\\\')\\\\ninputs = feature_extractor(images=image, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\n\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/swin-tiny-patch4-window7-224', 'api_call': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': [{'transformers': 'AutoFeatureExtractor', 'PIL': 'Image', 'requests': 'requests'}], 'example_code': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.' </s>\"\n"}
{"text": "<s>[INST]لتوليد نصوص متطورة في مجال تجهيز اللغة الطبيعية، قم بتوفير التحديثات اللازمة لبناء نموذج مولِّد نصوص يستخدم التعلُم الآلي. يجب أن يكون النموذج قادرًا على توليد نصوص بشكل طبيعي ومفهوم، وقم بتجهيز فريقك لتحقيق الأداء القصوى ضد النماذج الأخرى في مجال تجهيز اللّغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library: AutoTokenizer and AutoModelForCausalLM.\\\\n2. Load the tokenizer and the pre-trained model 'Salesforce/codegen-2B-multi'. This model has been specifically designed for program synthesis tasks and can generate code in multiple programming languages based on human-readable descriptions.\\\\n3. Tokenize the input human-readable description using the tokenizer's encode method, which converts the description into a sequence of input_ids that the model can understand.\\\\n4. Pass the input_ids to the model's generate method, specifying the maximum length of the generated code. This will return the generated_ids corresponding to the predicted code.\\\\n5. Finally, decode the generated_ids using the tokenizer's decode method to obtain the generated code as text.\\\", '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained('Salesforce/codegen-2B-multi')\\\\nmodel = AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\\\\ntext = 'Create a function that prints Hello, World!'\\\\ninput_ids = tokenizer.encode(text, return_tensors='pt').input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\ngenerated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\\\", 'api_call': \\\"AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Salesforce/codegen-2B-multi', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\\\", 'api_arguments': {'input_ids': 'input_ids', 'max_length': 128}, 'python_environment_requirements': [{'transformers': 'AutoTokenizer, AutoModelForCausalLM'}], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-2B-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-2B-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval, MTPB'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B). The checkpoint included in this repository is denoted as CodeGen-Multi 2B, where Multi means the model is initialized with CodeGen-NL 2B and further pre-trained on a dataset of multiple programming languages, and 2B refers to the number of trainable parameters.' </s>\"\n"}
{"text": "<s>[INST]يقوم البرنامج بتعلم اللغة الطبيعية وإنشاء نصوص بشكل تلقائي. استخدم هذه التقنية لتطوير تطبيقات الذكاء الاصطناعي ومعالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. This includes the AutoModelForCausalLM for the text generation model and AutoTokenizer for tokenizing input text.\\\\n2. Use the from_pretrained method of the AutoModelForCausalLM class to load the pre-trained DialoGPT model 'microsoft/DialoGPT-large'. This model has been trained for large-scale conversational interactions, which is exactly what we need for a chatbot doing customer service for a bank.\\\\n3. Also, load the appropriate tokenizer for the DialoGPT model using the from_pretrained method of the AutoTokenizer class.\\\\n4. Now, when a customer asks a question, tokenize the input, generate a response using the DialoGPT model, and then decode the output to produce a human-readable response.\\\", '<<<code>>>': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-large')\\\\nmodel = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/DialoGPT-large', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_arguments': ['bot_input_ids', 'max_length', 'pad_token_id'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-large)\\\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-large)', 'performance': {'dataset': 'Reddit discussion thread', 'accuracy': 'Comparable to human response quality under a single-turn conversation Turing test'}, 'description': 'DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multi-turn conversations. The model is trained on 147M multi-turn dialogues from Reddit discussion threads.' </s>\"\n"}
{"text": "<s>[INST]يتعين عليك تطوير برنامج في مجال معالجة اللغة الطبيعية لترجمة النصوص بدون الحاجة إلى تصنيفات سابقة. نحتاج إلى حل قادر على تحويل النصوص إلى تصنيفات دون الحاجة إلى تدريب مسبق لمستخدمي التطبيق.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'pipeline' function from the transformers library.\\\\n2. Create a translation pipeline with the 'translation_de_to_es' task using the 'Helsinki-NLP/opus-mt-de-es' model to handle German to Spanish translation.\\\\n3. Pass the text in German from the online chat as an input to the pipeline to get the translated text in Spanish.\\\\n4. The translated text can then be used for further communication in the chat.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\\ngerman_text = 'Guten Morgen! Wie geht es Ihnen?'\\\\n# Replace this with the text from the online chat\\\\ntranslated_text = translation(text=german_text)\\\", 'api_call': \\\"pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-de-es', 'api_call': \\\"pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\\ntranslated_text = translation('Guten Tag')\\\", 'performance': {'dataset': 'Tatoeba.de.es', 'accuracy': {'BLEU': 48.5, 'chr-F': 0.676}}, 'description': 'A German to Spanish translation model based on the OPUS dataset and trained using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece tokenization.' </s>\"\n"}
{"text": "<s>[INST]استخدم واجهة برمجيات التطبيقات الخاصة بنا لتصنيف الصور باستخدام تقنيات الرؤية الحاسوبية. يمكنك تحميل الصور وتعيين الفئات المختلفة، ثم قم بتشغيل النموذج لتصنيف الصور تلقائيا وتحليلها.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries including requests, PIL, and transformers.\\\\n2. Use the requests library to fetch the image from the provided URL.\\\\n3. Open the image using the Image class from the PIL package.\\\\n4. Load the pre-trained model 'nateraw/vit-age-classifier' using the ViTForImageClassification.from_pretrained() method, designed for age classification.\\\\n5. Load the transforms using the ViTFeatureExtractor.from_pretrained() method.\\\\n6. Apply the transforms to the input image and convert the result into a PyTorch tensor.\\\\n7. Pass the transformed input to the model and obtain the output logits.\\\\n8. Get the probabilities and predictions of the age by applying softmax on the logits and finding the index with the highest probability.\\\\n9. Finally, return the predicted age of the person in the image.\\\", '<<<code>>>': 'import requests\\\\nfrom PIL import Image\\\\nfrom io import BytesIO\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\n\\\\nurl = \\\"<url>\\\"\\\\nr = requests.get(url)\\\\nim = Image.open(BytesIO(r.content))\\\\nmodel = ViTForImageClassification.from_pretrained(\\\\'nateraw/vit-age-classifier\\\\')\\\\ntransforms = ViTFeatureExtractor.from_pretrained(\\\\'nateraw/vit-age-classifier\\\\')\\\\ninputs = transforms(im, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**inputs)\\\\nproba = output.logits.softmax(1)\\\\npreds = proba.argmax(1)\\\\npredicted_age = preds.item()', 'api_call': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nateraw/vit-age-classifier', 'api_call': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/vit-age-classifier'}, 'python_environment_requirements': [['requests', 'PIL', 'transformers']], 'example_code': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom io import BytesIO\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\n\\\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\\\nim = Image.open(BytesIO(r.content))\\\\n\\\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\\\n\\\\ninputs = transforms(im, return_tensors='pt')\\\\noutput = model(**inputs)\\\\n\\\\nproba = output.logits.softmax(1)\\\\npreds = proba.argmax(1)\\\", 'performance': {'dataset': 'fairface', 'accuracy': None}, 'description': \\\"A vision transformer finetuned to classify the age of a given person's face.\\\" </s>\"\n"}
{"text": "<s>[INST]نطاق تقسيم الصور باستخدام رؤية الحاسوب. يحتاج عميلنا إلى تطوير برنامج يقوم بتقسيم الصور بناءً على المحتوى والمواضيع المعرفية. يتعين أن يدعم البرنامج استخدام تقنيات تحليل البيانات والذكاء الاصطناعي.? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import DetrForSegmentation, DetrFeatureExtractor\\\\nfrom PIL import Image\\\\nimport io, requests\\\\n\\\\nurl = 'your_image_url'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50-panoptic')\\\\nmodel = DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nprocessed_sizes = torch.as_tensor(inputs['pixel_values'].shape[-2:]).unsqueeze(0)\\\\nresult = feature_extractor.post_process_panoptic(outputs, processed_sizes)[0]\\\\npanoptic_seg = Image.open(io.BytesIO(result['png_string']))\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'facebook/detr-resnet-50-panoptic', 'api_call': \\\"from transformers import DetrForSegmentation, DetrFeatureExtractor\\\\nfrom PIL import Image\\\\nimport io, requests\\\\n\\\\nurl = 'your_image_url'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50-panoptic')\\\\nmodel = DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nprocessed_sizes = torch.as_tensor(inputs['pixel_values'].shape[-2:]).unsqueeze(0)\\\\nresult = feature_extractor.post_process_panoptic(outputs, processed_sizes)[0]\\\\npanoptic_seg = Image.open(io.BytesIO(result['png_string']))\\\", 'api_arguments': ['image'], 'python_environment_requirements': ['torch', 'numpy', 'transformers', 'PIL', 'requests', 'io'], 'example_code': ['import io', 'import requests', 'from PIL import Image', 'import torch', 'import numpy', 'from transformers import DetrFeatureExtractor, DetrForSegmentation', 'from transformers.models.detr.feature_extraction_detr import rgb_to_id', \\\"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\", 'image = Image.open(requests.get(url, stream=True).raw)', \\\"feature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50-panoptic')\\\", \\\"model = DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\\\", \\\"inputs = feature_extractor(images=image, return_tensors='pt')\\\", 'outputs = model(**inputs)', \\\"processed_sizes = torch.as_tensor(inputs['pixel_values'].shape[-2:]).unsqueeze(0)\\\", 'result = feature_extractor.post_process_panoptic(outputs, processed_sizes)[0]', \\\"panoptic_seg = Image.open(io.BytesIO(result['png_string']))\\\", 'panoptic_seg = numpy.array(panoptic_seg, dtype=numpy.uint8)', 'panoptic_seg_id = rgb_to_id(panoptic_seg)'], 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': {'box_AP': 38.8, 'segmentation_AP': 31.1, 'PQ': 43.4}}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطوير نموذج لإنشاء صور بدون شروط باستخدام تقنيات رؤية الحاسوب. هل يمكنك مساعدتي في تحقيق ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, install the 'diffusers' package using !pip install diffusers.\\\\n2. Then import the DiffusionPipeline from the diffusers package.\\\\n3. Load the 'google/ncsnpp-celebahq-256' model using the from_pretrained() method of DiffusionPipeline. This is a score-based generative model pre-trained on human faces (CelebA-HQ dataset) and specifically optimized for generating high-fidelity images of human faces.\\\\n4. Call the loaded model object to generate a new sample image.\\\\n5. Save the generated image as a PNG file to your local storage or display it directly.\\\", '<<<code>>>': \\\"!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = 'google/ncsnpp-celebahq-256'\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\n\\\\ndef generate_random_face():\\\\n    image = sde_ve()[0]\\\\n    image.save('sde_ve_generated_face.png')\\\", 'api_call': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/ncsnpp-celebahq-256', 'api_call': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\\\", 'api_arguments': {'model_id': 'google/ncsnpp-celebahq-256'}, 'python_environment_requirements': [['diffusers']], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = google/ncsnpp-celebahq-256\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[sample]\\\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.' </s>\"\n"}
{"text": "<s>[INST]عميلنا يقوم بتطوير تطبيق لإنشاء صور جديدة دون شروط بواسطة تقنية رؤية الحاسوب. نحن بحاجة إلى برنامج يمكنه تحليل الصور وتوليد صور بشكل تلقائي لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('google/ddpm-ema-cat-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, install the 'diffusers' package to use the DDPMPipeline class.\\\\n2. Import DDPMPipeline from the diffusers package.\\\\n3. Load the pre-trained DDPM-EMA cat generator model using the from_pretrained method and the provided model id 'google/ddpm-ema-cat-256'.\\\\n4. Generate an image of a cat using the loaded model by calling the loaded model.\\\\n5. Save the generated image to a file.\\\\n6. Using this code, you will be able to generate high-quality images of cats.\\\", '<<<code>>>': \\\"!pip install diffusers\\\\nfrom diffusers import DDPMPipeline\\\\nmodel_id = 'google/ddpm-ema-cat-256'\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save('ddpm_generated_image.png')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-ema-cat-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/ddpm-ema-cat-256', 'api_call': \\\"DDPMPipeline.from_pretrained('google/ddpm-ema-cat-256')\\\", 'api_arguments': ['model_id'], 'python_environment_requirements': [['!pip install diffusers']], 'example_code': 'from diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-ema-cat-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images, and supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. On the unconditional CIFAR10 dataset, it achieves an Inception score of 9.46 and a state-of-the-art FID score of 3.17.' </s>\"\n"}
{"text": "<s>[INST]يجب على المستخدم تحديد المدخلات النصية والخروجية المطلوبة لمهمة تحويل النص إلى نص من قبل نظام معالجة اللغة الطبيعية. ثم، يجب على المستخدم تنفيذ نموذج توليد النصوص الذي يفي بالمتطلبات المحددة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We import the required T5ForConditionalGeneration class for text-to-text generation from the transformers package.\\\\n2. Load the required pretrained model 'castorini/doc2query-t5-base-msmarco', which has been specifically trained on the MS MARCO dataset for generating search queries from document content.\\\\n3. Use the model to generate search queries by passing the article text as input, and setting the max_length parameter, which defines the maximum length of generated search queries.\\\", '<<<code>>>': \\\"from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\n\\\\nmodel = T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\\\\ntokenizer = T5Tokenizer.from_pretrained('castorini/doc2query-t5-base-msmarco')\\\\ninput_ids = tokenizer.encode('Article text here...', return_tensors='pt')\\\\noutputs = model.generate(input_ids, max_length=10, num_return_sequences=5)\\\\n\\\\ngenerated_queries = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\\\", 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'castorini/doc2query-t5-base-msmarco', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\\\", 'api_arguments': 'text, max_length', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'MS MARCO', 'accuracy': 'Not specified'}, 'description': 'A T5 model trained on the MS MARCO dataset for generating queries from documents.' </s>\"\n"}
{"text": "<s>[INST]يُشدد على فريق التعليم الآلي تعقب النص الصوتي لإجراء تحليل عميق للنصوص المسموعة وتحويلها إلى نصوص مسموعة بصوت طبيعي باستخدام واجهة تحويل النص إلى كلام.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and classes such as SpeechT5Processor, SpeechT5ForTextToSpeech, and SpeechT5HifiGan.\\\\n2. Load the pre-trained SpeechT5 model 'microsoft/speecht5_tts', which has been fine-tuned for text-to-speech tasks.\\\\n3. Use an appropriate processor to convert the input text to a format that the model can interpret.\\\\n4. Generate the synthesized speech for the given input text using the model and a suitable feature extractor.\\\\n5. Convert the generated output to an audio file that can be played back.\\\", '<<<code>>>': 'from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\\\\nimport soundfile as sf\\\\ntext = \\\"Hello, I am your AI voice assistant.\\\"\\\\nprocessor = SpeechT5Processor.from_pretrained(\\\\'microsoft/speecht5_tts\\\\')\\\\nmodel = SpeechT5ForTextToSpeech.from_pretrained(\\\\'microsoft/speecht5_tts\\\\')\\\\nvocoder = SpeechT5HifiGan.from_pretrained(\\\\'microsoft/speecht5_hifigan\\\\')\\\\ninputs = processor(text=text, return_tensors=\\\\'pt\\\\')\\\\nspeech = model.generate_speech(inputs[\\\\'input_ids\\\\'], speaker_embeddings=None, vocoder=vocoder)\\\\nsf.write(\\\\'response.wav\\\\', speech.numpy(), samplerate=16000)', 'api_call': \\\"SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/speecht5_tts', 'api_call': \\\"SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\\\", 'api_arguments': ['text', 'return_tensors', 'input_ids', 'speaker_embeddings', 'vocoder'], 'python_environment_requirements': ['!pip install git+https://github.com/huggingface/transformers sentencepiece datasets'], 'example_code': \\\"from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\nimport soundfile as sf\\\\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_tts')\\\\nmodel = SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\\\\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\\\\ninputs = processor(text='Hello, my dog is cute', return_tensors='pt')\\\\nembeddings_dataset = load_dataset('Matthijs/cmu-arctic-xvectors', split='validation')\\\\nspeaker_embeddings = torch.tensor(embeddings_dataset[7306]['xvector']).unsqueeze(0)\\\\nspeech = model.generate_speech(inputs['input_ids'], speaker_embeddings, vocoder=vocoder)\\\\nsf.write('speech.wav', speech.numpy(), samplerate=16000)\\\", 'performance': {'dataset': 'LibriTTS', 'accuracy': 'Not specified'}, 'description': 'SpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS. It is a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. It can be used for a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.' </s>\"\n"}
{"text": "<s>[INST]استخدم خدمة التعرف التلقائي على الكلام لتحويل النص التالي إلى نص مكتوب: \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary packages from the transformers and datasets library. This includes WhisperProcessor and WhisperForConditionalGeneration for the Automatic Speech Recognition (ASR) model.\\\\n2. We use the from_pretrained method to load the pre-trained model 'openai/whisper-large-v2'. This model has been trained to recognize and transcribe human speech to text.\\\\n3. We need to preprocess the audio using the WhisperProcessor. It'll take the raw audio file and decode the waveform into some input features that the model can use.\\\\n4. We forward the preprocessed audio to the WhisperForConditionalGeneration model.\\\\n5. The model returns the IDs of the predicted tokens, which we can convert back into text using the processor.\\\", '<<<code>>>': \\\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\\naudio_path = 'path/to/audio_file.wav' # replace with the path to your audio file\\\\ninput_features = processor.load_and_process_audio_file(audio_path, return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\\", 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai/whisper-large-v2', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", 'api_arguments': {'forced_decoder_ids': \\\"WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')\\\"}, 'python_environment_requirements': [['transformers', 'datasets']], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', \\\"processor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\\\", \\\"model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", 'model.config.forced_decoder_ids = None', \\\"ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')\\\", \\\"sample = ds[0]['audio']\\\", \\\"input_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\\\", 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)'], 'performance': {'dataset': 'LibriSpeech test-clean', 'accuracy': 3.0003583080317573}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتصميم مركبات ذاتية القيادة في مجال رؤية الحاسوب ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\\\", '<<<api_provider>>>': 'Keras', '<<<explanation>>>': \\\"1. Import the necessary libraries, such as Image from PIL, from_pretrained_keras from huggingface_hub, and TensorFlow, numpy as well as requests.\\\\n2. Load the blurry image using the Image.open() method provided by PIL. The image can be a file on disk, or it can be fetched from a URL.\\\\n3. Cast and resize the image as a TensorFlow tensor with the required dimensions (256, 256).\\\\n4. Import the deblurring model 'google/maxim-s3-deblurring-gopro' using the from_pretrained_keras method from Hugging Face.\\\\n5. Predict the deblurred image by calling the model.predict method with input image tensor.\\\", '<<<code>>>': 'from huggingface_hub import from_pretrained_keras\\\\nfrom PIL import Image\\\\nimport tensorflow as tf\\\\nimport numpy as np\\\\nimport requests\\\\n\\\\nurl = \\\\'image_path_or_url\\\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw if \\\"http\\\" in url else url)\\\\nimage = np.array(image)\\\\nimage = tf.convert_to_tensor(image)\\\\nimage = tf.image.resize(image, (256, 256))\\\\nmodel = from_pretrained_keras(\\\\'google/maxim-s3-deblurring-gopro\\\\')\\\\ndeblurred_image = model.predict(tf.expand_dims(image, 0))', 'api_call': \\\"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\\\", 'provider': 'Keras', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/maxim-s3-deblurring-gopro', 'api_call': \\\"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\\\", 'api_arguments': ['image'], 'python_environment_requirements': [['huggingface_hub', 'PIL', 'tensorflow', 'numpy', 'requests']], 'example_code': 'from huggingface_hub import from_pretrained_keras\\\\nfrom PIL import Image\\\\nimport tensorflow as tf\\\\nimport numpy as np\\\\nimport requests\\\\nurl = https://github.com/sayakpaul/maxim-tf/raw/main/images/Deblurring/input/1fromGOPR0950.png\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nimage = np.array(image)\\\\nimage = tf.convert_to_tensor(image)\\\\nimage = tf.image.resize(image, (256, 256))\\\\nmodel = from_pretrained_keras(google/maxim-s3-deblurring-gopro)\\\\npredictions = model.predict(tf.expand_dims(image, 0))', 'performance': {'dataset': 'GoPro', 'accuracy': {'PSNR': 32.86, 'SSIM': 0.961}}, 'description': 'MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]قم بتوليد تعليمات أو توجيهات لعملية معالجة اللغة الطبيعية Fill-Mask من خلال التفاعل مع نظام يستخدم هذه التقنية لملء الفجوات في النصوص بكلمات مناسبة، وتسجيل العملية على شكل فيديو توضح تطبيق هذه التقنية بشكل فعال ودقيق.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"AutoModel.from_pretrained('nlpaueb/legal-bert-small-uncased')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the AutoTokenizer and AutoModel classes from the transformers library provided by Hugging Face.\\\\n2. Load the tokenizer and model 'nlpaueb/legal-bert-small-uncased' using the from_pretrained() method for AutoTokenizer and AutoModel, respectively. This model is specifically tuned for legal documents, contracts, agreements, and rights.\\\\n3. Create an input sentence with a masked token to be filled. Use the tokenizer to tokenize the input sentence with the masked token.\\\\n4. Feed the tokenized input to the model to get a prediction. Retrieve the top predicted token(s) for the masked position.\\\\n5. Use the tokenizer to decode the predicted token(s) and obtain the completed sentence.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModel\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'nlpaueb/legal-bert-small-uncased\\\\')\\\\nmodel = AutoModel.from_pretrained(\\\\'nlpaueb/legal-bert-small-uncased\\\\')\\\\n\\\\nmasked_sentence = \\\"The party agrees to pay a penalty if the contract is not <mask> within a specified time.\\\"\\\\ntokens = tokenizer(masked_sentence, return_tensors=\\\\'pt\\\\')\\\\nprediction = model(**tokens)\\\\npredicted_index = prediction.logits.argmax(-1).item()\\\\npredicted_token = tokenizer.convert_ids_to_tokens(predicted_index)\\\\n\\\\nfilled_sentence = masked_sentence.replace(\\\\'<mask>\\\\', predicted_token)', 'api_call': \\\"AutoModel.from_pretrained('nlpaueb/legal-bert-small-uncased')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nlpaueb/legal-bert-small-uncased', 'api_call': \\\"AutoModel.from_pretrained('nlpaueb/legal-bert-small-uncased')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nlpaueb/legal-bert-small-uncased'}, 'python_environment_requirements': [{'transformers': 'AutoTokenizer, AutoModel'}], 'example_code': \\\"from transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-small-uncased')\\\\nmodel = AutoModel.from_pretrained('nlpaueb/legal-bert-small-uncased')\\\", 'performance': {'dataset': 'Legal Corpora', 'accuracy': 'Comparable to larger models'}, 'description': 'LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications. This is the light-weight version of BERT-BASE (33% the size of BERT-BASE) pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint.' </s>\"\n"}
{"text": "<s>[INST]يتطلب تدريب نموذج الرؤية الحاسوبية على تقسيم الصور إلى أجزاء مختلفة. يجب عليك استخدام بيانات الصور الموجودة بشكل مسبق لتدريب النموذج. يجب تقديم الصور كإدخال لنموذج التعلم الآلي لتعلم كيفية تقسيم الصور بدقة. اختبر النموذج بعد التدريب باستخدام صور جديدة للتحقق من كفاءته في تقسيم الصور بدقة وفاعلية.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"'OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)'\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, including classes from the transformers package and Image from the PIL package. This includes OneFormerForUniversalSegmentation for the segmentation model and Image for processing image data.\\\\n2. Use the from_pretrained method of the OneFormerForUniversalSegmentation class to load the pre-trained model 'shi-labs/oneformer_coco_swin_large'. This model has been trained for multiple segmentation tasks, which is exactly what we need for identifying road signs, pedestrians, and vehicles in images.\\\\n3. Load the image data from a file or acquire it in real-time from the autonomous vehicle's camera.\\\\n4. This model can then be used to perform universal image segmentation upon the provided image, resulting in the identification of various objects in the image.\\\", '<<<code>>>': \\\"from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimage = Image.open('image_path.jpg') # replace 'image_path.jpg' with path to your image\\\\nprocessor = OneFormerProcessor.from_pretrained('shi-labs/oneformer_coco_swin_large')\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_coco_swin_large')\\\\nsemantic_inputs = processor(images=image, task_inputs=['semantic'], return_tensors='pt')\\\\nsemantic_outputs = model(**semantic_inputs)\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\", 'api_call': \\\"'OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)'\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'shi-labs/oneformer_coco_swin_large', 'api_call': \\\"'OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)'\\\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/coco.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_coco_swin_large)\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)\\\\n\\\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\\\nsemantic_outputs = model(**semantic_inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ydshieh/coco_dataset_script', 'accuracy': 'Not provided'}, 'description': 'OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>\"\n"}
{"text": "<s>[INST]يشمل التحدي في مجال الرؤية الحاسوبية من الصورة إلى الصورة مهمة جديدة. لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Load the required libraries and utilities such as torch, Image from PIL, MLSDdetector from controlnet_aux, and functions from diffusers.\\\\n2. Set the checkpoint and prompt information as needed.\\\\n3. Load the source image from the provided URL and your prompt text; then, process the image using MLSDdetector.\\\\n4. Create a ControlNetModel by loading the pretrained model from the checkpoint while specifying the desired torch_dtype, which is torch.float16.\\\\n5. Create a diffusion pipeline by loading a pretrained StableDiffusionControlNetPipeline with the controlnet model, torch_dtype, and any other required configurations.\\\\n6. Generate the image using the pipeline and the provided prompt, and then save the generated image to a specified output file.', '<<<code>>>': 'import torch\\\\nfrom PIL import Image\\\\nfrom controlnet_aux import MLSDdetector\\\\nfrom diffusers import ControlNetModel, StableDiffusionControlNetPipeline, UniPCMultistepScheduler\\\\nfrom diffusers.utils import load_image\\\\n\\\\ncheckpoint = \\\\'lllyasviel/control_v11p_sd15_mlsd\\\\'\\\\nimage_url = \\\"https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\\\"\\\\nprompt = \\\"royal chamber with fancy bed\\\"\\\\n\\\\nimage = load_image(image_url)\\\\nprocessor = MLSDdetector.from_pretrained(\\\\'lllyasviel/ControlNet\\\\')\\\\ncontrol_image = processor(image)\\\\ncontrol_image.save(\\\\'./images/control.png\\\\')\\\\n\\\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n    \\\\'runwayml/stable-diffusion-v1-5\\\\', controlnet=controlnet, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(0)\\\\n\\\\ngenerated_image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\ngenerated_image.save(\\\\'images/image_out.png\\\\')', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/control_v11p_sd15_mlsd', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\\\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate', 'controlnet_aux']], 'example_code': \\\"import torch\\\\nimport os\\\\nfrom huggingface_hub import HfApi\\\\nfrom pathlib import Path\\\\nfrom diffusers.utils import load_image\\\\nfrom PIL import Image\\\\nimport numpy as np\\\\nfrom controlnet_aux import MLSDdetector\\\\nfrom diffusers import (\\\\n ControlNetModel,\\\\n StableDiffusionControlNetPipeline,\\\\n UniPCMultistepScheduler,\\\\n)\\\\ncheckpoint = lllyasviel/control_v11p_sd15_mlsd\\\\nimage = load_image(\\\\n https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\\\\n)\\\\nprompt = royal chamber with fancy bed\\\\nprocessor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\\\ncontrol_image = processor(image)\\\\ncontrol_image.save(./images/control.png)\\\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(0)\\\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\nimage.save('images/image_out.png')\\\", 'performance': {'dataset': 'MLSD', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج لتوليد أسئلة وإجابات ذات صلة بالجداول باستخدام اللغة العربية بتقنيات معالجة اللغات الطبيعية. يجب أن تبدأ الأسئلة المولدة بعبارة \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the TapasTokenizer and TapasForQuestionAnswering classes from the transformers library.', 'code': 'from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\n\\\\nmodel = TapasForQuestionAnswering.from_pretrained(\\\\'google/tapas-base-finetuned-wtq\\\\')\\\\ntokenizer = TapasTokenizer.from_pretrained(\\\\'google/tapas-base-finetuned-wtq\\\\')\\\\n\\\\ntable_data = \\\"\\\"\\\"\\\\nTeam | Goals | Assists\\\\n-------|-------|---------\\\\nTeam A | 5     | 3\\\\nTeam B | 2     | 1\\\\n\\\"\\\"\\\"\\\\n\\\\nquestions = [\\\\n    \\\"Which team scored more goals?\\\",\\\\n    \\\"How many assists were there in total?\\\",\\\\n]\\\\n\\\\nfor question in questions:\\\\n    inputs = tokenizer(question, table_data, return_tensors=\\\"pt\\\")\\\\n    outputs = model(**inputs)\\\\n    answer = tokenizer.convert_logits_to_predictions(inputs, outputs.logits)\\\\n    print(f\\\"Question: {question}\\\\\\\\nAnswer: {answer[0]}\\\\\\\\n\\\")', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-base-finetuned-wtq', 'api_call': 'from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\n\\\\nmodel = TapasForQuestionAnswering.from_pretrained(\\\\'google/tapas-base-finetuned-wtq\\\\')\\\\ntokenizer = TapasTokenizer.from_pretrained(\\\\'google/tapas-base-finetuned-wtq\\\\')\\\\n\\\\ntable_data = \\\"\\\"\\\"\\\\nTeam | Goals | Assists\\\\n-------|-------|---------\\\\nTeam A | 5     | 3\\\\nTeam B | 2     | 1\\\\n\\\"\\\"\\\"\\\\n\\\\nquestions = [\\\\n    \\\"Which team scored more goals?\\\",\\\\n    \\\"How many assists were there in total?\\\",\\\\n]\\\\n\\\\nfor question in questions:\\\\n    inputs = tokenizer(question, table_data, return_tensors=\\\"pt\\\")\\\\n    outputs = model(**inputs)\\\\n    answer = tokenizer.convert_logits_to_predictions(inputs, outputs.logits)\\\\n    print(f\\\"Question: {question}\\\\\\\\nAnswer: {answer[0]}\\\\\\\\n\\\")', 'api_arguments': {'model_name': 'google/tapas-base-finetuned-wtq'}, 'python_environment_requirements': {'transformers': '4.12.0'}, 'example_code': \\\"from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\n\\\\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')\\\\ntokenizer = TapasTokenizer.from_pretrained('google/tapas-base-finetuned-wtq')\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.4638}, 'description': 'TAPAS base model fine-tuned on WikiTable Questions (WTQ). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion, and then fine-tuned on SQA, WikiSQL, and finally WTQ. It can be used for answering questions related to a table.'}} </s>\"\n"}
{"text": "<s>[INST]أنشئ جدول تصنيفي يحتوي على بيانات عناصر اللغة الطبيعية للترجمة، مثل الكلمات والعبارات والجمل، ثم بعد ذلك قم بتصنيفها حسب تصنيف الرموز الطبيعية لمعالجة اللغات: [قائمة بعناصر أو كائنات تحتوي على مجموعة متنوعة من الخصائص]? [/INST]{'domain': 'Natural Language Processing Token Classification', 'api_call': \\\"SequenceTagger.load('flair/pos-english')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load(\\\\'flair/pos-english\\\\')\\\\nsentence = Sentence(\\\"Your english sentence\\\")\\\\ntagger.predict(sentence)\\\\nfor entity in sentence.get_spans(\\\\'pos\\\\'):\\\\n    print(entity)', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Part-of-Speech Tagging', 'api_name': 'flair/pos-english', 'api_call': 'from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load(\\\\'flair/pos-english\\\\')\\\\nsentence = Sentence(\\\"Your english sentence\\\")\\\\ntagger.predict(sentence)\\\\nfor entity in sentence.get_spans(\\\\'pos\\\\'):\\\\n    print(entity)', 'api_arguments': 'sentence', 'python_environment_requirements': 'flair (pip install flair)', 'example_code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\ntagger = SequenceTagger.load(flair/pos-english)\\\\n\\\\nsentence = Sentence(I love Berlin.)\\\\n\\\\ntagger.predict(sentence)\\\\n\\\\nprint(sentence)\\\\n\\\\nfor entity in sentence.get_spans('pos'):\\\\n print(entity)\\\", 'performance': {'dataset': 'Ontonotes', 'accuracy': '98.19'}, 'description': 'This is the standard part-of-speech tagging model for English that ships with Flair. It predicts fine-grained POS tags based on Flair embeddings and LSTM-CRF.'}} </s>\"\n"}
{"text": "<s>[INST]يرجى تصميم نموذج يقوم بتحويل الصوت إلى نص لتسهيل استخدامه في مجال معالجة اللغة الطبيعية والإجابة عن أسئلة عن جداول البيانات.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-1024shot')\\\", '<<<api_provider>>>': 'PyTorch Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'AutoTokenizer', 'AutoModelForSeq2SeqLM' from transformers, and 'pandas' for working with the data.\\\\n2. Load the table-based QA model 'neulab/omnitab-large-1024shot', which has been pretrained for few-shot table-based question answering.\\\\n3. Create the data as a pandas DataFrame containing the electronic devices information.\\\\n4. Specify the query, which will be used to compare the devices and find out the one with the highest battery capacity.\\\\n5. Encode the table and query using the tokenizer and generate the answer using the model.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport pandas as pd\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'neulab/omnitab-large-1024shot\\\\')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'neulab/omnitab-large-1024shot\\\\')\\\\n\\\\ndata = {\\\\n    \\\\'Device\\\\': [\\\\'Device1\\\\', \\\\'Device2\\\\', \\\\'Device3\\\\'],\\\\n    \\\\'Battery\\\\': [4000, 5000, 4500]\\\\n}\\\\ntable = pd.DataFrame(data)\\\\nquery = \\\"Which device has the highest battery capacity?\\\"\\\\n\\\\nencoding = tokenizer(table=table, query=query, return_tensors=\\\\'pt\\\\')\\\\noutputs = model.generate(**encoding)\\\\nanswer = tokenizer.batch_decode(outputs, skip_special_tokens=True)\\\\n\\\\nprint(answer)', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-1024shot')\\\", 'provider': 'PyTorch Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'neulab/omnitab-large-1024shot', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-1024shot')\\\", 'api_arguments': {'table': 'pd.DataFrame.from_dict(data)', 'query': 'str'}, 'python_environment_requirements': [['transformers', 'pandas']], 'example_code': 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport pandas as pd\\\\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-1024shot)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-1024shot)\\\\ndata = {\\\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\\\n city: [athens, paris, st. louis, athens, beijing, london]\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = In which year did beijing host the Olympic Games?\\\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))', 'performance': {'dataset': 'wikitablequestions', 'accuracy': 'Not provided'}, 'description': 'OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. neulab/omnitab-large-1024shot (based on BART architecture) is initialized with microsoft/tapex-large and continuously pretrained on natural and synthetic data (SQL2NL model trained in the 1024-shot setting).' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير تقنيات لاستخراج الميزات متعددة الوسائط، ونريد التقدم في البحث والتنمية لتحسين أداء النظم الذكية القائمة على التعلم الآلي.? [/INST]'<<<domain>>>': 'Multimodal Feature Extraction', '<<<api_call>>>': \\\"AutoModel.from_pretrained('microsoft/unixcoder-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries 'AutoTokenizer' and 'AutoModel' from the transformers package.\\\\n2. Create a tokenizer using the 'AutoTokenizer.from_pretrained()' function with the model name 'microsoft/unixcoder-base'.\\\\n3. Load the 'microsoft/unixcoder-base' model using 'AutoModel.from_pretrained()'.\\\\n4. Tokenize the code comments and abstract syntax trees (AST) using the tokenizer object created earlier.\\\\n5. Use the model to generate embeddings and feature representation of the code comments and AST.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'microsoft/unixcoder-base\\\\')\\\\nmodel = AutoModel.from_pretrained(\\\\'microsoft/unixcoder-base\\\\')\\\\ninputs = tokenizer([\\\"code comment here\\\", \\\"AST representation here\\\"], return_tensors=\\\"pt\\\", padding=True, truncation=True)\\\\noutputs = model(**inputs)\\\\nfeatures = outputs[0]', 'api_call': \\\"AutoModel.from_pretrained('microsoft/unixcoder-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/unixcoder-base', 'api_call': \\\"AutoModel.from_pretrained('microsoft/unixcoder-base')\\\", 'api_arguments': {'tokenizer': \\\"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\\"}, 'python_environment_requirements': [{'transformers': 'from transformers import AutoTokenizer, AutoModel'}], 'example_code': \\\"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\\\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>\"\n"}
{"text": "<s>[INST]عندما تأتون إلى ميدان اللعب الافتراضي، استخدموا أداة التحكم المتعددة الوضعيات للاندماج بين النص والصورة. قوموا بإنشاء رسومات تعبيرية تعكس النص المقروء باستخدام امكانيات الرسم والتلوين المختلفة.? [/INST]{'domain': 'Multimodal Text-to-Image', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0', torch_dtype=torch.float16)\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': \\\"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = 'andite/anything-v4.0'\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to('cuda')\\\\nprompt = 'futuristic city landscape'\\\\nimage = pipe(prompt).images[0]\\\\nimage.save('futuristic_city_landscape.png')\\\", 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \\\"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = 'andite/anything-v4.0'\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to('cuda')\\\\nprompt = 'futuristic city landscape'\\\\nimage = pipe(prompt).images[0]\\\\nimage.save('futuristic_city_landscape.png')\\\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.'}} </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى تطوير برنامج لتحويل الملفات الصوتية إلى تسجيلات صوتية بجودة عالية. يجب أن يكون البرنامج قادرًا على معالجة تنسيقات الصوت المختلفة وتحسين الجودة بشكل فعال.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"pipeline('audio-to-audio', model='JorisCos/DPTNet_Libri1Mix_enhsingle_16k')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the required pipeline function from the transformers library provided by Hugging Face.\\\\n2. Utilize the pipeline function to create an audio-to-audio model, which is designed to enhance the quality of audio files.\\\\n3. Specify the model \\\\'JorisCos/DPTNet_Libri1Mix_enhsingle_16k\\\\' to be loaded. This model is trained for the \\\"enh_single\\\" task of the Libri1Mix dataset, which involves enhancing audio quality for files with multiple speakers.\\\\n4. The created audio quality enhancement model can be used to process audio files and produce clean audio files with enhanced quality.', '<<<code>>>': \\\"from transformers import pipeline\\\\naudio_enhancer = pipeline('audio-to-audio', model='JorisCos/DPTNet_Libri1Mix_enhsingle_16k')\\\\nenhanced_audio = audio_enhancer(input_audio_file)\\\", 'api_call': \\\"pipeline('audio-to-audio', model='JorisCos/DPTNet_Libri1Mix_enhsingle_16k')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'JorisCos/DPTNet_Libri1Mix_enhsingle_16k', 'api_call': \\\"pipeline('audio-to-audio', model='JorisCos/DPTNet_Libri1Mix_enhsingle_16k')\\\", 'api_arguments': {'model': 'JorisCos/DPTNet_Libri1Mix_enhsingle_16k'}, 'python_environment_requirements': [['transformers', 'asteroid']], 'example_code': '', 'performance': {'dataset': 'Libri1Mix', 'si_sdr': 14.829670037349064, 'si_sdr_imp': 11.379888731489366, 'sdr': 15.395712644737149, 'sdr_imp': 11.893049845524112, 'sir': 'Infinity', 'sir_imp': 'NaN', 'sar': 15.395712644737149, 'sar_imp': 11.893049845524112, 'stoi': 0.9301948391058859, 'stoi_imp': 0.13427501556534832}, 'description': 'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نظام يستخدم معالجة اللغة الطبيعية والرؤية الحاسوبية لتحويل الصور إلى نصوص، حيث يمكن توليد وصف نصي دقيق لمحتوى الصور بشكل متعدد الوسائط، واستخدم مجموعة بيانات متعددة وكبيرة لتحقيق أداء متميز ودقيق.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the Pix2StructForConditionalGeneration and Pix2StructProcessor classes from the transformers library provided by Hugging Face.\\\\n2. We load the pretrained model 'google/pix2struct-base' that is specially designed for generating textual descriptions for images. This model has been trained on various tasks, including image captioning.\\\\n3. Using the model and the processor, we can convert the input image data into a textual description.\\\\n4. The generated description can be used as an alternative text for images on websites to provide better accessibility to users with visual impairments.\\\", '<<<code>>>': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(\\\\'google/pix2struct-base\\\\')\\\\nprocessor = Pix2StructProcessor.from_pretrained(\\\\'google/pix2struct-base\\\\')\\\\ninputs = processor(images=input_image, return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(**inputs)\\\\ndescription = processor.batch_decode(outputs, skip_special_tokens=True)[0]', 'api_call': \\\"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/pix2struct-base', 'api_call': \\\"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\\\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': [{'transformers': '4.15.0', 'torch': '1.10.1'}], 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نظام لتصنيف النصوص في معالجة اللغة الطبيعية بدقة عالية. نحتاج إلى برنامج يستطيع تصنيف النصوص تلقائياً دون الحاجة لتدخل يدوي من الخارج.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include DistilBertTokenizer and DistilBertForSequenceClassification from the transformers package.\\\\n2. Use the from_pretrained method to load both the tokenizer and the pretrained model with the given model name 'distilbert-base-uncased-finetuned-sst-2-english'. This model is specifically trained to predict sentiment for binary sentiment classification.\\\\n3. Given a movie review, use the tokenizer to convert it into a set of tokens that are input to the model.\\\\n4. The model's output logits can be used to determine the predicted class label, which can be either positive or negative sentiment.\\\\n5. Classifying movie reviews this way allows us to sort movies based on user preferences.\\\", '<<<code>>>': \\\"from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\\\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\\\\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\\\\ninputs = tokenizer('Movie review text here...', return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_class_id = logits.argmax().item()\\\\nsentiment_label = model.config.id2label[predicted_class_id]\\\", 'api_call': \\\"DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilbert-base-uncased-finetuned-sst-2-english', 'api_call': \\\"DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\\\", 'api_arguments': ['inputs'], 'python_environment_requirements': [['torch', 'transformers']], 'example_code': \\\"import torch\\\\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\\\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\\\\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\\\\ninputs = tokenizer('Hello, my dog is cute', return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_class_id = logits.argmax().item()\\\\nmodel.config.id2label[predicted_class_id]\\\", 'performance': {'dataset': 'glue', 'accuracy': 0.911}, 'description': 'This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.' </s>\"\n"}
{"text": "<s>[INST]قم بتدريب نموذج التعلم الآلي على تصنيف الصور باستخدام البيانات المتاحة. ثم قم بتقييم أداء النموذج على مجموعة اختبار من الصور وقياس دقته. يمكنك استخدام الطرق المختلفة في رؤية الحاسوب لتحسين أداء النموذج.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries for working with images and loading the pretrained model. This includes the 'timm' library, which makes it easy to load the model, and the 'Image' class from PIL for working with image data.\\\\n2. Use the 'timm.create_model()' function to load the pretrained 'convnextv2_huge.fcmae_ft_in1k' model. This model has been trained on a wide range of image classes and should be able to identify various types of food in images.\\\\n3. Load an image, which could be taken by the robot's camera or from a file.\\\\n4. Apply any necessary preprocessing steps to the image, set the model to evaluation mode, and then use the model to classify the image.\\\\n5. Extract the top 5 most probable class indices and their corresponding class probabilities.\\\", '<<<code>>>': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimg = Image.open(urlopen('https://some_host.com/your_image_url_here.jpg'))\\\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\", 'api_call': \\\"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'convnextv2_huge.fcmae_ft_in1k', 'api_call': \\\"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': [['timm']], 'example_code': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimg = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))\\\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\\\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 86.256}, 'description': 'A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k.' </s>\"\n"}
{"text": "<s>[INST]أنا متخصص في تمثيل قدرات لغوية. هل تحتاج إلى مساعدة في تحليل الصور المصنفة من دون استخدام الإشارات؟? [/INST]{'domain': 'Computer Vision Zero-Shot Image Classification', 'api_call': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import ChineseCLIPModel, ChineseCLIPProcessor\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = \\\\'https://example.com/product_image.jpg\\\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nmodel = ChineseCLIPModel.from_pretrained(\\\\'OFA-Sys/chinese-clip-vit-large-patch14\\\\')\\\\nprocessor = ChineseCLIPProcessor.from_pretrained(\\\\'OFA-Sys/chinese-clip-vit-large-patch14\\\\')\\\\ntexts = [\\\"电子设备\\\", \\\"服装\\\", \\\"厨房用品\\\", \\\"汽车配件\\\"]\\\\ninputs = processor(text=texts, images=image, return_tensors=\\\\'pt\\\\', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Image Classification', 'api_name': 'chinese-clip-vit-large-patch14', 'api_call': 'from transformers import ChineseCLIPModel, ChineseCLIPProcessor\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = \\\\'https://example.com/product_image.jpg\\\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nmodel = ChineseCLIPModel.from_pretrained(\\\\'OFA-Sys/chinese-clip-vit-large-patch14\\\\')\\\\nprocessor = ChineseCLIPProcessor.from_pretrained(\\\\'OFA-Sys/chinese-clip-vit-large-patch14\\\\')\\\\ntexts = [\\\"电子设备\\\", \\\"服装\\\", \\\"厨房用品\\\", \\\"汽车配件\\\"]\\\\ninputs = processor(text=texts, images=image, return_tensors=\\\\'pt\\\\', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)', 'api_arguments': {'model_name': 'OFA-Sys/chinese-clip-vit-large-patch14'}, 'python_environment_requirements': {'libraries': ['transformers', 'PIL', 'requests']}, 'example_code': 'from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\\\\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\\\\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = []\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nimage_features = model.get_image_features(**inputs)\\\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True) # normalize\\\\ninputs = processor(text=texts, padding=True, return_tensors=pt)\\\\ntext_features = model.get_text_features(**inputs)\\\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True) # normalize\\\\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\\\\nprobs = logits_per_image.softmax(dim=1) # probs: [[0.0066, 0.0211, 0.0031, 0.9692]]', 'performance': {'dataset': 'MUGE Text-to-Image Retrieval, Flickr30K-CN Retrieval, COCO-CN Retrieval, CIFAR10, CIFAR100, DTD, EuroSAT, FER, FGV, KITTI, MNIST, PASCAL VOC', 'accuracy': 'Varies depending on the dataset'}, 'description': 'Chinese-CLIP-ViT-Large-Patch14 is a large version of the Chinese CLIP model, with ViT-L/14 as the image encoder and RoBERTa-wwm-base as the text encoder. Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It is designed for zero-shot image classification tasks.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج لتحويل النصوص صوتية إلى نصوص مكتوبة بشكل تلقائي باستخدام تقنية التعرف على الكلام الآلي.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('nguyenvulebinh/wav2vec2-base-vietnamese-250h')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We import the necessary classes from the transformers and soundfile packages. This includes the Wav2Vec2Processor for processing audio data and the Wav2Vec2ForCTC for the speech recognition model.', 'code': 'from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nimport soundfile as sf\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained(\\\\'nguyenvulebinh/wav2vec2-base-vietnamese-250h\\\\')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\\\'nguyenvulebinh/wav2vec2-base-vietnamese-250h\\\\')\\\\n\\\\n# Replace \\\\'audio_path.wav\\\\' with the location of your audio file\\\\nspeech, _ = sf.read(\\\\'audio_path.wav\\\\')\\\\ninput_values = processor(speech, return_tensors=\\\"pt\\\").input_values\\\\nlogits = model(input_values).logits\\\\n\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'nguyenvulebinh/wav2vec2-base-vietnamese-250h', 'api_call': 'from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nimport soundfile as sf\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained(\\\\'nguyenvulebinh/wav2vec2-base-vietnamese-250h\\\\')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\\\'nguyenvulebinh/wav2vec2-base-vietnamese-250h\\\\')\\\\n\\\\n# Replace \\\\'audio_path.wav\\\\' with the location of your audio file\\\\nspeech, _ = sf.read(\\\\'audio_path.wav\\\\')\\\\ninput_values = processor(speech, return_tensors=\\\"pt\\\").input_values\\\\nlogits = model(input_values).logits\\\\n\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'api_arguments': ['file'], 'python_environment_requirements': ['transformers', 'datasets', 'soundfile', 'torch'], 'example_code': \\\"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nfrom datasets import load_dataset\\\\nimport soundfile as sf\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained(nguyenvulebinh/wav2vec2-base-vietnamese-250h)\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(nguyenvulebinh/wav2vec2-base-vietnamese-250h)\\\\n\\\\ndef map_to_array(batch):\\\\n speech, _ = sf.read(batch[file])\\\\n batch[speech] = speech\\\\n return batch\\\\n\\\\nds = map_to_array({\\\\n file: 'audio-test/t1_0001-00010.wav'\\\\n})\\\\n\\\\ninput_values = processor(ds[speech], return_tensors=pt, padding=longest).input_values\\\\n\\\\nlogits = model(input_values).logits\\\\n\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'performance': {'dataset': [{'name': 'VIVOS', 'accuracy': 6.15}, {'name': 'Common Voice vi', 'accuracy': 11.52}]}, 'description': 'Vietnamese end-to-end speech recognition using wav2vec 2.0. Pre-trained on 13k hours of Vietnamese youtube audio (un-label data) and fine-tuned on 250 hours labeled of VLSP ASR dataset on 16kHz sampled speech audio.'}} </s>\"\n"}
{"text": "<s>[INST]قُم بتحميل مجموعة بيانات تحتوي على عينات صوتية لمهمة تحويل الصوت إلى صوت.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': 'ConvTasNet_Libri3Mix_sepclean_8k()', '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': 'ConvTasNet_Libri3Mix_sepclean_8k()', 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ConvTasNet_Libri3Mix_sepclean_8k', 'api_call': 'ConvTasNet_Libri3Mix_sepclean_8k()', 'api_arguments': {'n_src': 3, 'sample_rate': 8000, 'segment': 3, 'task': 'sep_clean', 'train_dir': 'data/wav8k/min/train-360', 'valid_dir': 'data/wav8k/min/dev', 'kernel_size': 16, 'n_filters': 512, 'stride': 8, 'bn_chan': 128, 'hid_chan': 512, 'mask_act': 'relu', 'n_blocks': 8, 'n_repeats': 3, 'skip_chan': 128, 'lr': 0.001, 'optimizer': 'adam', 'weight_decay': 0.0, 'batch_size': 24, 'early_stop': True, 'epochs': 200, 'half_lr': True, 'num_workers': 4}, 'python_environment_requirements': ['Asteroid'], 'example_code': '', 'performance': {'dataset': 'Libri3Mix', 'accuracy': {'si_sdr': 8.581797049575108, 'si_sdr_imp': 11.977037288467368, 'sdr': 9.305885208641385, 'sdr_imp': 12.3943409734845, 'sir': 16.42030534048559, 'sir_imp': 19.508759460400984, 'sar': 10.641943911079238, 'sar_imp': -56.4345187842095, 'stoi': 0.8365148408724333, 'stoi_imp': 0.24401766199806396}}, 'description': 'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri3Mix dataset.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء واجهة تستخدم خوارزميات التعلم العميق لتحويل الصور إلى نصوص ووصف الحالات المتعددة في تصنيف الفيديو. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries from transformers, such as AutoImageProcessor and TimesformerForVideoClassification.\\\\n2. Load the pretrained model 'facebook/timesformer-base-finetuned-k600' using TimesformerForVideoClassification.from_pretrained() method.\\\\n3. Process the video frames using AutoImageProcessor and create a torch tensor of the frames.\\\\n4. Pass the processed video tensor into the model to get the logits.\\\\n5. Find the predicted class index with the highest probability in the logits.\\\\n6. Finally, use the predicted class index to obtain the label of the action that is classified by the model.\\\", '<<<code>>>': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(\\\\'facebook/timesformer-base-finetuned-k600\\\\')\\\\nmodel = TimesformerForVideoClassification.from_pretrained(\\\\'facebook/timesformer-base-finetuned-k600\\\\')\\\\ninputs = processor(images=video, return_tensors=\\\\'pt\\\\')\\\\n\\\\n with torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n predicted_class_idx = logits.argmax(-1).item()\\\\n print(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/timesformer-base-finetuned-k600', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\\\", 'api_arguments': ['images'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k600)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k600)\\\\ninputs = processor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-600', 'accuracy': None}, 'description': 'TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]يجب تصميم نموذج لمعالجة اللغة الطبيعية للإجابة على الأسئلة المتعلقة بالجداول، وهو أمر حاسم لتحسين الأداء ودقة النظام.? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('google/tapas-large-finetuned-sqa')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': \\\"from transformers import TapasForQuestionAnswering\\\\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-large-finetuned-sqa')\\\\n# Use the model to answer questions based on the provided financial data\\\\n# Replace `financial_data_table` and `user_question` with the relevant input\\\\nanswer = model(question=user_question, table=financial_data_table)\\\", 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-large-finetuned-sqa', 'api_call': \\\"from transformers import TapasForQuestionAnswering\\\\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-large-finetuned-sqa')\\\\n# Use the model to answer questions based on the provided financial data\\\\n# Replace `financial_data_table` and `user_question` with the relevant input\\\\nanswer = model(question=user_question, table=financial_data_table)\\\", 'api_arguments': ['question', 'table'], 'python_environment_requirements': ['transformers'], 'example_code': 'https://huggingface.co/google/tapas-large-finetuned-sqa', 'performance': {'dataset': 'msr_sqa', 'accuracy': 0.7289}, 'description': 'TAPAS large model fine-tuned on Sequential Question Answering (SQA). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned on SQA. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).'}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء سؤال مكتوب باللغة العربية واستخدام تسمية الصورة للحصول على الإجابة باستخدام الترجمة متعددة الوسائط.? [/INST]{'domain': 'Multimodal Visual Question Answering', 'api_call': \\\"ViltModel.from_pretrained('tufa15nik/vilt-finetuned-vqasi')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import ViltModel, ViltTokenizer\\\\nmodel = ViltModel.from_pretrained(\\\\'tufa15nik/vilt-finetuned-vqasi\\\\')\\\\ntokenizer = ViltTokenizer.from_pretrained(\\\\'tufa15nik/vilt-finetuned-vqasi\\\\')\\\\nquestion = \\\"<insert_question_here>\\\"\\\\ninputs = tokenizer(question, return_tensors=\\\"pt\\\")\\\\nimage_inputs = {\\\"image\\\": image}  # Load image content\\\\noutputs = model([inputs, image_inputs], return_dict=True)\\\\nanswer = outputs[\\\"answer\\\"][0]', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'vilt-finetuned-vqasi', 'api_call': 'from transformers import ViltModel, ViltTokenizer\\\\nmodel = ViltModel.from_pretrained(\\\\'tufa15nik/vilt-finetuned-vqasi\\\\')\\\\ntokenizer = ViltTokenizer.from_pretrained(\\\\'tufa15nik/vilt-finetuned-vqasi\\\\')\\\\nquestion = \\\"<insert_question_here>\\\"\\\\ninputs = tokenizer(question, return_tensors=\\\"pt\\\")\\\\nimage_inputs = {\\\"image\\\": image}  # Load image content\\\\noutputs = model([inputs, image_inputs], return_dict=True)\\\\nanswer = outputs[\\\"answer\\\"][0]', 'api_arguments': {'model': 'tufa15nik/vilt-finetuned-vqasi', 'tokenizer': 'tufa15nik/vilt-finetuned-vqasi'}, 'python_environment_requirements': {'transformers': '>=4.11.3'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the VQASI dataset by tufa15nik using the ViLT architecture. The model is designed to answer questions based on the content of an input image.'}} </s>\"\n"}
{"text": "<s>[INST]العميل يرغب في تطوير نظام تعرف الصوت التلقائي باستخدام الحواسيب. نحن بحاجة إلى برنامج يمكنه تحويل النصوص الصوتية إلى نصوص مكتوبة بدقة وفعالية.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\", 'api_provider': 'PyTorch Transformers', 'explanation': '1. First, import the necessary classes from the transformers package: Wav2Vec2Processor and Data2VecForCTC.', 'code': \\\"from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/data2vec-audio-base-960h')\\\\nmodel = Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\\ninput_values = processor(conference_audio, return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'PyTorch Transformers', 'functionality': 'Automatic Speech Recognition', 'api_name': 'data2vec-audio-base-960h', 'api_call': \\\"from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/data2vec-audio-base-960h')\\\\nmodel = Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\\ninput_values = processor(conference_audio, return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers', 'datasets', 'torch', 'jiwer'], 'example_code': 'from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\n# load model and processor\\\\nprocessor = Wav2Vec2Processor.from_pretrained(facebook/data2vec-audio-base-960h)\\\\nmodel = Data2VecForCTC.from_pretrained(facebook/data2vec-audio-base-960h)\\\\n\\\\n# load dummy dataset and read soundfiles\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\n\\\\n# tokenize\\\\ninput_values = processor(ds[0][audio][array],, return_tensors=pt, padding=longest).input_values # Batch size 1\\\\n\\\\n# retrieve logits\\\\nlogits = model(input_values).logits\\\\n\\\\n# take argmax and decode\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'performance': {'dataset': 'librispeech_asr', 'accuracy': {'clean': 2.77, 'other': 7.08}}, 'description': \\\"Facebook's Data2Vec-Audio-Base-960h model is an Automatic Speech Recognition model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It can be used for transcribing audio files and achieves competitive performance on major benchmarks of speech recognition. The model is based on the Data2Vec framework which uses the same learning method for either speech, NLP, or computer vision.\\\"}} </s>\"\n"}
{"text": "<s>[INST]نقوم بإنشاء نموذج لمعالجة اللغة الطبيعية لقياس التشابه بين الجمل، وذلك من خلال استخدام تقنيات التعلم الآلي ومعالجة اللغات الطبيعية. يلزمنا استخدام مجموعة البيانات المناسبة لتدريب النموذج وضبطه لتحقيق أداء ممتاز.? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v1')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. First, import the SentenceTransformer class from the sentence-transformers library.', 'code': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nimport numpy as np\\\\n\\\\nquery = \\\"What do customers say about the hotel\\\\'s breakfast?\\\"\\\\nreviews = [...] # List of customer reviews\\\\n\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-MiniLM-L12-v1\\\\')\\\\nembeddings = model.encode([query] + reviews)\\\\nquery_embedding = embeddings[0]\\\\nreview_embeddings = embeddings[1:]\\\\n\\\\nsimilarities = cosine_similarity([query_embedding], review_embeddings)[0]\\\\ntop_n_indices = np.argsort(-similarities)[:n]\\\\ntop_n_reviews = [reviews[idx] for idx in top_n_indices]', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Sentence Transformers', 'api_name': 'sentence-transformers/all-MiniLM-L12-v1', 'api_call': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nimport numpy as np\\\\n\\\\nquery = \\\"What do customers say about the hotel\\\\'s breakfast?\\\"\\\\nreviews = [...] # List of customer reviews\\\\n\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-MiniLM-L12-v1\\\\')\\\\nembeddings = model.encode([query] + reviews)\\\\nquery_embedding = embeddings[0]\\\\nreview_embeddings = embeddings[1:]\\\\n\\\\nsimilarities = cosine_similarity([query_embedding], review_embeddings)[0]\\\\ntop_n_indices = np.argsort(-similarities)[:n]\\\\ntop_n_reviews = [reviews[idx] for idx in top_n_indices]', 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v1')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': [{'name': 'Sentence Embeddings Benchmark', 'url': 'https://seb.sbert.net'}], 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model that maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}} </s>\"\n"}
{"text": "<s>[INST]هل ترغب في إنشاء نموذج تعلم تعزيزي يستطيع تقديم فيديو يصور مشهدًا للحيوانات في البرية؟ يمكنني مساعدتك في ذلك.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"PPO.load_from_hub('araffin/ppo-LunarLander-v2', 'ppo-LunarLander-v2.zip')\\\", '<<<api_provider>>>': 'Stable-Baselines3', '<<<explanation>>>': '1. Import the required libraries load_from_hub from huggingface_sb3, PPO from stable_baselines3, make_vec_env from stable_baselines3.common.env_util and evaluate_policy from stable_baselines3.common.evaluation.\\\\n2. Load the trained model from the Hugging Face model hub using the load_from_hub function and the provided model name and file. The model is a PPO (Proximal Policy Optimization) trained on the LunarLander-v2 environment.\\\\n3. Pass the loaded model into PPO.load() method to create the PPO model for the LunarLander-v2 game.\\\\n4. Create the LunarLander-v2 environment using the make_vec_env function.\\\\n5. You can now use this model to make decisions in the LunarLander-v2 environment and beat your friend at the game.', '<<<code>>>': \\\"from huggingface_sb3 import load_from_hub\\\\nfrom stable_baselines3 import PPO\\\\nfrom stable_baselines3.common.env_util import make_vec_env\\\\ncheckpoint = load_from_hub('araffin/ppo-LunarLander-v2', 'ppo-LunarLander-v2.zip')\\\\nmodel = PPO.load(checkpoint)\\\\nenv = make_vec_env('LunarLander-v2', n_envs=1)\\\", 'api_call': \\\"PPO.load_from_hub('araffin/ppo-LunarLander-v2', 'ppo-LunarLander-v2.zip')\\\", 'provider': 'Stable-Baselines3', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'araffin/ppo-LunarLander-v2', 'api_call': \\\"PPO.load_from_hub('araffin/ppo-LunarLander-v2', 'ppo-LunarLander-v2.zip')\\\", 'api_arguments': {'checkpoint': 'araffin/ppo-LunarLander-v2.zip'}, 'python_environment_requirements': [['huggingface_sb3', 'stable_baselines3']], 'example_code': {'load_model': 'from huggingface_sb3 import load_from_hub\\\\ncheckpoint = load_from_hub(araffin/ppo-LunarLander-v2, ppo-LunarLander-v2.zip)\\\\nmodel = PPO.load(checkpoint)', 'evaluate': 'from stable_baselines3.common.env_util import make_vec_env\\\\nfrom stable_baselines3.common.evaluation import evaluate_policy\\\\nenv = make_vec_env(LunarLander-v2, n_envs=1)\\\\nmean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\\\\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})'}, 'performance': {'dataset': 'LunarLander-v2', 'accuracy': '283.49 +/- 13.74'}, 'description': 'This is a trained model of a PPO agent playing LunarLander-v2 using the stable-baselines3 library.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام التعرف التلقائي على الكلام الصوتي، ونحتاج إلى إنشاء نموذج يمكنه التعرف على الكلام بدقة وسرعة عالية.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"Wav2Vec2ForCTC.from_pretrained('nguyenvulebinh/wav2vec2-base-vietnamese-250h')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary libraries, like Wav2Vec2Processor and Wav2Vec2ForCTC from the transformers package for speech recognition. We should also import soundfile for audio file reading and torch for computation.\\\\n2. We then load the pre-trained Vietnamese speech recognition model using the 'nguyenvulebinh/wav2vec2-base-vietnamese-250h' identifier.\\\\n3. We read the Vietnamese audio file and convert it into a suitable format for further processing.\\\\n4. Having chosen the audio file, the model processes the audio and outputs a list of transcribed texts.\\\\n5. These transcriptions can then be used to compile an audio summary of the latest Vietnamese news.\\\", '<<<code>>>': \\\"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nimport soundfile as sf\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained('nguyenvulebinh/wav2vec2-base-vietnamese-250h')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained('nguyenvulebinh/wav2vec2-base-vietnamese-250h')\\\\n\\\\nspeech, _ = sf.read('your_audio_file.wav')\\\\n# replace 'your_audio_file.wav' with the path to your audio file\\\\ninput_values = processor(speech, return_tensors='pt').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('nguyenvulebinh/wav2vec2-base-vietnamese-250h')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nguyenvulebinh/wav2vec2-base-vietnamese-250h', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('nguyenvulebinh/wav2vec2-base-vietnamese-250h')\\\", 'api_arguments': ['file'], 'python_environment_requirements': [['transformers', 'datasets', 'soundfile', 'torch']], 'example_code': \\\"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nfrom datasets import load_dataset\\\\nimport soundfile as sf\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained(nguyenvulebinh/wav2vec2-base-vietnamese-250h)\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(nguyenvulebinh/wav2vec2-base-vietnamese-250h)\\\\n\\\\ndef map_to_array(batch):\\\\n speech, _ = sf.read(batch[file])\\\\n batch[speech] = speech\\\\n return batch\\\\n\\\\nds = map_to_array({\\\\n file: 'audio-test/t1_0001-00010.wav'\\\\n})\\\\n\\\\ninput_values = processor(ds[speech], return_tensors=pt, padding=longest).input_values\\\\n\\\\nlogits = model(input_values).logits\\\\n\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'performance': {'dataset': [{'name': 'VIVOS', 'accuracy': 6.15}, {'name': 'Common Voice vi', 'accuracy': 11.52}]}, 'description': 'Vietnamese end-to-end speech recognition using wav2vec 2.0. Pre-trained on 13k hours of Vietnamese youtube audio (un-label data) and fine-tuned on 250 hours labeled of VLSP ASR dataset on 16kHz sampled speech audio.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي لتصنيف الرموز في مجال معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"SequenceTagger.load('flair/ner-english')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries from the flair package including Sentence and SequenceTagger.\\\\n2. Load the pre-trained model 'flair/ner-english' using the SequenceTagger.load method.\\\\n3. Create a Sentence object with the input text. The Sentence class is used for handling sentence-level operations, such as tokenization and named entity recognition.\\\\n4. Use the predict method of the SequenceTagger to identify the named entities in the input text.\\\\n5. Iterate over the identified entities and classify them as person, location, organization, or other, based on their detected tags.\\\", '<<<code>>>': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\ntagger = SequenceTagger.load('flair/ner-english')\\\\nsentence = Sentence('Input text here...')\\\\ntagger.predict(sentence)\\\\n\\\\nentities = {}\\\\nfor entity in sentence.get_spans('ner'):\\\\n    entities[entity.text] = entity.tag\\\\n\\\\nprint(entities)\\\", 'api_call': \\\"SequenceTagger.load('flair/ner-english')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'flair/ner-english', 'api_call': \\\"SequenceTagger.load('flair/ner-english')\\\", 'api_arguments': ['sentence'], 'python_environment_requirements': [['flair']], 'example_code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\n# load tagger\\\\ntagger = SequenceTagger.load('flair/ner-english')\\\\n\\\\n# make example sentence\\\\nsentence = Sentence('George Washington went to Washington')\\\\n\\\\n# predict NER tags\\\\ntagger.predict(sentence)\\\\n\\\\n# print sentence\\\\nprint(sentence)\\\\n\\\\n# print predicted NER spans\\\\nprint('The following NER tags are found:')\\\\n\\\\n# iterate over entities and print\\\\nfor entity in sentence.get_spans('ner'):\\\\n    print(entity)\\\", 'performance': {'dataset': 'conll2003', 'accuracy': '93.06'}, 'description': 'This is the standard 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نماذج لمعالجة اللغة الطبيعية، ونحتاج إلى استخلاص الميزات اللازمة لتحليل النصوص بدقة وفعالية.? [/INST]'<<<domain>>>': 'Natural Language Processing Feature Extraction', '<<<api_call>>>': \\\"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary modules from the transformers library, specifically AutoTokenizer and AutoModel.\\\\n2. Load the pre-trained 'dmis-lab/biobert-v1.1' model for feature extraction. This model is particularly suitable for handling biomedical texts since it has been specifically designed and trained for that purpose.\\\\n3. Tokenize the input medical text using the appropriate tokenizer, which can be obtained from the AutoTokenizer module.\\\\n4. Use the resulting tokenized input to feed the pre-trained model, which will output the text's features that can be used to represent the medical concepts and relationships present in the text.\\\", '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-v1.1')\\\\nmodel = AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\\\\ninput_text = 'Medical text content...'\\\\ninputs = tokenizer(input_text, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nfeatures = outputs.last_hidden_state\\\", 'api_call': \\\"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \\\"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نموذج لتصنيف مقاطع الفيديو من خلال الرؤية الحاسوبية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"AutoModelForVideoClassification.from_pretrained('dangle124/videomae-base-finetuned-RealLifeViolenceSituations-subset')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary class, 'AutoModelForVideoClassification', from the transformers library.\\\\n2. Load the pre-trained model 'dangle124/videomae-base-finetuned-RealLifeViolenceSituations-subset'. This model is specifically tuned for detecting real-life violence in videos.\\\\n3. The loaded model can then be used to process video files or video streams.\\\\n4. As the model is trained to classify videos containing real-life violence events, it will be able to accurately identify whether the input video contains any violent situations or not.\\\", '<<<code>>>': \\\"from transformers import AutoModelForVideoClassification\\\\nmodel = AutoModelForVideoClassification.from_pretrained('dangle124/videomae-base-finetuned-RealLifeViolenceSituations-subset')\\\\n\\\\n# Code to load video, preprocess it and predict using the 'model' object should be added here.\\\", 'api_call': \\\"AutoModelForVideoClassification.from_pretrained('dangle124/videomae-base-finetuned-RealLifeViolenceSituations-subset')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'videomae-base-finetuned-RealLifeViolenceSituations-subset', 'api_call': \\\"AutoModelForVideoClassification.from_pretrained('dangle124/videomae-base-finetuned-RealLifeViolenceSituations-subset')\\\", 'api_arguments': {'model_name': 'dangle124/videomae-base-finetuned-RealLifeViolenceSituations-subset'}, 'python_environment_requirements': [{'transformers': '4.27.2', 'pytorch': '1.13.1', 'datasets': '2.10.1', 'tokenizers': '0.13.2'}], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': 0.9533}, 'description': 'This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It is trained for video classification task, specifically for RealLifeViolenceSituations.' </s>\"\n"}
{"text": "<s>[INST]يتعلق تصنيف الصوت بإنشاء نماذج تعلم آلي لتعريف وتصنيف الأصوات في مجال الصور المتعددة الوسائط. ستقوم شركتنا بتصميم نماذج لتصنيف الصوت في بيئات مختلفة، ونحتاج إلى تقدير دقة وفعالية هذه النماذج.? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'https://path_to_image.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'https://path_to_image.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\\\ngenerated_ids = model.generate(pixel_values)\\\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\\\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي لتحديد صور الكمبيوتر.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"ViTModel.from_pretrained('facebook/dino-vitb16')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We use the Vision Transformer (ViT) model 'facebook/dino-vitb16', which is pretrained on a large collection of images in a self-supervised fashion. This model can extract features from images to perform classification tasks.\\\\n2. We first import the necessary libraries: ViTFeatureExtractor from transformers, ViTModel from transformers, Image from PIL and requests.\\\\n3. We load an image from a URL and convert it to a PIL Image object.\\\\n4. We instantiate the ViTFeatureExtractor using the from_pretrained method with the 'facebook/dino-vitb16' model.\\\\n5. We also load the main ViTModel using the from_pretrained method with the 'facebook/dino-vitb16' model.\\\\n6. The feature extraction is performed by passing the image to the feature_extractor object, and this is then passed to the model, obtaining the last_hidden_state as the extracted image features.\\\\n7. These features can then be used for further classification tasks.\\\", '<<<code>>>': \\\"from transformers import ViTFeatureExtractor, ViTModel\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlast_hidden_states = outputs.last_hidden_state\\\", 'api_call': \\\"ViTModel.from_pretrained('facebook/dino-vitb16')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \\\"ViTModel.from_pretrained('facebook/dino-vitb16')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': [{'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}], 'example_code': \\\"from transformers import ViTFeatureExtractor, ViTModel\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlast_hidden_states = outputs.last_hidden_state\\\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج تصنيف الصوت باستخدام بيانات التدريب المقدمة وقم بتدريب النموذج باستخدام خوارزمية التعلم الآلي المناسبة.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"pipeline('audio-classification', model='MIT/ast-finetuned-audioset-10-10-0.4593')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' function provided by the transformers library from Hugging Face.\\\\n2. Load the MIT/ast-finetuned-audioset-10-10-0.4593 model using the 'pipeline' function with 'audio-classification' as the task identifier. This model is fine-tuned on the AudioSet dataset and can perform audio classification tasks.\\\\n3. To detect bird sounds among the ambient noises, you can pass your audio recordings as spectrograms (essentially, an image representing the audio information) to the loaded model. The model will identify the presence of bird sounds within the input audio data.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\naudio_classifier = pipeline('audio-classification', model='MIT/ast-finetuned-audioset-10-10-0.4593')\\\\nresult = audio_classifier(audio_data_as_spectrogram)\\\", 'api_call': \\\"pipeline('audio-classification', model='MIT/ast-finetuned-audioset-10-10-0.4593')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ast-finetuned-audioset-10-10-0.4593', 'api_call': \\\"pipeline('audio-classification', model='MIT/ast-finetuned-audioset-10-10-0.4593')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'AudioSet', 'accuracy': ''}, 'description': 'Audio Spectrogram Transformer (AST) model fine-tuned on AudioSet. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.' </s>\"\n"}
{"text": "<s>[INST]أنشئ جدول تصنيفي لمعالجة اللغة الطبيعية لمهمة تلخيص النصوص يحتوي على البيانات المطلوبة وقم بتصنيفها وفقا لمعايير تصنيف الرموز المحددة.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('mrm8488-bert2bert_shared-spanish-finetuned-summarization')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary libraries, which are 'torch' and 'transformers'. Make sure to import 'AutoTokenizer' and 'AutoModelForSeq2SeqLM' from the 'transformers' package.\\\", 'code': \\\"from torch import device, cuda\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nmodel_checkpoint = 'mrm8488/bert2bert_shared-spanish-finetuned-summarization'\\\\ndevice = 'cuda' if cuda.is_available() else 'cpu'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\\\\ntext = 'Texto en español en el que necesitas resumir aquí...'\\\\ninputs = tokenizer([text], padding=True, truncation=True, max_length=512, return_tensors=pt)\\\\ninput_ids = inputs.input_ids.to(device)\\\\nattention_mask = inputs.attention_mask.to(device)\\\\noutput = model.generate(input_ids, attention_mask=attention_mask)\\\\nsummary = tokenizer.decode(output[0], skip_special_tokens=True)\\\", 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Text2Text Generation', 'api_name': 'mrm8488/bert2bert_shared-spanish-finetuned-summarization', 'api_call': \\\"from torch import device, cuda\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nmodel_checkpoint = 'mrm8488/bert2bert_shared-spanish-finetuned-summarization'\\\\ndevice = 'cuda' if cuda.is_available() else 'cpu'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\\\\ntext = 'Texto en español en el que necesitas resumir aquí...'\\\\ninputs = tokenizer([text], padding=True, truncation=True, max_length=512, return_tensors=pt)\\\\ninput_ids = inputs.input_ids.to(device)\\\\nattention_mask = inputs.attention_mask.to(device)\\\\noutput = model.generate(input_ids, attention_mask=attention_mask)\\\\nsummary = tokenizer.decode(output[0], skip_special_tokens=True)\\\", 'api_arguments': ['text'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': 'def generate_summary(text):\\\\n  inputs = tokenizer([text], padding=max_length, truncation=True, max_length=512, return_tensors=pt)\\\\n  input_ids = inputs.input_ids.to(device)\\\\n  attention_mask = inputs.attention_mask.to(device)\\\\n  output = model.generate(input_ids, attention_mask=attention_mask)\\\\n  return tokenizer.decode(output[0], skip_special_tokens=True)\\\\ntext = Your text here...\\\\ngenerate_summary(text)', 'performance': {'dataset': 'mlsum', 'accuracy': {'Rouge1': 26.24, 'Rouge2': 8.9, 'RougeL': 21.01, 'RougeLsum': 21.02}}, 'description': 'Spanish BERT2BERT (BETO) fine-tuned on MLSUM ES for summarization'}} </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج للإجابة على الأسئلة النصية عن طريق معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. This includes TapasTokenizer and TapasForQuestionAnswering for tackling the table question answering task.\\\\n2. Use the from_pretrained method of the TapasTokenizer and TapasForQuestionAnswering classes to load the pre-trained model 'lysandre/tapas-temporary-repo'.\\\\n3. This model is capable of handling questions related to a table in a conversational setting, making it ideal for your startup's project of extracting relevant information based on a user's question.\\\\n4. Provide the table and the user's question to the model, and it will return the predicted answer coordinates and aggregation indices, which you can use to extract the desired information.\\\", '<<<code>>>': \\\"from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\ntokenizer = TapasTokenizer.from_pretrained('lysandre/tapas-temporary-repo')\\\\nmodel = TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\\ninputs = tokenizer(table=table, queries=question, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lysandre/tapas-temporary-repo', 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\", 'api_arguments': ['model_name', 'question', 'table'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"tokenizer = TapasTokenizer.from_pretrained('lysandre/tapas-temporary-repo')\\\\nmodel = TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\\\ninputs = tokenizer(table=table, queries=question, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())\\\", 'performance': {'dataset': 'SQA', 'accuracy': 'Not provided'}, 'description': 'TAPAS base model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion and can be used for answering questions related to a table in a conversational set-up.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير تقنية التصنيف التلقائي لصور الكمبيوتر، ونرغب في مساعدتكم على تحسين الأداء التصنيفي للصور والصور الفوتوغرافية الخاصة بكم.? [/INST]{'domain': 'Computer Vision Image Classification', 'api_call': \\\"timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimg = Image.open(urlopen('https://your-image-url-here'))\\\\nmodel = timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))\\\", 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification, Feature Map Extraction, Image Embeddings', 'api_name': 'convnext_base.fb_in1k', 'api_call': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimg = Image.open(urlopen('https://your-image-url-here'))\\\\nmodel = timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))\\\", 'api_arguments': {'pretrained': 'True', 'features_only': 'True', 'num_classes': '0'}, 'python_environment_requirements': ['timm'], 'example_code': ['from urllib.request import urlopen', 'from PIL import Image', 'import timm', \\\"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\\", \\\"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\", 'model = model.eval()', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'output = model(transforms(img).unsqueeze(0))'], 'performance': {'dataset': 'imagenet-1k', 'accuracy': '83.82%'}, 'description': 'A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لإنشاء صور بشكل غير مشروط باستخدام تقنيات رؤية الحاسوب للاكتشاف الكائنات، ونريد توليد صور جديدة بناءً على التصنيفات المتوفرة للصور الحالية.? [/INST]{'domain': 'Computer Vision Object Detection', 'api_call': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'in_game_image.jpg'  # Replace this with the path to your image\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'in_game_image.jpg'  # Replace this with the path to your image\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \\\"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\\\"}} </s>\"\n"}
{"text": "<s>[INST]قم بتوليد ترجمة آلية للنصوص من اللغة العربية إلى الإنجليزية باستخدام تقنيات معالجة اللغات الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes and methods from the transformers library, such as pipeline and the AutoModelForSeq2SeqLM.\\\\n2. Load the 'Helsinki-NLP/opus-mt-en-ru' model using the from_pretrained method of the AutoModelForSeq2SeqLM class. This model has been trained to translate English text to Russian.\\\\n3. Create a pipeline object using the pipeline function and specify 'translation_en_to_ru' as the task and provide the model obtained in the previous step.\\\\n4. Use the created pipeline object to translate English text on your website to Russian.\\\", '<<<code>>>': \\\"from transformers import pipeline, AutoModelForSeq2SeqLM\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\\ntranslation = pipeline('translation_en_to_ru', model=model)\\\\ntranslated_text = translation('Your English website content here')\\\", 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-en-ru', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers', 'torch', 'tensorflow']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ru', model='Helsinki-NLP/opus-mt-en-ru')\\\\ntranslated_text = translation('Hello, how are you?')\\\", 'performance': {'dataset': 'newstest2019-enru', 'accuracy': '27.1'}, 'description': 'Helsinki-NLP/opus-mt-en-ru is a translation model trained on the OPUS dataset, which translates English text to Russian. It is based on the Marian NMT framework and can be used with Hugging Face Transformers.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا في مجال رؤية الحاسوب والكشف عن الأجسام بتصميم مركبات ذاتية القيادة، ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To build the object detection application, we import the necessary packages, namely YolosFeatureExtractor and YolosForObjectDetection from the transformers library, Image from PIL, and requests.\\\\n2. We use the YolosFeatureExtractor.from_pretrained method to load the feature extractor for the 'hustvl/yolos-tiny' model.\\\\n3. We use the YolosForObjectDetection.from_pretrained method to create an instance of the object detection model with the same pretrained model name 'hustvl/yolos-tiny'.\\\\n4. Then, we open an image (loaded from an URL) and convert it to the appropriate format using the feature extractor.\\\\n5. We pass the processed input image to the model and obtain the outputs (logits and bounding boxes) which can be used to identify objects in the image.\\\", '<<<code>>>': \\\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\n\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\", 'api_call': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hustvl/yolos-tiny', 'api_call': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': \\\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '28.7 AP'}, 'description': 'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.' </s>\"\n"}
{"text": "<s>[INST]يتطلب بناء نظام تجسير لإنتاج الصور التجريبي على الحواسيب القدرة على توليد صور مفصلة واقعية بدون قيود.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\\ngenerated_image = pipeline().images[0]\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'myunus1/diffmodels_galaxies_scratchbook', 'api_call': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\\ngenerated_image = pipeline().images[0]\\\", 'api_arguments': {'from_pretrained': 'myunus1/diffmodels_galaxies_scratchbook'}, 'python_environment_requirements': {'package': 'diffusers', 'import': 'from diffusers import DDPMPipeline'}, 'example_code': {'initialize_pipeline': \\\"pipeline = DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", 'generate_image': 'image = pipeline().images[0]', 'display_image': 'image'}, 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'This model is a diffusion model for unconditional image generation of cute 🦋.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتضمين حقل <mask> في النص لاستكشاف النصائج المختلفة وتقديم جملة معنوية متكاملة.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We will first use the transformers library to create a pipeline to translate the Dutch text to English.', 'code': \\\"from transformers import pipeline\\\\ndutch_text = 'Hallo, Ik hoop dat alles goed gaat met jou en je team. We zijn erg blij met de vooruitgang van ons project en waarderen uw inspanningen. Laten we een vergadering plannen om onze volgende stappen te bespreken. Kunt u de komende week een geschikte tijd voorstellen? Met vriendelijke groet.'\\\\ntranslation_pipeline = pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\\ntranslated_text = translation_pipeline(dutch_text)[0]['translation_text']\\\", 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Translation', 'api_name': 'Helsinki-NLP/opus-mt-nl-en', 'api_call': \\\"from transformers import pipeline\\\\ndutch_text = 'Hallo, Ik hoop dat alles goed gaat met jou en je team. We zijn erg blij met de vooruitgang van ons project en waarderen uw inspanningen. Laten we een vergadering plannen om onze volgende stappen te bespreken. Kunt u de komende week een geschikte tijd voorstellen? Met vriendelijke groet.'\\\\ntranslation_pipeline = pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\\ntranslated_text = translation_pipeline(dutch_text)[0]['translation_text']\\\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\\\ntranslated_text = translation('Hallo, hoe gaat het met je?')[0]['translation_text']\\\", 'performance': {'dataset': 'Tatoeba.nl.en', 'accuracy': {'BLEU': 60.9, 'chr-F': 0.749}}, 'description': 'A Dutch to English translation model based on the OPUS dataset, using a transformer-align architecture with normalization and SentencePiece pre-processing.'}} </s>\"\n"}
{"text": "<s>[INST]استخدم تقنيات معالجة اللغة الطبيعية للجواب على أسئلة عن الجداول، ابدأ بـ \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"TapasTokenizer.from_pretrained('google/tapas-base-finetuned-sqa')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\ntapas_pipeline = pipeline('table-question-answering', model='google/tapas-base-finetuned-sqa')\\\\nquestion = 'How many goals did player A score?'\\\\ntable = [['Player', 'Goals'], ['Player A', 5], ['Player B', 3]]\\\\nresult = tapas_pipeline(question=question, table=table)\\\", 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-base-finetuned-sqa', 'api_call': \\\"from transformers import pipeline\\\\ntapas_pipeline = pipeline('table-question-answering', model='google/tapas-base-finetuned-sqa')\\\\nquestion = 'How many goals did player A score?'\\\\ntable = [['Player', 'Goals'], ['Player A', 5], ['Player B', 3]]\\\\nresult = tapas_pipeline(question=question, table=table)\\\", 'api_arguments': ['question', 'table'], 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntapas_pipeline = pipeline('table-question-answering', model='google/tapas-base-finetuned-sqa')\\\\n\\\\n# Define the table and question\\\\nquestion = 'How many goals did player A score?'\\\\ntable = [['Player', 'Goals'], ['Player A', 5], ['Player B', 3]]\\\\n\\\\n# Get the answer\\\\nresult = tapas_pipeline(question=question, table=table)\\\\nprint(result)\\\", 'performance': {'dataset': 'msr_sqa', 'accuracy': 0.6874}, 'description': 'TAPAS base model fine-tuned on Sequential Question Answering (SQA). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia and fine-tuned on SQA. It can be used for answering questions related to a table in a conversational set-up.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نموذج لمعالجة اللغة الطبيعية لتصنيف النصوص. هل يمكنك مساعدتي في تنفيذ ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"pipeline('text-classification', model='roberta-base-openai-detector')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We first import the necessary libraries, which are 'pipeline' from transformers.\\\\n2. We then use the 'pipeline' function to load the model, which in this case is 'roberta-base-openai-detector'. This model has been fine-tuned to detect GPT-2 generated text.\\\\n3. The loaded model will be used for text classification, where it will identify whether the given text is influenced by AI-generated content.\\\\n4. The model will evaluate the text and return either positive or negative, depending on whether the content seems to be AI-generated or not.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'text-classification\\\\', model=\\\\'roberta-base-openai-detector\\\\')\\\\narticle_text = \\\"Text from the news article...\\\"\\\\nresult = classifier(article_text)', 'api_call': \\\"pipeline('text-classification', model='roberta-base-openai-detector')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'roberta-base-openai-detector', 'api_call': \\\"pipeline('text-classification', model='roberta-base-openai-detector')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\npipe = pipeline(text-classification, model=roberta-base-openai-detector)\\\\nprint(pipe(Hello world! Is this content AI-generated?))', 'performance': {'dataset': 'WebText', 'accuracy': '95%'}, 'description': 'RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model.' </s>\"\n"}
{"text": "<s>[INST]يقوم العميل بتطوير تطبيق لتصنيف الجداول. نحتاج إلى برنامج يقوم بتحويل الجداول إلى نص لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': 'TF_Decision_Trees(input_features, target)', '<<<api_provider>>>': 'Keras', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': 'TF_Decision_Trees(input_features, target)', 'provider': 'Keras', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'TF_Decision_Trees', 'api_call': 'TF_Decision_Trees(input_features, target)', 'api_arguments': ['input_features', 'target'], 'python_environment_requirements': [['tensorflow >= 7.0']], 'example_code': 'https://github.com/tdubon/TF-GB-Forest/blob/c0cf4c7e3e29d819b996cfe4eecc1f2728115e52/TFDecisionTrees_Final.ipynb', 'performance': {'dataset': 'Census-Income Data Set', 'accuracy': 96.57}, 'description': \\\"Use TensorFlow's Gradient Boosted Trees model in binary classification of structured data. Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is trained on the US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables. The task is to determine whether a person makes over 50k a year.\\\" </s>\"\n"}
{"text": "<s>[INST]يركز ابحاثنا على تطوير نماذج تعتمد على الذكاء الصناعي لتقنيات تحويل الصوت إلى صوت في مجال معالجة اللغة الطبيعية.? [/INST]{'domain': 'Audio Audio-to-Audio', 'api_call': \\\"pipeline('translation', model='facebook/xm_transformer_sm_all-en')\\\", 'api_provider': 'Fairseq', 'explanation': '1. Import the pipeline function from the transformers library.', 'code': 'from transformers import pipeline\\\\ntranslation_pipeline = pipeline(\\\\'translation\\\\', model=\\\\'facebook/xm_transformer_sm_all-en\\\\')\\\\ninput_sentence = \\\"Hello, how are you?\\\"\\\\ntarget_language = \\\"fr\\\" # French\\\\ntranslated_sentence = translation_pipeline(f\\\"{input_sentence} <text_to_translate> {target_language}\\\")', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Fairseq', 'functionality': 'speech-to-speech-translation', 'api_name': 'facebook/xm_transformer_sm_all-en', 'api_call': 'from transformers import pipeline\\\\ntranslation_pipeline = pipeline(\\\\'translation\\\\', model=\\\\'facebook/xm_transformer_sm_all-en\\\\')\\\\ninput_sentence = \\\"Hello, how are you?\\\"\\\\ntarget_language = \\\"fr\\\" # French\\\\ntranslated_sentence = translation_pipeline(f\\\"{input_sentence} <text_to_translate> {target_language}\\\")', 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \\\"translation_pipeline('Hello, how are you?')\\\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A speech-to-speech translation model that can be loaded on the Inference API on-demand.'}} </s>\"\n"}
{"text": "<s>[INST]قم باستخدام تقنيات معالجة الصور والفيديو لتحويل صور الحيوانات في البرية إلى فيديو محسن. هل يمكنك تقديم المساعدة في تحقيق ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary packages, including transformers and torch.\\\\n2. Load the 'lllyasviel/sd-controlnet-seg' pre-trained ControlNetModel from Hugging Face, which is trained for image segmentation tasks that can help in understanding images within your platformer game.\\\\n3. Preprocess the image data obtained from the game according to the model's requirements (e.g., resizing, normalization).\\\\n4. Feed the preprocessed image data into the trained ControlNetModel to generate the segmented image.\\\\n5. Use the segmented output image to better understand the game environment.\\\", '<<<code>>>': \\\"from transformers import ControlNetModel\\\\nimport torch\\\\n\\\\nmodel = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\\npreprocessed_image = preprocess_game_image(game_image) # Replace with a function to preprocess your game images\\\\nseg_output = model(preprocessed_image.to(torch_dtype))\\\", 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/sd-controlnet-seg', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\\\", 'api_arguments': ['torch_dtype'], 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate']], 'example_code': \\\"image = pipe(house, image, num_inference_steps=20).images[0]\\\\nimage.save('./images/house_seg_out.png')\\\", 'performance': {'dataset': 'ADE20K', 'accuracy': 'Trained on 164K segmentation-image, caption pairs'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.' </s>\"\n"}
{"text": "<s>[INST]استخدم واجهة برمجة تطبيقاتنا لتحويل النص إلى صوت عالي الجودة في مجال معالجة اللغة الطبيعية والإجابة على الأسئلة. يمكنك تكامل الصوت في تطبيقك عبر الاتصال بالـ API وإعطاء تجربة صوتية رائعة لمستخدميك.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/bert-base-cased-squad2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. We use the AutoModelForQuestionAnswering class provided by the transformers library and load the pre-trained model 'deepset/bert-base-cased-squad2'. This model is designed for question answering tasks and can provide relevant answers to users' questions.\\\", 'code': \\\"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained('deepset/bert-base-cased-squad2')\\\\ntokenizer = AutoTokenizer.from_pretrained('deepset/bert-base-cased-squad2')\\\\nqa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\\\\ncontext = 'The restaurant is located at 123 Main Street, offers Italian cuisine, and accepts reservations online.'\\\\nquestion = 'Where is the restaurant located?'\\\\nanswer = qa_pipeline({'context': context, 'question': question})\\\\nanswer_text = answer['answer']\\\", 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'deepset/bert-base-cased-squad2', 'api_call': \\\"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel = AutoModelForQuestionAnswering.from_pretrained('deepset/bert-base-cased-squad2')\\\\ntokenizer = AutoTokenizer.from_pretrained('deepset/bert-base-cased-squad2')\\\\nqa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\\\\ncontext = 'The restaurant is located at 123 Main Street, offers Italian cuisine, and accepts reservations online.'\\\\nquestion = 'Where is the restaurant located?'\\\\nanswer = qa_pipeline({'context': context, 'question': question})\\\\nanswer_text = answer['answer']\\\", 'api_arguments': 'context, question', 'python_environment_requirements': 'transformers', 'example_code': \\\"qa_pipeline({'context': 'This model can be loaded on the Inference API on-demand.', 'question': 'Where can this model be loaded?'})\\\", 'performance': {'dataset': 'squad_v2', 'accuracy': {'exact_match': 71.152, 'f1': 74.671}}, 'description': 'This is a BERT base cased model trained on SQuAD v2'}} </s>\"\n"}
{"text": "<s>[INST]قم بتنفيذ نموذج تكوين صور عشوائية في مجال رؤية الحاسوب يعتمد على تقنيات توليد النصوص الطبيعية، حيث يتم استخدام الخوارزميات والشبكات العصبية لإنشاء صور تامة الشكل بشكل آلي بدون إشارات دلالية، ثم قم بتقييم أداء النموذج من حيث جودة التصوير والامتياز في إنتاج الصور.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary DDPMPipeline class from the diffusers package to work with unconditional image generation models.\\\\n2. We then use the from_pretrained method of the DDPMPipeline class to load the pre-trained model 'myunus1/diffmodels_galaxies_scratchbook'. This model has been trained for unconditional image synthesis tasks, with a focus on generating galaxy-like shapes.\\\\n3. Once the model is loaded, we can use it to generate an image of a galaxy-like shape, which can be used by the astrophysicist for their research project.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\\nimage = pipeline().images[0]\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'myunus1/diffmodels_galaxies_scratchbook', 'api_call': \\\"DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", 'api_arguments': {'from_pretrained': 'myunus1/diffmodels_galaxies_scratchbook'}, 'python_environment_requirements': [{'package': 'diffusers', 'import': 'from diffusers import DDPMPipeline'}], 'example_code': {'initialize_pipeline': \\\"pipeline = DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\\\", 'generate_image': 'image = pipeline().images[0]', 'display_image': 'image'}, 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'This model is a diffusion model for unconditional image generation of cute 🦋.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي في مجال تصنيف النصوص بالمعالجة اللغوية الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Then, we use the pipeline function to create a sentiment analysis model, and we specify the model 'ProsusAI/finbert' to be loaded. This model has been fine-tuned for financial sentiment classification tasks, making it suitable for analyzing the sentiment of a cryptocurrency news article.\\\\n3. We can now use the loaded model to analyze the sentiment of the given news article. The model will classify the sentiment as 'positive', 'negative', or 'neutral'.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'sentiment-analysis\\\\', model=\\\\'ProsusAI/finbert\\\\')\\\\nresult = classifier(\\\"Bitcoin has recently seen an impressive surge in value, leading to a renewed interest in cryptocurrencies. This has created a lot of excitement among investors, with many predicting a profitable future. However, the market remains volatile, and it would be wise for investors to approach with caution, as sudden fluctuations can result in significant losses.\\\")', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ProsusAI/finbert', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline; classifier = pipeline('sentiment-analysis', model='ProsusAI/finbert'); classifier('your_text_here')\\\", 'performance': {'dataset': 'Financial PhraseBank', 'accuracy': 'Not provided'}, 'description': 'FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير تقنيات معالجة الصور والنصوص لتحسين التفاعل بين الصور والنصوص، ونحتاج إلى تقدير عمق الفهم المتعدد الوسائط لتحسين دقة التفاعل.? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary libraries, such as TrOCRProcessor and VisionEncoderDecoderModel from transformers, Image from PIL, requests, and torch.', 'code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\nimport torch\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\\\n\\\\npixel_values = processor(image, return_tensors='pt').pixel_values\\\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\\\", 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-stage1', 'api_call': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\nimport torch\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\\\n\\\\npixel_values = processor(image, return_tensors='pt').pixel_values\\\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\\\", 'api_arguments': {'url': 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'torch'], 'example_code': \\\"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\\\nfrom PIL import Image\\\\nimport requests\\\\nimport torch\\\\n\\\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\\\n\\\\npixel_values = processor(image, return_tensors='pt').pixel_values\\\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\\\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.'}} </s>\"\n"}
{"text": "<s>[INST]نحن مهتمون بإنشاء نموذج لتصنيف عمق الصور في مجال رؤية الحواسيب. هل يمكنك مساعدتنا في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries by importing the pipeline function from the transformers library.\\\\n2. Use the pipeline function to create a depth estimation model, specifying 'sayakpaul/glpn-nyu-finetuned-diode-221221-102136' as the model to be loaded.\\\\n3. The loaded model will estimate depth values for a given input image.\\\\n4. The robot's camera system can use the output from this model to identify the distance between the robot and objects on the floor.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ndepth_estimator = pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\\\\ndepth_map = depth_estimator(image)\\\", 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221221-102136', 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\\\", 'api_arguments': [], 'python_environment_requirements': [['Transformers 4.24.0', 'Pytorch 1.12.1+cu116', 'Datasets 2.8.0', 'Tokenizers 0.13.2']], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4222, 'Mae': 0.411, 'Rmse': 0.6292, 'Abs Rel': 0.3778, 'Log Mae': 0.1636, 'Log Rmse': 0.224, 'Delta1': 0.432, 'Delta2': 0.6806, 'Delta3': 0.8068}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.? [/INST]{'domain': 'Multimodal Text-to-Image', 'api_call': \\\"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\ntext_to_image = pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\\\\ngenerated_image = text_to_image('analog style landscape')\\\", 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \\\"from transformers import pipeline\\\\ntext_to_image = pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\\\\ngenerated_image = text_to_image('analog style landscape')\\\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \\\"text_to_image('analog style landscape')\\\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \\\"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\\\"}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء جدول تصنيفي يحتوي على أسماء العناصر النصية والصور وقم بتصنيف كل عنصر تبعاً للوسيط المعبر المناسب.? [/INST]{'domain': 'Multimodal Text-to-Image', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion', torch_dtype=torch.float16)\\\", 'api_provider': 'Hugging Face', 'explanation': '1. We first import the required class and tools from the diffusers package and torch.', 'code': \\\"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\npipe = StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion', torch_dtype=torch.float16)\\\\npipe = pipe.to('cuda')\\\\nprompt = 'archer arcane style magical princess with golden hair'\\\\nimage = pipe(prompt).images[0]\\\\nimage.save('./magical_princess.png')\\\", 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \\\"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\npipe = StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion', torch_dtype=torch.float16)\\\\npipe = pipe.to('cuda')\\\\nprompt = 'archer arcane style magical princess with golden hair'\\\\nimage = pipe(prompt).images[0]\\\\nimage.save('./magical_princess.png')\\\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = nitrosocke/nitro-diffusion\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = archer arcane style magical princess with golden hair\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.'}} </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذجًا لتوليد صور لا شرطية في مجال رؤية الحاسوب.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('ddpm-celebahq-256')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. First, install the necessary diffusers package using \\\"!pip install diffusers\\\".\\\\n2. Import the DDPMPipeline class from the diffusers library.\\\\n3. Use the DDPMPipeline.from_pretrained method to load the pre-trained model \\\\'google/ddpm-celebahq-256\\\\', which has been trained on high-quality human face images.\\\\n4. Call the loaded model to generate a human-like face image.\\\\n5. Save the generated image as \\\"movie_poster.png\\\" to use it as the main movie poster background.', '<<<code>>>': \\\"!pip install diffusers\\\\nfrom diffusers import DDPMPipeline\\\\nmodel_id = 'google/ddpm-celebahq-256'\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().sample\\\\nimage[0].save('movie_poster.png')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('ddpm-celebahq-256')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/ddpm-celebahq-256', 'api_call': \\\"DDPMPipeline.from_pretrained('ddpm-celebahq-256')\\\", 'api_arguments': ['model_id'], 'python_environment_requirements': [['diffusers']], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-celebahq-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm()[sample]\\\\nimage[0].save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نموذج لتوليد نص من النصوص في مجال معالجة اللغة الطبيعية. هل يمكنك مساعدتي في تنفيذ هذا المهمة؟? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-large-seq2seq')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are AutoTokenizer and AutoModelForSeq2SeqLM from the transformers library.\\\\n2. Load the GODEL-v1_1-large-seq2seq model and its tokenizer using the AutoModelForSeq2SeqLM.from_pretrained and AutoTokenizer.from_pretrained functions respectively.\\\\n3. Define a function called 'generate' which takes instruction, knowledge, and dialog as input. The function should combine the dialog using 'EOS' as a separator and create a query with the instruction, knowledge, and dialog.\\\\n4. Convert the query into input_ids using the tokenizer.\\\\n5. Generate a response using the model's 'generate' function, specifying the desired output length and sampling parameters.\\\\n6. Decode the output ids using the tokenizer to get the final empathic response.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'microsoft/GODEL-v1_1-large-seq2seq\\\\')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'microsoft/GODEL-v1_1-large-seq2seq\\\\')\\\\n\\\\ndef generate(instruction, knowledge, dialog):\\\\n    if knowledge != \\\\'\\\\':\\\\n        knowledge = \\\\'[KNOWLEDGE] \\\\' + knowledge\\\\n    dialog = \\\\' EOS \\\\'.join(dialog)\\\\n    query = f\\\"{instruction} [CONTEXT] {dialog} {knowledge}\\\"\\\\n    input_ids = tokenizer(query, return_tensors=\\\\'pt\\\\').input_ids\\\\n    outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n    return output\\\\n\\\\ninstruction = \\\"Instruction: given a dialog context, you need to response empathically.\\\"\\\\nknowledge = \\\"\\\"\\\\ndialog = [\\\\n    \\\\'Does money buy happiness?\\\\',\\\\n    \\\\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\\\',\\\\n    \\\\'What is the best way to buy happiness ?\\\\'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-large-seq2seq')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/GODEL-v1_1-large-seq2seq', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-large-seq2seq')\\\", 'api_arguments': {'instruction': 'Instruction: given a dialog context, you need to response empathically.', 'knowledge': '', 'dialog': ['Does money buy happiness?', 'It is a question. Money buys you a lot of things, but not enough to buy happiness.', 'What is the best way to buy happiness ?']}, 'python_environment_requirements': [{'transformers': 'AutoTokenizer, AutoModelForSeq2SeqLM'}], 'example_code': \\\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != '':\\\\n knowledge = '[KNOWLEDGE] ' + knowledge\\\\n dialog = ' EOS '.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f'Instruction: given a dialog context, you need to response empathically.'\\\\nknowledge = ''\\\\ndialog = [\\\\n 'Does money buy happiness?',\\\\n 'It is a question. Money buys you a lot of things, but not enough to buy happiness.',\\\\n 'What is the best way to buy happiness ?'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\\\", 'performance': {'dataset': 'Reddit discussion thread, instruction and knowledge grounded dialogs', 'accuracy': 'Not provided'}, 'description': 'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.' </s>\"\n"}
{"text": "<s>[INST]عزيزي الطالب، يمكنك تطوير نظام تعلم آلي يمكنه تحويل النص إلى كلام بصوت مسموع. يجتمع نادينا الطلابي كل أسبوعين لاستخدام نظام \\\"نص إلى كلام\\\" الذي قمت بإنشائه. ستقوم بتطبيق نظام التعلم الآلي الخاص بك للتنافس في مسابقة \\\"كرة القدم الافتراضية\\\". نتمنى لك التوفيق!? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", '<<<api_provider>>>': 'speechbrain', '<<<explanation>>>': \\\"1. Import the necessary libraries and modules, including 'speechbrain', 'torchaudio', 'Tacotron2', and 'HIFIGAN'.\\\\n2. Use the 'from_hparams' method of the Tacotron2 class to load the pre-trained Tacotron2 model with a German source 'padmalcom/tts-tacotron2-german'.\\\\n3. Use the 'from_hparams' method of the HIFIGAN class to load the pre-trained HiFIGAN vocoder with a German dataset under the source 'padmalcom/tts-hifigan-german'.\\\\n4. Encode the factory tour script text using the Tacotron2 model to obtain the mel spectrogram (mel_output, mel_length, and alignment).\\\\n5. Decode the mel spectrogram using the HiFIGAN vocoder to generate the speech waveforms.\\\\n6. Save the generated audio to a .wav file using 'torchaudio.save'.\\\", '<<<code>>>': 'import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2\\\\nfrom speechbrain.pretrained import HIFIGAN\\\\n\\\\ntacotron2 = Tacotron2.from_hparams(source=\\\\'padmalcom/tts-tacotron2-german\\\\')\\\\nhifi_gan = HIFIGAN.from_hparams(source=\\\\'padmalcom/tts-hifigan-german\\\\')\\\\n\\\\ntour_script = \\\"Factory tour script in German...\\\"\\\\nmel_output, mel_length, alignment = tacotron2.encode_text(tour_script)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\n\\\\ntorchaudio.save(\\\\'factory_tour_audio.wav\\\\', waveforms.squeeze(1), 22050)', 'api_call': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", 'provider': 'speechbrain', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tts-hifigan-german', 'api_call': \\\"HIFIGAN.from_hparams(source='padmalcom/tts-hifigan-german', savedir=tmpdir_vocoder)\\\", 'api_arguments': ['mel_output'], 'python_environment_requirements': [['speechbrain']], 'example_code': \\\"import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2\\\\nfrom speechbrain.pretrained import HIFIGAN\\\\ntacotron2 = Tacotron2.from_hparams(source=padmalcom/tts-tacotron2-german, savedir=tmpdir_tts)\\\\nhifi_gan = HIFIGAN.from_hparams(source=padmalcom/tts-hifigan-german, savedir=tmpdir_vocoder)\\\\nmel_output, mel_length, alignment = tacotron2.encode_text(Mary had a little lamb)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\\\", 'performance': {'dataset': 'custom German dataset', 'accuracy': 'Not specified'}, 'description': 'A HiFIGAN vocoder trained on a generated German dataset using mp3_to_training_data. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا ببناء نموذج لتصنيف الصور في مجال الرؤية الحاسوبية، حيث نرغب في تصنيف الصور إلى فئات مختلفة بناءً على محتوى الصور والمعلومات المتاحة.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers and PIL packages. This includes SwinForImageClassification for the image classification model and Image for processing image data.\\\\n2. We then use the from_pretrained method of the SwinForImageClassification class to load the pre-trained model 'microsoft/swin-tiny-patch4-window7-224'. This model has been trained on a large dataset (ImageNet-1k) and can be useful for scene classification tasks.\\\\n3. We load the image data from a file or a set of consecutive image frames from a TV show.\\\\n4. This model can then be used to analyze an image and classify the scene content in it.\\\", '<<<code>>>': \\\"from transformers import SwinForImageClassification\\\\nfrom PIL import Image\\\\nimage = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with path to your image\\\\nmodel = SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'api_call': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/swin-tiny-patch4-window7-224', 'api_call': \\\"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\\\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': [{'transformers': 'AutoFeatureExtractor', 'PIL': 'Image', 'requests': 'requests'}], 'example_code': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير تكنولوجيا تحويل النص إلى صوت عالي الجودة، ونحتاج إلى تطوير نموذج يستطيع تحويل النصوص إلى كلام باللغة العربية بشكل دقيق وواضح.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-es-css10')\\\", '<<<api_provider>>>': 'Fairseq', '<<<explanation>>>': \\\"1. We first import the necessary libraries, such as fairseq, IPython and the methods load_model_ensemble_and_task_from_hf_hub, TTSHubInterface from fairseq.\\\\n2. We then use the load_model_ensemble_and_task_from_hf_hub method to load the pre-trained model 'facebook/tts_transformer-es-css10'. This model has been trained for text-to-speech tasks in Spanish.\\\\n3. This model can then be used to convert the text of the article (in Spanish) into synthesized speech. The synthesized audio is then played using IPython.display.Audio.\\\", '<<<code>>>': \\\"from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\n\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n    'facebook/tts_transformer-es-css10',\\\\n    arg_overrides={'vocoder': 'hifigan', 'fp16': False}\\\\n)\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\n\\\\ntext = 'Hola, esta es una prueba.'\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)\\\", 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-es-css10')\\\", 'provider': 'Fairseq', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/tts_transformer-es-css10', 'api_call': \\\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-es-css10')\\\", 'api_arguments': {'arg_overrides': {'vocoder': 'hifigan', 'fp16': False}}, 'python_environment_requirements': [['fairseq', 'IPython']], 'example_code': 'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n facebook/tts_transformer-es-css10,\\\\n arg_overrides={vocoder: hifigan, fp16: False}\\\\n)\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\ntext = Hola, esta es una prueba.\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)', 'performance': {'dataset': 'CSS10', 'accuracy': None}, 'description': 'Transformer text-to-speech model from fairseq S^2. Spanish single-speaker male voice trained on CSS10.' </s>\"\n"}
{"text": "<s>[INST]يقوم فريقنا بتطوير نماذج وتقنيات لفصل الأجسام في الصور الرقمية باستخدام تقنيات الرؤية الحاسوبية، ونحتاج إلى قدرة على تقدير عمق الأشياء في الصور الملتقطة لتحسين النتائج لمعالجة الأسئلة المتعلقة بالجداول في مجال معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='Filosofas/DialoGPT-medium-PALPATINE2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create a text generation pipeline using the 'text-generation' and the  pre-trained DialoGPT model 'Filosofas/DialoGPT-medium-PALPATINE2'. This model is specifically designed to hold conversational interactions which makes it a good choice for building a chatbot.\\\\n3. Now, you will have a conversational agent initialized that can provide human-like conversational responses based on user input.\\\\n4. Call the pipeline with user input to generate the chatbot's response.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nchatbot = pipeline(\\\\'text-generation\\\\', model=\\\\'Filosofas/DialoGPT-medium-PALPATINE2\\\\')\\\\nuser_input = \\\"I\\\\'m feeling really stressed and overwhelmed.\\\"\\\\nresponse = chatbot(user_input)[0][\\\\'generated_text\\\\']\\\\nprint(response)', 'api_call': \\\"pipeline('text-generation', model='Filosofas/DialoGPT-medium-PALPATINE2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DialoGPT-medium-PALPATINE2', 'api_call': \\\"pipeline('text-generation', model='Filosofas/DialoGPT-medium-PALPATINE2')\\\", 'api_arguments': 'message', 'python_environment_requirements': ['transformers'], 'example_code': \\\"api('Input a message to start chatting with Filosofas/DialoGPT-medium-PALPATINE2.')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A DialoGPT model trained for generating human-like conversational responses.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير تطبيق يساعد ذوي الإعاقة البصرية على الوصول إلى المحتوى. نحتاج إلى ميزة تحويل النصوص في الوثائق إلى أسئلة ليتم الرد عليها من قبل مستخدمي التطبيق عن طريق الصوت أو النص.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline method to set the task to 'question-answering' and load the pre-trained model 'xhyi/layoutlmv3_docvqa_t11c5000'.\\\\n3. This model has been trained on financial documents and can answer questions based on those documents.\\\\n4. Once the model is created, you can query the document by passing the context (the document text) and the question you want to be answered.\\\\n5. The model will return an answer for the query, which can be helpful in finding the dividends paid by the company 'RenewX'.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\'question-answering\\\\', model=\\\\'xhyi/layoutlmv3_docvqa_t11c5000\\\\')\\\\nquestion = \\\"What are the dividends paid by RenewX?\\\"\\\\ncontext = \\\"Financial document text here...\\\"\\\\nresult = qa_pipeline({\\\"question\\\": question, \\\"context\\\": context})', 'api_call': \\\"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'xhyi/layoutlmv3_docvqa_t11c5000', 'api_call': \\\"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\\\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'LayoutLMv3 model trained for document question answering task.' </s>\"\n"}
{"text": "<s>[INST]تقوم الشركة بتطوير نظام لتحديد العمق في رؤية الحاسوب، ونحتاج إلى بناء نموذج يستند إلى البيانات المتوفرة لتحقيق ذلك.? [/INST]{'domain': 'Computer Vision Depth Estimation', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import AutoModel\\\\nfrom PIL import Image\\\\nimport torch\\\\n\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\\\\nimage = Image.open('image_path.jpg') # replace 'image_path.jpg' with path to your image\\\\ninput_tensor = torch.tensor(image).unsqueeze(0) # Convert image to tensor and add batch dimension\\\\ndepth_estimate = model(input_tensor)\\\", 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-014502', 'api_call': \\\"from transformers import AutoModel\\\\nfrom PIL import Image\\\\nimport torch\\\\n\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\\\\nimage = Image.open('image_path.jpg') # replace 'image_path.jpg' with path to your image\\\\ninput_tensor = torch.tensor(image).unsqueeze(0) # Convert image to tensor and add batch dimension\\\\ndepth_estimate = model(input_tensor)\\\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3476, 'Mae': 0.2763, 'Rmse': 0.4088, 'Abs Rel': 0.3308, 'Log Mae': 0.1161, 'Log Rmse': 0.17, 'Delta1': 0.5682, 'Delta2': 0.8301, 'Delta3': 0.9279}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics.'}} </s>\"\n"}
{"text": "<s>[INST]يجب عليك تشغيل التطبيق لتصنيف مقاطع الفيديو باستخدام التقنيات البصرية للحاسوب. بإمكانك تحديد لغة النص واختيار النبرة والسرعة المناسبة للنطق.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the required classes from the transformers package, including VideoMAEFeatureExtractor and VideoMAEForPreTraining. Import numpy for handling input arrays and torch for tensors.\\\\n2. Then, use the from_pretrained method of the VideoMAEForPreTraining class to load the pre-trained model 'MCG-NJU/videomae-base-short-ssv2'. This model is designed for video classification tasks and will be used to classify the type of exercise in a given video.\\\\n3. Extract the features using VideoMAEFeatureExtractor class and prepare the video for input to the model.\\\\n4. Using the model, classify the video into different categories corresponding to the type of exercise being performed.\\\", '<<<code>>>': \\\"from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = load_video_data('exercise_video.mp4') \\\\n# Replace 'load_video_data' function with appropriate way to read video data.\\\\n\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\\n\\\\npixel_values = feature_extractor(video, return_tensors='pt').pixel_values\\\\n\\\\nnum_frames = len(video)\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\n\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss\\\", 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'videomae-base-ssv2', 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\\\", 'api_arguments': 'video', 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\\\npixel_values = feature_extractor(video, return_tensors=pt).pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss', 'performance': {'dataset': 'Something-Something-v2', 'accuracy': ''}, 'description': 'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج لفئة تصنيف الفيديو في مجال رؤية الحاسوب.? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import AutoModelForVideoClassification\\\\nmodel = AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb')\\\\n# Preprocessing video(s) and extracting features is not shown in this example.\\\", 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb', 'api_call': \\\"from transformers import AutoModelForVideoClassification\\\\nmodel = AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb')\\\\n# Preprocessing video(s) and extracting features is not shown in this example.\\\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch, tokenizers, datasets', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': 0.7298}, 'description': 'This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.5482, Accuracy: 0.7298.'}} </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي لتصنيف النصوص بدون تدريب مسبق.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='xlm-roberta-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a masked language modeling model using the 'xlm-roberta-base' model. This model is pre-trained on 100 languages and can generate suitable replacements for masked words without fine-tuning.\\\\n3. Create a unmasker object that can be used to fill in masked words in text inputs.\\\\n4. The unmasker can be utilized in a conversational AI bot whenever a masked word is detected in the input sentence.\\\\n5. Let the AI bot use the unmasker object to complete the sentences by replacing the masked words with suitable choices.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'xlm-roberta-base\\\\')\\\\nunmasked_result = unmasker(\\\"Hello, I\\\\'m a <mask> model.\\\")', 'api_call': \\\"pipeline('fill-mask', model='xlm-roberta-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'xlm-roberta-base', 'api_call': \\\"pipeline('fill-mask', model='xlm-roberta-base')\\\", 'api_arguments': {'model': 'xlm-roberta-base'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='xlm-roberta-base')\\\\nunmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': 'CommonCrawl', 'accuracy': 'N/A'}, 'description': 'XLM-RoBERTa is a multilingual version of RoBERTa pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It can be used for masked language modeling and is intended to be fine-tuned on a downstream task.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام تعرف صوتي تلقائي لتحويل الكلام اللفظي إلى نص مكتوب، ونريد تحسين أداء النظام ودقة التعرف على الكلمات من خلال المزيد من البيانات التدريبية وتطوير نماذج التعرف على الصوت.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'AutoModel' from transformers and 'Image' from PIL.\\\\n2. Use the 'from_pretrained' method of the AutoModel class to load the model, which in this case is 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'.\\\\n3. This model has been fine-tuned for depth estimation tasks, which can help you estimate the depth of various rooms from an image.\\\\n4. Load an image of the room, and then apply the model to that image to get an estimate of the room's depth.\\\\n5. This will assist the customers in visualizing the space on your real estate platform.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nfrom PIL import Image\\\\nroom_image = Image.open('path_to_room_image.jpg')\\\\n# replace 'path_to_room_image.jpg' with the actual path to your room image\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\\\\ninputs = feature_extractor(images=room_image, return_tensors='pt')\\\\ndepth_estimation = model(**inputs)\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221116-054332', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\\\", 'api_arguments': {'model_name': 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'}, 'python_environment_requirements': [{'transformers': '4.24.0', 'pytorch': '1.13.0+cu117', 'tokenizers': '0.13.2'}], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.6028, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج لتصنيف الرموز في معالجة اللغة الطبيعية باستخدام تقنيات التصنيف الرمزي.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes, such as AutoTokenizer and AutoModelForTokenClassification, from the transformers library provided by Hugging Face.\\\\n2. Load the pre-trained model 'd4data/biomedical-ner-all', which is capable of recognizing 107 bio-medical entities from text, using AutoModelForTokenClassification.\\\\n3. Load the corresponding tokenizer for the model using AutoTokenizer.\\\\n4. Create a pipeline using the loaded model, tokenizer, and the pipeline function from the transformers library.\\\\n5. Use the created pipeline to analyze the biomedical paper and extract bio-medical entities from the text.\\\", '<<<code>>>': \\\"from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\\\\nmodel = AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\\ntokenizer = AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\\\\nbiomedical_ner = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy='simple')\\\\nentities = biomedical_ner(biomedical_paper_text)\\\", 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'd4data/biomedical-ner-all', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\", 'api_arguments': {'model': 'AutoModelForTokenClassification.from_pretrained(d4data/biomedical-ner-all)', 'tokenizer': 'AutoTokenizer.from_pretrained(d4data/biomedical-ner-all)', 'aggregation_strategy': 'simple'}, 'python_environment_requirements': [{'transformers': 'from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification'}], 'example_code': 'pipe(The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.)', 'performance': {'dataset': 'Maccrobat', 'accuracy': 'Not provided'}, 'description': 'An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نص تقني يشرح كيفية تحويل النصوص بشكل طبيعي بمجال معالجة اللغات.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"translation_pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library to create the translation pipeline.\\\\n2. Load the German to English translation model 'Helsinki-NLP/opus-mt-de-en' for translating the German news article.\\\\n3. Use the translation pipeline to convert the text from German to English, in order to cater to the company's international website.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_de_to_en\\\\', model=\\\\'Helsinki-NLP/opus-mt-de-en\\\\')\\\\ngerman_text = \\\"Der Bundesrat hat heute neue Maßnahmen für Unternehmen angekündigt, um sie aufgrund der wirtschaftlichen Auswirkungen von COVID-19 zu unterstützen. Die Firmen, die am meisten betroffen sind, erhalten Finanzhilfe von der Regierung, um den Arbeitsplatzverlust zu verhindern. Diese Maßnahmen halten die Wirtschaft des Landes am Laufen und schützen die Arbeitsplätze.\\\"\\\\ntranslated_text = translator(german_text)\\\\nenglish_text = translated_text[0][\\\\'translation_text\\\\']', 'api_call': \\\"translation_pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-de-en', 'api_call': \\\"translation_pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\\\", 'api_arguments': ['source languages: de', 'target languages: en'], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'opus', 'accuracy': {'newssyscomb2009.de.en': 29.4, 'news-test2008.de.en': 27.8, 'newstest2009.de.en': 26.8, 'newstest2010.de.en': 30.2, 'newstest2011.de.en': 27.4, 'newstest2012.de.en': 29.1, 'newstest2013.de.en': 32.1, 'newstest2014-deen.de.en': 34.0, 'newstest2015-ende.de.en': 34.2, 'newstest2016-ende.de.en': 40.4, 'newstest2017-ende.de.en': 35.7, 'newstest2018-ende.de.en': 43.7, 'newstest2019-deen.de.en': 40.1, 'Tatoeba.de.en': 55.4}}, 'description': 'A German to English translation model trained on the OPUS dataset using the Hugging Face Transformers library.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام لتحويل النصوص إلى صور متعددة الوسائط، ونريد توجيه النظام لفهم وتحليل النصوص بشكل دقيق ودقيق.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler), torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We import the necessary classes from the diffusers library, including StableDiffusionPipeline and EulerDiscreteScheduler. We also import torch.\\\\n2. We load the pre-trained model 'stabilityai/stable-diffusion-2-1-base', which is designed for text-to-image generation tasks, along with the EulerDiscreteScheduler model loader.\\\\n3. We provide the text prompt 'an elephant flying with balloons over the plains of Africa' to guide the image generation.\\\\n4. The created pipeline processes the prompt to generate an image illustrating the described scenario. The image can then be saved to a file.\\\", '<<<code>>>': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nimport torch\\\\nmodel_id = \\\"stabilityai/stable-diffusion-2-1-base\\\"\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\\\"scheduler\\\")\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(\\\"cuda\\\")\\\\nprompt = \\\"an elephant flying with balloons over the plains of Africa\\\"\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(\\\"elephant_flying.png\\\")', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler), torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler), torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors']], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nimport torch\\\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>\"\n"}
{"text": "<s>[INST]نوصي بإنشاء نادٍ طلابي يجتمع كل أسبوعين من أجل لعب كرة القدم النصية الافتراضية. قدم لهم أداة للعب مقابل وكيل معالجة النصوص الآلي لتوليف النصوص.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-large-seq2seq')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required AutoTokenizer and AutoModelForSeq2SeqLM classes from the transformers library provided by Hugging Face.\\\\n2. Use AutoTokenizer.from_pretrained and AutoModelForSeq2SeqLM.from_pretrained on 'microsoft/GODEL-v1_1-large-seq2seq' to obtain the pre-trained tokenizer and model.\\\\n3. Define a function to generate a response using the pre-trained model. This function takes the instruction of responding empathically, dialog context, and any knowledge to be used for grounding the response.\\\\n4. Prepare input by joining dialog context using the 'EOS' token, and then format the input string with instruction, dialog, and knowledge.\\\\n5. Tokenize the input string, obtain input_ids, and generate a response from the model.\\\\n6. Decode the output to get the empathic response to the user's query.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'microsoft/GODEL-v1_1-large-seq2seq\\\\')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'microsoft/GODEL-v1_1-large-seq2seq\\\\')\\\\n\\\\ndef generate_empathic_response(instruction, knowledge, dialog):\\\\n    if knowledge != \\\\'\\\\':\\\\n        knowledge = \\\\'[KNOWLEDGE] \\\\' + knowledge\\\\n    dialog_str = \\\\' EOS \\\\'.join(dialog)\\\\n    query = f\\\"{instruction} [CONTEXT] {dialog_str} {knowledge}\\\"\\\\n    input_ids = tokenizer(query, return_tensors=\\\\'pt\\\\').input_ids\\\\n    outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n    return response\\\\n\\\\ninstruction = \\\\'Instruction: given a dialog context, you need to respond empathically.\\\\'\\\\nknowledge = \\\\'\\\\'\\\\ndialog_context = [\\\\n    \\\\'Does money buy happiness?\\\\',\\\\n    \\\\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\\\',\\\\n    \\\\'What is the best way to buy happiness ?\\\\'\\\\n]\\\\n\\\\nempathic_response = generate_empathic_response(instruction, knowledge, dialog_context)\\\\nprint(empathic_response)', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-large-seq2seq')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/GODEL-v1_1-large-seq2seq', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-large-seq2seq')\\\", 'api_arguments': {'instruction': 'Instruction: given a dialog context, you need to response empathically.', 'knowledge': '', 'dialog': ['Does money buy happiness?', 'It is a question. Money buys you a lot of things, but not enough to buy happiness.', 'What is the best way to buy happiness ?']}, 'python_environment_requirements': [{'transformers': 'AutoTokenizer, AutoModelForSeq2SeqLM'}], 'example_code': \\\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != '':\\\\n knowledge = '[KNOWLEDGE] ' + knowledge\\\\n dialog = ' EOS '.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f'Instruction: given a dialog context, you need to response empathically.'\\\\nknowledge = ''\\\\ndialog = [\\\\n 'Does money buy happiness?',\\\\n 'It is a question. Money buys you a lot of things, but not enough to buy happiness.',\\\\n 'What is the best way to buy happiness ?'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\\\", 'performance': {'dataset': 'Reddit discussion thread, instruction and knowledge grounded dialogs', 'accuracy': 'Not provided'}, 'description': 'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.' </s>\"\n"}
{"text": "<s>[INST]قم بتطبيق تقنيات معالجة اللغات الطبيعية لإنشاء واجهة تفاعلية تسمح بإدخال نصوص وصور متعددة لتوليد توصيف محتوى متعدد الوسائط.? [/INST]{'domain': 'Multimodal Text-to-Image', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\\\", 'api_provider': 'Hugging Face', 'explanation': '1. Import the necessary classes from the diffusers package, which includes StableDiffusionPipeline and DPMSolverMultistepScheduler.', 'code': \\\"from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\\\nimport torch\\\\n\\\\nprompt = 'an autumn landscape with trees shedding leaves along a peaceful river at sunset'\\\\nmodel_id = 'darkstorm2150/Protogen_v2.2_Official_Release'\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to('cuda')\\\\n\\\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\\\nimage.save('./result.jpg')\\\", 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \\\"from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\\\nimport torch\\\\n\\\\nprompt = 'an autumn landscape with trees shedding leaves along a peaceful river at sunset'\\\\nmodel_id = 'darkstorm2150/Protogen_v2.2_Official_Release'\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to('cuda')\\\\n\\\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\\\nimage.save('./result.jpg')\\\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\\\nimport torch\\\\nprompt = (\\\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\\\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\\\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\\\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\\\n)\\\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to(cuda)\\\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \\\"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\\\"}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لتصنيف النصوص والصور باستخدام تقنيات معالجة اللغة الطبيعية والرؤية الحاسوبية، تمهيداً بإنشاء سؤال يتعلق بوثيقة متعددة الوسائط والإجابة عليه.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the pipeline function from the transformers package.\\\\n2. Create an instance of OCR-DocVQA-Donut pipeline using the provided model \\\\'jinhybr/OCR-DocVQA-Donut\\\\'. This model uses both vision and text processing capabilities to answer questions based on visual documents.\\\\n3. You now have a document-question-answering pipeline tailored for OCR and can use it for extracting titles from different documents.\\\\n4. To use the pipeline, pass in the image path of the document you wish to process and the question \\\"What is the title?\\\".', '<<<code>>>': \\\"from transformers import pipeline\\\\ndoc_vqa = pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\\ntitle = doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\\\", 'api_call': \\\"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': \\\"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\\\", 'api_arguments': 'image_path, question', 'python_environment_requirements': ['transformers'], 'example_code': \\\"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\\\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>\"\n"}
{"text": "<s>[INST]لتحسين الدقة في تصنيف الصور باستخدام الرؤية الحاسوبية، قم بتدريب نموذج التعلم الآلي على مجموعة بيانات الصور المعينة. ثمّ، قم بتقديم تحديا للطلاب بالاستفادة من النموذج المدرب وإضافة عنصر منافسية بتوظيف وكيل التعلم الآلي كخصم.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', model='saltacc/anime-ai-detect')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create an image classification pipeline by specifying the required model 'saltacc/anime-ai-detect'. This model is a BEiT classifier trained to differentiate between AI-generated and human-created anime artworks.\\\\n3. Use the created pipeline object to classify the anime artwork based on the input image.\\\\n4. The model will return a classification result that indicates if the anime art was created by an AI or a human artist.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nimport cv2\\\\n\\\\nimg = cv2.imread('your_anime_art_image.jpg')\\\\nclassifier = pipeline('image-classification', model='saltacc/anime-ai-detect')\\\\nresult = classifier(img)\\\", 'api_call': \\\"pipeline('image-classification', model='saltacc/anime-ai-detect')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'saltacc/anime-ai-detect', 'api_call': \\\"pipeline('image-classification', model='saltacc/anime-ai-detect')\\\", 'api_arguments': ['image'], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'aibooru and imageboard sites', 'accuracy': '96%'}, 'description': 'A BEiT classifier to see if anime art was made by an AI or a human.' </s>\"\n"}
{"text": "<s>[INST]قم بتدريب نموذج اللغة الطبيعية على أسئلة وأجوبة ذاتية التقويم باستخدام مجموعة البيانات المعينة من أجل تطوير نظام فعال للإجابة عن الأسئلة.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"AutoModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' function from the transformers library.\\\\n2. Create a question-answering pipeline using the 'bert-large-uncased-whole-word-masking-finetuned-squad' model, which is a BERT large model specifically fine-tuned for question-answering tasks.\\\\n3. Pass the question and the context (text passage) to the pipeline, which will then return the answer.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad')\\\\nresult = qa_pipeline({'question': 'What is the capital of France?', 'context': 'Paris is the capital of France.'})\\\\nprint(result['answer'])\\\", 'api_call': \\\"AutoModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bert-large-uncased-whole-word-masking-finetuned-squad', 'api_call': \\\"AutoModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\\\", 'api_arguments': {'model_name_or_path': 'bert-large-uncased-whole-word-masking', 'dataset_name': 'squad', 'do_train': True, 'do_eval': True, 'learning_rate': 3e-05, 'num_train_epochs': 2, 'max_seq_length': 384, 'doc_stride': 128, 'output_dir': './examples/models/wwm_uncased_finetuned_squad/', 'per_device_eval_batch_size': 3, 'per_device_train_batch_size': 3}, 'python_environment_requirements': [{'torch': '1.9.0', 'transformers': '4.9.2'}], 'example_code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad')\\\\nresult = qa_pipeline({'question': 'What is the capital of France?', 'context': 'Paris is the capital of France.'})\\\\nprint(result)\\\", 'performance': {'dataset': 'SQuAD', 'accuracy': {'f1': 93.15, 'exact_match': 86.91}}, 'description': 'BERT large model (uncased) whole word masking finetuned on SQuAD. The model was pretrained on BookCorpus and English Wikipedia. It was trained with two objectives: Masked language modeling (MLM) and Next sentence prediction (NSP). This model should be used as a question-answering model.' </s>\"\n"}
{"text": "<s>[INST]العميل يرغب في تطوير تطبيق يستخدم رؤية الحاسوب لتحويل الصور إلى صور أخرى. نحن بحاجة إلى برنامج يقوم بتحويل الصور من نوع إلى نوع آخر لمستخدمي هذا التطبيق.? [/INST]{'domain': 'Computer Vision Image-to-Image', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-hed')\\\", 'api_provider': 'Hugging Face', 'explanation': '', 'code': \\\"from PIL import Image\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom controlnet_aux import HEDdetector, load_image\\\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png')\\\\nimage = hed(image)\\\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-hed', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\noil_painting_image = pipe('oil painting of handsome old man, masterpiece', image, num_inference_steps=20).images[0]\\\\noil_painting_image.save('images/man_hed_out.png')\\\", 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-hed', 'api_call': \\\"from PIL import Image\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom controlnet_aux import HEDdetector, load_image\\\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png')\\\\nimage = hed(image)\\\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-hed', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\noil_painting_image = pipe('oil painting of handsome old man, masterpiece', image, num_inference_steps=20).images[0]\\\\noil_painting_image.save('images/man_hed_out.png')\\\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \\\"from PIL import Image\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom controlnet_aux import HEDdetector\\\\nfrom diffusers.utils import load_image\\\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png)\\\\nimage = hed(image)\\\\ncontrolnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-hed, torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\nimage = pipe(oil painting of handsome old man, masterpiece, image, num_inference_steps=20).images[0]\\\\nimage.save('images/man_hed_out.png')\\\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.'}} </s>\"\n"}
{"text": "<s>[INST]عميلنا يعمل في مجال تعلم الآلة البصري والطرق المتعددة للرسم البياني. نحتاج إلى تطوير برنامج قادر على التعلم من البيانات المتعددة والرسوم البيانية لتحسين الأداء وتحسين التواصل بين المستخدمين.? [/INST]'<<<domain>>>': 'Multimodal Graph Machine Learning', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary libraries and modules, including the transformers library provided by Hugging Face.\\\\n2. We then use the from_pretrained method to load the pre-trained model 'janpase97/codeformer-pretrained'. This model can generate code that can be used to control an NPC in a game like GTA5.\\\\n3. We need to design a neural network for the model. In this case, it could be as simple as a 2-layer fully connected network to control the driving actions of the NPC based on inputs such as traffic conditions and vehicle speed.\\\\n4. The model is then trained with a data cap on the selected dataset (e.g., MNIST) using the PyTorch framework.\\\\n5. We check if the targeted game process is running and attach the trained model to the game.\\\\n6. The AI model can then control the NPC van and make it drive properly in traffic.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport torch\\\\nimport torch.optim as optim\\\\nimport torch.nn as nn\\\\nfrom torchvision import datasets, transforms\\\\n\\\\ndef train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):\\\\n    data_processed = 0\\\\n    data_cap_bytes = data_cap_gb * (1024 ** 3)\\\\n    epoch = 0\\\\n    while data_processed < data_cap_bytes:\\\\n        running_loss = 0.0\\\\n        for i, data in enumerate(data_loader, 0):\\\\n            inputs, labels = data\\\\n            inputs, labels = inputs.to(device), labels.to(device)\\\\n            data_processed += inputs.nelement() * inputs.element_size()\\\\n            if data_processed >= data_cap_bytes:\\\\n                break\\\\n            optimizer.zero_grad()\\\\n            outputs = model(inputs.view(-1, 28 * 28))\\\\n            loss = criterion(outputs, labels)\\\\n            loss.backward()\\\\n            optimizer.step()\\\\n            running_loss += loss.item()\\\\n        epoch += 1\\\\n    print(f\\\\'Epoch {epoch}, Loss: {running_loss / (i + 1)}\\\\')\\\\n    print(f\\\\'Data processed: {data_processed / (1024 ** 3):.2f} GB\\\\')\\\\n    return model\\\\n\\\\n# Load the model\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\"janpase97/codeformer-pretrained\\\")\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\"janpase97/codeformer-pretrained\\\")\\\\ndevice = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\\\n\\\\n# Load training data\\\\ntrain_loader = torch.utils.data.DataLoader(\\\\n    datasets.MNIST(\\\"mnist_data\\\", train=True, download=True, transform=transforms.ToTensor()),\\\\n    batch_size=64, shuffle=True)\\\\n\\\\n# Neural network\\\\nclass NanoCircuit(nn.Module):\\\\n    def __init__(self):\\\\n        super(NanoCircuit, self).__init__()\\\\n        self.fc1 = nn.Linear(784, 128)\\\\n        self.fc2 = nn.Linear(128, 10)\\\\n\\\\n    def forward(self, x):\\\\n        x = x.view(-1, 784)\\\\n        x = torch.relu(self.fc1(x))\\\\n        x = self.fc2(x)\\\\n        return x\\\\n\\\\n# Create and train the model\\\\nmy_model = NanoCircuit()\\\\nmy_model.to(device)\\\\ncriterion = nn.CrossEntropyLoss()\\\\noptimizer = optim.SGD(my_model.parameters(), lr=0.001, momentum=0.9)\\\\ntrained_model = train_with_data_cap(my_model, train_loader, criterion, optimizer, device, data_cap_gb=10)', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GTA5_PROCESS_LEARNING_AI', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\\\", 'api_arguments': {'model': 'NanoCircuit', 'data_loader': 'train_loader', 'criterion': 'nn.CrossEntropyLoss', 'optimizer': 'optim.SGD', 'device': 'torch.device', 'data_cap_gb': 10}, 'python_environment_requirements': [['contextlib', 'os', 'matplotlib', 'numpy', 'torch', 'torch.nn', 'torch.optim', 'requests', 'torchvision', 'psutil', 'time', 'subprocess', 'onnxruntime', 'numexpr', 'transformers']], 'example_code': {'import_libraries': ['import contextlib', 'import os', 'from matplotlib import pyplot as plt', 'import numpy as np', 'import torch', 'import torch.nn as nn', 'import torch.optim as optim', 'import requests', 'from torchvision import datasets, transforms', 'import psutil', 'import time', 'import subprocess', 'import onnxruntime as ort', 'import matplotlib.pyplot as plt', 'import numpy as np', 'import numexpr as ne', 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM'], 'define_neural_network': ['class NanoCircuit(nn.Module):', ' def init(self):', ' super(NanoCircuit, self).init()', ' self.fc1 = nn.Linear(784, 128)', ' self.fc2 = nn.Linear(128, 10)', 'def forward(self, x):', ' x = x.view(-1, 784)', ' x = torch.relu(self.fc1(x))', ' x = self.fc2(x)', ' return x'], 'train_with_data_cap': ['def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):', ' data_processed = 0', ' data_cap_bytes = data_cap_gb * (1024 ** 3)', ' epoch = 0', 'while data_processed < data_cap_bytes:', ' running_loss = 0.0', ' for i, data in enumerate(data_loader, 0):', ' inputs, labels = data', ' inputs, labels = inputs.to(device), labels.to(device)', ' data_processed += inputs.nelement() * inputs.element_size()', ' if data_processed >= data_cap_bytes:', ' break', ' optimizer.zero_grad()', ' outputs = model(inputs.view(-1, 28 * 28))', ' loss = criterion(outputs, labels)', ' loss.backward()', ' optimizer.step()', ' running_loss += loss.item()', 'epoch += 1', 'print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})', 'print(fData processed: {data_processed / (1024 ** 3):.2f} GB)', 'return model']}, 'performance': {'dataset': 'MNIST', 'accuracy': 'Not specified'}, 'description': 'This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بالبدء في تصنيف الصور باستخدام تقنيات الرؤية الحاسوبية والنصوص. هل يمكنك مساعدتي في تحقيق ذلك في مجال النصوص والصور المرتبطة؟? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import Text2ImagePipeline from the transformers package provided by Hugging Face.\\\\n2. Use the Text2ImagePipeline to create a text-to-image model, specifying the 'Linaqruf/anything-v3.0' model.\\\\n3. This model can be used to generate images based on text descriptions, so you provide the model with the client's description to create an image for the book cover.\\\", '<<<code>>>': 'from transformers import Text2ImagePipeline\\\\ntext2image = Text2ImagePipeline(model=\\\\'Linaqruf/anything-v3.0\\\\')\\\\ndescription = \\\"A mysterious forest with a full moon shining through the trees and a dark figure walking in the shadows.\\\"\\\\noutput_image = text2image(description)', 'api_call': \\\"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \\\"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>\"\n"}
{"text": "<s>[INST]سنقوم بعقد جلسة تدريبية في مجال التعلم التعزيزي. قم بقراءة وفهم نظريات ومفاهيم التعلم التعزيزي. بعد ذلك، بدأ في تطبيق النماذج الحاسوبية والخوارزميات المختلفة لتدريب النماذج في بهدف تحسين أداءها على المهمة المعينة.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"load_from_hub(repo_id='sb3/td3-Ant-v3,filename='{MODEL FILENAME}.zip',)\\\", '<<<api_provider>>>': 'Stable-Baselines3', '<<<explanation>>>': '1. We will utilize the Stable-Baselines3 library, which provides a collection of reinforcement learning algorithms and pre-trained agents.\\\\n2. From this library, we will select the TD3 algorithm (Twin Delayed Deep Deterministic Policy Gradient) as it has been proven to be effective in continuous control tasks.\\\\n3. We will use the RL Zoo, a training framework for Stable Baselines3 reinforcement learning agents, to train the TD3 agent in a hospital environment.\\\\n4. After training the agent, it can be deployed in a navigation robot, allowing it to autonomously navigate the hospital.', '<<<code>>>': \\\"from stable_baselines3.common import load_from_hub\\\\nimport gym\\\\n\\\\n# Load pre-trained model\\\\nmodel = load_from_hub(repo_id='sb3/td3-Ant-v3', filename='{MODEL FILENAME}.zip')\\\\n\\\\n# Create the hospital environment (custom environment required)\\\\nenv = gym.make('HospitalNavigation-v0')\\\\n\\\\n# Enjoy the trained agent\\\\nobs = env.reset()\\\\ndone = False\\\\nwhile not done:\\\\n    action, _states = model.predict(obs)\\\\n    obs, rewards, done, info = env.step(action)\\\\n    env.render()\\\", 'api_call': \\\"load_from_hub(repo_id='sb3/td3-Ant-v3,filename='{MODEL FILENAME}.zip',)\\\", 'provider': 'Stable-Baselines3', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'td3-Ant-v3', 'api_call': \\\"load_from_hub(repo_id='sb3/td3-Ant-v3,filename='{MODEL FILENAME}.zip',)\\\", 'api_arguments': ['algo', 'env', 'f'], 'python_environment_requirements': [['RL Zoo: https://github.com/DLR-RM/rl-baselines3-zoo', 'SB3: https://github.com/DLR-RM/stable-baselines3', 'SB3 Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib']], 'example_code': ['python -m rl_zoo3.load_from_hub --algo td3 --env Ant-v3 -orga sb3 -f logs/', 'python enjoy.py --algo td3 --env Ant-v3 -f logs/', 'python train.py --algo td3 --env Ant-v3 -f logs/', 'python -m rl_zoo3.push_to_hub --algo td3 --env Ant-v3 -f logs/ -orga sb3'], 'performance': {'dataset': 'Ant-v3', 'accuracy': '5822.96 +/- 93.33'}, 'description': 'This is a trained model of a TD3 agent playing Ant-v3 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.' </s>\"\n"}
{"text": "<s>[INST]تهدف الشركة إلى تطوير تقنية لتقدير عمق الرؤية الحاسوبية في التصوير، مع التركيز على دقة التقدير وسرعة الاستجابة للنظام.? [/INST]{'domain': 'Computer Vision Depth Estimation', 'api_call': \\\"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import DPTForDepthEstimation\\\\ndepth_estimator = DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\\\\ndepth_map = depth_estimator.predict(image)\\\", 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'tiny-random-DPTForDepthEstimation', 'api_call': \\\"from transformers import DPTForDepthEstimation\\\\ndepth_estimator = DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\\\\ndepth_map = depth_estimator.predict(image)\\\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random DPT model for depth estimation using Hugging Face Transformers library.'}} </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نظام يستخدم تقنيات معالجة اللغة الطبيعية لملء الفجوات في النصوص، حيث يقوم النظام بتحليل النصوص وتحديد المكان المناسب لوضع الكلمات الناقصة. نهدف إلى تحسين أداء النظام من خلال تحسين وتطوير خوارزمية معالجة اللغة الطبيعية لدينا.? [/INST]{'domain': 'Natural Language Processing Fill-Mask', 'api_call': 'pipeline(\\\\'fill-mask\\\\', model=\\\"cl-tohoku/bert-base-japanese-whole-word-masking\\\")', 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the 'pipeline' function from the transformers package.\\\", 'code': 'from transformers import pipeline\\\\nfill_mask = pipeline(\\\\'fill-mask\\\\', model=\\\"cl-tohoku/bert-base-japanese-whole-word-masking\\\")\\\\ninput_sentence = \\\"彼女は[MASK]を飲んでいます。\\\"\\\\noutput = fill_mask(input_sentence)\\\\nword_predictions = [prediction[\\\"token_str\\\"] for prediction in output]\\\\nprint(word_predictions)', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Fill-Mask', 'api_name': 'cl-tohoku/bert-base-japanese-whole-word-masking', 'api_call': 'from transformers import pipeline\\\\nfill_mask = pipeline(\\\\'fill-mask\\\\', model=\\\"cl-tohoku/bert-base-japanese-whole-word-masking\\\")\\\\ninput_sentence = \\\"彼女は[MASK]を飲んでいます。\\\"\\\\noutput = fill_mask(input_sentence)\\\\nword_predictions = [prediction[\\\"token_str\\\"] for prediction in output]\\\\nprint(word_predictions)', 'api_arguments': {'model': 'cl-tohoku/bert-base-japanese-whole-word-masking'}, 'python_environment_requirements': {'transformers': 'latest'}, 'example_code': \\\"fill_mask('[MASK]')\\\", 'performance': {'dataset': 'Japanese Wikipedia', 'accuracy': 'Not provided'}, 'description': 'This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization. Additionally, the model is trained with the whole word masking enabled for the masked language modeling (MLM) objective.'}} </s>\"\n"}
{"text": "<s>[INST]سيقوم فريق البحث الخاص بنا في مجال تجهيز اللغة الطبيعية ببناء نموذج للإجابة على الأسئلة المتعلقة بالجداول، نريد القدرة على استخدام هذا النموذج لتوقع الإجابة على أسئلة عن الرواتب بناءً على البيانات المتاحة.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"BartForConditionalGeneration.from_pretrained('microsoft/tapex-base-finetuned-wtq')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include TapexTokenizer and BartForConditionalGeneration from the transformers package, and pandas for creating data tables.\\\\n2. Load the tokenizer and the model using their respective from_pretrained methods.\\\\n3. Convert the provided table_data into a pandas DataFrame.\\\\n4. Use the tokenizer to encode the table and the query.\\\\n5. Call the model's generate method to obtain an answer to the query.\\\\n6. Decode the answer using the tokenizer's batch_decode method.\\\", '<<<code>>>': 'from transformers import TapexTokenizer, BartForConditionalGeneration\\\\nimport pandas as pd\\\\ntokenizer = TapexTokenizer.from_pretrained(\\\\'microsoft/tapex-base-finetuned-wtq\\\\')\\\\nmodel = BartForConditionalGeneration.from_pretrained(\\\\'microsoft/tapex-base-finetuned-wtq\\\\')\\\\ntable_data = [{\\\"year\\\": 1896, \\\"city\\\": \\\"Athens\\\"}, {\\\"year\\\": 1900, \\\"city\\\": \\\"Paris\\\"}, {\\\"year\\\": 1904, \\\"city\\\": \\\"St. Louis\\\"}, {\\\"year\\\": 2004, \\\"city\\\": \\\"Athens\\\"}, {\\\"year\\\": 2008, \\\"city\\\": \\\"Beijing\\\"}, {\\\"year\\\": 2012, \\\"city\\\": \\\"London\\\"}]\\\\ntable = pd.DataFrame(table_data)\\\\nquery = \\\"In which year did Beijing host the Olympic Games?\\\"\\\\nencoding = tokenizer(table=table, query=query, return_tensors=\\\\'pt\\\\')\\\\noutputs = model.generate(**encoding)\\\\nanswer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('microsoft/tapex-base-finetuned-wtq')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/tapex-base-finetuned-wtq', 'api_call': \\\"BartForConditionalGeneration.from_pretrained('microsoft/tapex-base-finetuned-wtq')\\\", 'api_arguments': {'tokenizer': 'TapexTokenizer.from_pretrained(microsoft/tapex-base-finetuned-wtq)', 'model': 'BartForConditionalGeneration.from_pretrained(microsoft/tapex-base-finetuned-wtq)', 'table': 'pd.DataFrame.from_dict(data)', 'query': 'query'}, 'python_environment_requirements': [['transformers', 'pandas']], 'example_code': 'from transformers import TapexTokenizer, BartForConditionalGeneration\\\\nimport pandas as pd\\\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-base-finetuned-wtq)\\\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-base-finetuned-wtq)\\\\ndata = {\\\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\\\n city: [athens, paris, st. louis, athens, beijing, london]\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = In which year did beijing host the Olympic Games?\\\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))', 'performance': {'dataset': 'wikitablequestions', 'accuracy': 'Not provided'}, 'description': 'TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لترجمة نصوص في مجال معالجة اللغات الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries from the transformers package.\\\\n2. Use the 'from_pretrained()' function to load the pre-trained model 'Helsinki-NLP/opus-mt-ROMANCE-en', which translates Romance languages (French, Spanish, Portuguese, Italian, Romanian, etc.) into English.\\\\n3. Create a function that wraps the model, and when given a text input in one of the supported Romance languages, translates it into English.\\\\n4. Integrate this function into the English-speaking toy so that it can receive text inputs and respond accordingly.\\\", '<<<code>>>': 'from transformers import MarianTokenizer, MarianMTModel\\\\nmodel_name = \\\\'Helsinki-NLP/opus-mt-ROMANCE-en\\\\'\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\n\\\\ndef translate(input_text):\\\\n    batch = tokenizer.prepare_translation_batch(src_texts=[input_text])\\\\n    translated = model.generate(**batch)\\\\n    return tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\\\\n\\\\ntranslated_text = translate(\\\"Input the Romance language text here\\\")', 'api_call': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-ROMANCE-en', 'api_call': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", 'api_arguments': ['source languages', 'target languages'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': '', 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': 62.2, 'chr-F': 0.75}}, 'description': 'A model for translating Romance languages to English, trained on the OPUS dataset. It supports multiple source languages such as French, Spanish, Portuguese, Italian, and Romanian, among others. The model is based on the transformer architecture and uses normalization and SentencePiece for pre-processing.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا ببناء منتج يتيح تحويل الصور إلى نصوص متعددة الوسائط، لذلك نريد التنبؤ بتوصيف الصور بناءً على بعض مجموعات البيانات المتوفرة.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model for Optical Character Recognition (OCR), which in this case is 'kha-white/manga-ocr-base'.\\\\n3. This loaded model was specifically designed to work with Japanese text and has been trained primarily for Japanese manga image-to-text tasks.\\\\n4. After loading the OCR model, use the pipeline to provide the manga image and the model will return the recognized text from the manga image.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nocr_pipeline = pipeline(\\\\'ocr\\\\', model=\\\\'kha-white/manga-ocr-base\\\\')\\\\nimage_path = \\\"path/to/your/manga/image.jpg\\\"\\\\nwith open(image_path, \\\\'rb\\\\') as image_file:\\\\n    image = image_file.read()\\\\nrecognized_text = ocr_pipeline(image)', 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'api_arguments': 'image', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء جدول بيانات يحتوي على العوامل التنبؤية والمتوقعة، ثم قم بتطبيق تقنية الانحدار الخطي للتنبؤ بالقيم المستقبلية.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load(hf_hub_download('srg/outhimar_64-Close-regression', 'sklearn_model.joblib'))\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'joblib' from joblib and 'hf_hub_download' from a library like huggingface_hub.\\\\n2. Load the pre-trained tabular regression model 'srg/outhimar_64-Close-regression' from Hugging Face Model Hub using the joblib.load function.\\\\n3. Prepare the input data with the necessary parameters like temperature, time of the day, and humidity.\\\\n4. Use the loaded model to predict the energy consumption for a day based on input data.\\\", '<<<code>>>': \\\"from joblib import load\\\\nfrom huggingface_hub import hf_hub_download\\\\nmodel = load(hf_hub_download('srg/outhimar_64-Close-regression', 'sklearn_model.joblib'))\\\\ninputs = {'temperature': 25, 'time': 12, 'humidity': 60}\\\\n# replace values with your actual data\\\\nprediction = model.predict([inputs])[0]\\\", 'api_call': \\\"joblib.load(hf_hub_download('srg/outhimar_64-Close-regression', 'sklearn_model.joblib'))\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'srg/outhimar_64-Close-regression', 'api_call': \\\"joblib.load(hf_hub_download('srg/outhimar_64-Close-regression', 'sklearn_model.joblib'))\\\", 'api_arguments': ['X_train', 'y_train'], 'python_environment_requirements': [['scikit-learn', 'dabl']], 'example_code': \\\"Pipeline(steps=[('easypreprocessor',EasyPreprocessor(types= continuous dirty_float low_card_int ... date free_string useless\\\\nDate False False False ... True False False\\\\nOpen True False False ... False False False\\\\nHigh True False False ... False False False\\\\nLow True False False ... False False False\\\\nAdj Close True False False ... False False False\\\\nVolume True False False ... False False False[6 rows x 7 columns])),('ridge', Ridge(alpha=10))])\\\", 'performance': {'dataset': 'outhimar_64', 'accuracy': {'r2': 0.999858, 'neg_mean_squared_error': -1.067685}}, 'description': 'Baseline Model trained on outhimar_64 to apply regression on Close. Disclaimer: This model is trained with dabl library as a baseline, for better results, use AutoTrain. Logs of training including the models tried in the process can be found in logs.txt.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير منتج للمعالجة اللغوية الطبيعية والإجابة عن الأسئلة، ونريد استخدام بعض البيانات المتاحة للتنبؤ بإجابات الأسئلة وتحسين الأداء.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face Transformer.\\\\n2. Use the pipeline function to create a question-answering model.\\\\n3. Specify the model 'sultan/BioM-ELECTRA-Large-SQuAD2' to be loaded. This model is fine-tuned for the SQuAD2.0 dataset and is trained specifically for handling biomedical texts.\\\\n4. The created model can be used to answer questions related to medical findings present in the research text.\\\\n5. Provide the research text as context and ask relevant questions to extract important information.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\\nresult = qa_pipeline({'context': 'your_research_text', 'question': 'your_medical_question'})\\\", 'api_call': \\\"pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sultan/BioM-ELECTRA-Large-SQuAD2', 'api_call': \\\"pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\", 'api_arguments': None, 'python_environment_requirements': [['transformers', 'sentencepiece']], 'example_code': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\\\nresult = qa_pipeline({'context': 'your_context', 'question': 'your_question'})\\\", 'performance': {'dataset': 'SQuAD2.0 Dev', 'accuracy': {'exact': 84.33420365535248, 'f1': 87.49354241889522}}, 'description': 'BioM-ELECTRA-Large-SQuAD2 is a fine-tuned version of BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts, on the SQuAD2.0 dataset. Fine-tuning the biomedical language model on the SQuAD dataset helps improve the score on the BioASQ challenge. This model is suitable for working with BioASQ or biomedical QA tasks.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نظام لتعريف وتحويل نصوص الكلام في مقاطع الصوت إلى نصوص مكتوبة تلقائيًا باستخدام تقنيات التعرف الآلي على الكلام. هل يمكنك مساعدتي في تحقيق هذا؟? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import necessary libraries like 'WhisperProcessor', 'WhisperForConditionalGeneration' from transformers and 'load_dataset' from datasets.\\\", 'code': '', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Transcription and Translation', 'api_name': 'openai/whisper-medium', 'api_call': '', 'api_arguments': ['sample', 'sampling_rate', 'language', 'task', 'skip_special_tokens'], 'python_environment_requirements': ['transformers', 'datasets'], 'example_code': 'from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\n\\\\nprocessor = WhisperProcessor.from_pretrained(openai/whisper-medium)\\\\nmodel = WhisperForConditionalGeneration.from_pretrained(openai/whisper-medium)\\\\n\\\\nmodel.config.forced_decoder_ids = None\\\\n\\\\nds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\\\\nsample = ds[0][audio]\\\\ninput_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\\\\n\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)', 'performance': {'dataset': [{'name': 'LibriSpeech (clean)', 'accuracy': 2.9}, {'name': 'LibriSpeech (other)', 'accuracy': 5.9}, {'name': 'Common Voice 11.0', 'accuracy': 53.87}]}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and was trained on either English-only data or multilingual data.'}} </s>\"\n"}
{"text": "<s>[INST]يعمل الباحث على تطوير نموذج لتوليد النصوص الطبيعية بتقنية معالجة اللغات الطبيعية. نحن بحاجة إلى برنامج يستطيع تحويل النصوص إلى كلام لمستخدمي هذا المجال.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the transformers library provided by Hugging Face.\\\\n2. Import the 'pipeline' function from the transformers library.\\\\n3. Use the pipeline function to create a conversational model with the 'ingen51/DialoGPT-medium-GPT4' model, which is a GPT-4 based model specifically trained for generating human-like conversational responses.\\\\n4. Pass a message or a series of messages to the created pipeline to generate a conversational reply.\\\\n5. Embed the pipeline in your product to generate text based on user inputs while chatting.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nchat_pipeline = pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\\\\nuser_message = 'Hello, how are you?'\\\\ngenerated_response = chat_pipeline(user_message)\\\", 'api_call': \\\"pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ingen51/DialoGPT-medium-GPT4', 'api_call': \\\"pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\\\", 'api_arguments': ['message'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"conversational_pipeline('Hello, how are you?')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A GPT-4 model for generating conversational responses in a dialogue setting.' </s>\"\n"}
{"text": "<s>[INST]قم بتدريب نموذج الرؤية الحاسوبية في تقدير العمق للصور بواسطة بناء مجموعة بيانات كبيرة وتحديد أنواع مختلفة من الصور المتعلقة بالرؤية الحاسوبية مثل السيارات، الحيوانات، الأثاث وغيرها.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To create a depth estimation model, import the pipeline function from the transformers library.\\\\n2. Use the pipeline function to create a depth estimation model by specifying the 'depth-estimation' pipeline and the pre-trained model 'sayakpaul/glpn-kitti-finetuned-diode-221214-123047'.\\\\n3. This model has been fine-tuned on the diode-subset dataset and can be used for depth estimation tasks needed in your computer vision application.\\\\n4. To use the model, simply provide an input image, and the model will return depth estimations for the objects present in the image.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ndepth_estimator = pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\\\", 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-kitti-finetuned-diode-221214-123047', 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2']], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3497, 'Mae': 0.2847, 'Rmse': 0.3977, 'Abs Rel': 0.3477, 'Log Mae': 0.1203, 'Log Rmse': 0.1726, 'Delta1': 0.5217, 'Delta2': 0.8246, 'Delta3': 0.9436}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى إنشاء نموذج يتمكن من تحويل النص إلى كلام بصوت طبيعي باستخدام تقنيات متقدمة في مجال معالجة اللغات الطبيعية. يجب على النموذج أن يستطيع تحويل النص إلى كلام بصوت واضح وجودة عالية لتحسين تجربة المستخدم وضمان فعالية العملية.? [/INST]{'domain': 'Audio Text-to-Speech', 'api_call': \\\"unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')\\\", 'api_provider': 'Fairseq', 'explanation': '1. First, we need to install and import the necessary packages, including HuggingFace Hub and Fairseq.', 'code': 'from huggingface_hub import ModelHubMixin\\\\nfrom fairseq import unit\\\\nmodel = unit.TTS.from_pretrained(\\\\'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10\\\\')\\\\n# provide a sentence to convert text-to-speech\\\\nsentence = \\\"Hello, how are you?\\\"\\\\naudio = model(sentence)', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Fairseq', 'functionality': 'Text-to-Speech', 'api_name': 'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10', 'api_call': 'from huggingface_hub import ModelHubMixin\\\\nfrom fairseq import unit\\\\nmodel = unit.TTS.from_pretrained(\\\\'facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10\\\\')\\\\n# provide a sentence to convert text-to-speech\\\\nsentence = \\\"Hello, how are you?\\\"\\\\naudio = model(sentence)', 'api_arguments': None, 'python_environment_requirements': 'huggingface_hub, fairseq', 'example_code': None, 'performance': {'dataset': 'covost2', 'accuracy': None}, 'description': 'A text-to-speech model trained on multiple datasets including mtedx, covost2, europarl_st, and voxpopuli. Supports English, Spanish, French, and Italian languages.'}} </s>\"\n"}
{"text": "<s>[INST]تطوير نظام لتقسيم الصور في مجال رؤية الحاسوب.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, including SegformerFeatureExtractor, SegformerForSemanticSegmentation, PIL, and requests.\\\\n2. Instantiate the feature extractor and the model using the from_pretrained method. The model we'll be using is 'nvidia/segformer-b5-finetuned-ade-640-640', which is specifically designed for semantic segmentation tasks.\\\\n3. Obtain the image from the drone's camera, and use the PIL library to open and preprocess it.\\\\n4. Use the feature extractor to transform the image into an input tensor that is compatible with the segmentation model.\\\\n5. Pass the input tensor through the model to obtain the segmentation logits, which can then be used to identify various objects in the environment.\\\", '<<<code>>>': \\\"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\\nimage_url = 'http://path_to_drone_image.jpg'\\\\nimage = Image.open(requests.get(image_url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\", 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nvidia/segformer-b5-finetuned-ade-640-640', 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\\\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20K', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى تنفيذ عملية تصنيف جداولية باستخدام البيانات التي تم جمعها وتجهيزها مسبقًا. الهدف هو بناء نموذج يستطيع تصنيف البيانات بدقة عالية وفقًا للمعايير المحددة.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv')))\\\", '<<<api_provider>>>': 'Scikit-learn', '<<<explanation>>>': \\\"1. We use the Scikit-learn library to create a machine learning model for wine quality classification. We load the pre-trained model from the Hugging Face Model Hub, which is stored in a joblib format.\\\\n2. Import the necessary libraries such as huggingface_hub, joblib, and pandas.\\\\n3. Download the pre-trained model and dataset using the Hugging Face Hub's cached_download and hf_hub_url functions.\\\\n4. Load the model using joblib.load and read the data using pandas to create a dataframe that contains the wine samples' features and target values.\\\\n5. We can now use the loaded model to predict the wine quality based on the input features. The model's performance can also be assessed using its score method.\\\", '<<<code>>>': \\\"from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\n\\\\nREPO_ID = 'julien-c/wine-quality'\\\\nFILENAME = 'sklearn_model.joblib'\\\\ndata_file = cached_download(hf_hub_url(REPO_ID, 'winequality-red.csv'))\\\\nmodel = joblib.load(cached_download(hf_hub_url(REPO_ID, FILENAME)))\\\\n\\\\nwinedf = pd.read_csv(data_file, sep=';')\\\\nX = winedf.drop(['quality'], axis=1)\\\\nY = winedf['quality']\\\\nlabels = model.predict(X[:3])\\\\nmodel_score = model.score(X, Y)\\\", 'api_call': \\\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv')))\\\", 'provider': 'Scikit-learn', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'julien-c/wine-quality', 'api_call': \\\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv')))\\\", 'api_arguments': ['X'], 'python_environment_requirements': [['huggingface_hub', 'joblib', 'pandas']], 'example_code': 'from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nREPO_ID = julien-c/wine-quality\\\\nFILENAME = sklearn_model.joblib\\\\nmodel = joblib.load(cached_download(\\\\n hf_hub_url(REPO_ID, FILENAME)\\\\n))\\\\ndata_file = cached_download(\\\\n hf_hub_url(REPO_ID, winequality-red.csv)\\\\n)\\\\nwinedf = pd.read_csv(data_file, sep=;)\\\\nX = winedf.drop([quality], axis=1)\\\\nY = winedf[quality]\\\\nprint(X[:3])\\\\nlabels = model.predict(X[:3])\\\\nmodel.score(X, Y)', 'performance': {'dataset': 'julien-c/wine-quality', 'accuracy': 0.6616635397123202}, 'description': 'A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نماذج تعلم آلي لتصنيف النصوص بشكل فوري دون الحاجة إلى تدريب مسبق، ونحتاج إلى تقدير قدرة هذه النماذج على التعرف على الفئات المختلفة من دون توجيه مسبق.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('typeform/distilbert-base-uncased-mnli')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the AutoTokenizer and AutoModelForSequenceClassification classes from the transformers library.\\\\n2. Load the 'typeform/distilbert-base-uncased-mnli' pre-trained model, which is optimized for zero-shot classification tasks.\\\\n3. Use the AutoTokenizer to tokenize the customer comments.\\\\n4. Employ the AutoModelForSequenceClassification to make predictions for sentiment categories such as 'positive', 'negative', or 'neutral'.\\\\n5. Determine the sentiment class with the highest probability for each customer comment.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'typeform/distilbert-base-uncased-mnli\\\\')\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\\\'typeform/distilbert-base-uncased-mnli\\\\')\\\\n\\\\ninput_text = \\\"I really love the taste of this product!\\\"\\\\ninputs = tokenizer(input_text, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\nlabel_mapping = {0: \\\\'positive\\\\', 1: \\\\'negative\\\\', 2: \\\\'neutral\\\\'}\\\\npredicted_class = label_mapping[outputs.logits.argmax().item()]', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('typeform/distilbert-base-uncased-mnli')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'typeform/distilbert-base-uncased-mnli', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('typeform/distilbert-base-uncased-mnli')\\\", 'api_arguments': {'tokenizer': 'AutoTokenizer.from_pretrained(typeform/distilbert-base-uncased-mnli)', 'model': 'AutoModelForSequenceClassification.from_pretrained(typeform/distilbert-base-uncased-mnli)'}, 'python_environment_requirements': [{'transformers': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification'}], 'example_code': 'tokenizer = AutoTokenizer.from_pretrained(typeform/distilbert-base-uncased-mnli)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(typeform/distilbert-base-uncased-mnli)', 'performance': {'dataset': 'multi_nli', 'accuracy': 0.8206875508543532}, 'description': 'This is the uncased DistilBERT model fine-tuned on Multi-Genre Natural Language Inference (MNLI) dataset for the zero-shot classification task.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذجًا لتصنيف النصوص الغير محددة الموضوع باستخدام معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' function from the transformers library.\\\\n2. Use the 'pipeline' function to create a zero-shot classification model by providing the model name 'valhalla/distilbart-mnli-12-1' as an argument. This model is pretrained on a multi-choice NLI benchmark dataset, making it suitable for zero-shot classification.\\\\n3. Define the categories in which the customer reviews should be classified (e.g., 'customer service', 'product quality', 'shipping', 'pricing').\\\\n4. For each customer review, use the created classifier to perform zero-shot classification and assign the most relevant category to the review.\\\\n5. This way, reviews can be automatically categorized without having an annotated dataset for training.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'valhalla/distilbart-mnli-12-1\\\\')\\\\n\\\\nreview = \\\"The product arrived late and the packaging was damaged.\\\"\\\\ncategories = [\\\\'customer service\\\\', \\\\'product quality\\\\', \\\\'shipping\\\\', \\\\'pricing\\\\']\\\\n\\\\nresult = classifier(review, categories)\\\\nprint(result[\\\\'labels\\\\'][0])  # prints the most relevant category', 'api_call': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'valhalla/distilbart-mnli-12-1', 'api_call': \\\"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-1')\\\", 'api_arguments': {'model_name_or_path': 'valhalla/distilbart-mnli-12-1'}, 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'MNLI', 'matched_accuracy': 87.08, 'mismatched_accuracy': 87.5}, 'description': 'distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is designed for zero-shot classification tasks.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد صور غير مشروطة لمشاهد تصور الحيوانات في البرية باستخدام تقنيات الرؤية الحاسوبية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we need to install the diffusers package and import the DDPMPipeline class.\\\\n2. Next, we use the from_pretrained method to load the pre-trained model 'ocariz/universe_1400'. This model has been trained specifically for generating high-quality images of the universe.\\\\n3. We can then use this model to generate a space-themed image that can be used as a desktop wallpaper.\\\\n4. The generated image can be saved to a file or displayed using image manipulation tools.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\\nimage = pipeline().images[0]\\\\nimage.save('space_wallpaper.png')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ocariz/universe_1400', 'api_call': \\\"DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\", 'api_arguments': '', 'python_environment_requirements': ['diffusers'], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is a diffusion model for unconditional image generation of the universe trained for 1400 epochs.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام للكشف عن الأجسام في الصور ومقاطع الفيديو باستخدام تقنيات الرؤية الحاسوبية، ونحتاج إلى قدرة على تقدير عمق الأشياء في المشاهد التي تلتقطها الكاميرات.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes from the transformers and PIL packages, such as YolosForObjectDetection for the object detection model, Image for image processing, and requests for fetching image data.\\\\n2. Use the from_pretrained method of the YolosForObjectDetection class to load the pre-trained model 'hustvl/yolos-small', which is optimized for real-time object detection tasks like detecting vehicles on the streets.\\\\n3. Load image data from an image file or get it in real-time from the traffic surveillance cameras.\\\\n4. Pass the image data to the model, which will analyze the image to identify the vehicles within the frame, giving you the necessary information for traffic monitoring.\\\", '<<<code>>>': \\\"from transformers import YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimage = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with the path to your image\\\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'api_call': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hustvl/yolos-small', 'api_call': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\\\", 'api_arguments': {'model_name': 'hustvl/yolos-small'}, 'python_environment_requirements': [{'packages': ['transformers', 'PIL', 'requests']}], 'example_code': {'import': ['from transformers import YolosFeatureExtractor, YolosForObjectDetection', 'from PIL import Image', 'import requests'], 'url': 'http://images.cocodataset.org/val2017/000000039769.jpg', 'image': 'Image.open(requests.get(url, stream=True).raw)', 'feature_extractor': \\\"YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\\\", 'model': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\\\", 'inputs': \\\"feature_extractor(images=image, return_tensors='pt')\\\", 'outputs': 'model(**inputs)', 'logits': 'outputs.logits', 'bboxes': 'outputs.pred_boxes'}, 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '36.1 AP'}, 'description': 'YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نظام معالجة اللغة الطبيعية لإنشاء نصوص جديدة استنادًا إلى النصوص المدخلة، ونريد تقدير دقة التوليف والتعبير بالنص.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the pipeline function from the transformers library provided by Hugging Face.', 'code': '', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Grammar Synthesis', 'api_name': 'pszemraj/flan-t5-large-grammar-synthesis', 'api_call': '', 'api_arguments': ['raw_text'], 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\ncorrector = pipeline(\\\\n 'text2text-generation',\\\\n 'pszemraj/flan-t5-large-grammar-synthesis',\\\\n )\\\\n\\\\nraw_text = 'i can has cheezburger'\\\\nresults = corrector(raw_text)\\\\nprint(results)\\\", 'performance': {'dataset': 'jfleg', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of google/flan-t5-large for grammar correction on an expanded version of the JFLEG dataset.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتحليل النصوص وتحويلها إلى نصوص أخرى باستخدام تقنيات معالجة اللغة الطبيعية لتوليد النصوص.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('t5-3b')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('t5-3b')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 't5-3b', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('t5-3b')\\\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"input_text = 'translate English to French: The quick brown fox jumps over the lazy dog'; inputs = tokenizer.encode(input_text, return_tensors='pt'); outputs = model.generate(inputs); translated_text = tokenizer.decode(outputs[0])\\\", 'performance': {'dataset': 'c4', 'accuracy': 'See research paper, Table 14'}, 'description': 'T5-3B is a Text-To-Text Transfer Transformer (T5) model with 3 billion parameters. It is designed for various NLP tasks such as translation, summarization, question answering, and sentiment analysis. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on multiple supervised and unsupervised tasks.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library by Hugging Face.\\\\n2. Use the pipeline function to create a text-generation model, specifying the model name 'PygmalionAI/pygmalion-1.3b'.\\\\n3. This model is specially designed for generating conversational responses, ideal for creating a chatbot capable of providing hints to players.\\\\n4. To interact with the chatbot, provide it with an input prompt structured as follows: CHARACTER persona, DIALOGUE history, and USER input.\\\\n5. The model will provide a generated response as the output based on the input prompt.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nhint_chatbot = pipeline(\\\\'text-generation\\\\', \\\\'PygmalionAI/pygmalion-1.3b\\\\')\\\\ninput_prompt = \\\"HintBot\\\\'s Persona: A helpful bot providing hints to players when they are stuck in the game.\\\\\\\\nYou: I\\\\'m stuck in the maze, can you help me?\\\"\\\\nresponse = hint_chatbot(input_prompt)', 'api_call': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pygmalion-1.3b', 'api_call': \\\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\\\", 'api_arguments': 'input_prompt', 'python_environment_requirements': [['transformers']], 'example_code': \\\"[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\\\", 'performance': {'dataset': '56MB of dialogue data', 'accuracy': 'Not provided'}, 'description': \\\"Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\\\" </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي لتصنيف الرموز في مجال معالجة اللغة الطبيعية لقياس تشابه الجمل.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the required class SentenceTransformer from the package sentence_transformers.\\\\n2. Then, initialize the model using the pre-trained model 'sentence-transformers/distilbert-base-nli-stsb-mean-tokens'.\\\\n3. Encode the given movie plot summaries into 768-dimensional dense vector representations using the model's 'encode' method.\\\\n4. Calculate the pairwise similarities between the embeddings, and find the pair with the highest similarity score.\\\\n5. Return the index of the most similar pair of movie plot summaries.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nimport numpy as np\\\\n\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/distilbert-base-nli-stsb-mean-tokens\\\\')\\\\nmovie_plots = [\\\"A hero with a mysterious past embarks on a journey to save a princess from an evil sorcerer.\\\",\\\\n               \\\"In a post-apocalyptic world, survivors struggle to maintain their humanity as they battle against mutated creatures.\\\",\\\\n               \\\"A young wizard goes on a quest to find a mythical sword, accompanied by a fierce warrior and a skilled archer.\\\"]\\\\nembeddings = model.encode(movie_plots)\\\\nplot_similarity_matrix = cosine_similarity(embeddings)\\\\nnp.fill_diagonal(plot_similarity_matrix, 0)\\\\nmost_similar_pair_idx = np.unravel_index(np.argmax(plot_similarity_matrix, axis=None), plot_similarity_matrix.shape)\\\\n\\\\nmost_similar_pair_idx', 'api_call': \\\"SentenceTransformer('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-stsb-mean-tokens', 'api_call': \\\"SentenceTransformer('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير أداة لتحويل النص إلى صورة متعددة الوسائط، ونريد توجيه التنبؤ بخصائص الصورة بناءً على الوصف النصي المقابل في مجال تقدير العمق برؤية الحاسوب.? [/INST]{'domain': 'Computer Vision Depth Estimation', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import AutoModel\\\\nfrom PIL import Image\\\\nimage = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with path to your image\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\\\\ndepth_map = model(image)\\\", 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-230103-091356', 'api_call': \\\"from transformers import AutoModel\\\\nfrom PIL import Image\\\\nimage = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with path to your image\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\\\\ndepth_map = model(image)\\\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, datasets==2.8.0, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.436, 'Mae': 0.4251, 'Rmse': 0.6169, 'Abs Rel': 0.45, 'Log Mae': 0.1721, 'Log Rmse': 0.2269, 'Delta1': 0.3828, 'Delta2': 0.6326, 'Delta3': 0.8051}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نماذج لتعرف التلقائي للكلام الصوتي باستخدام تقنيات الذكاء الاصطناعي، ونحتاج إلى تحسين دقة تعرف الكلام للغات متعددة.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-small')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\\\\naudio_path = 'path/to/your/audio_file.wav'\\\\n# replace 'path/to/your/audio_file.wav' with the actual path to your audio file\\\\nsample_rate = 16000\\\\n# replace 16000 with your actual sample rate\\\\naudio_data = read_audio_file(audio_path, sample_rate)\\\\ninput_features = processor(audio_data, sampling_rate=sample_rate, return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\\\nprint(transcription)\\\", 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Transcription and Translation', 'api_name': 'openai/whisper-small', 'api_call': \\\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-small')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\\\\naudio_path = 'path/to/your/audio_file.wav'\\\\n# replace 'path/to/your/audio_file.wav' with the actual path to your audio file\\\\nsample_rate = 16000\\\\n# replace 16000 with your actual sample rate\\\\naudio_data = read_audio_file(audio_path, sample_rate)\\\\ninput_features = processor(audio_data, sampling_rate=sample_rate, return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\\\nprint(transcription)\\\", 'api_arguments': {'language': 'english', 'task': 'transcribe'}, 'python_environment_requirements': {'transformers': 'latest', 'datasets': 'latest'}, 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', 'processor = WhisperProcessor.from_pretrained(openai/whisper-small)', 'model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-small)', 'model.config.forced_decoder_ids = None', 'ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)', 'sample = ds[0][audio]', 'input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features', 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)', 'print(transcription)'], 'performance': {'dataset': 'LibriSpeech (clean) test set', 'accuracy': '3.432 WER'}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and supports transcription and translation in various languages.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نماذج مركبة لتحويل الصور إلى نص، ونحتاج إلى دقة عالية في تفسير الصور متعددة الوسائط لمعالجة النصوص بتقنية معالجة اللغة الطبيعية.? [/INST]{'domain': 'Natural Language Processing Text Classification', 'api_call': \\\"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\nsentiment_analysis = pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\\nresult = sentiment_analysis('I love this product!')\\\", 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'sentiment_analysis_generic_dataset', 'api_call': \\\"from transformers import pipeline\\\\nsentiment_analysis = pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\\nresult = sentiment_analysis('I love this product!')\\\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': \\\"sentiment_analysis('I love this product!')\\\", 'performance': {'dataset': 'generic_dataset', 'accuracy': 'Not specified'}, 'description': 'This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text classification.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا ببناء نموذج لمعالجة اللغة الطبيعية لتصنيف النصوص، ونريد التنبؤ بصنف النص بناءً على بعض مجموعات البيانات المتوفرة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, classes, and functions.\\\\n2. Load the pre-trained model, 'zhayunduo/roberta-base-stocktwits-finetuned', and its tokenizer specifically fine-tuned for analyzing sentiments in stock-related comments.\\\\n3. Create an inference pipeline using the tokenizer and the model, and pass in the stock-related messages to predict whether they are bullish (optimistic) or bearish (pessimistic).\\\\n4. Print or return the predicted sentiments.\\\", '<<<code>>>': \\\"from transformers import RobertaForSequenceClassification, RobertaTokenizer, pipeline\\\\ntokenizer_loaded = RobertaTokenizer.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\\\nmodel_loaded = RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\\\nnlp = pipeline('text-classification', model=model_loaded, tokenizer=tokenizer_loaded)\\\\nmessages = ['The stock is going to soar!', 'Sell now or regret it!', 'Steady growth in the long term']\\\\nresults = nlp(messages)\\\\nprint(results)\\\", 'api_call': \\\"RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'zhayunduo/roberta-base-stocktwits-finetuned', 'api_call': \\\"RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\\", 'api_arguments': {'model': 'RobertaForSequenceClassification', 'tokenizer': 'RobertaTokenizer'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import RobertaForSequenceClassification, RobertaTokenizer\\\\nfrom transformers import pipeline\\\\nimport pandas as pd\\\\nimport emoji\\\\ntokenizer_loaded = RobertaTokenizer.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\\\nmodel_loaded = RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\\\nnlp = pipeline(text-classification, model=model_loaded, tokenizer=tokenizer_loaded)\\\\nsentences = pd.Series(['just buy','just sell it','entity rocket to the sky!','go down','even though it is going up, I still think it will not keep this trend in the near future'])\\\\nsentences = list(sentences)\\\\nresults = nlp(sentences)\\\\nprint(results)\\\", 'performance': {'dataset': 'stocktwits', 'accuracy': 0.9343}, 'description': \\\"This model is fine-tuned with roberta-base model on 3,200,000 comments from stocktwits, with the user-labeled tags 'Bullish' or 'Bearish'.\\\" </s>\"\n"}
{"text": "<s>[INST]عن طريق استخدام خوارزمية تعلم الآلة التي تستهدف تصنيف الجداول، قم ببناء نظام للتنبؤ بالقيم الرقمية بناءً على البيانات المقدمة. استخدم البيانات التي تم توفيرها بدون تصنيف مسبق لتدريب النموذج الخاص بك.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. First, import the necessary libraries, which are 'joblib' for loading the trained model and 'pandas' for handling the dataset.\\\\n2. Use the 'joblib.load' function to load the pre-trained model from the file 'model.joblib'.\\\\n3. Read the dataset with the housing price information as a pandas DataFrame.\\\\n4. Select only the required features listed in the config.json file for the model, and then rename the columns according to the feature name format expected by the model.\\\\n5. Finally, use the loaded model to predict the housing prices based on the input dataset.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'jwan2021/autotrain-us-housing-prices-1771761511', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': {'data': 'pandas.DataFrame'}, 'python_environment_requirements': [{'joblib': 'latest', 'pandas': 'latest'}], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'jwan2021/autotrain-data-us-housing-prices', 'accuracy': {'Loss': 134406.507, 'R2': 0.861, 'MSE': 18065109105.27, 'MAE': 103271.843, 'RMSLE': 0.139}}, 'description': 'A model trained using AutoTrain for predicting US housing prices. The model is trained on the jwan2021/autotrain-data-us-housing-prices dataset and is a single column regression model with an ID of 1771761511.' </s>\"\n"}
{"text": "<s>[INST]يُطلب من فريق التعقب إجراء تحقيق عميق للرؤية الحاسوبية للحصول على إحداث على صور لوحة مرسومة يدويًا وتحويلها إلى صور حقيقية لوحات فنية باستخدام واجهة التعليم الآلي.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class AutoModel from the transformers library provided by Hugging Face.\\\\n2. We then use the from_pretrained method of the AutoModel class to load the pre-trained model 'sayakpaul/glpn-nyu-finetuned-diode-221122-082237'. This model has been fine-tuned on the diode-subset dataset and can perform depth estimation tasks.\\\\n3. This model can be used to analyze images taken by the autonomous vehicle in real-time to estimate the depth information of the surrounding objects.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221122-082237', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\\\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': ['transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3421, 'Mae': 0.27, 'Rmse': 0.4042, 'Abs Rel': 0.3279, 'Log Mae': 0.1132, 'Log Rmse': 0.1688, 'Delta1': 0.5839, 'Delta2': 0.8408, 'Delta3': 0.9309}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>\"\n"}
{"text": "<s>[INST]يعتبر التصنيف الصفري من معالجة اللغة الطبيعية مجالًا حيويًا في تطوير تقنيات التعلم الآلي لملء الفجوة. نقوم بتطبيق موديلاتنا على البيانات من مجالات متنوعة دون الحاجة إلى تصنيف مسبق أو تدريب.? [/INST]{'domain': 'Natural Language Processing Fill-Mask', 'api_call': \\\"AutoModelForMaskedLM.from_pretrained('bert-base-chinese')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import AutoTokenizer, AutoModelForMaskedLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'bert-base-chinese\\\\')\\\\nmodel = AutoModelForMaskedLM.from_pretrained(\\\\'bert-base-chinese\\\\')\\\\n\\\\nsentence = \\\"我喜欢学习[MASK]。\\\"\\\\ninputs = tokenizer(sentence, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\npredictions = outputs.logits.argmax(dim=-1)\\\\npredicted_word = tokenizer.decode(predictions[0, -3]).strip()', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Masked Language Modeling', 'api_name': 'bert-base-chinese', 'api_call': 'from transformers import AutoTokenizer, AutoModelForMaskedLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'bert-base-chinese\\\\')\\\\nmodel = AutoModelForMaskedLM.from_pretrained(\\\\'bert-base-chinese\\\\')\\\\n\\\\nsentence = \\\"我喜欢学习[MASK]。\\\"\\\\ninputs = tokenizer(sentence, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**inputs)\\\\npredictions = outputs.logits.argmax(dim=-1)\\\\npredicted_word = tokenizer.decode(predictions[0, -3]).strip()', 'api_arguments': {'pretrained_model_name': 'bert-base-chinese'}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModelForMaskedLM'}, 'example_code': 'tokenizer = AutoTokenizer.from_pretrained(bert-base-chinese)\\\\nmodel = AutoModelForMaskedLM.from_pretrained(bert-base-chinese)', 'performance': {'dataset': '[More Information Needed]', 'accuracy': '[More Information Needed]'}, 'description': 'This model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper). It can be used for masked language modeling.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطبيق تقنيات معالجة الصوت لتحويل الصوت إلى صوت. هل يمكنك مساعدتي في إعادة صياغة الجملة التالية إلى اللغة الإنجليزية؟? [/INST]{'domain': 'Audio Audio-to-Audio', 'api_call': \\\"separator.from_hparams(source='speechbrain/sepformer-wsj02mix')\\\", 'api_provider': 'SpeechBrain', 'explanation': '1. Install the SpeechBrain package and import the necessary classes and functions.', 'code': \\\"from speechbrain.pretrained import SepformerSeparation as separator\\\\nimport torchaudio\\\\nmodel = separator.from_hparams(source='speechbrain/sepformer-wsj02mix', savedir='pretrained_models/sepformer-wsj02mix')\\\\nest_sources = model.separate_file(path='path_to_mixed_audio_file.wav')\\\\ntorchaudio.save('source1_output.wav', est_sources[:, :, 0].detach().cpu(), 8000)\\\\ntorchaudio.save('source2_output.wav', est_sources[:, :, 1].detach().cpu(), 8000)\\\", 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'SpeechBrain', 'functionality': 'Audio Source Separation', 'api_name': 'sepformer-wsj02mix', 'api_call': \\\"from speechbrain.pretrained import SepformerSeparation as separator\\\\nimport torchaudio\\\\nmodel = separator.from_hparams(source='speechbrain/sepformer-wsj02mix', savedir='pretrained_models/sepformer-wsj02mix')\\\\nest_sources = model.separate_file(path='path_to_mixed_audio_file.wav')\\\\ntorchaudio.save('source1_output.wav', est_sources[:, :, 0].detach().cpu(), 8000)\\\\ntorchaudio.save('source2_output.wav', est_sources[:, :, 1].detach().cpu(), 8000)\\\", 'api_arguments': ['path'], 'python_environment_requirements': ['pip install speechbrain'], 'example_code': ['from speechbrain.pretrained import SepformerSeparation as separator', 'import torchaudio', \\\"model = separator.from_hparams(source=speechbrain/sepformer-wsj02mix, savedir='pretrained_models/sepformer-wsj02mix')\\\", \\\"est_sources = model.separate_file(path='speechbrain/sepformer-wsj02mix/test_mixture.wav')\\\", 'torchaudio.save(source1hat.wav, est_sources[:, :, 0].detach().cpu(), 8000)', 'torchaudio.save(source2hat.wav, est_sources[:, :, 1].detach().cpu(), 8000)'], 'performance': {'dataset': 'WSJ0-2Mix', 'accuracy': '22.4 dB'}, 'description': 'This repository provides all the necessary tools to perform audio source separation with a SepFormer model, implemented with SpeechBrain, and pretrained on WSJ0-2Mix dataset.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام كشف الأجسام من خلال تقنية الرؤية الحاسوبية، ونريد استخدام البيانات المتاحة للتنبؤ بمواقع وتصنيفات الأجسام في الصور.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8m-csgo-player-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and classes, including 'YOLO' and 'render_result' from the ultralyticsplus package.\\\\n2. Create an instance of the YOLO class by providing the model name 'keremberke/yolov8m-csgo-player-detection'.\\\\n3. Override some model parameters to adjust the confidence threshold, IoU threshold, non-maximum suppression settings, and maximum detected objects.\\\\n4. Use the 'predict' method of the model to perform object detection on a given image URL.\\\\n5. Print the detected boxes for the CS:GO players.\\\\n6. Create a rendered version of the input image with the detected players, and visualize the results using the 'show' method.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage_url = 'https://example.com/csgo_image.jpg'\\\\nresults = model.predict(image_url)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image_url, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8m-csgo-player-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8m-csgo-player-detection', 'api_call': \\\"YOLO('keremberke/yolov8m-csgo-player-detection')\\\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.892}, 'description': \\\"An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\\\" </s>\"\n"}
{"text": "<s>[INST]قم بتحميل التطبيق على جهاز الكمبيوتر الخاص بك وابدأ في تحويل النصوص ضمن معالجة اللغة الطبيعية إلى تصنيف الرموز باستخدام الأدوات المتاحة. ثم اختبر دقة النتائج عن طريق تصنيف الرموز ومراجعتها.? [/INST]{'domain': 'Natural Language Processing Token Classification', 'api_call': \\\"SequenceTagger.load('flair/upos-english')\\\", 'api_provider': 'Flair', 'explanation': '', 'code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load('flair/upos-english')\\\\ninput_text = 'The quick brown fox jumps over the lazy dog.'\\\\nsentence = Sentence(input_text)\\\\ntagger.predict(sentence)\\\\nprint(sentence)\\\\nfor entity in sentence.get_spans('pos'):\\\\n    print(entity)\\\", 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Flair', 'functionality': 'Part-of-Speech Tagging', 'api_name': 'flair/upos-english', 'api_call': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load('flair/upos-english')\\\\ninput_text = 'The quick brown fox jumps over the lazy dog.'\\\\nsentence = Sentence(input_text)\\\\ntagger.predict(sentence)\\\\nprint(sentence)\\\\nfor entity in sentence.get_spans('pos'):\\\\n    print(entity)\\\", 'api_arguments': ['sentence'], 'python_environment_requirements': 'pip install flair', 'example_code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load('flair/upos-english')\\\\nsentence = Sentence('I love Berlin.')\\\\ntagger.predict(sentence)\\\\nprint(sentence)\\\\nfor entity in sentence.get_spans('pos'):\\\\n    print(entity)\\\", 'performance': {'dataset': 'ontonotes', 'accuracy': 98.6}, 'description': 'This is the standard universal part-of-speech tagging model for English that ships with Flair. It predicts universal POS tags such as ADJ, ADP, ADV, AUX, CCONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, SYM, VERB, and X. The model is based on Flair embeddings and LSTM-CRF.'}} </s>\"\n"}
{"text": "<s>[INST]يجب تطوير نموذج جديد لتصنيف الفيديوهات في مجال رؤية الحاسوب، ونحن بحاجة لتدريب النموذج على مجموعة متنوعة من البيانات الفيديوية المتاحة لتحقيق أقصى قدر من الدقة والفعالية.? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include 'VideoMAEImageProcessor', 'VideoMAEForPreTraining', 'torch' and 'numpy'.\\\\n2. Load the VideoMAE model, which has been pretrained on a large dataset. This model will analyze short video clips and classify the activities within them automatically.\\\\n3. The VideoMAEImageProcessor will preprocess your input video frames, while the VideoMAEForPreTraining model will analyze the frames and predict the activity class.\\\\n4. The model will return a list of probabilities associated with each class. Pick the class with the highest probability for the final classification result.\\\", '<<<code>>>': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-short')\\\\nmodel = VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short')\\\\ninput_tensor = processor(video, return_tensors='pt').pixel_values\\\\n\\\\noutputs = model(input_tensor)\\\\nactivity_classification = outputs['logits'].argmax(dim=-1).item()\\\", 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'MCG-NJU/videomae-base-short', 'api_call': \\\"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'MCG-NJU/videomae-base-short'}, 'python_environment_requirements': [{'packages': ['transformers']}], 'example_code': 'from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\\\nimport numpy as np\\\\nimport torch\\\\nnum_frames = 16\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-short)\\\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short)\\\\npixel_values = processor(video, return_tensors=pt).pixel_values\\\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\\\nloss = outputs.loss', 'performance': {'dataset': 'Kinetics-400', 'accuracy': 'Not provided'}, 'description': 'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء صورة جديدة بدون شروط باستخدام تقنيات رؤية الحواسيب.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class from the diffusers package. This includes DDPMPipeline for the unconditional image generation model.\\\\n2. We then use the from_pretrained method of the DDPMPipeline class to load the pre-trained model 'ocariz/butterfly_200'. This model has been trained for unconditional image synthesis tasks, with a specific focus on generating images of cute butterflies.\\\\n3. This model can then be used to generate butterfly images, which can be utilized in your marketing materials for environmental causes.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\\nimage = pipeline().images[0]\\\\nimage.save('butterfly_generated_image.png')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ocariz/butterfly_200', 'api_call': \\\"DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\", 'api_arguments': '', 'python_environment_requirements': ['diffusers'], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is a diffusion model for unconditional image generation of cute butterflies trained for 200 epochs.' </s>\"\n"}
{"text": "<s>[INST]لإنشاء نموذج لتصنيف الصور باستخدام رؤية الحاسوب، قم بجمع مجموعة من الصور الصحيحة للتدريب. يمكنك استخدام تقنيات العلم الآلي مثل الشبكات العصبية العميقة. يمكنك الاستفادة من مكتبة مفتوحة المصدر لتسريع العملية. استخدم بيانات الاختبار لتقييم أداء النموذج. قم بتحسين النموذج بتعديل المعلمات وتحسين الخوارزميات. سيساعدك هذا على تدريب النموذج على التعرف على الصور بدقة. لا تنسى توثيق عملية التدريب لمستقبل الصيانة والتطوير.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, including timm, PIL, and urllib.request.\\\\n2. Load an image sample or an image from the dataset, using the Image.open() method from the PIL library and the urlopen() method from urllib.request.\\\\n3. Create the model 'convnext_base.fb_in1k' pretrained on the ImageNet-1k dataset using the timm.create_model() function. This model is suitable for image classification, feature map extraction, and image embeddings.\\\\n4. Evaluate the model using the model.eval() method.\\\\n5. Resolve the model data configuration using the timm.data.resolve_model_data_config() function.\\\\n6. Create image transforms using the timm.data.create_transform() function. These transforms will help preprocess images for the model.\\\\n7. Using the model and transforms, obtain the embeddings for all images in your dataset.\\\\n8. Compute the similarity between embeddings of each pair of images to cluster similar images together.\\\", '<<<code>>>': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\n\\\\nimg = Image.open(urlopen('https://your-image-url-here'))\\\\nmodel = timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\nimage_embeddings = model(transforms(img).unsqueeze(0))\\\\n\\\\n# Calculate similarity between image_embeddings and cluster similar images together\\\", 'api_call': \\\"timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'convnext_base.fb_in1k', 'api_call': \\\"timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\", 'api_arguments': {'pretrained': 'True', 'features_only': 'True', 'num_classes': '0'}, 'python_environment_requirements': [['timm']], 'example_code': ['from urllib.request import urlopen', 'from PIL import Image', 'import timm', \\\"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\\", \\\"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\\\", 'model = model.eval()', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'output = model(transforms(img).unsqueeze(0))'], 'performance': {'dataset': 'imagenet-1k', 'accuracy': '83.82%'}, 'description': 'A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتقسيم الصور في مجال رؤية الحاسوب، ونريد تنبؤ حدود الأجسام في الصور بناءً على تدريب النموذج على مجموعة متنوعة من البيانات.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import required classes - OneFormerProcessor and OneFormerForUniversalSegmentation for processing and segmentation, and Image for handling image data.\\\\n2. Use the from_pretrained method of OneFormerForUniversalSegmentation class to load the pre-trained model 'shi-labs/oneformer_ade20k_swin_tiny'. This model can perform semantic, instance, and panoptic segmentation tasks.\\\\n3. Load the image data from the file or obtain it in real-time from a surveillance camera feed.\\\\n4. Using the processor, create inputs for each segmentation task and use the model to generate outputs with the specific task conditional tokens (semantic, instance, panoptic).\\\\n5. Process the output according to the specific task using the corresponding post-processing method in the processor (post_process_semantic_segmentation, post_process_instance_segmentation, post_process_panoptic_segmentation).\\\", '<<<code>>>': \\\"from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimage = Image.open('image_path.jpg')\\\\n# replace 'image_path.jpg' with path to your image\\\\nprocessor = OneFormerProcessor.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\\nsemantic_inputs = processor(images=image, task_inputs=['semantic'], return_tensors='pt')\\\\nsemantic_outputs = model(**semantic_inputs)\\\\ninstance_inputs = processor(images=image, task_inputs=['instance'], return_tensors='pt')\\\\ninstance_outputs = model(**instance_inputs)\\\\npanoptic_inputs = processor(images=image, task_inputs=['panoptic'], return_tensors='pt')\\\\npanoptic_outputs = model(**panoptic_inputs)\\\", 'api_call': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'shi-labs/oneformer_ade20k_swin_tiny', 'api_call': \\\"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\\\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\\\n\\\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\\\nsemantic_outputs = model(**semantic_inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\n\\\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\\\ninstance_outputs = model(**instance_inputs)\\\\n\\\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\\\n\\\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\\\npanoptic_outputs = model(**panoptic_inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتحويل الصور إلى نصوص وإنشاء توصيفات نصية للصور. هل يمكنك مساعدتي في توليد نص من صورة؟? [/INST]{'domain': 'Multimodal Text-to-Image', 'api_call': \\\"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\\\", 'api_provider': 'Hugging Face', 'explanation': '1. We first import the pipeline function from the transformers library provided by Hugging Face.', 'code': \\\"from transformers import pipeline\\\\ntext_to_image = pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\\\\ngenerated_image = text_to_image('analog style futuristic cityscape')\\\", 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \\\"from transformers import pipeline\\\\ntext_to_image = pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\\\\ngenerated_image = text_to_image('analog style futuristic cityscape')\\\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \\\"text_to_image('analog style landscape')\\\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \\\"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\\\"}} </s>\"\n"}
{"text": "<s>[INST]نقدم لكم خدمة تحويل النصوص إلى كلام مسموع عبر منتج مبتكر يعتمد على تقنيات حديثة في مجال المعالجة الصوتية، ويمكننا تنبؤ بجودة صوت الناتج باستخدام بعض مجموعات البيانات المتاحة.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"pipeline('text-to-speech', model='mio/Artoria')\\\", '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a text-to-speech model with the specified model, which is 'mio/Artoria'.\\\\n3. This model has been trained on the 'fate' dataset and is capable of converting text input into speech output.\\\\n4. Provide the input text that you want the model to convert into speech.\\\\n5. The model will generate an audio representation of the input text, which can be played back as speech.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntts = pipeline(\\\\'text-to-speech\\\\', model=\\\\'mio/Artoria\\\\')\\\\ntext_to_read_aloud = \\\"This is the sample text to be read out loud by the TTS model.\\\"\\\\nspeech_audio = tts(text_to_read_aloud)', 'api_call': \\\"pipeline('text-to-speech', model='mio/Artoria')\\\", 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mio/Artoria', 'api_call': \\\"pipeline('text-to-speech', model='mio/Artoria')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; tts = pipeline('text-to-speech', model='mio/Artoria'); tts('s')\\\", 'performance': {'dataset': 'fate', 'accuracy': 'Not provided'}, 'description': 'This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتجميع ملخص نصي احترافي للنصوص العلمية المتعلقة بمجال معالجة اللغة الطبيعية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('cointegrated/rut5-base-absum')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the necessary classes 'T5ForConditionalGeneration' and 'T5Tokenizer' from the transformers library.\\\\n2. Then, use 'T5ForConditionalGeneration.from_pretrained()' to load the pre-trained model 'cointegrated/rut5-base-absum', which is specifically designed for abstractive Russian summarization.\\\\n3. Load the tokenizer with 'T5Tokenizer.from_pretrained()'.\\\\n4. Use the tokenizer to tokenize the input text from the Russian newspaper article.\\\\n5. Generate the summary using the model.generate() method with the appropriate parameters, such as max_length, num_beams, etc.\\\\n6. Decode the generated summary using tokenizer.decode() and return the summarized text.\\\", '<<<code>>>': 'from transformers import T5ForConditionalGeneration, T5Tokenizer\\\\n\\\\nMODEL_NAME = \\\\'cointegrated/rut5-base-absum\\\\'\\\\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\\\\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\\\\n\\\\ndef summarize(text, max_length=1000, num_beams=3, do_sample=False, repetition_penalty=10.0):\\\\n    input_text = \\\"summarize: \\\" + text\\\\n    input_tokens = tokenizer(input_text, return_tensors=\\\\'pt\\\\', padding=True)\\\\n    with torch.no_grad():\\\\n        summary_tokens = model.generate(\\\\n            input_tokens,\\\\n            max_length=max_length,\\\\n            num_beams=num_beams,\\\\n            do_sample=do_sample,\\\\n            repetition_penalty=repetition_penalty\\\\n        )\\\\n    summary_text = tokenizer.decode(summary_tokens[0], skip_special_tokens=True)\\\\n    return summary_text\\\\n\\\\narticle_text = \\\"Russian newspaper article text here...\\\"\\\\nsummarized_text = summarize(article_text)', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('cointegrated/rut5-base-absum')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cointegrated/rut5-base-absum', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('cointegrated/rut5-base-absum')\\\", 'api_arguments': {'n_words': 'int', 'compression': 'float', 'max_length': 'int', 'num_beams': 'int', 'do_sample': 'bool', 'repetition_penalty': 'float'}, 'python_environment_requirements': [{'transformers': 'latest', 'torch': 'latest'}], 'example_code': \\\"import torch\\\\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\\\\nMODEL_NAME = 'cointegrated/rut5-base-absum'\\\\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\\\\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\\\\nmodel.cuda();\\\\nmodel.eval();\\\\ndef summarize(\\\\n text, n_words=None, compression=None,\\\\n max_length=1000, num_beams=3, do_sample=False, repetition_penalty=10.0, \\\\n <strong>kwargs\\\\n):\\\\n \\\\n Summarize the text\\\\n The following parameters are mutually exclusive:\\\\n - n_words (int) is an approximate number of words to generate.\\\\n - compression (float) is an approximate length ratio of summary and original text.\\\\n \\\\n if n_words:\\\\n text = '[{}] '.format(n_words) + text\\\\n elif compression:\\\\n text = '[{0:.1g}] '.format(compression) + text\\\\n x = tokenizer(text, return_tensors='pt', padding=True).to(model.device)\\\\n with torch.inference_mode():\\\\n out = model.generate(\\\\n </strong>x, \\\\n max_length=max_length, num_beams=num_beams, \\\\n do_sample=do_sample, repetition_penalty=repetition_penalty, \\\\n **kwargs\\\\n )\\\\n return tokenizer.decode(out[0], skip_special_tokens=True)\\\", 'performance': {'dataset': ['csebuetnlp/xlsum', 'IlyaGusev/gazeta', 'mlsum'], 'accuracy': 'Not provided'}, 'description': 'This is a model for abstractive Russian summarization, based on cointegrated/rut5-base-multitask and fine-tuned on 4 datasets.' </s>\"\n"}
{"text": "<s>[INST]نحتاج في مجال معالجة اللغة الطبيعية وتوليد النصوص إلى تصميم نماذج لتصنيف النصوص بدون تدريب مسبق. نحتاج أيضًا إلى تقدير عمق المحتوى والمفهوم في النصوص التي سيتم معالجتها.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('bigcode/santacoder', trust_remote_code=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. This includes 'AutoModelForCausalLM' for the text generation model and 'AutoTokenizer' for tokenization.\\\\n2. Load the 'bigcode/santacoder' model using the 'AutoModelForCausalLM.from_pretrained()' function.\\\\n3. Tokenize and encode the input Python code snippet using the 'AutoTokenizer.from_pretrained()' function.\\\\n4. Generate the completed code snippet using the 'model.generate()' function and decode the generated tokens with the tokenizer.\\\\n5. The completed code snippet will be output.\\\", '<<<code>>>': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ncheckpoint = \\\\'bigcode/santacoder\\\\'\\\\ndevice = \\\\'cuda\\\\' if torch.cuda.is_available() else \\\\'cpu\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\\\\n\\\\ninputs = tokenizer.encode(\\\"def print_hello_world():\\\", return_tensors=\\\\'pt\\\\').to(device)\\\\noutputs = model.generate(inputs)\\\\ncompleted_code = tokenizer.decode(outputs[0])\\\\nprint(completed_code)', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('bigcode/santacoder', trust_remote_code=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bigcode/santacoder', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('bigcode/santacoder', trust_remote_code=True)\\\", 'api_arguments': ['inputs'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ncheckpoint = bigcode/santacoder\\\\ndevice = cuda # for GPU usage or cpu for CPU usage\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\\\\ninputs = tokenizer.encode(def print_hello_world():, return_tensors=pt).to(device)\\\\noutputs = model.generate(inputs)\\\\nprint(tokenizer.decode(outputs[0]))', 'performance': {'dataset': 'bigcode/the-stack', 'accuracy': {'pass@1 on MultiPL HumanEval (Python)': 0.18, 'pass@10 on MultiPL HumanEval (Python)': 0.29, 'pass@100 on MultiPL HumanEval (Python)': 0.49, 'pass@1 on MultiPL MBPP (Python)': 0.35, 'pass@10 on MultiPL MBPP (Python)': 0.58, 'pass@100 on MultiPL MBPP (Python)': 0.77, 'pass@1 on MultiPL HumanEval (JavaScript)': 0.16, 'pass@10 on MultiPL HumanEval (JavaScript)': 0.27, 'pass@100 on MultiPL HumanEval (JavaScript)': 0.47, 'pass@1 on MultiPL MBPP (Javascript)': 0.28, 'pass@10 on MultiPL MBPP (Javascript)': 0.51, 'pass@100 on MultiPL MBPP (Javascript)': 0.7, 'pass@1 on MultiPL HumanEval (Java)': 0.15, 'pass@10 on MultiPL HumanEval (Java)': 0.26, 'pass@100 on MultiPL HumanEval (Java)': 0.41, 'pass@1 on MultiPL MBPP (Java)': 0.28, 'pass@10 on MultiPL MBPP (Java)': 0.44, 'pass@100 on MultiPL MBPP (Java)': 0.59, 'single_line on HumanEval FIM (Python)': 0.44, 'single_line on MultiPL HumanEval FIM (Java)': 0.62, 'single_line on MultiPL HumanEval FIM (JavaScript)': 0.6, 'BLEU on CodeXGLUE code-to-text (Python)': 18.13}}, 'description': 'The SantaCoder models are a series of 1.1B parameter models trained on the Python, Java, and JavaScript subset of The Stack (v1.1) (which excluded opt-out requests). The main model uses Multi Query Attention, was trained using near-deduplication and comment-to-code ratio as filtering criteria and using the Fill-in-the-Middle objective. In addition there are several models that were trained on datasets with different filter parameters and with architecture and objective variations.' </s>\"\n"}
{"text": "<s>[INST]قم بفتح تطبيق معالجة اللغة الطبيعية وقم بإدخال الجملة التالية: \\\"أحنا عندنا نادي طلابي بيتجمع كل توتين عشان نلعب كورة القدم الافتراضية. زودهم بأداة تقدر تلعب ضد وكيل التعلم الآلي\\\". ثم اختر اللهجة المفضلة واستمع للنص باستخدام تقنية تحويل النص إلى كلام.? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the SentenceTransformer from the sentence_transformers package.', 'code': \\\"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\\nsentences = ['This is the English message in the ad campaign',\\\\n             'Ceci est le message français dans la campagne publicitaire',\\\\n             'Dies ist deutscher Text in der Werbekampagne']\\\\nembeddings = model.encode(sentences)\\\\n(similarity_matrix)\\\", 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Sentence Embeddings', 'api_name': 'sentence-transformers/nli-mpnet-base-v2', 'api_call': \\\"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\\nsentences = ['This is the English message in the ad campaign',\\\\n             'Ceci est le message français dans la campagne publicitaire',\\\\n             'Dies ist deutscher Text in der Werbekampagne']\\\\nembeddings = model.encode(sentences)\\\\n(similarity_matrix)\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير تطبيق لقسمة الصور لنظام الرؤية الحاسوبية، ونريد إنشاء نموذج يتنبأ بالأشكال المختلفة في الصور بدقة وسرعة عالية.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8s-building-segmentation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary Python packages, like YOLO and render_result from the ultralyticsplus library.\\\\n2. Load the YOLOv8 model for building segmentation in satellite images using 'keremberke/yolov8s-building-segmentation'.\\\\n3. Configure the model by setting values for 'conf', 'iou', 'agnostic_nms', and 'max_det' parameters. These parameters influence the confidence threshold, intersection over union, non-maximum suppression, and maximum allowed detections.\\\\n4. Prepare the input satellite image by providing a URL or local image file.\\\\n5. Use the predict method of the loaded model to analyze the image and generate segmentation results.\\\\n6. Extract the bounding boxes and masks from the results.\\\\n7. Render the segmented image using the render_result function.\\\\n8. Display the segmented image with buildings highlighted.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-building-segmentation')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://path/to/satellite/image.jpg'  # replace with your URL or local path\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8s-building-segmentation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8s-building-segmentation', 'api_call': \\\"YOLO('keremberke/yolov8s-building-segmentation')\\\", 'api_arguments': ['conf', 'iou', 'agnostic_nms', 'max_det', 'image'], 'python_environment_requirements': [['ultralyticsplus==0.0.21']], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8s-building-segmentation')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'print(results[0].masks)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.661, 'mAP@0.5(mask)': 0.651}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.' </s>\"\n"}
{"text": "<s>[INST]نحتاج في شركتنا المتخصصة في مجال معالجة اللغة الطبيعية وتصنيف الصفر إلى تصميم نماذج لتصنيف النصوص بدون تدريب مسبق، ونحتاج إلى تقدير عمق المحتوى والمفهوم في النصوص التي سيتم معالجتها في مجال معالجة اللغة الطبيعية ومشابهة الجمل.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the SentenceTransformer class from the sentence_transformers package.\\\\n2. Initialize the SentenceTransformer model using the 'sentence-transformers/all-roberta-large-v1' pre-trained model. This model has been designed to map sentences into a high-dimensional dense vector space.\\\\n3. Encode the headlines using the model, which will generate embeddings for each headline.\\\\n4. Compare the generated embeddings to measure the similarity between news headlines.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nheadlines = [\\\"Breaking: Scientists Discover Cure for Cancer\\\", \\\"Stunning Discovery: New Cancer Treatment Found\\\"]\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-roberta-large-v1\\\\')\\\\nembeddings = model.encode(headlines)', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/all-roberta-large-v1', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 1024 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطوير نموذج لاستخراج الميزات من اللغة الطبيعية باستخدام تقنيات معالجة اللغة الطبيعية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Feature Extraction', '<<<api_call>>>': \\\"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To extract features from the biomedical text, you'll need the 'AutoModel' and 'AutoTokenizer' classes from the Hugging Face Transformers library.\\\\n2. Load the BioBERT model ('dmis-lab/biobert-v1.1') using the 'AutoModel.from_pretrained()' function.\\\\n3. Load the tokenizer associated with the BioBERT model, using 'AutoTokenizer.from_pretrained()'.\\\\n4. Tokenize the input biomedical text using the tokenizer with 'tokenizer.encode()'.\\\\n5. Pass the tokenized input to the model to extract features using 'model(tensor_input)'.\\\\n6. The output tensor contains the features extracted from the biomedical text.\\\", '<<<code>>>': 'from transformers import AutoModel, AutoTokenizer\\\\n\\\\nmodel_checkpoint = \\\\'dmis-lab/biobert-v1-1\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\\\nmodel = AutoModel.from_pretrained(model_checkpoint)\\\\n\\\\ntext = \\\"Biomedical text to be analyzed.\\\"\\\\ntokenized_text = tokenizer.encode(text, return_tensors=\\\\'pt\\\\')\\\\n\\\\nwith torch.no_grad():\\\\n    features = model(tokenized_text)[0]', 'api_call': \\\"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \\\"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية باستخدام تقنيات الرؤية الحاسوبية لاكتشاف وتحديد الكائنات. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8s-hard-hat-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the required Python packages: ultralyticsplus and ultralytics.\\\\n2. Import the YOLO model and render_result function from the ultralyticsplus library.\\\\n3. Load the 'keremberke/yolov8s-hard-hat-detection' model to detect hard hats in the image provided.\\\\n4. Configure the model with the appropriate confidence level, IoU threshold, object categories, and maximum number of detections.\\\\n5. Provide an image URL or local image file that contains workers on the construction site.\\\\n6. Use the predict method to detect hard hats in the provided image and then utilize the render_result function to visualize the detected objects.\\\\n7. Finally, display the rendered image to evaluate the safety measures taken by the workers.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-hard-hat-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'construction_site_image_url.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0]);\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8s-hard-hat-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8s-hard-hat-detection', 'api_call': \\\"YOLO('keremberke/yolov8s-hard-hat-detection')\\\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-hard-hat-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.834}, 'description': 'An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج لمعالجة اللغة الطبيعية يقوم بتحويل النص إلى نصوص أخرى لغرض إنتاج نص يمكن فهمه من قبل البشر، هذا النموذج مخصص لتوفير حلاً لمستخدمين بإعاقة بصرية.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('t5-3b')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Firstly, import T5ForConditionalGeneration and a suitable tokenizer class from the transformers library.\\\\n2. We load the 't5-3b' model using the from_pretrained method, which is designed for various NLP tasks such as translation, summarization, question answering, and sentiment analysis.\\\\n3. Next, we tokenize and encode the given input text using the appropriate tokenizer for the model.\\\\n4. Then, we generate the French translation of the English text using the model's generate method.\\\\n5. Finally, the translated text is obtained by decoding the model's output.\\\", '<<<code>>>': 'from transformers import T5ForConditionalGeneration, T5Tokenizer\\\\n\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'t5-3b\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'t5-3b\\\\')\\\\n\\\\ninput_text = \\\"translate English to French: The quick brown fox jumps over the lazy dog\\\"\\\\ninputs = tokenizer(input_text, return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs.input_ids)\\\\ntranslated_text = tokenizer.decode(outputs[0])', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('t5-3b')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 't5-3b', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('t5-3b')\\\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"input_text = 'translate English to French: The quick brown fox jumps over the lazy dog'; inputs = tokenizer.encode(input_text, return_tensors='pt'); outputs = model.generate(inputs); translated_text = tokenizer.decode(outputs[0])\\\", 'performance': {'dataset': 'c4', 'accuracy': 'See research paper, Table 14'}, 'description': 'T5-3B is a Text-To-Text Transfer Transformer (T5) model with 3 billion parameters. It is designed for various NLP tasks such as translation, summarization, question answering, and sentiment analysis. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on multiple supervised and unsupervised tasks.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام انتاج صور بشكل غير مشروط باستخدام تقنيات الرؤية الحاسوبية، ونرغب في خلق صور واقعية وجديدة تماماً لتحسين كفاءة اكتشاف الأجسام في الصور.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the ultralyticsplus library, which includes the YOLO object detector and render_result function.\\\\n2. Instantiate the YOLO object detection model using the 'keremberke/yolov8n-table-extraction' checkpoint. This model is trained to detect tables from images.\\\\n3. Configure the model's detection parameters like confidence threshold, Intersection over Union (IoU) threshold, Non-Maximum Suppression (NMS), and maximum detections.\\\\n4. Load the image of the document containing tables.\\\\n5. Use the .predict() method to perform the table detection in the image.\\\\n6. The resulting object will contain detected bounding boxes and class labels for detected tables.\\\\n7. Use the render_result function to visualize the detection results.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8n-table-extraction')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path/to/image_with_tables.jpg'\\\\nresults = model.predict(image)\\\\ndetected_tables = results[0].boxes\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8n-table-extraction', 'api_call': \\\"YOLO('keremberke/yolov8n-table-extraction')\\\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8n-table-extraction')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'table-extraction', 'accuracy': 0.967}, 'description': \\\"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\\\" </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير منتج للإجابة عن الأسئلة المتعلقة بالوثائق متعددة الوسائط، ونرغب في توفير توقعات دقيقة حول كيفية إيجاد إجابات لهذه الأسئلة من خلال المنتج الجديد.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required classes LayoutXLMForQuestionAnswering and LayoutXLMTokenizer from the transformers library provided by Hugging Face.\\\\n2. Load the pretrained model 'fimu-docproc-research/CZ_DVQA_layoutxlm-base' using the LayoutXLMForQuestionAnswering.from_pretrained() function. This model is designed to answer questions based on a document containing a mix of text and tables.\\\\n3. Similarly, load the tokenizer using the LayoutXLMTokenizer.from_pretrained() function.\\\\n4. Use the tokenizer to encode the document text and the question. Pass the tokenized inputs to the model for inference.\\\\n5. The model will return an answer based on the document contents.\\\", '<<<code>>>': \\\"from transformers import LayoutXLMForQuestionAnswering, LayoutXLMTokenizer\\\\nmodel = LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\\\\ntokenizer = LayoutXLMTokenizer.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\\\\ninputs = tokenizer(document_text, question, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nanswer = tokenizer.decode(outputs.start_logits.argmax(), outputs.end_logits.argmax() + 1)\\\", 'api_call': \\\"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'CZ_DVQA_layoutxlm-base', 'api_call': \\\"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Document Question Answering model based on LayoutXLM.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج تعلم آلي لتصنيف النصوص بدون إشراف في معالجة اللغة الطبيعية. يجب توضيح الخطوات اللازمة لتطوير النموذج واختباره باستخدام مجموعة متنوعة من النصوص.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"CLIPModel.from_pretrained('laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create an instance of the pipeline for the zero-shot-classification task.\\\\n3. Load the specified model 'laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind'. This model is designed for zero-shot image classification and can identify objects in photos without requiring training images.\\\\n4. Use the created classifier with an input image and a list of potential class names to identify the objects in the photo.\\\\n5. The model will predict the class with the highest probability score for the given image.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nclip = pipeline('zero-shot-classification', model='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind')\\\\nclassification_results = clip(image, class_names=['cat', 'dog', 'fish'])\\\", 'api_call': \\\"CLIPModel.from_pretrained('laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind', 'api_call': \\\"CLIPModel.from_pretrained('laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind')\\\", 'api_arguments': 'image, class_names', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline; clip = pipeline('zero-shot-classification', model='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind'); clip(image, class_names=['cat', 'dog', 'fish'])\\\", 'performance': {'dataset': 'ImageNet-1k', 'accuracy': '79.1 - 79.4'}, 'description': 'A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نموذج للإجابة على أسئلة الجداول بشكل تلقائي باستخدام تقنيات معالجة اللغة الطبيعية. هل يمكنك مساعدتي في هذا المجال؟? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')\\\", 'api_provider': 'Transformers', 'explanation': '1. Install the transformers library, provided by Hugging Face.', 'code': \\\"from transformers import AutoTokenizer, AutoModelForTableQuestionAnswering, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained('google/tapas-mini-finetuned-wtq')\\\\nmodel = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')\\\\ntable_qa_pipeline = pipeline('table-question-answering', model=model, tokenizer=tokenizer)\\\", 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-mini-finetuned-wtq', 'api_call': \\\"from transformers import AutoTokenizer, AutoModelForTableQuestionAnswering, pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained('google/tapas-mini-finetuned-wtq')\\\\nmodel = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')\\\\ntable_qa_pipeline = pipeline('table-question-answering', model=model, tokenizer=tokenizer)\\\", 'api_arguments': \\\"tokenizer = AutoTokenizer.from_pretrained('google/tapas-mini-finetuned-wtq'); model = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')\\\", 'python_environment_requirements': 'transformers', 'example_code': \\\"from transformers import pipeline\\\\n\\\\nnlp = pipeline('table-question-answering', model='google/tapas-mini-finetuned-wtq', tokenizer='google/tapas-mini-finetuned-wtq')\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.2854}, 'description': 'TAPAS mini model fine-tuned on WikiTable Questions (WTQ). It is pretrained on a large corpus of English data from Wikipedia and can be used for answering questions related to a table.'}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء تطبيق يساعد المستخدمين على تحويل مقاطع صوتية إلى مقاطع صوتية بدقة عالية وسهولة. يرجى تضمين واجهة مستخدم بديهية وعملية لتحميل الملفات الصوتية واستيرادها وتحويلها بنقرة واحدة.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the 'pipeline' function from the transformers library.\\\\n2. Use the 'pipeline' function to create an audio source separation model by specifying the model name 'Awais/Audio_Source_Separation'.\\\\n3. This model is trained to perform source separation tasks, meaning it can separate different sources of audio (e.g., vocals and instruments) from a given audio file.\\\\n4. Apply the model on an audio file provided by the musician to separate the vocals from the instrumental sounds.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\naudio_separator = pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\\\\nseparated_sources = audio_separator(music_audio_file_path)\\\", 'api_call': \\\"pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Awais/Audio_Source_Separation', 'api_call': \\\"pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\\\", 'api_arguments': 'audio', 'python_environment_requirements': ['Asteroid'], 'example_code': '', 'performance': {'dataset': 'Libri2Mix', 'accuracy': {'si_sdr': 14.764543634468069, 'si_sdr_imp': 14.764029375607246, 'sdr': 15.29337970745095, 'sdr_imp': 15.114146605113111, 'sir': 24.092904661115366, 'sir_imp': 23.913669683141528, 'sar': 16.06055906916849, 'sar_imp': -51.980784441287454, 'stoi': 0.9311142440593033, 'stoi_imp': 0.21817376142710482}}, 'description': 'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى تطبيق يستخدم تعلم تعزيزي في تطوير برنامج يقوم بتحويل الصور إلى صور أخرى لمستخدمي التطبيق الذين يعانون من إعاقة بصرية، مما يمكنهم من تحسين تجربتهم والاستفادة من السمات المتعددة للصور المطورة.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the necessary libraries, including ControlNetModel from diffusers, LineartAnimeDetector from controlnet_aux, and CLIPTextModel from transformers.\\\\n2. Load the ControlNet model for creating anime-style images. This model will be used to generate high-quality lineart anime images based on text descriptions.\\\\n3. Define a prompt containing the description of the desired image, like \\\"A warrior girl in the jungle\\\".\\\\n4. Preprocess the image with LineartAnimeDetector, which is used to detect lineart anime in the loaded image.\\\\n5. Load the text encoder from the Hugging Face model hub, and initialize the ControlNetModel.\\\\n6. Set up a diffusion pipeline, using StableDiffusionControlNetPipeline, with the loaded text encoder and control network.\\\\n7. Generate the anime-style image based on the prompt, the image, and the configured pipeline.\\\\n8. Save the generated image as a file or display it.', '<<<code>>>': 'from diffusers import ControlNetModel\\\\nfrom controlnet_aux import LineartAnimeDetector\\\\nfrom transformers import CLIPTextModel\\\\nimport torch\\\\nfrom PIL import Image\\\\nprompt = \\\"A warrior girl in the jungle\\\"\\\\ntext_encoder = CLIPTextModel.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', subfolder=\\\\'text_encoder\\\\', num_hidden_layers=11, torch_dtype=torch.float16)\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/control_v11p_sd15s2_lineart_anime\\\\', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(0)\\\\nresult_image = pipe(prompt, num_inference_steps=30, generator=generator).images[0]\\\\nresult_image.save(\\\\'output_image.png\\\\')', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\\\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': [['pip install diffusers transformers accelerate', 'pip install controlnet_aux==0.3.0']], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import LineartAnimeDetector', 'from transformers import CLIPTextModel', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15s2_lineart_anime', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15s2_lineart_anime/resolve/main/images/input.png', ')', 'image = image.resize((512, 512))', 'prompt = A warrior girl in the jungle', 'processor = LineartAnimeDetector.from_pretrained(lllyasviel/Annotators)', 'control_image = processor(image)', 'control_image.save(./images/control.png)', 'text_encoder = CLIPTextModel.from_pretrained(runwayml/stable-diffusion-v1-5, subfolder=text_encoder, num_hidden_layers=11, torch_dtype=torch.float16)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \\\"image.save('images/image_out.png')\\\"], 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.' </s>\"\n"}
{"text": "<s>[INST]يقدم العاملون في مجال تحويل النصوص إلى صور صوتية خدمات تجميع النصوص بشكل متقن وفعال، ونحتاج إلى تقدير درجة تلخيص النصوص بدقة وإتقان بمجال معالجة اللغات الطبيعية والترجمة.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"MarianMTModel.from_pretrained('pytorch-models/opus-mt-tc-big-en-pt')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library, which includes MarianMTModel and MarianTokenizer.\\\\n2. Define the source text as a list containing multiple English sentences.\\\\n3. Load the tokenizer and model using the model name 'pytorch-models/opus-mt-tc-big-en-pt'.\\\\n4. Use the tokenizer to preprocess the source text and pass it to the model for generating the translated text.\\\\n5. Decode the translated text using the tokenizer and skip the special tokens.\\\", '<<<code>>>': 'from transformers import MarianMTModel, MarianTokenizer\\\\n\\\\nsrc_text = [\\\\n    \\\"Tom tried to stab me.\\\",\\\\n    \\\"He has been to Hawaii several times.\\\"\\\\n]\\\\n\\\\nmodel_name = \\\\'pytorch-models/opus-mt-tc-big-en-pt\\\\'\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\n\\\\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\\\\'pt\\\\', padding=True))\\\\n\\\\nfor t in translated:\\\\n    print(tokenizer.decode(t, skip_special_tokens=True))', 'api_call': \\\"MarianMTModel.from_pretrained('pytorch-models/opus-mt-tc-big-en-pt')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-tc-big-en-pt', 'api_call': \\\"MarianMTModel.from_pretrained('pytorch-models/opus-mt-tc-big-en-pt')\\\", 'api_arguments': {'src_text': 'list of text strings with language tokens'}, 'python_environment_requirements': [{'transformers': '4.16.2'}], 'example_code': 'from transformers import MarianMTModel, MarianTokenizer\\\\nsrc_text = [\\\\n &gt;&gt;por&lt;&lt; Tom tried to stab me.,\\\\n &gt;&gt;por&lt;&lt; He has been to Hawaii several times.\\\\n]\\\\nmodel_name = pytorch-models/opus-mt-tc-big-en-pt\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\ntranslated = model.generate(**tokenizer(src_text, return_tensors=pt, padding=True))\\\\nfor t in translated:\\\\n print( tokenizer.decode(t, skip_special_tokens=True) )', 'performance': {'dataset': [{'name': 'flores101-devtest', 'accuracy': 50.4}, {'name': 'tatoeba-test-v2021-08-07', 'accuracy': 49.6}]}, 'description': 'Neural machine translation model for translating from English (en) to Portuguese (pt). This model is part of the OPUS-MT project, an effort to make neural machine translation models widely available and accessible for many languages in the world.' </s>\"\n"}
{"text": "<s>[INST]أنشئ ملخصًا لنص قصير بتقنية معالجة اللغة الطبيعية يستخدم التحليل النصي لتلخيص نقاط رئيسية.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"pipeline('summarization', model='philschmid/bart-large-cnn-samsum')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a summarization model. Specify the model 'philschmid/bart-large-cnn-samsum' to be loaded. This model is trained to generate abstractive summaries of conversations.\\\\n3. The created summarizer can be used to provide a summarized version of the conversations in the app.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nsummarizer = pipeline('summarization', model='philschmid/bart-large-cnn-samsum')\\\\nconversation_summary = summarizer(conversation_text)\\\", 'api_call': \\\"pipeline('summarization', model='philschmid/bart-large-cnn-samsum')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'philschmid/bart-large-cnn-samsum', 'api_call': \\\"pipeline('summarization', model='philschmid/bart-large-cnn-samsum')\\\", 'api_arguments': {'model': 'philschmid/bart-large-cnn-samsum'}, 'python_environment_requirements': [{'transformers': 'latest'}], 'example_code': \\\"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\\\nconversation = '''Jeff: Can I train a 🤗 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n'''\\\\nsummarizer(conversation)\\\", 'performance': {'dataset': 'samsum', 'accuracy': {'eval_rouge1': 42.621, 'eval_rouge2': 21.9825, 'eval_rougeL': 33.034, 'eval_rougeLsum': 39.6783, 'test_rouge1': 41.3174, 'test_rouge2': 20.8716, 'test_rougeL': 32.1337, 'test_rougeLsum': 38.4149}}, 'description': 'philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير تقنيات تحويل النصوص إلى كلام لإنشاء تطبيقات صوتية مبتكرة وذات جودة عالية، ونرغب في دمج هذه التقنيات لتحقيق تجربة صوتية مميزة وملهمة.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': \\\"Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\\\", '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': \\\"1. We import the necessary classes from the espnet2.bin.tts_inference package. This includes Text2Speech for the text-to-speech model.\\\\n2. We then use the from_pretrained method of the Text2Speech class to load the pre-trained model 'espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best'. This model has been trained to convert Chinese text input into speech output in Chinese.\\\\n3. The created text-to-speech system can be used to process the input text and generate the corresponding audio file.\\\\n4. For saving the audio file, we use the soundfile library, which provides a convenient way to write the generated speech to a file.\\\", '<<<code>>>': \\\"import soundfile\\\\nfrom espnet2.bin.tts_inference import Text2Speech\\\\ntext2speech = Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\\\\ntext = '春江潮水连海平，海上明月共潮生'\\\\nspeech = text2speech(text)['wav']\\\\nsoundfile.write('output.wav', speech.numpy(), text2speech.fs, 'PCM_16')\\\", 'api_call': \\\"Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\\\", 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best', 'api_call': \\\"Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['torch', 'espnet_model_zoo']], 'example_code': 'import soundfile\\\\nfrom espnet2.bin.tts_inference import Text2Speech\\\\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\\\\ntext = 春江潮水连海平，海上明月共潮生\\\\nspeech = text2speech(text)[wav]\\\\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)', 'performance': {'dataset': 'csmsc', 'accuracy': 'Not specified'}, 'description': 'A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نماذج تجربيات جيل النص الطبيعي لنصوص الجيل النصي، ونحتاج إلى تقدير الجمل المحتملة لدعم توليد النظام.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the necessary libraries, which include the T5ForConditionalGeneration class and the T5Tokenizer from the transformers library.', 'code': 'from transformers import T5ForConditionalGeneration, T5Tokenizer\\\\n\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'castorini/doc2query-t5-base-msmarco\\\\')\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'castorini/doc2query-t5-base-msmarco\\\\')\\\\n\\\\ninput_text = \\\"<organization\\\\'s technical document>\\\"\\\\nencoded_input = tokenizer.encode(input_text, return_tensors=\\\"pt\\\")\\\\ngenerated_queries = model.generate(encoded_input, max_length=50)\\\\ndecoded_queries = tokenizer.batch_decode(generated_queries, skip_special_tokens=True)', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Text2Text Generation', 'api_name': 'castorini/doc2query-t5-base-msmarco', 'api_call': 'from transformers import T5ForConditionalGeneration, T5Tokenizer\\\\n\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'castorini/doc2query-t5-base-msmarco\\\\')\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'castorini/doc2query-t5-base-msmarco\\\\')\\\\n\\\\ninput_text = \\\"<organization\\\\'s technical document>\\\"\\\\nencoded_input = tokenizer.encode(input_text, return_tensors=\\\"pt\\\")\\\\ngenerated_queries = model.generate(encoded_input, max_length=50)\\\\ndecoded_queries = tokenizer.batch_decode(generated_queries, skip_special_tokens=True)', 'api_arguments': 'text, max_length', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'MS MARCO', 'accuracy': 'Not specified'}, 'description': 'A T5 model trained on the MS MARCO dataset for generating queries from documents.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بالعمل في مجال معالجة اللغة الطبيعية، ونود تطوير نظام لترجمة النصوص بين اللغات المختلفة بشكل فعال وسلس.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"translation_pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required pipeline function from the transformers library provided by Hugging Face.\\\\n2. Create a translation pipeline for translating German text to English text using the 'Helsinki-NLP/opus-mt-de-en' model. This model is specifically trained for German to English translations and achieved reasonable performance scores on various translation benchmarks.\\\\n3. Use the created translation pipeline to translate the sales emails from German to English.\\\\n4. The translated text can then be used by the sales team to address the client's needs expressed in the email.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntranslation_pipeline = pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\\\\ntranslated_text = translation_pipeline(german_sales_email)\\\", 'api_call': \\\"translation_pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-de-en', 'api_call': \\\"translation_pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\\\", 'api_arguments': ['source languages: de', 'target languages: en'], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'opus', 'accuracy': {'newssyscomb2009.de.en': 29.4, 'news-test2008.de.en': 27.8, 'newstest2009.de.en': 26.8, 'newstest2010.de.en': 30.2, 'newstest2011.de.en': 27.4, 'newstest2012.de.en': 29.1, 'newstest2013.de.en': 32.1, 'newstest2014-deen.de.en': 34.0, 'newstest2015-ende.de.en': 34.2, 'newstest2016-ende.de.en': 40.4, 'newstest2017-ende.de.en': 35.7, 'newstest2018-ende.de.en': 43.7, 'newstest2019-deen.de.en': 40.1, 'Tatoeba.de.en': 55.4}}, 'description': 'A German to English translation model trained on the OPUS dataset using the Hugging Face Transformers library.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي في تصنيف الصور بنظام الرؤية الحاسوبية.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include AutoFeatureExtractor and RegNetForImageClassification from transformers, torch, and load_dataset from datasets.\\\\n2. Load a dataset containing a collection of images, which can be obtained from sources like Hugging Face's Dataset Hub.\\\\n3. Preprocess the input image data using AutoFeatureExtractor, which helps in preparing the image data to be fed to the RegNet model.\\\\n4. Load the pre-trained image classification model 'zuppif/regnet-y-040' using RegNetForImageClassification.from_pretrained(). This model is trained to categorize images based on their content.\\\\n5. Use the model to classify the images and retrieve the predicted label for each image.\\\", '<<<code>>>': \\\"from transformers import AutoFeatureExtractor, RegNetForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset('<your-dataset>')  # replace with your dataset\\\\nimage = dataset['test']['image'][0]  # sample image from the dataset\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('zuppif/regnet-y-040')\\\\nmodel = RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\\ninputs = feature_extractor(image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/regnet-y-008', 'api_call': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'zuppif/regnet-y-040'}, 'python_environment_requirements': [{'transformers': 'AutoFeatureExtractor, RegNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}], 'example_code': 'from transformers import AutoFeatureExtractor, RegNetForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset(huggingface/cats-image)\\\\nimage = dataset[test][image][0]\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\\\ninputs = feature_extractor(image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n... logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لاكتشاف نشاط الصوت في مجال الصوتيات، ويجب على المستخدمين توفير مجموعات بيانات كافية لضمان أداء موثوق للنموذج.? [/INST]{'domain': 'Audio Voice Activity Detection', 'api_call': \\\"Pipeline.from_pretrained('pyannote/speaker-diarization@2.1')\\\", 'api_provider': 'pyannote.audio', 'explanation': \\\"1. Import the 'Pipeline' class from the pyannote.audio library.\\\", 'code': \\\"from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained('pyannote/speaker-diarization@2.1')\\\\naudio_file_path = 'path/to/your/audio_file.wav'\\\\ndiarization = pipeline(audio_file_path)\\\", 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'pyannote.audio', 'functionality': 'Speaker diarization', 'api_name': 'johnislarry/cloned-pyannote-speaker-diarization-endpoint', 'api_call': \\\"from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained('pyannote/speaker-diarization@2.1')\\\\naudio_file_path = 'path/to/your/audio_file.wav'\\\\ndiarization = pipeline(audio_file_path)\\\", 'api_arguments': ['num_speakers', 'min_speakers', 'max_speakers', 'segmentation_onset'], 'python_environment_requirements': 'pyannote.audio 2.0', 'example_code': {'load_pipeline': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)', 'apply_pipeline': 'diarization = pipeline(audio.wav)', 'save_output': 'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)'}, 'performance': {'dataset': [{'name': 'AISHELL-4', 'accuracy': {'DER%': 14.61, 'FA%': 3.31, 'Miss%': 4.35, 'Conf%': 6.95}}, {'name': 'AMI Mix-Headset only_words', 'accuracy': {'DER%': 18.21, 'FA%': 3.28, 'Miss%': 11.07, 'Conf%': 3.87}}, {'name': 'AMI Array1-01 only_words', 'accuracy': {'DER%': 29.0, 'FA%': 2.71, 'Miss%': 21.61, 'Conf%': 4.68}}, {'name': 'CALLHOME Part2', 'accuracy': {'DER%': 30.24, 'FA%': 3.71, 'Miss%': 16.86, 'Conf%': 9.66}}, {'name': 'DIHARD 3 Full', 'accuracy': {'DER%': 20.99, 'FA%': 4.25, 'Miss%': 10.74, 'Conf%': 6.0}}, {'name': 'REPERE Phase 2', 'accuracy': {'DER%': 12.62, 'FA%': 1.55, 'Miss%': 3.3, 'Conf%': 7.76}}, {'name': 'VoxConverse v0.0.2', 'accuracy': {'DER%': 12.76, 'FA%': 3.45, 'Miss%': 3.85, 'Conf%': 5.46}}]}, 'description': 'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.'}} </s>\"\n"}
{"text": "<s>[INST]على مستوى نموذج التعلم الآلي الذي تم مشاركته، يقوم بتحليل بيانات جدولية لتوقع قيم متغير مستنتج ما بناءً على المتغيرات التنبؤية. يجب تدريب النموذج ببيانات التدريب الجدولية واختباره بالبيانات غير المستخدمة لضمان كفاءته. عبارة عن ميزة حساسة لمواصفات البيانات المستخدمة والإعدادات المعتمدة. لذلك، يجب توفير بيئة اختبار مناسبة واختبارها بشكل دوري للتحقق من أداء النموذج.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. First, we need to load the tabular regression model provided by Hugging Face. This can be done using the joblib library.\\\\n2. Next, we need to read and preprocess the data in the format required by the model. This involves selecting the necessary feature columns and renaming them.\\\\n3. Then, we simply use the loaded model to make predictions for CO2 emissions. The predictions are returned as a numeric array, where each element represents the predicted CO2 emission for the corresponding input example.', '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nimport json\\\\n\\\\n# Load the pretrained model\\\\nmodel = joblib.load('model.joblib')\\\\n\\\\n# Load the feature configuration\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\n\\\\n# Load and preprocess the vehicle lineup data\\\\ndata = pd.read_csv('vehicle_lineup.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\n\\\\n# Make predictions for the CO2 emissions\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'farouk97/autotrain-test7-2644pc-linearregr-38619101723', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'farouk97/autotrain-data-test7-2644pc-linearregr', 'accuracy': {'Loss': 0.145, 'R2': 0.0, 'MSE': 0.021, 'MAE': 0.099, 'RMSLE': 0.101}}, 'description': 'A tabular regression model trained using AutoTrain to predict CO2 emissions (in grams).' </s>\"\n"}
{"text": "<s>[INST]عيِّن سؤالًا حول الجواب المرئي باستخدام استخراج الإجابة.? [/INST]{'domain': 'Multimodal Visual Question Answering', 'api_call': \\\"pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\nvqa_pipeline = pipeline(\\\\'visual-question-answering\\\\', model=\\\\'azwierzc/vilt-b32-finetuned-vqa-pl\\\\')\\\\nimage_path = \\\"path/to/image.jpg\\\"\\\\nquestion_polish = \\\"Pytanie po polsku tutaj\\\"\\\\nanswer = vqa_pipeline(image=image_path, question=question_polish)', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'azwierzc/vilt-b32-finetuned-vqa-pl', 'api_call': 'from transformers import pipeline\\\\nvqa_pipeline = pipeline(\\\\'visual-question-answering\\\\', model=\\\\'azwierzc/vilt-b32-finetuned-vqa-pl\\\\')\\\\nimage_path = \\\"path/to/image.jpg\\\"\\\\nquestion_polish = \\\"Pytanie po polsku tutaj\\\"\\\\nanswer = vqa_pipeline(image=image_path, question=question_polish)', 'api_arguments': {'image': 'path_to_image', 'question': 'question_text'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the Polish language.'}} </s>\"\n"}
{"text": "<s>[INST]يمكن أن تكون تقنيات معالجة اللغات الطبيعية والصور مفيدة في مجال Multimodal Graph Machine Learning لتحسين أداء نماذج التعلم الآلي على البيانات المتعددة الوسائط. يمكنك إنشاء تعليمات توجيهية تشرح كيفية استخدام البيانات الرقمية والرسم البياني وتدريب النماذج المتعددة الوسائط لتحقيق تقدم في تصنيف البيانات والتنبؤ على المستوى الموجود داخل الرسوم البيانية.? [/INST]'<<<domain>>>': 'Multimodal Graph Machine Learning', '<<<api_call>>>': \\\"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required library, AutoModel, from the transformers package to use the Graphormer model.\\\\n2. Load the pretrained 'graphormer-base-pcqm4mv1' model to leverage its capabilities in predicting the quantum properties of materials.\\\\n3. The Graphormer model has been trained on the PCQM4M-LSC dataset and demonstrates excellent performance in graph representation tasks and graph classification tasks.\\\\n4. You can now use the Graphormer model for your material science research, such as predicting quantum properties of various materials or fine-tuning the model for downstream specific tasks.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\\\", 'api_call': \\\"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': \\\"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers']], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.' </s>\"\n"}
{"text": "<s>[INST]عميلنا يرغب في تطوير تطبيق يستند إلى معالجة اللغة الطبيعية والصوت والصور لإنشاء مقاطع الفيديو دون الحاجة إلى تدريب مسبق. نحتاج إلى برنامج قادر على تحويل النص إلى فيديو لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Video', '<<<api_call>>>': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import necessary libraries from the Tune-A-Video package, torch library and others.\\\\n2. Load the pre-trained models from the Hugging Face model hub, which include the diffusion model and the UNet3DConditionModel as specified in the API call.\\\\n3. Create a pipeline object 'pipe' with the combination of both models.\\\\n4. Set the textual prompt as 'a bear enjoying a beautiful sunset in the forest'.\\\\n5. Pass the prompt and other parameters like video_length, height, width for generating the video to the 'pipe'.\\\\n6. Save the generated video to a specified location.\\\", '<<<code>>>': \\\"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\\\nfrom tuneavideo.util import save_videos_grid\\\\nimport torch\\\\n\\\\npretrained_model_path = 'nitrosocke/redshift-diffusion'\\\\nunet_model_path = 'Tune-A-Video-library/redshift-man-skiing'\\\\nprompt = 'a bear enjoying a beautiful sunset in the forest'\\\\n\\\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to('cuda')\\\\npipe.enable_xformers_memory_efficient_attention()\\\\n\\\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\\\nsave_videos_grid(video, f'./{prompt}.gif')\\\", 'api_call': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'redshift-man-skiing', 'api_call': \\\"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet', torch_dtype=torch.float16), torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': [['torch', 'tuneavideo']], 'example_code': \\\"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\\\nfrom tuneavideo.util import save_videos_grid\\\\nimport torch\\\\npretrained_model_path = nitrosocke/redshift-diffusion\\\\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\\\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\nprompt = (redshift style) spider man is skiing\\\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\\\nsave_videos_grid(video, f./{prompt}.gif)\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': \\\"Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\\\" </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نظام للمحادثات في معالجة اللغات الطبيعية بحيث يتمكن من الإجابة على الأسئلة المتعلقة بالمحادثات بشكل ذكي ونريد توقع ردود الفعل باستخدام البيانات المتوفرة.? [/INST]{'domain': 'Natural Language Processing Conversational', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('Zixtrauce/JohnBot')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We import the necessary class AutoModelForCausalLM from the transformers library provided by Hugging Face.', 'code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nmodel = AutoModelForCausalLM.from_pretrained('Zixtrauce/JohnBot')\\\\ntokenizer = AutoTokenizer.from_pretrained('Zixtrauce/JohnBot')\\\\n\\\\ndef generate_response(customer_message):\\\\n    input_tensor = tokenizer.encode(customer_message + tokenizer.eos_token, return_tensors='pt')\\\\n    reply_tensor = model.generate(input_tensor, max_length=100, pad_token_id=tokenizer.eos_token_id)\\\\n    response_text = tokenizer.decode(reply_tensor[:, input_tensor.shape[-1]:][0], skip_special_tokens=True)\\\\n    return response_text\\\", 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'Zixtrauce/JohnBot', 'api_call': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nmodel = AutoModelForCausalLM.from_pretrained('Zixtrauce/JohnBot')\\\\ntokenizer = AutoTokenizer.from_pretrained('Zixtrauce/JohnBot')\\\\n\\\\ndef generate_response(customer_message):\\\\n    input_tensor = tokenizer.encode(customer_message + tokenizer.eos_token, return_tensors='pt')\\\\n    reply_tensor = model.generate(input_tensor, max_length=100, pad_token_id=tokenizer.eos_token_id)\\\\n    response_text = tokenizer.decode(reply_tensor[:, input_tensor.shape[-1]:][0], skip_special_tokens=True)\\\\n    return response_text\\\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'Input a message to start chatting with Zixtrauce/JohnBot.', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'JohnBot is a conversational model based on the gpt2 architecture and trained using the Hugging Face Transformers library. It can be used for generating text responses in a chat-based interface.'}} </s>\"\n"}
{"text": "<s>[INST]نقوم بتشغيل خوارزمية معالجة اللغة الطبيعية ومعالجة البيانات البصرية بالتوازي في مشهد يصور الحيوانات في البرية لتوليد فيديو من وصف نصي وتصنيف النص والصور. هل تحتاج إلى مساعدة في هذا؟? [/INST]'<<<domain>>>': 'Multimodal Graph Machine Learning', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import necessary libraries, including PyTorch and Transformers.\\\\n2. Create a simple neural network class (NanoCircuit) that takes in the MNIST dataset input and outputs logits for each of the ten possible classes.\\\\n3. Define a function (train_with_data_cap) that takes in the created model, data_loader, criterion, optimizer, device, and a data_cap_gb parameter. This function will train the model on the dataset, running an epoch in a loop until the amount of data processed surpasses the data_cap_gb limit.\\\\n4. Then, use the trained model to perform predictions on game-related processing like GTA5.\\\\n5. To use the model in-game, pass the AI model instance to the game using its PID and make sure the targeted application is running to interact with it.', '<<<code>>>': 'import torch\\\\nimport torch.nn as nn\\\\nimport torch.optim as optim\\\\nfrom torchvision import datasets, transforms\\\\n\\\\nclass NanoCircuit(nn.Module):\\\\n    def __init__(self):\\\\n        super(NanoCircuit, self).__init__()\\\\n        self.fc1 = nn.Linear(784, 128)\\\\n        self.fc2 = nn.Linear(128, 10)\\\\n\\\\n    def forward(self, x):\\\\n        x = x.view(-1, 784)\\\\n        x = torch.relu(self.fc1(x))\\\\n        x = self.fc2(x)\\\\n        return x\\\\n\\\\ndef train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):\\\\n    data_processed = 0\\\\n    data_cap_bytes = data_cap_gb * (1024 ** 3)\\\\n    epoch = 0\\\\n    while data_processed < data_cap_bytes:\\\\n        running_loss = 0.0\\\\n        for i, data in enumerate(data_loader, 0):\\\\n            inputs, labels = data\\\\n            inputs, labels = inputs.to(device), labels.to(device)\\\\n            data_processed += inputs.nelement() * inputs.element_size()\\\\n            if data_processed >= data_cap_bytes:\\\\n                break\\\\n            optimizer.zero_grad()\\\\n            outputs = model(inputs.view(-1, 28 * 28))\\\\n            loss = criterion(outputs, labels)\\\\n            loss.backward()\\\\n            optimizer.step()\\\\n            running_loss += loss.item()\\\\n        epoch += 1\\\\n    return model\\\\n\\\\n# Training the model with a specific data cap\\\\ntrain_loader = torch.utils.data.DataLoader(\\\\n    datasets.MNIST(\\\\'data\\\\', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\\\\n    batch_size=batch_size, shuffle=True)\\\\ndevice = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\\\nmodel = NanoCircuit().to(device)\\\\ncriterion = nn.CrossEntropyLoss()\\\\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\\\\ndata_cap_gb = 10\\\\ntrained_model = train_with_data_cap(model, train_loader, criterion, optimizer, device, data_cap_gb)', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GTA5_PROCESS_LEARNING_AI', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\\\", 'api_arguments': {'model': 'NanoCircuit', 'data_loader': 'train_loader', 'criterion': 'nn.CrossEntropyLoss', 'optimizer': 'optim.SGD', 'device': 'torch.device', 'data_cap_gb': 10}, 'python_environment_requirements': [['contextlib', 'os', 'matplotlib', 'numpy', 'torch', 'torch.nn', 'torch.optim', 'requests', 'torchvision', 'psutil', 'time', 'subprocess', 'onnxruntime', 'numexpr', 'transformers']], 'example_code': {'import_libraries': ['import contextlib', 'import os', 'from matplotlib import pyplot as plt', 'import numpy as np', 'import torch', 'import torch.nn as nn', 'import torch.optim as optim', 'import requests', 'from torchvision import datasets, transforms', 'import psutil', 'import time', 'import subprocess', 'import onnxruntime as ort', 'import matplotlib.pyplot as plt', 'import numpy as np', 'import numexpr as ne', 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM'], 'define_neural_network': ['class NanoCircuit(nn.Module):', ' def init(self):', ' super(NanoCircuit, self).init()', ' self.fc1 = nn.Linear(784, 128)', ' self.fc2 = nn.Linear(128, 10)', 'def forward(self, x):', ' x = x.view(-1, 784)', ' x = torch.relu(self.fc1(x))', ' x = self.fc2(x)', ' return x'], 'train_with_data_cap': ['def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):', ' data_processed = 0', ' data_cap_bytes = data_cap_gb * (1024 ** 3)', ' epoch = 0', 'while data_processed < data_cap_bytes:', ' running_loss = 0.0', ' for i, data in enumerate(data_loader, 0):', ' inputs, labels = data', ' inputs, labels = inputs.to(device), labels.to(device)', ' data_processed += inputs.nelement() * inputs.element_size()', ' if data_processed >= data_cap_bytes:', ' break', ' optimizer.zero_grad()', ' outputs = model(inputs.view(-1, 28 * 28))', ' loss = criterion(outputs, labels)', ' loss.backward()', ' optimizer.step()', ' running_loss += loss.item()', 'epoch += 1', 'print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})', 'print(fData processed: {data_processed / (1024 ** 3):.2f} GB)', 'return model']}, 'performance': {'dataset': 'MNIST', 'accuracy': 'Not specified'}, 'description': 'This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى تطوير برنامج في تجهيز اللغة الطبيعية لتصنيف النصوص. المستخدم يحتاج إلى تحويل النصوص إلى تصنيفات وفقًا لنوعها ومحتواها.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('madhurjindal/autonlp-Gibberish-Detector-492513457')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries: AutoModelForSequenceClassification and AutoTokenizer from the transformers library provided by Hugging Face.\\\\n2. Load the pretrained model 'madhurjindal/autonlp-Gibberish-Detector-492513457' using AutoModelForSequenceClassification.from_pretrained() method.\\\\n3. Load the tokenizer associated with the model using the AutoTokenizer.from_pretrained() method.\\\\n4. Tokenize the given input text using the tokenizer and generate the input tensors.\\\\n5. Pass the input tensors to the model and obtain the model outputs.\\\\n6. Analyze the model outputs to determine if the given text input is gibberish or not.\\\", '<<<code>>>': 'from transformers import AutoModelForSequenceClassification, AutoTokenizer\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\\\'madhurjindal/autonlp-Gibberish-Detector-492513457\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'madhurjindal/autonlp-Gibberish-Detector-492513457\\\\')\\\\ninputs = tokenizer(\\\"Given text input here\\\", return_tensors=\\\"pt\\\")\\\\noutputs = model(**inputs)\\\\nprediction = outputs.logits.argmax(dim=1)\\\\nif prediction.item() == 1:\\\\n    print(\\\"The given text is gibberish.\\\")\\\\nelse:\\\\n    print(\\\"The given text is not gibberish.\\\")', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('madhurjindal/autonlp-Gibberish-Detector-492513457')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'madhurjindal/autonlp-Gibberish-Detector-492513457', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('madhurjindal/autonlp-Gibberish-Detector-492513457')\\\", 'api_arguments': {'inputs': 'I love AutoNLP'}, 'python_environment_requirements': [{'transformers': 'AutoModelForSequenceClassification', 'AutoTokenizer': 'from_pretrained'}], 'example_code': 'from transformers import AutoModelForSequenceClassification, AutoTokenizer\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\\\\ninputs = tokenizer(I love AutoNLP, return_tensors=pt)\\\\noutputs = model(**inputs)', 'performance': {'dataset': 'madhurjindal/autonlp-data-Gibberish-Detector', 'accuracy': 0.9735624586913417}, 'description': 'A multi-class text classification model for detecting gibberish text. Trained using AutoNLP and DistilBERT.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في تصنيف الفيديو؟? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We first import the necessary classes from the transformers and numpy packages.', 'code': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\\\\ninputs = processor(video, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\n  predicted_class_idx = logits.argmax(-1).item()\\\\n  print('Predicted gesture:', model.config.id2label[predicted_class_idx])\\\", 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'videomae-small-finetuned-kinetics', 'api_call': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\\\\ninputs = processor(video, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\n  predicted_class_idx = logits.argmax(-1).item()\\\\n  print('Predicted gesture:', model.config.id2label[predicted_class_idx])\\\", 'api_arguments': {'video': 'list(np.random.randn(16, 3, 224, 224))'}, 'python_environment_requirements': ['transformers', 'numpy', 'torch'], 'example_code': \\\"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\\\\ninputs = processor(video, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\\\", 'performance': {'dataset': 'Kinetics-400', 'accuracy': {'top-1': 79.0, 'top-5': 93.8}}, 'description': 'VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتشغيل خوارزمية التعرف على الصور في مشهد يصور الحيوانات في البرية لتصنيف الصور. هل تتطلب مساعدة في هذا؟? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary DDPMPipeline class from the diffusers package.\\\\n2. Use the from_pretrained method of the DDPMPipeline class to load the pre-trained model 'schdoel/sd-class-AFHQ-32'. This model has been trained for unconditional image synthesis tasks of cute animals, which is perfect for generating images for our pet daycare marketing campaign.\\\\n3. We then use the pipeline to generate a cute animal image and save it for our campaign purposes.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\\nimage = pipeline().images[0]\\\\nimage.save('cute_animal_image.png')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sd-class-pandas-32', 'api_call': \\\"DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\", 'api_arguments': {'pretrained_model': 'schdoel/sd-class-AFHQ-32'}, 'python_environment_requirements': [{'package': 'diffusers', 'import': 'from diffusers import DDPMPipeline'}], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': 'AFHQ', 'accuracy': 'Not provided'}, 'description': 'This model is a diffusion model for unconditional image generation of cute 🦋.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتصنيف النصوص والصور في مجال معالجة اللغة الطبيعية والترجمة، ونرغب في استخدامه لتحديد تصنيف النصوص والصور بدقة عالية وكفاءة.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a translation model for Italian to English. Specify the model 'Helsinki-NLP/opus-mt-it-en' to be loaded, which has been trained on a large corpus of text to handle the translation task.\\\\n3. The created translation pipeline can be used to translate Italian text to English.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')\\\\ntranslated_email = translation(italian_email_text)\\\", 'api_call': \\\"pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-it-en', 'api_call': \\\"pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')('Ciao mondo!')\\\", 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': {'newssyscomb2009.it.en': 35.3, 'newstest2009.it.en': 34.0, 'Tatoeba.it.en': 70.9}, 'chr-F': {'newssyscomb2009.it.en': 0.6, 'newstest2009.it.en': 0.594, 'Tatoeba.it.en': 0.808}}}, 'description': 'A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتصنيف النصوص باستخدام المعالجة اللغوية الطبيعية. ونريد تحسين فهم أنواع النصوص المختلفة بناءً على التصنيف الأوتوماتيكي.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the required pipeline function from the transformers library provided by Hugging Face.\\\\n2. We use the pipeline function to instantiate a text classification model.\\\\n3. Set the model to 'Seethal/sentiment_analysis_generic_dataset', which has been trained for sentiment analysis tasks.\\\\n4. Use the model to analyze the sentiment of text reviews, which can be used to help inform product recommendations to users based on their sentiment.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsentiment_analyzer = pipeline(\\\\'text-classification\\\\', model=\\\\'Seethal/sentiment_analysis_generic_dataset\\\\')\\\\nsentiment = sentiment_analyzer(\\\"I love this product!\\\")', 'api_call': \\\"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentiment_analysis_generic_dataset', 'api_call': \\\"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': \\\"sentiment_analysis('I love this product!')\\\", 'performance': {'dataset': 'generic_dataset', 'accuracy': 'Not specified'}, 'description': 'This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text classification.' </s>\"\n"}
{"text": "<s>[INST]سيقوم فريقنا بتطوير نماذج لتوليد الصور بشكل غير مشروط في تقنيات الرؤية الحاسوبية. يتعين عليك توفير البيانات اللازمة لتدريب النموذج، وتحديد العمق المطلوب لكل عنصر في الصورة أو المشهد.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\\\ngenerated_image = pipeline().images[0]\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'johnowhitaker/sd-class-wikiart-from-bedrooms', 'api_call': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\\\ngenerated_image = pipeline().images[0]\\\", 'api_arguments': '', 'python_environment_requirements': 'diffusers', 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': 'https://huggingface.co/datasets/huggan/wikiart', 'accuracy': 'Not provided'}, 'description': 'This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart.'}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء فيديو باستخدام تقنيات معالجة اللغات الطبيعية من النص الوصفي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في تنفيذ ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary packages from the transformers library. This includes T5ForConditionalGeneration for summarization tasks and a pipeline from transformers.\\\\n2. Load the pre-trained model 'pszemraj/long-t5-tglobal-base-16384-book-summary' using the from_pretrained method of the T5ForConditionalGeneration class. This model is specifically designed for summarizing long texts like books.\\\\n3. Set up a summarization pipeline with the loaded model.\\\\n4. Pass the entire long text of the book to the summarization pipeline, and it will generate a condensed summary of the book.\\\\n5. Print or store the summary as needed.\\\", '<<<code>>>': 'from transformers import pipeline, T5ForConditionalGeneration\\\\nimport torch\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\\\')\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model, device=0 if torch.cuda.is_available() else -1)\\\\nlong_text = \\\"Your 300-page book content here\\\"\\\\nresult = summarizer(long_text)\\\\nsummary = result[0][\\\\'summary_text\\\\']\\\\nprint(summary)', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pszemraj/long-t5-tglobal-base-16384-book-summary', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\", 'api_arguments': ['long_text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don't want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\\\", 'performance': {'dataset': 'kmfoda/booksum', 'accuracy': {'ROUGE-1': 36.408, 'ROUGE-2': 6.065, 'ROUGE-L': 16.721, 'ROUGE-LSUM': 33.34}}, 'description': 'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطبيق تقنية تصنيف الصور باستخدام الرؤية الحاسوبية لتحديد الحيوانات في البرية. هل يمكنك مساعدتي في توليد هذا التطبيق؟? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary classes and functions from the transformers and torch libraries. This includes AutoImageProcessor for image preprocessing and ResNetForImageClassification for the image classification model.\\\\n2. We then use the from_pretrained method of the ResNetForImageClassification class to load the pre-trained model 'microsoft/resnet-50'. This model has been trained for image classification tasks, which is exactly what we need for determining the categories of items on an e-commerce site.\\\\n3. We load and preprocess the image using the AutoImageProcessor. The processed image is then fed to the model to classify the object in the image.\\\\n4. The model outputs the logits, and we use the argmax function to determine the predicted label for the image.\\\\n5. Finally, we print the predicted label, which corresponds to the item category we're looking for.\\\", '<<<code>>>': \\\"from transformers import AutoImageProcessor, ResNetForImageClassification\\\\nimport torch\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('microsoft/resnet-50')\\\\nmodel = ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\\\\n\\\\nimage = 'path_to_image.jpg'  # replace with the image's file path\\\\ninputs = processor(image, return_tensors='pt')\\\\n\\\\nlogits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/resnet-50', 'api_call': \\\"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\\\", 'api_arguments': {'from_pretrained': 'microsoft/resnet-50'}, 'python_environment_requirements': [{'transformers': 'AutoImageProcessor, ResNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}], 'example_code': 'from transformers import AutoImageProcessor, ResNetForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset(huggingface/cats-image)\\\\nimage = dataset[test][image][0]\\\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/resnet-50)\\\\nmodel = ResNetForImageClassification.from_pretrained(microsoft/resnet-50)\\\\ninputs = processor(image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': '~0.5% top1'}, 'description': 'ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback.' </s>\"\n"}
{"text": "<s>[INST]يرجى توليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية باستخدام تقنيات معالجة اللغة الطبيعية. يهدف الفيديو إلى تصنيف الحيوانات باستخدام تقنيات رؤية الحاسوب. هل يمكنك مساعدتي في هذا؟? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"TimesformerForVideoClassification.from_pretrained('fcakyon/timesformer-large-finetuned-k400')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and classes, including AutoImageProcessor, TimesformerForVideoClassification, numpy, and torch.\\\\n2. Load the 'fcakyon/timesformer-large-finetuned-k400' pre-trained model using the TimesformerForVideoClassification.from_pretrained utility.\\\\n3. This model can then be used to analyze a video on a specific topic and classify it into one of the 400 possible Kinetics-400 labels. When using the model for video classification, you would first use the AutoImageProcessor to process the video and then use the TimesformerForVideoClassification model to predict the class of the video content.\\\\n4. The predicted class can help you understand which topic the video is about.\\\", '<<<code>>>': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\nvideo = list(np.random.randn(96, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(\\\\'fcakyon/timesformer-large-finetuned-k400\\\\')\\\\nmodel = TimesformerForVideoClassification.from_pretrained(\\\\'fcakyon/timesformer-large-finetuned-k400\\\\')\\\\n\\\\ninputs = processor(video, return_tensors=\\\\'pt\\\\')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\\"Predicted class:\\\", model.config.id2label[predicted_class_idx])', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('fcakyon/timesformer-large-finetuned-k400')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'fcakyon/timesformer-large-finetuned-k400', 'api_call': \\\"TimesformerForVideoClassification.from_pretrained('fcakyon/timesformer-large-finetuned-k400')\\\", 'api_arguments': ['video'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(96, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(fcakyon/timesformer-large-finetuned-k400)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(fcakyon/timesformer-large-finetuned-k400)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Kinetics-400', 'accuracy': 'Not provided'}, 'description': \\\"TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper 'TimeSformer: Is Space-Time Attention All You Need for Video Understanding?' by Tong et al.\\\" </s>\"\n"}
{"text": "<s>[INST]تطوير تطبيق للإجابة على الأسئلة المرئية متعددة الوسائط. نطلب برنامجًا يحول النصوص إلى كلام لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"ViltModel.from_pretrained('tufa15nik/vilt-finetuned-vqasi')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To achieve this goal, we can use a Visual Question Answering (VQA) model from Hugging Face Transformers. Specifically, we can use `tufa15nik/vilt-finetuned-vqasi`, which is a VQA model that has been fine-tuned on the VQASI dataset and is designed to answer questions about an input image.\\\\n2. First, import the required classes from the transformers package. This includes ViltModel for the visual question answering model and the tokenizer.\\\\n3. Load the VQA model using the from_pretrained method of the ViltModel class and the address 'tufa15nik/vilt-finetuned-vqasi'.\\\\n4. This model can then be used to analyze an image and answer questions based on the content of the image.\\\", '<<<code>>>': \\\"from transformers import ViltModel, ViltTokenizer\\\\nmodel = ViltModel.from_pretrained('tufa15nik/vilt-finetuned-vqasi')\\\\ntokenizer = ViltTokenizer.from_pretrained('tufa15nik/vilt-finetuned-vqasi')\\\", 'api_call': \\\"ViltModel.from_pretrained('tufa15nik/vilt-finetuned-vqasi')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'vilt-finetuned-vqasi', 'api_call': \\\"ViltModel.from_pretrained('tufa15nik/vilt-finetuned-vqasi')\\\", 'api_arguments': {'model': 'tufa15nik/vilt-finetuned-vqasi', 'tokenizer': 'tufa15nik/vilt-finetuned-vqasi'}, 'python_environment_requirements': [{'transformers': '>=4.11.3'}], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the VQASI dataset by tufa15nik using the ViLT architecture. The model is designed to answer questions based on the content of an input image.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء مجموعة دراسية للاجتماع أسبوعيًا والنقاش حول مجال الصوتيات وتحويل الصوت إلى صوت. ثم قم بتقديم وحدة لتقييم التشابه بين الإشارات الصوتية في هذا السياق.? [/INST]{'domain': 'Audio Audio-to-Audio', 'api_call': 'S2THubInterface()', 'api_provider': 'Fairseq', 'explanation': '1. Import the required libraries and modules such as S2THubInterface, torchaudio, and fairseq.', 'code': 'import torchaudio\\\\nfrom fairseq import hub_utils, load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\n\\\\ncache_dir = os.getenv(\\\\'HUGGINGFACE_HUB_CACHE\\\\')\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n    \\\\'facebook/xm_transformer_s2ut_hk-en\\\\',\\\\n    arg_overrides={\\\\'config_yaml\\\\': \\\\'config.yaml\\\\', \\\\'task\\\\': \\\\'speech_to_text\\\\'},\\\\n    cache_dir=cache_dir,\\\\n)\\\\n\\\\nmodel = models[0].cpu()\\\\ncfg[\\\\'task\\\\'].cpu = True\\\\ngenerator = task.build_generator([model], cfg)\\\\n\\\\naudio, _ = torchaudio.load(\\\"/path/to/hokkien_speech/audio/file\\\")\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\ntranslated_speech = S2THubInterface.get_prediction(task, model, generator, sample)', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Fairseq', 'functionality': 'Speech-to-speech translation', 'api_name': 'xm_transformer_s2ut_hk-en', 'api_call': 'import torchaudio\\\\nfrom fairseq import hub_utils, load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\n\\\\ncache_dir = os.getenv(\\\\'HUGGINGFACE_HUB_CACHE\\\\')\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n    \\\\'facebook/xm_transformer_s2ut_hk-en\\\\',\\\\n    arg_overrides={\\\\'config_yaml\\\\': \\\\'config.yaml\\\\', \\\\'task\\\\': \\\\'speech_to_text\\\\'},\\\\n    cache_dir=cache_dir,\\\\n)\\\\n\\\\nmodel = models[0].cpu()\\\\ncfg[\\\\'task\\\\'].cpu = True\\\\ngenerator = task.build_generator([model], cfg)\\\\n\\\\naudio, _ = torchaudio.load(\\\"/path/to/hokkien_speech/audio/file\\\")\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\ntranslated_speech = S2THubInterface.get_prediction(task, model, generator, sample)', 'api_arguments': {'task': 'speech_to_text', 'model': 'facebook/xm_transformer_s2ut_hk-en', 'generator': 'task.build_generator([model], cfg)', 'sample': 'S2THubInterface.get_model_input(task, audio)'}, 'python_environment_requirements': {'fairseq': 'latest', 'torchaudio': 'latest', 'huggingface_hub': 'latest'}, 'example_code': \\\"import json\\\\nimport os\\\\nfrom pathlib import Path\\\\nimport IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n facebook/xm_transformer_s2ut_hk-en,\\\\n arg_overrides={config_yaml: config.yaml, task: speech_to_text},\\\\n cache_dir=cache_dir,\\\\n)\\\\nmodel = models[0].cpu()\\\\ncfg[task].cpu = True\\\\ngenerator = task.build_generator([model], cfg)\\\\naudio, _ = torchaudio.load(/path/to/an/audio/file)\\\\nsample = S2THubInterface.get_model_input(task, audio)\\\\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\\\\nlibrary_name = fairseq\\\\ncache_dir = (\\\\n cache_dir or (Path.home() / .cache / library_name).as_posix()\\\\n)\\\\ncache_dir = snapshot_download(\\\\n ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name\\\\n)\\\\nx = hub_utils.from_pretrained(\\\\n cache_dir,\\\\n model.pt,\\\\n .,\\\\n archive_map=CodeHiFiGANVocoder.hub_models(),\\\\n config_yaml=config.json,\\\\n fp16=False,\\\\n is_vocoder=True,\\\\n)\\\\nwith open(f{x['args']['data']}/config.json) as f:\\\\n vocoder_cfg = json.load(f)\\\\nassert (\\\\n len(x[args][model_path]) == 1\\\\n), Too many vocoder models in the input\\\\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\nipd.Audio(wav, rate=sr)\\\", 'performance': {'dataset': 'TED, drama, TAT domain', 'accuracy': 'Not provided'}, 'description': 'Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq for Hokkien-English. Trained with supervised data in TED, drama, TAT domain, and weakly supervised data in drama domain.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتوليد النصوص باستخدام تقنيات معالجة اللغة الطبيعية، ونهدف إلى إنتاج نصوص طبيعية بشكل تلقائي وسلس.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='bigscience/test-bloomd-6b3')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the library from transformers, which is 'pipeline'.\\\\n2. Call the 'pipeline' function with the task type 'text-generation' and the pre-trained model 'bigscience/test-bloomd-6b3'. This model will be used for generating the text based on a given input.\\\\n3. To generate the story, provide an initial seed text as input to the model.\\\\n4. The model will output a continuation of the given seed text in the form of a story.\\\\n5. Participants can then use this generated text as a starting point and build their models based on the provided architecture.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ngenerator = pipeline('text-generation', model='bigscience/test-bloomd-6b3')\\\\nstory_seed = 'Once upon a time in a magical land...'\\\\ngenerated_story = generator(story_seed)\\\", 'api_call': \\\"pipeline('text-generation', model='bigscience/test-bloomd-6b3')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bigscience/test-bloomd-6b3', 'api_call': \\\"pipeline('text-generation', model='bigscience/test-bloomd-6b3')\\\", 'api_arguments': ['input_text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline; generator = pipeline('text-generation', model='bigscience/test-bloomd-6b3'); generator('Once upon a time')\\\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'A text generation model from Hugging Face, using the bigscience/test-bloomd-6b3 architecture. It can be used for generating text based on a given input.' </s>\"\n"}
{"text": "<s>[INST]توفر منصتنا خدمة التصنيف الصوري البديهية باستخدام الرؤية الحاسوبية. يرجى استخدام واجهة برمجة التطبيقات لتحقيق الصنف. ستحصل على النتائج بدقة وفعالية عالية.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"CLIPModel.from_pretrained('patrickjohncyh/fashion-clip')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. This includes CLIPProcessor for pre-processing the image and text data and CLIPModel for the zero-shot image classification model.\\\\n2. Load the pre-trained model 'patrickjohncyh/fashion-clip' using the from_pretrained method of the CLIPModel class. This model is specifically trained for fashion industry applications.\\\\n3. Load the processor that matches the model using the from_pretrained method of the CLIPProcessor class.\\\\n4. Prepare the inputs to the model, which includes the image and text data. The text data should include names of the clothing categories you wish to classify the image.\\\\n5. Use the model to get logits_per_image, which represent the probability distribution of the image belonging to each of the categories mentioned earlier. The softmax function is used to convert logits into probabilities.\\\", '<<<code>>>': 'from transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(\\\\'patrickjohncyh/fashion-clip\\\\')\\\\nprocessor = CLIPProcessor.from_pretrained(\\\\'patrickjohncyh/fashion-clip\\\\')\\\\nimage_path = \\\"path/to/image.jpg\\\" # replace with the path to the customer\\\\'s image\\\\nimage = Image.open(image_path)\\\\nclass_names = \\\"t-shirt, dress, shoes, pants, hat\\\"\\\\ninputs = processor(text=class_names, images=image, return_tensors=\\\\'pt\\\\', padding=True)\\\\nlogits_per_image = model(**inputs).logits_per_image\\\\nprobs = logits_per_image.softmax(dim=-1).tolist()[0]', 'api_call': \\\"CLIPModel.from_pretrained('patrickjohncyh/fashion-clip')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'patrickjohncyh/fashion-clip', 'api_call': \\\"CLIPModel.from_pretrained('patrickjohncyh/fashion-clip')\\\", 'api_arguments': {'image': 'File', 'class_names': 'String (comma-separated)'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained('patrickjohncyh/fashion-clip'); processor = CLIPProcessor.from_pretrained('patrickjohncyh/fashion-clip'); inputs = processor(text='blue shoes', images=image, return_tensors='pt', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\\\", 'performance': {'dataset': [{'name': 'FMNIST', 'accuracy': 0.83}, {'name': 'KAGL', 'accuracy': 0.73}, {'name': 'DEEP', 'accuracy': 0.62}]}, 'description': 'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.' </s>\"\n"}
{"text": "<s>[INST]عميلنا يرغب في تطوير تطبيق لمعالجة اللغة الطبيعية Text2Text Generation. نحن بحاجة إلى برنامج يستطيع تحليل النصوص وتصنيفها بدقة.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'google/flan-t5-base\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'google/flan-t5-base\\\\')\\\\ninput_text = \\\"respond to message: \\\" + input_message\\\\ninput_ids = tokenizer(input_text, return_tensors=\\\\'pt\\\\').input_ids\\\\noutputs = model.generate(input_ids)\\\\nresponse_text = tokenizer.decode(outputs[0])', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Language model', 'api_name': 'google/flan-t5-base', 'api_call': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'google/flan-t5-base\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'google/flan-t5-base\\\\')\\\\ninput_text = \\\"respond to message: \\\" + input_message\\\\ninput_ids = tokenizer(input_text, return_tensors=\\\\'pt\\\\').input_ids\\\\noutputs = model.generate(input_ids)\\\\nresponse_text = tokenizer.decode(outputs[0])', 'api_arguments': 'input_text', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-base)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-base)\\\\ninput_text = translate English to German: How old are you?\\\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))', 'performance': {'dataset': [{'name': 'MMLU', 'accuracy': '75.2%'}]}, 'description': 'FLAN-T5 is a language model fine-tuned on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering.'}} </s>\"\n"}
{"text": "<s>[INST]يقوم الخبير بتطوير برنامج في مجال معالجة اللغة الطبيعية لتوليد النصوص. يرجى إنشاء تعليمات للبرمجة لتحويل النص إلى كلام مسموع لمستخدمي التطبيق.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='TehVenom/PPO_Pygway-V8p4_Dev-6b')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Begin by importing the required 'pipeline' function from the transformers library.\\\\n2. Use the 'pipeline' function to create a text generation model with the specified model 'TehVenom/PPO_Pygway-V8p4_Dev-6b'. This is a mixed model which will provide a variety of responses, making the chatbot engaging for users with different interests.\\\\n3. Once the text generation model is created, you can provide user input to the model to generate contextual and diverse responses using the 'generate()' method.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\n\\\\ntext_generator = pipeline('text-generation', model='TehVenom/PPO_Pygway-V8p4_Dev-6b')\\\\nuser_input = 'User input goes here...'\\\\ngenerated_response = text_generator(user_input)\\\", 'api_call': \\\"pipeline('text-generation', model='TehVenom/PPO_Pygway-V8p4_Dev-6b')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'TehVenom/PPO_Pygway-V8p4_Dev-6b', 'api_call': \\\"pipeline('text-generation', model='TehVenom/PPO_Pygway-V8p4_Dev-6b')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'TODO card. Mix of (GPT-J-6B-Janeway + PPO_HH_GPT-J) + Pygmalion-6b-DEV (V8 / Part 4). At a ratio of GPT-J-6B-Janeway - 20%, PPO_HH_GPT-J - 20%, Pygmalion-6b DEV (V8 / Part 4) - 60%.' </s>\"\n"}
{"text": "<s>[INST]قم بتدريب نموذج التصنيف الجدولي على مجموعة البيانات المعطاة لتصنيف البيانات بدقة عالية، وضع في الاعتبار الأبعاد المتعددة للبيانات وتصحيح الأوزان بشكل صحيح.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary libraries (joblib and pandas).\\\\n2. Load the pre-trained binary classification model ('desertdev/autotrain-imdb-sentiment-analysis-44994113085') using the joblib library.\\\\n3. Read and preprocess the input data (movie reviews in CSV format) using pandas. Select the required features and format the column names accordingly.\\\\n4. Use the loaded model to make predictions on the processed data.\\\\n5. Analyze the predictions to categorize movie reviews as positive or negative, and use this information to make movie recommendations.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\ndata = pd.read_csv('data.csv')\\\\nfeatures = config['features']\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'desertdev/autotrain-imdb-sentiment-analysis-44994113085', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'desertdev/autotrain-data-imdb-sentiment-analysis', 'accuracy': 0.565}, 'description': 'A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.' </s>\"\n"}
{"text": "<s>[INST]تعمل شركتنا على تطوير نموذج لتحويل النصوص إلى إجابات على جداول عن طريق معالجة اللغة الطبيعية ونرغب في تحسين النموذج لتحسين جودة الإجابات المولدة.? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"TapasForQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-sqa')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import necessary libraries and modules, such as 'TapasForQuestionAnswering' from transformers.\\\", 'code': 'from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\nimport pandas as pd\\\\n\\\\nmodel_name = \\\\'google/tapas-mini-finetuned-sqa\\\\'\\\\ntokenizer = TapasTokenizer.from_pretrained(model_name)\\\\nmodel = TapasForQuestionAnswering.from_pretrained(model_name)\\\\n\\\\ndata = {\\\\'date\\\\':[\\\\'2023-01-01\\\\', \\\\'2023-01-02\\\\', \\\\'2023-01-03\\\\',],\\\\n        \\\\'city\\\\':[\\\\'San Francisco\\\\', \\\\'San Francisco\\\\', \\\\'San Francisco\\\\',],\\\\n        \\\\'weather\\\\':[\\\\'Rain\\\\', \\\\'Sun\\\\', \\\\'Rain\\\\',]}\\\\n\\\\ntable = pd.DataFrame(data)\\\\n\\\\nqueries = \\\"Which days will it rain in San Francisco?\\\"\\\\n\\\\ninputs = tokenizer(table=table, queries=queries, padding=\\\\'max_length\\\\', return_tensors=\\\"pt\\\")\\\\noutputs = model(**inputs)\\\\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())\\\\n\\\\nanswers = tokenizer.convert_coordinates_to_text(table=table, token_coordinates=predicted_answer_coordinates[0])\\\\n\\\\nprint(answers)', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-mini-finetuned-sqa', 'api_call': 'from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\nimport pandas as pd\\\\n\\\\nmodel_name = \\\\'google/tapas-mini-finetuned-sqa\\\\'\\\\ntokenizer = TapasTokenizer.from_pretrained(model_name)\\\\nmodel = TapasForQuestionAnswering.from_pretrained(model_name)\\\\n\\\\ndata = {\\\\'date\\\\':[\\\\'2023-01-01\\\\', \\\\'2023-01-02\\\\', \\\\'2023-01-03\\\\',],\\\\n        \\\\'city\\\\':[\\\\'San Francisco\\\\', \\\\'San Francisco\\\\', \\\\'San Francisco\\\\',],\\\\n        \\\\'weather\\\\':[\\\\'Rain\\\\', \\\\'Sun\\\\', \\\\'Rain\\\\',]}\\\\n\\\\ntable = pd.DataFrame(data)\\\\n\\\\nqueries = \\\"Which days will it rain in San Francisco?\\\"\\\\n\\\\ninputs = tokenizer(table=table, queries=queries, padding=\\\\'max_length\\\\', return_tensors=\\\"pt\\\")\\\\noutputs = model(**inputs)\\\\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())\\\\n\\\\nanswers = tokenizer.convert_coordinates_to_text(table=table, token_coordinates=predicted_answer_coordinates[0])\\\\n\\\\nprint(answers)', 'api_arguments': ['model_name', 'table', 'queries'], 'python_environment_requirements': ['transformers'], 'example_code': 'N/A', 'performance': {'dataset': 'msr_sqa', 'accuracy': 0.5148}, 'description': 'TAPAS mini model fine-tuned on Sequential Question Answering (SQA)'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء فيديو من وصف نصي لمشهد يصور الحيوانات في البرية باستخدام تقنيات التصنيف البصري الحاسوبي. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-ssv2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries: VideoMAEFeatureExtractor and VideoMAEForVideoClassification from the transformers package, numpy as np, and torch.\\\\n2. Load the pre-trained video classification model 'videomae-small-finetuned-ssv2' using the from_pretrained method of VideoMAEForVideoClassification class. This model is trained for video classification tasks and can be utilized for classifying different types of workouts in user-submitted videos.\\\\n3. To classify a workout video, first, extract the features of the video using the VideoMAEFeatureExtractor with the from_pretrained method.\\\\n4. Then, create a list of the frames from the video and input them into the feature extractor to prepare them for the model.\\\\n5. Finally, run the model with the extracted features as inputs to generate the logits (unnormalized probabilities) for each class. Choose the class with the highest logits as the predicted class, and this will determine the type of workout that the user is performing.\\\", '<<<code>>>': \\\"from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained('MCG-NJU/videomae-small-finetuned-ssv2')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-ssv2')\\\\ninputs = feature_extractor(video, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(f'Predicted class: {model.config.id2label[predicted_class_idx]}')\\\", 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-ssv2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'videomae-small-finetuned-ssv2', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-ssv2')\\\", 'api_arguments': {'model_name': 'MCG-NJU/videomae-small-finetuned-ssv2'}, 'python_environment_requirements': [{'transformers': 'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification', 'numpy': 'import numpy as np', 'torch': 'import torch'}], 'example_code': 'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'Something-Something V2', 'accuracy': {'top-1': 66.8, 'top-5': 90.3}}, 'description': 'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.' </s>\"\n"}
{"text": "<s>[INST]يجب على الباحثين في مجال معالجة اللغة الطبيعية والإجابة على الأسئلة تقديم تقدير دقيق لخصائص النصوص الملتقطة لتحسين أداء نظام الإجابة الآلي على الأسئلة.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"pipeline(task='question-answering', model='thatdramebaazguy/roberta-base-squad')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. To create a program to answer questions from a legal document, first import the necessary libraries, which are 'pipeline' from transformers.\\\", 'code': 'from transformers import pipeline\\\\nqa_pipeline = pipeline(task=\\\\'question-answering\\\\', model=\\\\'thatdramebaazguy/roberta-base-squad\\\\'); legal_document = \\\\'Content of the legal document...\\\\'\\\\nquestions = [\\\\'Question 1...\\\\', \\\\'Question 2...\\\\', \\\\'Question 3...\\\\']\\\\n# Replace with your questions\\\\nfor question in questions:\\\\n    context = {\\\\'context\\\\': legal_document, \\\\'question\\\\': question}\\\\n    answer = qa_pipeline(context)\\\\n    print(f\\\"{question}: {answer[\\\\'answer\\\\']}\\\")', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Question Answering', 'api_name': 'thatdramebaazguy/roberta-base-squad', 'api_call': 'from transformers import pipeline\\\\nqa_pipeline = pipeline(task=\\\\'question-answering\\\\', model=\\\\'thatdramebaazguy/roberta-base-squad\\\\'); legal_document = \\\\'Content of the legal document...\\\\'\\\\nquestions = [\\\\'Question 1...\\\\', \\\\'Question 2...\\\\', \\\\'Question 3...\\\\']\\\\n# Replace with your questions\\\\nfor question in questions:\\\\n    context = {\\\\'context\\\\': legal_document, \\\\'question\\\\': question}\\\\n    answer = qa_pipeline(context)\\\\n    print(f\\\"{question}: {answer[\\\\'answer\\\\']}\\\")', 'api_arguments': {'model_name': 'thatdramebaazguy/roberta-base-squad', 'tokenizer': 'thatdramebaazguy/roberta-base-squad', 'revision': 'v1.0', 'task': 'question-answering'}, 'python_environment_requirements': {'huggingface': {'transformers': 'latest'}}, 'example_code': \\\"model_name = 'thatdramebaazguy/roberta-base-squad'\\\\nqa_pipeline = pipeline(model=model_name, tokenizer=model_name, revision='v1.0', task='question-answering')\\\", 'performance': {'dataset': [{'name': 'SQuADv1', 'accuracy': {'exact_match': 83.6045, 'f1': 91.1709}}, {'name': 'MoviesQA', 'accuracy': {'exact_match': 51.6494, 'f1': 68.2615}}]}, 'description': 'This is Roberta Base trained to do the SQuAD Task. This makes a QA model capable of answering questions.'}} </s>\"\n"}
{"text": "<s>[INST]يقوم فريق البحث بتطوير نموذج تصنيف لأسئلة الجداول في مجال تحليل واستجابة اللغة الطبيعية، ونحتاج إلى تدريب النموذج على تصنيف البيانات الجدولية بدقة عالية.? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-sqa')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\n\\\\ntable_qa_pipeline = pipeline('table-question-answering', model='google/tapas-medium-finetuned-sqa')\\\\n# replace 'table' with revenue data in the required format\\\\nresult = table_qa_pipeline(table=table, query='What is the total revenue?')\\\", 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Transformers', 'functionality': 'Table Question Answering', 'api_name': 'google/tapas-medium-finetuned-sqa', 'api_call': \\\"from transformers import pipeline\\\\n\\\\ntable_qa_pipeline = pipeline('table-question-answering', model='google/tapas-medium-finetuned-sqa')\\\\n# replace 'table' with revenue data in the required format\\\\nresult = table_qa_pipeline(table=table, query='What is the total revenue?')\\\", 'api_arguments': 'table, query', 'python_environment_requirements': 'transformers', 'example_code': \\\"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntable_qa_pipeline = pipeline('table-question-answering', model='google/tapas-medium-finetuned-sqa')\\\\n\\\\n# Provide the table and query\\\\nresult = table_qa_pipeline(table=table, query='What is the total revenue?')\\\", 'performance': {'dataset': 'msr_sqa', 'accuracy': 0.6561}, 'description': 'TAPAS medium model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia and uses relative position embeddings. It can be used for answering questions related to a table in a conversational set-up.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتطوير برنامج للجيل العشوائي للصور في مجال رؤية الحاسوب. يجب على البرنامج أن يكون قادراً على إنشاء صور متنوعة بشكل غير مشروط. ينبغي أن يستخدم هذا البرنامج في تطبيقات التعرف على الصور في مجالات مختلفة.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Install the 'diffusers' library, which contains the necessary tools for working with diffusion-based generative models.\\\", 'code': \\\"!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = 'google/ncsnpp-celebahq-256'\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage_sample = sde_ve()\\\\nimage_sample[0].save('sde_ve_generated_image.png')\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ncsnpp-celebahq-256', 'api_call': \\\"!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = 'google/ncsnpp-celebahq-256'\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage_sample = sde_ve()\\\\nimage_sample[0].save('sde_ve_generated_image.png')\\\", 'api_arguments': {'model_id': 'google/ncsnpp-celebahq-256'}, 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = google/ncsnpp-celebahq-256\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[sample]\\\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.'}} </s>\"\n"}
{"text": "<s>[INST]أنشئ جدول تبويبي يحتوي على بيانات تصنيفية للعناصر المختلفة.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': '1. Import the necessary libraries such as json, joblib, and pandas.\\\\n2. Load the pre-trained model using joblib.load() with the path of the saved model.\\\\n3. Read the configuration file to get the features needed for the model input.\\\\n4. Using pandas, load the dataset and filter out the required features from the data.\\\\n5. Use the trained model to predict whether companies have high carbon emission or not, based on the input features.', '<<<code>>>': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'jwan2021/autotrain-jwan-autotrain1-1768961489', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['json', 'joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'Validation Metrics', 'accuracy': 0.983}, 'description': 'Binary Classification model for Carbon Emissions prediction' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج تصنيف بدون مسبق تدريب لتحليل النصوص باستخدام تقنيات معالجة اللغات الطبيعية من النص الوصفي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في تنفيذ ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-classification', model='MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a zero-shot classification model.\\\\n3. Specify the model 'MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli' to be loaded. This model is trained for the task of natural language inference, which we can leverage for zero-shot classification.\\\\n4. The created classifier can be used to analyze a given news article and classify it into one or multiple categories.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nnews_classifier = pipeline('zero-shot-classification', model='MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli')\\\\nnews_article = 'The news article content goes here...'\\\\ncategories = ['politics', 'economy', 'entertainment', 'environment', 'technology', 'sports']\\\\nresult = news_classifier(news_article, categories, multi_label=True)\\\", 'api_call': \\\"pipeline('zero-shot-classification', model='MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli', 'api_call': \\\"pipeline('zero-shot-classification', model='MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli')\\\", 'api_arguments': ['sequence_to_classify', 'candidate_labels', 'multi_label'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli)\\\\nsequence_to_classify = Angela Merkel is a politician in Germany and leader of the CDU\\\\ncandidate_labels = [politics, economy, entertainment, environment]\\\\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\\\\nprint(output)', 'performance': {'dataset': [{'name': 'mnli_test_m', 'accuracy': 0.912}, {'name': 'mnli_test_mm', 'accuracy': 0.908}, {'name': 'anli_test', 'accuracy': 0.702}, {'name': 'anli_test_r3', 'accuracy': 0.64}, {'name': 'ling_test', 'accuracy': 0.87}, {'name': 'wanli_test', 'accuracy': 0.77}]}, 'description': 'This model was fine-tuned on the MultiNLI, Fever-NLI, Adversarial-NLI (ANLI), LingNLI and WANLI datasets, which comprise 885 242 NLI hypothesis-premise pairs. This model is the best performing NLI model on the Hugging Face Hub as of 06.06.22 and can be used for zero-shot classification. It significantly outperforms all other large models on the ANLI benchmark.' </s>\"\n"}
{"text": "<s>[INST]تلتزم شركتنا بتطوير نموذج للإجابة على الأسئلة المرئية متعددة الوسائط، حيث سيتم دمج تحليل الصور والنصوص لتقديم إجابات شاملة ودقيقة. نسعى إلى استخدام تقنيات التعلم الآلي لتحقيق هذه الغاية وتحقيق الاستدلال البصري واللغوي المتقدم للنموذج.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. We import necessary classes from the transformers package, plus additional packages to handle images and internet requests.\\\\n2. We use the from_pretrained method of the ViltForQuestionAnswering class to load the pre-trained model \\\\'dandelin/vilt-b32-finetuned-vqa\\\\'. This model is trained for visual question answering tasks and will help answer users\\\\' questions based on images.\\\\n3. We collect a live image from the smart home\\\\'s camera, in this example, an image from the internet is used.\\\\n4. The VILT model is then used to analyze the given image along with a user\\\\'s text-based question (such as \\\"What color is the car?\\\").\\\\n5. The model processes the input and provides an answer based on its understanding of both the image and the question.', '<<<code>>>': 'from transformers import ViltProcessor, ViltForQuestionAnswering\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = \\\\'http://images.cocodataset.org/val2017/000000039769.jpg\\\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntext = \\\"What color is the car?\\\"\\\\n\\\\nprocessor = ViltProcessor.from_pretrained(\\\\'dandelin/vilt-b32-finetuned-vqa\\\\')\\\\nmodel = ViltForQuestionAnswering.from_pretrained(\\\\'dandelin/vilt-b32-finetuned-vqa\\\\')\\\\nencoding = processor(image, text, return_tensors=\\\\'pt\\\\')\\\\noutputs = model(**encoding)\\\\nlogits = outputs.logits', 'api_call': \\\"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dandelin/vilt-b32-finetuned-vqa', 'api_call': \\\"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\\\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'text': 'How many cats are there?'}, 'python_environment_requirements': [{'transformers': 'ViltProcessor, ViltForQuestionAnswering', 'requests': 'requests', 'PIL': 'Image'}], 'example_code': 'from transformers import ViltProcessor, ViltForQuestionAnswering\\\\nimport requests\\\\nfrom PIL import Image\\\\n\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntext = How many cats are there?\\\\nprocessor = ViltProcessor.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\\\nmodel = ViltForQuestionAnswering.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\\\n\\\\nencoding = processor(image, text, return_tensors=pt)\\\\noutputs = model(**encoding)\\\\nlogits = outputs.logits\\\\nidx = logits.argmax(-1).item()\\\\nprint(Predicted answer:, model.config.id2label[idx])', 'performance': {'dataset': 'VQAv2', 'accuracy': 'to do'}, 'description': 'Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير نموذج لتقسيم الصور في مجال رؤية الحاسوب. قم بتدريب النموذج على مجموعة بيانات متنوعة لتحديد الحدود والأشكال في الصور. ابدأ بتقسيم صورة مرجعية إلى عدة أقسام باستخدام خوارزمية تعلم الآلة، ثم قم بتحسين النتائج بتغيير المعلمات وتقنيات المعالجة. استخدم أدوات الرؤية الحاسوبية المتقدمة لضمان دقة عالية وكفاءة في عملية تقسيم الصور.? [/INST]{'domain': 'Computer Vision Image Segmentation', 'api_call': \\\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We start by importing the necessary libraries, which include requests, torch, PIL, and transformers.', 'code': \\\"from PIL import Image\\\\nimport requests\\\\nimport torch\\\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\\n\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n\\\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result['segmentation']\\\", 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-coco-panoptic', 'api_call': \\\"from PIL import Image\\\\nimport requests\\\\nimport torch\\\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\\\n\\\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\\\\n\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\ninputs = processor(images=image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n\\\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result['segmentation']\\\", 'api_arguments': ['image'], 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': 'processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result[segmentation]', 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.'}} </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج لتوليد نصوص من نوع إلى نوع في مجال معالجة اللغة الطبيعية والجيل النصي.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\\\nsrc_text = \\\"This is an English text.\\\"\\\\nsrc_lang = \\\"en\\\"\\\\ntgt_lang = \\\"fr\\\"\\\\nmodel = M2M100ForConditionalGeneration.from_pretrained(\\\\'facebook/m2m100_1.2B\\\\')\\\\ntokenizer = M2M100Tokenizer.from_pretrained(\\\\'facebook/m2m100_1.2B\\\\')\\\\ntokenizer.src_lang = src_lang\\\\nencoded_input = tokenizer(src_text, return_tensors=\\\"pt\\\")\\\\ngenerated_tokens = model.generate(**encoded_input, forced_bos_token_id=tokenizer.get_lang_id(tgt_lang))\\\\ntranslated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Many-to-Many multilingual translation', 'api_name': 'facebook/m2m100_1.2B', 'api_call': 'from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\\\nsrc_text = \\\"This is an English text.\\\"\\\\nsrc_lang = \\\"en\\\"\\\\ntgt_lang = \\\"fr\\\"\\\\nmodel = M2M100ForConditionalGeneration.from_pretrained(\\\\'facebook/m2m100_1.2B\\\\')\\\\ntokenizer = M2M100Tokenizer.from_pretrained(\\\\'facebook/m2m100_1.2B\\\\')\\\\ntokenizer.src_lang = src_lang\\\\nencoded_input = tokenizer(src_text, return_tensors=\\\"pt\\\")\\\\ngenerated_tokens = model.generate(**encoded_input, forced_bos_token_id=tokenizer.get_lang_id(tgt_lang))\\\\ntranslated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]', 'api_arguments': {'encoded_input': 'encoded text', 'forced_bos_token_id': 'target language id'}, 'python_environment_requirements': 'sentencepiece', 'example_code': 'from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\\\nhi_text = chinese_text = model = M2M100ForConditionalGeneration.from_pretrained(facebook/m2m100_1.2B)\\\\ntokenizer = M2M100Tokenizer.from_pretrained(facebook/m2m100_1.2B)\\\\ntokenizer.src_lang = hi\\\\nencoded_hi = tokenizer(hi_text, return_tensors=pt)\\\\ngenerated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(fr))\\\\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)', 'performance': {'dataset': 'M2M100', 'accuracy': 'Not specified'}, 'description': 'M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token.'}} </s>\"\n"}
{"text": "<s>[INST]نحتاج في شركتنا المتخصصة في مجال معالجة اللغة الطبيعية وتصنيف الصفر إلى تصميم نماذج لتصنيف النصوص بدون تدريب مسبق، ونحتاج إلى تقدير عمق المحتوى والمفهوم في النصوص التي سيتم معالجتها.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"pipeline('token-classification', model='kredor/punctuate-all')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We need to import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. The pipeline function is then used to create a token classification model for punctuation prediction by specifying the 'kredor/punctuate-all' model.\\\\n3. The model is trained on 12 languages, including French, so it will work for the French text input.\\\\n4. We can pass the French conversation transcript text as input, and the model will return an updated version of the text with appropriate punctuation marks inserted.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\n\\\\npunctuator = pipeline('token-classification', model='kredor/punctuate-all')\\\\npunctuated_text = punctuator(french_conversation_transcript)\\\", 'api_call': \\\"pipeline('token-classification', model='kredor/punctuate-all')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kredor/punctuate-all', 'api_call': \\\"pipeline('token-classification', model='kredor/punctuate-all')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'multilingual', 'accuracy': 0.98}, 'description': 'A finetuned xlm-roberta-base model for punctuation prediction on twelve languages: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian.' </s>\"\n"}
{"text": "<s>[INST]لتحسين تمييز الجمل في مجال معالجة اللغة الطبيعية لتحسين الشبه بين الجمل، يجب عليك البدء بدورات تعليم المفاهيم الأساسية والأدوات المستخدمة. استخدم الخوارزميات المخصصة لمعالجة اللغة الطبيعية مثل Word2Vec وBERT لتحقيق النتائج المرجوة.? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = ['This is an example sentence', 'Each sentence is converted']\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Sentence Transformers', 'api_name': 'sentence-transformers/paraphrase-distilroberta-base-v2', 'api_call': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = ['This is an example sentence', 'Each sentence is converted']\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج تحويل صور من الصور إلى الصور باستخدام تقنية رؤية الحاسوب.? [/INST]{'domain': 'Multimodal Document Question Answer', 'api_call': \\\"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa', return_dict=True))\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Install required packages like PIL, pytesseract, PyTorch and transformers.', 'code': 'from transformers import pipeline, LayoutLMForQuestionAnswering\\\\nimport pytesseract\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\n# Get the image\\\\nimage_url = \\\"https://example.com/invoice_image.png\\\"\\\\nresponse = requests.get(image_url)\\\\nimg = Image.open(BytesIO(response.content))\\\\n\\\\n# Prepare query\\\\nquestion = \\\"Which supplier has sent this invoice?\\\"\\\\n\\\\n# Initialize the pipeline\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\\\'impira/layoutlm-document-qa\\\\', return_dict=True))\\\\n\\\\n# Get the answer\\\\nresult = nlp(img, question)', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': 'from transformers import pipeline, LayoutLMForQuestionAnswering\\\\nimport pytesseract\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\n# Get the image\\\\nimage_url = \\\"https://example.com/invoice_image.png\\\"\\\\nresponse = requests.get(image_url)\\\\nimg = Image.open(BytesIO(response.content))\\\\n\\\\n# Prepare query\\\\nquestion = \\\"Which supplier has sent this invoice?\\\"\\\\n\\\\n# Initialize the pipeline\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\\\'impira/layoutlm-document-qa\\\\', return_dict=True))\\\\n\\\\n# Get the answer\\\\nresult = nlp(img, question)', 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': ['SQuAD2.0', 'DocVQA'], 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.'}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لتلخيص النصوص باستخدام تقنيات معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required libraries: PegasusTokenizer, PegasusForConditionalGeneration from the transformers library provided by Hugging Face.\\\\n2. Use the model_name: 'human-centered-summarization/financial-summarization-pegasus'.\\\\n3. Load the tokenizer and the model using the PegasusTokenizer.from_pretrained() and PegasusForConditionalGeneration.from_pretrained() methods, respectively, with the provided model_name.\\\\n4. To use the model for summarization, first tokenize the input text (financial news) using the tokenizer. This will create the input_ids which are needed for the model.\\\\n5. Use the model.generate() method to obtain the summarized text by specifying input_ids, max_length, num_beams, and early_stopping parameters.\\\\n6. Decode the output using the tokenizer.decode() method, which provides the final summarized text.\\\", '<<<code>>>': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration\\\\nmodel_name = \\\\'human-centered-summarization/financial-summarization-pegasus\\\\'\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = \\\"<your_financial_news>\\\"\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=\\\\'pt\\\\').input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nsummary = tokenizer.decode(output[0], skip_special_tokens=True)', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'financial-summarization-pegasus', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabia’s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba’s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region’s third-largest lender. The entity’s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East’s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))', 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 35.206, 'ROUGE-2': 16.569, 'ROUGE-L': 30.128, 'ROUGE-LSUM': 30.171}}, 'description': 'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth', torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': '1. Import the required libraries and pipeline class, i.e., StableDiffusionDepth2ImgPipeline.\\\\n2. Instantiate the pipeline with the pre-trained model \\\\'stabilityai/stable-diffusion-2-depth\\\\'.\\\\n3. Use the pipeline object to generate an image based on the given prompt, \\\"a calm and peaceful beach with palm trees and white sand, no people or animals.\\\"\\\\n4. Provide negative prompts like \\\"people,\\\" \\\"animals,\\\" \\\"crowded,\\\" etc., to avoid generating images with unwanted elements.\\\\n5. You can adjust the \\\\'strength\\\\' parameter to influence the strength of the prompt in the final image.', '<<<code>>>': 'from diffusers import StableDiffusionDepth2ImgPipeline\\\\nimport torch\\\\n\\\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\\\'stabilityai/stable-diffusion-2-depth\\\\', torch_dtype=torch.float16).to(cuda)\\\\ngenerated_image = pipe(prompt=\\\"a calm and peaceful beach with palm trees and white sand, no people or animals\\\",\\\\n                      negative_prompt=\\\"people, animals, crowded\\\", strength=0.8).images[0]', 'api_call': \\\"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth', torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \\\"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth', torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': [['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors']], 'example_code': 'import torch\\\\nimport requests\\\\nfrom PIL import Image\\\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\\\n\\\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\\\n stabilityai/stable-diffusion-2-depth,\\\\n torch_dtype=torch.float16,\\\\n).to(cuda)\\\\n\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\\\nprompt = two tigers\\\\nn_propmt = bad, deformed, ugly, bad anotomy\\\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد نماذج تصنيف النصوص بمساعدة معالجة اللغة الطبيعية للتصنيف الصفري. هل يمكنك مساعدتي في ذلك؟? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"pipeline('zero-shot-classification', model='MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\", 'code': \\\"from transformers import pipeline\\\\ntext = 'Putin faces pressures on food prices and gas shortages.'\\\\nclassifier = pipeline('zero-shot-classification', model='MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli')\\\\ncandidate_labels = ['politics', 'economy', 'energy', 'crisis']\\\\nresult = classifier(text, candidate_labels)\\\\nprint(result)\\\", 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Classification', 'api_name': 'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli', 'api_call': \\\"from transformers import pipeline\\\\ntext = 'Putin faces pressures on food prices and gas shortages.'\\\\nclassifier = pipeline('zero-shot-classification', model='MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli')\\\\ncandidate_labels = ['politics', 'economy', 'energy', 'crisis']\\\\nresult = classifier(text, candidate_labels)\\\\nprint(result)\\\", 'api_arguments': {'model': 'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli', 'sequence_to_classify': 'Angela Merkel is a politician in Germany and leader of the CDU', 'candidate_labels': ['politics', 'economy', 'entertainment', 'environment'], 'multi_label': 'False'}, 'python_environment_requirements': {'transformers': '>=4.13'}, 'example_code': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli)\\\\nsequence_to_classify = Angela Merkel is a politician in Germany and leader of the CDU\\\\ncandidate_labels = [politics, economy, entertainment, environment]\\\\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\\\\nprint(output)', 'performance': {'dataset': {'mnli-m': 0.903, 'mnli-mm': 0.903, 'fever-nli': 0.777, 'anli-all': 0.579, 'anli-r3': 0.495}, 'accuracy': {'mnli-m': 0.903, 'mnli-mm': 0.903, 'fever-nli': 0.777, 'anli-all': 0.579, 'anli-r3': 0.495}}, 'description': 'This model was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs. This base model outperforms almost all large models on the ANLI benchmark. The base model is DeBERTa-v3-base from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective, see annex 11 of the original DeBERTa paper.'}} </s>\"\n"}
{"text": "<s>[INST]لإنشاء نص مولد معالجة اللغات الطبيعية Text2Text Generation، قم بتجهيز بيانات التدريب اللازمة التي تتضمن الأزواج النصية (النص الأصلي والنص المولد). ثم، قم بتحديد وتدريب نموذج توليد النصوص الذي يستند إلى تعلم الآلة بالسياق. كما يُوصى بضبط معلمات التدريب والتحقق باستخدام مقاييس الأداء المناسبة مثل BLEU و ROUGE. في نهاية التجربة، يمكنك تقييم جودة النتائج المولدة وضبط النموذج إذا لزم الأمر لتحقيق أداء أفضل.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import T5Tokenizer and T5ForConditionalGeneration from the transformers library.\\\\n2. Instantiate the tokenizer and model by loading the \\\\'google/flan-t5-large\\\\' pretrained model.\\\\n3. Use the tokenizer to convert the input text into input_ids. The input text should include the translation instruction: \\\"translate English to French: Hello, how are you?\\\"\\\\n4. Feed the input_ids to the model\\\\'s generate() function to obtain the translated text.\\\\n5. Decode the output to get the translated text in French.', '<<<code>>>': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'google/flan-t5-large\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'google/flan-t5-large\\\\')\\\\ninput_text = \\\"translate English to French: Hello, how are you?\\\"\\\\ninput_ids = tokenizer(input_text, return_tensors=\\\"pt\\\").input_ids\\\\noutputs = model.generate(input_ids)\\\\ntranslated_text = tokenizer.decode(outputs[0])', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/flan-t5-large', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-large')\\\", 'api_arguments': ['input_text', 'input_ids', 'outputs'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-large)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-large)\\\\ninput_text = translate English to German: How old are you?\\\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))', 'performance': {'dataset': [{'name': 'MMLU', 'accuracy': '75.2%'}]}, 'description': 'FLAN-T5 large is a language model fine-tuned on over 1000 tasks and multiple languages. It achieves state-of-the-art performance on several benchmarks, including 75.2% on five-shot MMLU. The model is based on pretrained T5 and fine-tuned with instructions for better zero-shot and few-shot performance. It can be used for research on language models, zero-shot NLP tasks, in-context few-shot learning NLP tasks, reasoning, question answering, and advancing fairness and safety research.' </s>\"\n"}
{"text": "<s>[INST]قم بتحديد النص والصورة الواردة ودمجهما لتطوير نموذج متعدد الوسائط لتحويل النص إلى صورة بمجال تصنيف الصور البصرية للحاسوب.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, such as urllib.request for fetching the image URL, PIL to open the image, and timm for the image classification model.\\\\n2. Assign the URL of the image to a variable and fetch the image using the urlopen method from urllib.request. Then, create an Image object using Image.open().\\\\n3. Load the pre-trained ConvNeXt-V2 image classification model through timm.create_model() with 'convnextv2_huge.fcmae_ft_in1k' as the model name and pretrained=True as an argument.\\\\n4. Set the model to evaluation mode, create proper transforms for the image using timm.data, and apply them to the input image after unsqueezing the tensor.\\\\n5. Calculate the top-5 probabilities and the class indices using torch.topk.\\\", '<<<code>>>': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimport torch\\\\nimg_url = 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\\\\nimg = Image.open(urlopen(img_url))\\\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))\\\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\\\", 'api_call': \\\"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'convnextv2_huge.fcmae_ft_in1k', 'api_call': \\\"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': [['timm']], 'example_code': \\\"from urllib.request import urlopen\\\\nfrom PIL import Image\\\\nimport timm\\\\nimg = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\\\nmodel = model.eval()\\\\ndata_config = timm.data.resolve_model_data_config(model)\\\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\\\noutput = model(transforms(img).unsqueeze(0))\\\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\\\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 86.256}, 'description': 'A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى تطوير نظام يستخدم معالجة اللغة الطبيعية لتوليد نصوص بطريقة تبدو وكأنها منشأة بشكل طبيعي. يجب أن يكون النظام قادرًا على توليد نصوص دقيقة ومعبرة تعتمد على تحليل عميق للنصوص والوسائط المتعددة.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': 'pipeline(\\\"text-generation\\\", model=\\\\'Salesforce/codegen-2B-multi\\\\')', 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the 'pipeline' class from the transformers package.\\\", 'code': 'from transformers import pipeline\\\\ngenerator = pipeline(\\\"text-generation\\\", model=\\\\'Salesforce/codegen-2B-multi\\\\')\\\\ninstruction = \\\"Create a Python function that calculates an employee\\\\'s salary based on their hourly rate and hours worked.\\\"\\\\ngenerated_code = generator(instruction, max_length=100)\\\\nprint(generated_code[0][\\\\'generated_text\\\\'])', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Program Synthesis', 'api_name': 'Salesforce/codegen-2B-multi', 'api_call': 'from transformers import pipeline\\\\ngenerator = pipeline(\\\"text-generation\\\", model=\\\\'Salesforce/codegen-2B-multi\\\\')\\\\ninstruction = \\\"Create a Python function that calculates an employee\\\\'s salary based on their hourly rate and hours worked.\\\"\\\\ngenerated_code = generator(instruction, max_length=100)\\\\nprint(generated_code[0][\\\\'generated_text\\\\'])', 'api_arguments': {'input_ids': 'input_ids', 'max_length': 128}, 'python_environment_requirements': {'transformers': 'AutoTokenizer, AutoModelForCausalLM'}, 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-2B-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-2B-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval, MTPB'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B). The checkpoint included in this repository is denoted as CodeGen-Multi 2B, where Multi means the model is initialized with CodeGen-NL 2B and further pre-trained on a dataset of multiple programming languages, and 2B refers to the number of trainable parameters.'}} </s>\"\n"}
{"text": "<s>[INST]أنشئ تعليمة صوتية لتصنيف المعلومات الجدولية تبدأ بجملة \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. Import joblib and pandas library to load the trained model and process data.\\\\n2. Load the pre-trained model 'abhishek/autotrain-iris-xgboost' saved as 'model.joblib'. This model is trained on the Iris dataset for multi-class classification, which can also be applied to classify different plant species in the park.\\\\n3. Load your plant data into a pandas DataFrame and preprocess it according to the requirements specified in the trained model's config file.\\\\n4. Once the data has been prepared, use the 'predict()' method of the loaded model to classify the plant species in the park. The output categories can then be used to schedule maintenance and development for different areas.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\n# Replace 'your_plant_data.csv' with the path to the file containing plant data for the park\\\\ndata = pd.read_csv('your_plant_data.csv')\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'abhishek/autotrain-iris-xgboost', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'scikit-learn/iris', 'accuracy': 0.8666666666666667}, 'description': 'A tabular classification model trained on the Iris dataset using XGBoost and AutoTrain. The model is capable of multi-class classification and has an accuracy of 86.67%.' </s>\"\n"}
{"text": "<s>[INST]قم بتشغيل نشاط الكشف عن الصوت في الصوت الذي تم توفيره.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. We first import the necessary modules from the transformers library, including AutoModelForCausalLM, AutoTokenizer, and set_seed.\\\\n2. We instantiate the pretrained text generation model \\\\'facebook/opt-66b\\\\' for the task.\\\\n3. We create a tokenizer to process the input text and the generated text.\\\\n4. We give the model a starting prompt like \\\"Create a fitness slogan: \\\". The model will then generate possible slogans based on the given prompt.\\\\n5. We can control the generated text using parameters like \\\\'do_sample\\\\', \\\\'num_return_sequences\\\\', and \\\\'max_length\\\\' to get suitable slogans.', '<<<code>>>': 'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\n\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'facebook/opt-66b\\\\', torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'facebook/opt-66b\\\\', use_fast=False)\\\\n\\\\nprompt = \\\"Create a fitness slogan: \\\"\\\\ninput_ids = tokenizer(prompt, return_tensors=\\\\'pt\\\\').input_ids.cuda()\\\\n\\\\nset_seed(32)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\\\\nslogans = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/opt-66b', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", 'api_arguments': ['input_ids', 'do_sample', 'num_return_sequences', 'max_length'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-66b, torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-66b, use_fast=False)\\\\nprompt = Hello, I am conscious and\\\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\\\nset_seed(32)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\\\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)', 'performance': {'dataset': 'GPT-3', 'accuracy': 'roughly matched'}, 'description': 'OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. OPT models are trained to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى برنامج في مجال معالجة اللغة الطبيعية يقوم بتلخيص النصوص. الهدف هو تطوير تطبيق يقوم بتحويل النص إلى ملخص قصير ومفيد لمستخدميه.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"PegasusForConditionalGeneration.from_pretrained('google/pegasus-cnn_dailymail')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary classes from the transformers package. This includes PegasusForConditionalGeneration and PegasusTokenizer.\\\\n2. We then use the from_pretrained method of the PegasusForConditionalGeneration class to load the pre-trained model 'google/pegasus-cnn_dailymail'. This model has been trained for abstractive summarization tasks, which is exactly what we need for summarizing key product features and benefits.\\\\n3. To use the model, we first tokenize the input text using the PegasusTokenizer. Next, we pass the tokenized input to the model which generates a summary.\\\\n4. The summary is then decoded back into human-readable text and can be used in press releases.\\\", '<<<code>>>': \\\"from transformers import PegasusForConditionalGeneration, PegasusTokenizer\\\\nmodel_name = 'google/pegasus-cnn_dailymail'\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ninputs = tokenizer.encode('Our latest product...', return_tensors='pt')\\\\n# replace 'Our latest product...' with the description of the product's features and benefits\\\\nsummary_ids = model.generate(inputs)\\\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\\\", 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('google/pegasus-cnn_dailymail')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/pegasus-cnn_dailymail', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('google/pegasus-cnn_dailymail')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import PegasusForConditionalGeneration, PegasusTokenizer\\\\nmodel_name = 'google/pegasus-cnn_dailymail'\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ninputs = tokenizer.encode('input_text', return_tensors='pt')\\\\nsummary_ids = model.generate(inputs)\\\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\\\", 'performance': {'dataset': 'cnn_dailymail', 'accuracy': '44.16/21.56/41.30'}, 'description': 'PEGASUS model for abstractive summarization, pretrained on the CNN/DailyMail dataset.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بفهم مدى تشابه الجمل بين النصوص باستخدام معالجة اللغة الطبيعية. يرجى تحليل درجة التشابه بين جملتين مختلفتين وإعطاء نتائجك. هل يمكنك مساعدتي في هذا المجال؟? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, install the 'sentence-transformers' library using pip.\\\\n2. Import 'SentenceTransformer' from the 'sentence_transformers' library.\\\\n3. Instantiate the SentenceTransformer with  pre-trained model 'sentence-transformers/distiluse-base-multilingual-cased-v2'.\\\\n4. Now, you can pass a list of sentences you want to compare as input to the 'encode' method of the SentenceTransformer. This method will return sentence embeddings in a high-dimensional space.\\\\n5. To measure similarity between sentences, you can calculate the cosine similarity between the embeddings, where a higher value indicates more similar sentences, and a lower value indicates less similar sentences.\\\\n6. Using this model, you can quickly identify similarities between the sentences you write and minimize repetition.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nsentences = [\\\"This is an example sentence.\\\", \\\"Each sentence has similarities.\\\"]\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/distiluse-base-multilingual-cased-v2\\\\')\\\\nembeddings = model.encode(sentences)', 'api_call': \\\"SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distiluse-base-multilingual-cased-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]لتصنيف البيانات الجدولية، استخدم خوارزميات تصنيف الجداول لتمييز الفئات المختلفة بناءً على السمات المختلفة. يجب تزويد النموذج بمجموعة من البيانات الجدولية المصنفة بشكل صحيح لتدريبه واختباره.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"AutoModel.from_pretrained('abhishek/autotrain-adult-census-xgboost')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. Import the necessary libraries (joblib, json, and pandas) to load the model and handle the dataset.\\\\n2. Load the pre-trained model 'abhishek/autotrain-adult-census-xgboost' using joblib.\\\\n3. Load the dataset in a pandas DataFrame by reading the CSV file.\\\\n4. Select and preprocess the necessary features according to the model's configuration file.\\\\n5. Use the pre-trained model to predict income levels ('above $50,000' or 'below $50,000') for each client in the dataset.\\\", '<<<code>>>': \\\"import joblib\\\\nimport json\\\\nimport pandas as pd\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('client_data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\n\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"AutoModel.from_pretrained('abhishek/autotrain-adult-census-xgboost')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'abhishek/autotrain-adult-census-xgboost', 'api_call': \\\"AutoModel.from_pretrained('abhishek/autotrain-adult-census-xgboost')\\\", 'api_arguments': ['data'], 'python_environment_requirements': [['json', 'joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'scikit-learn/adult-census-income', 'accuracy': 0.8750191923844618}, 'description': \\\"A binary classification model trained on the Adult Census Income dataset using the XGBoost algorithm. The model predicts whether an individual's income is above or below $50,000 per year.\\\" </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لإنشاء صورة غير مشروطة في مجال التحليل البصري، ونطمح لاستخدام التقنيات الحديثة في تحقيق توليد صور ذات جودة عالية واقعية دون الحاجة لشروط محددة.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DiffusionPipeline.from_pretrained('google/ncsnpp-church-256')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Firstly, we need to install the 'diffusers' library, which contains the required classes for using the 'google/ncsnpp-church-256' model.\\\", 'code': \\\"!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = 'google/ncsnpp-church-256'\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage_sample = sde_ve()[0]\\\\nimage_sample.save('generated_church_image.png')\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ncsnpp-church-256', 'api_call': \\\"!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = 'google/ncsnpp-church-256'\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage_sample = sde_ve()[0]\\\\nimage_sample.save('generated_church_image.png')\\\", 'api_arguments': 'model_id', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = google/ncsnpp-church-256\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[sample]\\\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and can generate high fidelity images of size 1024 x 1024.'}} </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نظام لكشف الكائنات في الصور باستخدام تقنيات الرؤية الحاسوبية، يُرجى تقديم تقدير لعمق الأشياء المعترف بها بواسطة النظام.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8m-forklift-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the required classes and functions from the ultralyticsplus package, which includes YOLO and render_result.\\\\n2. Use the YOLO() function to load the 'keremberke/yolov8m-forklift-detection' model trained for detecting forklifts and people in images.\\\\n3. Set the appropriate confidence level, Intersection over Union (IoU) threshold, and other necessary overrides for the model.\\\\n4. Process the desired input image for detecting forklifts and people using the predict() function of the instantiated model.\\\\n5. The results returned will contain boxes around detected objects, which can be printed.\\\\n6. Using the render_result function, you can visualize the detected objects in the image.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-forklift-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path_to_image.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8m-forklift-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8m-forklift-detection', 'api_call': \\\"YOLO('keremberke/yolov8m-forklift-detection')\\\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8m-forklift-detection')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'forklift-object-detection', 'accuracy': 0.846}, 'description': 'A YOLOv8 model for detecting forklifts and persons in images.' </s>\"\n"}
{"text": "<s>[INST]عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى برنامج يقوم بتحويل النص إلى كلام لمستخدمي هذا التطبيق في مجال معالجة اللغة الطبيعية وتصنيفها بدون تصنيف مسبق.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, including `AutoTokenizer` and `AutoModelForSequenceClassification` from `transformers` as well as `torch`.\\\\n2. Specify the model name, which is `'MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary'` and load its tokenizer and model.\\\\n3. Input your two sentences (premise and hypothesis) in separate variables.\\\\n4. Tokenize these sentences using the tokenizer's `__call__` method, and make sure to set `truncation=True` and `return_tensors='pt'`.\\\\n5. Perform a forward pass through the model by passing the tokenizer's output as input.\\\\n6. Softmax the logits output to transform them into probabilities.\\\\n7. Associate probabilities with their respective labels `entailment` and `not_entailment`.\\\\n8. Determine whether the two sentences contradict information or not using their respective probabilities.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nimport torch\\\\n\\\\ndevice = torch.device(\\\"cuda\\\") if torch.cuda.is_available() else torch.device(\\\"cpu\\\")\\\\nmodel_name = \\\\'MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\\\\n\\\\npremise = \\\"I first thought that I liked the movie, but upon second thought it was actually disappointing.\\\"\\\\nhypothesis = \\\"The movie was good.\\\"\\\\n\\\\ninput = tokenizer(premise, hypothesis, truncation=True, return_tensors=\\\\'pt\\\\')\\\\noutput = model(input[\\\"input_ids\\\"].to(device))\\\\n\\\\nprediction = torch.softmax(output[\\\"logits\\\"][0], -1).tolist()\\\\nlabel_names = [\\\\'entailment\\\\', \\\\'not_entailment\\\\']\\\\nprediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\\\\n\\\\nprint(prediction)', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary')\\\", 'api_arguments': {'premise': 'string', 'hypothesis': 'string'}, 'python_environment_requirements': [{'transformers': '>=4.13', 'torch': 'any'}], 'example_code': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nimport torch\\\\ndevice = torch.device(cuda) if torch.cuda.is_available() else torch.device(cpu)\\\\nmodel_name = MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\\\\npremise = I first thought that I liked the movie, but upon second thought it was actually disappointing.\\\\nhypothesis = The movie was good.\\\\ninput = tokenizer(premise, hypothesis, truncation=True, return_tensors=pt)\\\\noutput = model(input[input_ids].to(device))\\\\nprediction = torch.softmax(output[logits][0], -1).tolist()\\\\nlabel_names = [entailment, not_entailment]\\\\nprediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\\\\nprint(prediction)', 'performance': {'dataset': {'mnli-m-2c': {'accuracy': 0.925}, 'mnli-mm-2c': {'accuracy': 0.922}, 'fever-nli-2c': {'accuracy': 0.892}, 'anli-all-2c': {'accuracy': 0.676}, 'anli-r3-2c': {'accuracy': 0.665}, 'lingnli-2c': {'accuracy': 0.888}}}, 'description': 'This model was trained on 782 357 hypothesis-premise pairs from 4 NLI datasets: MultiNLI, Fever-NLI, LingNLI and ANLI. The base model is DeBERTa-v3-xsmall from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective.' </s>\"\n"}
{"text": "<s>[INST]قم بتجهيز نماذج الرؤية الحاسوبية للتصنيف الصوري بدون دعم حتى تكون قادرة على تمييز الأشياء التي لم تكن موجودة في تدريبها الأساسي، يُفضل استخدام أدوات لتحقيق ذلك كما لو أنهم يلعبون كرة القدم الافتراضية ضد وكيل التعلم الآلي.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. Import the necessary libraries: PIL for image manipulation, requests to download the image, and from transformers, we will use ChineseCLIPProcessor and ChineseCLIPModel.\\\\n2. Create instances of the ChineseCLIPProcessor and ChineseCLIPModel using the pre-trained model \\\\'OFA-Sys/chinese-clip-vit-large-patch14\\\\'.\\\\n3. Load the image file that you want to classify using the Image.open() method from PIL library.\\\\n4. Prepare the list of product types for zero-shot classification in variable named \\\"texts\\\" (i.e., [\\\\'electronics\\\\', \\\\'clothing\\\\', \\\\'toys\\\\']).\\\\n5. Use the processor to create inputs for both the image and potential product types.\\\\n6. Calculate image features and text features separately, and normalize them.\\\\n7. Combine image-text pairs and calculate the similarity score between them using the model.\\\\n8. Apply softmax on the logits to obtain the probabilities for each category.\\\\n9. The output provides the model\\\\'s predictions for the image representing one of the product types.', '<<<code>>>': \\\"from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\nmodel = ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\\nprocessor = ChineseCLIPProcessor.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\\nimage_url = 'https://example.com/path/to/image.jpg'\\\\nimage = Image.open(requests.get(image_url, stream=True).raw)\\\\ntexts = ['electronics', 'clothing', 'toys']\\\\ninputs = processor(images=image, texts=texts, return_tensors='pt', padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\\", 'api_call': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'chinese-clip-vit-large-patch14', 'api_call': \\\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\\\", 'api_arguments': {'model_name': 'OFA-Sys/chinese-clip-vit-large-patch14'}, 'python_environment_requirements': [{'libraries': ['transformers', 'PIL', 'requests']}], 'example_code': 'from PIL import Image\\\\nimport requests\\\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\\\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\\\\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\\\\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = []\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nimage_features = model.get_image_features(**inputs)\\\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True) # normalize\\\\ninputs = processor(text=texts, padding=True, return_tensors=pt)\\\\ntext_features = model.get_text_features(**inputs)\\\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True) # normalize\\\\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\\\\nprobs = logits_per_image.softmax(dim=1) # probs: [[0.0066, 0.0211, 0.0031, 0.9692]]', 'performance': {'dataset': 'MUGE Text-to-Image Retrieval, Flickr30K-CN Retrieval, COCO-CN Retrieval, CIFAR10, CIFAR100, DTD, EuroSAT, FER, FGV, KITTI, MNIST, PASCAL VOC', 'accuracy': 'Varies depending on the dataset'}, 'description': 'Chinese-CLIP-ViT-Large-Patch14 is a large version of the Chinese CLIP model, with ViT-L/14 as the image encoder and RoBERTa-wwm-base as the text encoder. Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It is designed for zero-shot image classification tasks.' </s>\"\n"}
{"text": "<s>[INST]يقوم فريقنا بتطوير نماذج وتقنيات لفصل الأجسام في الصور الرقمية باستخدام تقنيات الرؤية الحاسوبية، ونحتاج إلى قدرة على تقدير عمق الأشياء في الصور الملتقطة لتحسين النتائج.? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the required classes from the transformers library (SegformerFeatureExtractor and SegformerForSemanticSegmentation) and other libraries (PIL for image handling and requests for fetching images from URLs).\\\\n2. We load the pretrained 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024' model, which is a SegFormer model fine-tuned on the CityScapes dataset for image segmentation. This model can segment images into different classes like buildings, roads, and green spaces.\\\\n3. Using the SegformerFeatureExtractor, we preprocess the input aerial image and then pass it through the model, which generates the segmented output.\\\\n4. The output can then be analyzed to make decisions about city planning.\\\", '<<<code>>>': \\\"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\\naerial_image_url = 'https://example.com/aerial_image.jpg'\\\\n\\\\nimage = Image.open(requests.get(aerial_image_url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\", 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024', 'api_call': \\\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024'}, 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]استخدم تقنيات تحديد الكائنات في الرؤية الحاسوبية لتحديد الكائنات في الصور والفيديوهات.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8m-valorant-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the required libraries: ultralyticsplus and ultralytics.\\\\n2. Import YOLO and render_result classes from the ultralyticsplus library.\\\\n3. Load the YOLO object detection model for the Valorant game using the 'keremberke/yolov8m-valorant-detection' identifier.\\\\n4. Configure the model with the desired confidence (conf), intersection over union (iou), agnostic NMS, and maximum detections (max_det) settings.\\\\n5. Provide a screenshot (image) to the model.\\\\n6. The model will detect game objects such as dropped spikes, enemies, planted spikes, and teammates and return the bounding boxes.\\\\n7. Render the results to visualize the detected objects.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path/to/screenshot.jpg'\\\\nresults = model.predict(image)\\\\nbounding_boxes = results[0].boxes\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8m-valorant-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8m-valorant-detection', 'api_call': \\\"YOLO('keremberke/yolov8m-valorant-detection')\\\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'valorant-object-detection', 'accuracy': 0.965}, 'description': 'A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد مرحلة من التركيب النصي الى نص مقابلات في مجال توليد النصوص. هل بإمكانك مساعدتي في هذا؟? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-400M-distill')\\\", 'api_provider': 'Transformers', 'explanation': '', 'code': \\\"from transformers import BlenderbotForConditionalGeneration, Conversation\\\\nchatbot = BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-400M-distill')\\\\nconversation = Conversation(val)\\\\nresponse = chatbot.generate(conversation.tensor())\\\\nresponse_text = chatbot.tokenizer.batch_decode(response, skip_special_tokens=True)\\\", 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Conversational', 'api_name': 'facebook/blenderbot-400M-distill', 'api_call': \\\"from transformers import BlenderbotForConditionalGeneration, Conversation\\\\nchatbot = BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-400M-distill')\\\\nconversation = Conversation(val)\\\\nresponse = chatbot.generate(conversation.tensor())\\\\nresponse_text = chatbot.tokenizer.batch_decode(response, skip_special_tokens=True)\\\", 'api_arguments': \\\"['message']\\\", 'python_environment_requirements': ['transformers', 'torch'], 'example_code': 'Input a message to start chatting with facebook/blenderbot-400M-distill.', 'performance': {'dataset': 'blended_skill_talk', 'accuracy': 'Not specified'}, 'description': 'BlenderBot-400M-distill is a distilled version of the BlenderBot model, trained on the Blended Skill Talk dataset. It is designed for open-domain chatbot tasks and can generate text-to-text responses in a conversational manner. The model is based on the Transformers library and can be used with PyTorch, TensorFlow, and JAX.'}} </s>\"\n"}
{"text": "<s>[INST]سنقوم بتطوير تطبيق لمجال معالجة اللغة الطبيعية الذي يقوم بالإجابة عن أسئلة متعلقة بالجداول. يجب تصميم برنامج يتماشى مع احتياجات المستخدمين الذين يعانون من إعاقة بصرية.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library.\\\\n2. Initialize the pipeline with the 'table-question-answering' task and the model 'google/tapas-medium-finetuned-wtq'.\\\\n3. Define the table containing the employees' information and create a query to find the less talkative people for the one-month project.\\\\n4. Pass the table and query to the table_qa function to extract the appropriate employee(s) who fit the requirements.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntable_qa = pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\\ntable = {...}  # Employees information table\\\\nquery = 'Who are the least talkative employees for a one-month project?'\\\\nrecommended_employees = table_qa(table=table, query=query)\\\", 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/tapas-medium-finetuned-wtq', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", 'api_arguments': 'table, query', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntable_qa = pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\\n\\\\n# Define the table and the query\\\\ntable = {...}\\\\nquery = '...'\\\\n\\\\n# Get the answer\\\\nanswer = table_qa(table=table, query=query)\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.4324}, 'description': 'TAPAS medium model fine-tuned on WikiTable Questions (WTQ). This model is pretrained on a large corpus of English data from Wikipedia and is used for answering questions related to a table.' </s>\"\n"}
{"text": "<s>[INST]عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى برنامج يقوم بتحويل النص إلى كلام لمستخدمي هذا التطبيق في مجال الأسئلة والأجوبة عن الوثائق المتعددة. يركز هذا التطبيق على الفحص البصري متعدد الوسائط.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary library which is AutoModel from transformers.\\\\n2. Use the from_pretrained method of the AutoModel class to load the pre-trained model 'sheldonxxxx/OFA_model_weights'. This model has been designed for visual question answering tasks, which is exactly what we need for answering questions on movie scenes.\\\\n3. You can then use this loaded model to perform visual question answering tasks based on the input data (movie scenes and their corresponding questions).\\\\n4. Note that this is an unofficial mirror of the model weights for OFA, and it has been provided to help with faster downloads outside of China.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", 'api_call': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sheldonxxxx/OFA_model_weights', 'api_call': \\\"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.' </s>\"\n"}
{"text": "<s>[INST]يوجد فريق طلابي في مجال معالجة اللغات الطبيعية يلتقي كل أسبوعين للمشاركة في لعبة كرة القدم الافتراضية. قم بتزويدهم بأداة للعب ضد وكيل الترجمة الآلي.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. Import the required library (pipeline) from the transformers package provided by Hugging Face.\\\\n2. Use the pipeline function to create a translator that translates German text to Spanish.\\\\n3. Pass the German text \\\"Ich möchte ein Auto kaufen. Welche Farbe empfehlen Sie?\\\" to the created translator (translation).\\\\n4. The translator will translate the German input text into Spanish.', '<<<code>>>': 'from transformers import pipeline\\\\ntranslator = pipeline(\\\\'translation_de_to_es\\\\', model=\\\\'Helsinki-NLP/opus-mt-de-es\\\\')\\\\ngerman_text = \\\"Ich möchte ein Auto kaufen. Welche Farbe empfehlen Sie?\\\"\\\\nspanish_translation = translator(german_text)\\\\nresult = spanish_translation[0][\\\\'translation_text\\\\']', 'api_call': \\\"pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-de-es', 'api_call': \\\"pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\\\ntranslated_text = translation('Guten Tag')\\\", 'performance': {'dataset': 'Tatoeba.de.es', 'accuracy': {'BLEU': 48.5, 'chr-F': 0.676}}, 'description': 'A German to Spanish translation model based on the OPUS dataset and trained using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece tokenization.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد وصف نصي يصف مشهدًا يظهر تصنيف الفيديوهات في مجال رؤية الحاسوب باستخدام معالجة اللغة الطبيعية لتحديد وتصنيف الصور ومقاطع الفيديو. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and classes such as VideoReader from decord, VideoMAEFeatureExtractor and VideoMAEForVideoClassification from transformers, and hf_hub_download from huggingface_hub.\\\\n2. Create a VideoReader object to read the video file using the given file path.\\\\n3. Pick clip_len number of frames from the video with a specified frame_sample_rate.\\\\n4. Use VideoMAEFeatureExtractor to process the video frames, converting them into the appropriate format for the model.\\\\n5. Load the 'nateraw/videomae-base-finetuned-ucf101' pre-trained model using VideoMAEForVideoClassification and use it to analyze the video footage.\\\\n6. The model will produce logits that correspond to the probability of each action. Find the highest probability action and use the model's config.id2label dictionary to find the matching label.\\\\n7. Display the predicted label.\\\", '<<<code>>>': \\\"from decord import VideoReader, cpu\\\\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\\\nfrom huggingface_hub import hf_hub_download\\\\nfile_path = '<YOUR_VIDEO_FILE_PATH>'\\\\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\\\\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\\\\nvideo = videoreader.get_batch(indices).asnumpy()\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\\ninputs = feature_extractor(list(video), return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'videomae-base-finetuned-ucf101', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/videomae-base-finetuned-ucf101'}, 'python_environment_requirements': [['transformers', 'decord', 'huggingface_hub']], 'example_code': 'from decord import VideoReader, cpu\\\\nimport torch\\\\nimport numpy as np\\\\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\\\nfrom huggingface_hub import hf_hub_download\\\\nnp.random.seed(0)\\\\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\\\\n converted_len = int(clip_len * frame_sample_rate)\\\\n end_idx = np.random.randint(converted_len, seg_len)\\\\n start_idx = end_idx - converted_len\\\\n indices = np.linspace(start_idx, end_idx, num=clip_len)\\\\n indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\\\\n return indices\\\\nfile_path = hf_hub_download(\\\\n repo_id=nateraw/dino-clips, filename=archery.mp4, repo_type=space\\\\n)\\\\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\\\\nvideoreader.seek(0)\\\\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\\\\nvideo = videoreader.get_batch(indices).asnumpy()\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\\\\ninputs = feature_extractor(list(video), return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'UCF101', 'accuracy': 0.758209764957428}, 'description': 'VideoMAE Base model fine tuned on UCF101 for Video Action Recognition' </s>\"\n"}
{"text": "<s>[INST]يجب على المطور تطوير برنامج لتلخيص النصوص بجودة عالية لمساعدة الباحثين والطلاب في مجال معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary classes from the transformers library and custom tokenization module. This includes PegasusForConditionalGeneration for the text summarization model and PegasusTokenizer for processing Chinese text.\\\\n2. We then use the from_pretrained method to load the pre-trained model 'IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese'. This model has been fine-tuned on multiple Chinese text summarization datasets and is ideal for generating a summary of a Chinese article.\\\\n3. We load the text in the variable 'text' and tokenize it using the PegasusTokenizer.\\\\n4. The model processes the tokenized text and generates a summary, which can be obtained by decoding the tokens.\\\", '<<<code>>>': \\\"from transformers import PegasusForConditionalGeneration\\\\nfrom tokenizers_pegasus import PegasusTokenizer\\\\nmodel = PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntokenizer = PegasusTokenizer.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntext = '你的中文文章内容'\\\\ninputs = tokenizer(text, max_length=1024, return_tensors='pt')\\\\nsummary_ids = model.generate(inputs['input_ids'])\\\\nsummary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\\\", 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Randeng-Pegasus-238M-Summary-Chinese', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\", 'api_arguments': {'text': 'string', 'max_length': 'integer'}, 'python_environment_requirements': [['transformers', 'tokenizers_pegasus.py', 'data_utils.py']], 'example_code': \\\"from transformers import PegasusForConditionalGeneration\\\\nfrom tokenizers_pegasus import PegasusTokenizer\\\\nmodel = PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntokenizer = PegasusTokenizer.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntext = '1'\\\\ninputs = tokenizer(text, max_length=1024, return_tensors='pt')\\\\nsummary_ids = model.generate(inputs['input_ids'])\\\\ntokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\\\", 'performance': {'dataset': 'LCSTS', 'accuracy': {'rouge-1': 43.46, 'rouge-2': 29.59, 'rouge-L': 39.76}}, 'description': 'Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus. It is fine-tuned on 7 Chinese text summarization datasets including education, new2016zh, nlpcc, shence, sohu, thucnews, and weibo. The model can be used to generate summaries for Chinese text inputs.' </s>\"\n"}
{"text": "<s>[INST]يحتاج العمل بتقنيات تعزيز التعلم في مجال الروبوتات إلى تطوير نماذج تعليمية تستخدم البيانات لتدريب أنظمة تحكم ذكية للروبوتات، وذلك لتحقيق أداء متميز في التفاعل مع البيئة.? [/INST]'<<<domain>>>': 'Reinforcement Learning Robotics', '<<<api_call>>>': \\\"model_utils.load_model('model_utils.VC1_BASE_NAME')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary module 'model_utils' from the vc_models.models.vit package. This includes the functions required for loading and utilizing the VC-1 vision transformer model.\\\\n2. We then use the load_model function to load the pre-trained VC-1 model. This model is a vision transformer suitable for EmbodiedAI tasks, making it appropriate for office navigation and object manipulation.\\\\n3. With the model loaded, we can preprocess an input image captured by the robot's camera using the model_transforms helper.\\\\n4. Finally, we pass the preprocessed image through the model to obtain an embedding, which can be used for further analysis and action planning.\\\", '<<<code>>>': \\\"from vc_models.models.vit import model_utils\\\\nmodel, embd_size, model_transforms, model_info = model_utils.load_model(model_utils.VC1_BASE_NAME)\\\\nimg = your_image here  # Replace your_image with an actual image from the robot's camera\\\\ntransformed_img = model_transforms(img)\\\\nembedding = model(transformed_img)\\\", 'api_call': \\\"model_utils.load_model('model_utils.VC1_BASE_NAME')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Reinforcement Learning Robotics', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'VC1_BASE_NAME', 'api_call': \\\"model_utils.load_model('model_utils.VC1_BASE_NAME')\\\", 'api_arguments': 'img', 'python_environment_requirements': ['from vc_models.models.vit import model_utils'], 'example_code': 'model,embd_size,model_transforms,model_info = model_utils.load_model(model_utils.VC1_BASE_NAME)\\\\nimg = your_function_here ...\\\\ntransformed_img = model_transforms(img)\\\\nembedding = model(transformed_img)', 'performance': {'dataset': 'CortexBench', 'accuracy': 'Mean Success: 68.7%'}, 'description': 'The VC-1 model is a vision transformer (ViT) pre-trained on over 4,000 hours of egocentric videos from 7 different sources, together with ImageNet. The model is trained using Masked Auto-Encoding (MAE) and is available in two sizes: ViT-B and ViT-L. The model is intended for use for EmbodiedAI tasks, such as object manipulation and indoor navigation.' </s>\"\n"}
{"text": "<s>[INST]نحن بحاجة إلى إنشاء نموذج توليد نصوص لمعالجة اللغة الطبيعية المتخصص في تحويل النصوص إلى نصوص. يرجى تزويدنا بالإرشادات والإرشادات على كيفية تحديث وتحسين هذا النموذج.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('flexudy/t5-base-multi-sentence-doctor')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the necessary classes from the transformers library, which are AutoTokenizer and AutoModelWithLMHead.', 'code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'flexudy/t5-base-multi-sentence-doctor\\\\')\\\\nmodel = AutoModelWithLMHead.from_pretrained(\\\\'flexudy/t5-base-multi-sentence-doctor\\\\')\\\\ninput_text = \\\"repair_sentence: I hopes you got recieved thes attachmen with the report we discussed in meet yestday and pleas share you\\\\'re opinion.\\\"\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=\\\\'pt\\\\')\\\\noutputs = model.generate(input_ids, max_length=128, num_beams=1)\\\\ncorrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'flexudy/t5-base-multi-sentence-doctor\\\\')\\\\nmodel = AutoModelWithLMHead.from_pretrained(\\\\'flexudy/t5-base-multi-sentence-doctor\\\\')\\\\ninput_text = \\\"repair_sentence: I hopes you got recieved thes attachmen with the report we discussed in meet yestday and pleas share you\\\\'re opinion.\\\"\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=\\\\'pt\\\\')\\\\noutputs = model.generate(input_ids, max_length=128, num_beams=1)\\\\ncorrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتشغيل النظام المتعدد الوسائط لتحويل الصورة إلى نص ، وقم باستخدام الأداة المناسبة للتعرف على الصورة وتوليد النص المقابل لها بدقة. يجب تدريب نماذج التعلم الآلي التي تستخدمها على تحليل الصور بشكل فعال واستخراج المعلومات النصية المهمة منها.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"pipeline('text-generation', model='microsoft/git-base-coco')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which includes the 'pipeline' function from the transformers package.\\\\n2. Use the 'pipeline' function to create a text generation model with the pre-trained 'microsoft/git-base-coco' model. This model has been fine-tuned for the image-to-text generation task, making it suitable for generating captions for images.\\\\n3. Load the image that needs to be captioned and convert it into an appropriate format for feeding into the model.\\\\n4. Pass the image to the model to generate a caption.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nfrom PIL import Image\\\\n\\\\ndef generate_caption(image_path):\\\\n    image_to_text_model = pipeline('text-generation', model='microsoft/git-base-coco')\\\\n    image = Image.open(image_path)\\\\n    caption = image_to_text_model(image)\\\\n    return caption\\\\n\\\\n# Usage example\\\\ncaption_result = generate_caption('your_image_path.jpg')\\\", 'api_call': \\\"pipeline('text-generation', model='microsoft/git-base-coco')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/git-base-coco', 'api_call': \\\"pipeline('text-generation', model='microsoft/git-base-coco')\\\", 'api_arguments': 'image', 'python_environment_requirements': ['transformers'], 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام لمعالجة اللغة الطبيعية للإجابة عن الأسئلة، ونحتاج إلى تنبؤ بإجابات صحيحة باستخدام بيانات تدريب متاحة.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"pipeline('question-answering', model='bigwiz83/sapbert-from-pubmedbert-squad2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' function from transformers.\\\\n2. Load the model 'bigwiz83/sapbert-from-pubmedbert-squad2' using the 'pipeline' function, setting the functionality to 'question-answering'.\\\\n3. For each research paper abstract, provide a context and question to the model.\\\\n4. The model will then give an answer for the question based on the research paper abstract.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\'question-answering\\\\', model=\\\\'bigwiz83/sapbert-from-pubmedbert-squad2\\\\')\\\\nabstract = \\\"Research paper abstract here...\\\"\\\\nquestion = \\\"What is the main finding of the research paper?\\\"\\\\nresult = qa_pipeline({\\\\'context\\\\': abstract, \\\\'question\\\\': question})\\\\nanswer = result[\\\\'answer\\\\']', 'api_call': \\\"pipeline('question-answering', model='bigwiz83/sapbert-from-pubmedbert-squad2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bigwiz83/sapbert-from-pubmedbert-squad2', 'api_call': \\\"pipeline('question-answering', model='bigwiz83/sapbert-from-pubmedbert-squad2')\\\", 'api_arguments': ['context', 'question'], 'python_environment_requirements': [['transformers==4.7.0', 'torch==1.8.0', 'datasets==1.4.1', 'tokenizers==0.10.2']], 'example_code': \\\"qa_pipeline({'context': 'This model can be loaded on the Inference API on-demand.', 'question': 'Where can the model be loaded?'})\\\", 'performance': {'dataset': 'squad_v2', 'accuracy': '1.2582'}, 'description': 'This model is a fine-tuned version of cambridgeltl/SapBERT-from-PubMedBERT-fulltext on the squad_v2 dataset.' </s>\"\n"}
{"text": "<s>[INST]نحن في مجال معالجة اللغات الطبيعية والترجمة. يجب عليك تصميم خوارزميات لترجمة النصوص بين عدة لغات بدقة عالية وسلاسة، وضبطها على أن تعمل بكفاءة بمختلف الأنواع المعقدة من النصوص. يتوجب عليك التأكد من فهم عمق القواعد اللغوية والصيغ النحوية للغات المختلفة وتطوير الحلول لتقديم ترجمات دقيقة وسلسة.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the required libraries such as pipeline function and AutoModelForSeq2SeqLM from the transformers library provided by Hugging Face.\\\\n2. Load the pre-trained Helsinki-NLP/opus-mt-en-ru translation model using AutoModelForSeq2SeqLM.from_pretrained() function. This model is designed for translating English text to Russian.\\\\n3. Create a translation pipeline using the pipeline() function with 'translation_en_to_ru' and the loaded model to perform translations.\\\\n4. Pass the customer reviews as input to the pipeline to get the translated text in Russian.\\\", '<<<code>>>': 'from transformers import pipeline, AutoModelForSeq2SeqLM\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-ru\\\\')\\\\ntranslation = pipeline(\\\\'translation_en_to_ru\\\\', model=model)\\\\ntranslated_text = translation(\\\"Hello, how are you?\\\")', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-en-ru', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers', 'torch', 'tensorflow']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ru', model='Helsinki-NLP/opus-mt-en-ru')\\\\ntranslated_text = translation('Hello, how are you?')\\\", 'performance': {'dataset': 'newstest2019-enru', 'accuracy': '27.1'}, 'description': 'Helsinki-NLP/opus-mt-en-ru is a translation model trained on the OPUS dataset, which translates English text to Russian. It is based on the Marian NMT framework and can be used with Hugging Face Transformers.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نظام للتعرف التلقائي على الكلام الصوتي. يهدف النظام إلى تحويل التسجيلات الصوتية إلى نص تلقائياً من خلال تحليل الموجات الصوتية ومعالجتها بواسطة نماذج الذكاء الصناعي المتقدمة.? [/INST]{'domain': 'Audio Automatic Speech Recognition', 'api_call': \\\"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')\\\\naudio_paths = ['/path/to/lecture1.mp3', '/path/to/lecture2.mp3']\\\\n# Replace these file paths with the paths to your own audio files.\\\\ntranscriptions = model.transcribe(audio_paths)\\\", 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Speech Recognition', 'api_name': 'jonatasgrosman/wav2vec2-large-xlsr-53-japanese', 'api_call': \\\"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')\\\\naudio_paths = ['/path/to/lecture1.mp3', '/path/to/lecture2.mp3']\\\\n# Replace these file paths with the paths to your own audio files.\\\\ntranscriptions = model.transcribe(audio_paths)\\\", 'api_arguments': ['audio_paths'], 'python_environment_requirements': ['huggingsound', 'torch', 'librosa', 'datasets', 'transformers'], 'example_code': 'from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel(jonatasgrosman/wav2vec2-large-xlsr-53-japanese)\\\\naudio_paths = [/path/to/file.mp3, /path/to/another_file.wav]\\\\ntranscriptions = model.transcribe(audio_paths)', 'performance': {'dataset': 'common_voice', 'accuracy': {'WER': 81.8, 'CER': 20.16}}, 'description': 'Fine-tuned XLSR-53 large model for speech recognition in Japanese. Trained on Common Voice 6.1, CSS10, and JSUT datasets. Make sure your speech input is sampled at 16kHz.'}} </s>\"\n"}
{"text": "<s>[INST]بناءً على مشروع تصنيف البيانات الجدولية المتعلق بتقنية الرؤية الحاسوبية، نحتاج إلى برنامج يستطيع تحليل البيانات الجدولية المتنوعة وتصنيفها بشكل دقيق بإستخدام تقنيات تعلم الآلة. يجب أن يكون البرنامج قادرًا على تمييز بين العناصر المختلفة في البيانات الجدولية بدقة عالية.? [/INST]'<<<domain>>>': 'Tabular Tabular Regression', '<<<api_call>>>': \\\"pipeline('regression', model='julien-c/pokemon-predict-hp')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a regression model for predicting Pokemon HP, loading the specific pretrained model 'julien-c/pokemon-predict-hp'.\\\\n3. With the Pokemon data, the created model is ready to predict the HP of a Pokemon.\\\\n4. The input data should contain features of a Pokemon, and the model will return the predicted HP value.\\\\n5. The model is trained on julien-c/kaggle-rounakbanik-pokemon dataset and predicts HP based on the input data provided.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\npokemon_hp_predictor = pipeline('regression', model='julien-c/pokemon-predict-hp')\\\\ninput_data = {'feature1': value1, 'feature2': value2, ...}\\\\npredicted_hp = pokemon_hp_predictor(input_data)[0]['value']\\\", 'api_call': \\\"pipeline('regression', model='julien-c/pokemon-predict-hp')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'julien-c/pokemon-predict-hp', 'api_call': \\\"pipeline('regression', model='julien-c/pokemon-predict-hp')\\\", 'api_arguments': 'input_data', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'julien-c/kaggle-rounakbanik-pokemon', 'accuracy': {'mean_absolute_error': 15.909, 'model_loss': 647.605}}, 'description': 'A tabular regression model trained on the julien-c/kaggle-rounakbanik-pokemon dataset to predict the HP of Pokemon.' </s>\"\n"}
{"text": "<s>[INST]سنقوم بتحويل النص إلى كلام مسموع باستخدام تقنيات تحويل النص إلى كلام المتقدمة. يرجى تقديم النص الذي ترغب في تحويله وتحديد اللهجة والنبرة المطلوبة للصوت بخصوص تقنيات معالجة اللغة الطبيعية للجداول والإجابة عن الأسئلة.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\", '<<<api_provider>>>': 'PyTorch Transformers', '<<<explanation>>>': \\\"1. First, import the required libraries - Wav2Vec2Processor and Data2VecForCTC.\\\\n2. Load the pretrained model and processor using the 'facebook/data2vec-audio-base-960h' model.\\\\n3. Before transcribing, preprocess the audio dataset and read the sound files using the load_dataset method.\\\\n4. Tokenize the preprocessed audio input for feeding into the model using the Wav2Vec2Processor.\\\\n5. Obtain the logits for the input audio by passing it through the model.\\\\n6. Decode the logits into the most probable transcription using the argmax function and the batch_decode method from the processor.\\\", '<<<code>>>': \\\"from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/data2vec-audio-base-960h')\\\\nmodel = Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\\n\\\\nds = load_dataset('patrickvonplaten/librispeech_asr_dummy', 'clean', split='validation')\\\\ninput_values = processor(ds[0]['audio']['array'], return_tensors='pt', padding='longest').input_values\\\\nlogits = model(input_values).logits\\\\n\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\\", 'api_call': \\\"Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\", 'provider': 'PyTorch Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'data2vec-audio-base-960h', 'api_call': \\\"Data2VecForCTC.from_pretrained('facebook/data2vec-audio-base-960h')\\\", 'api_arguments': ['model_name'], 'python_environment_requirements': [['transformers', 'datasets', 'torch', 'jiwer']], 'example_code': 'from transformers import Wav2Vec2Processor, Data2VecForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\n# load model and processor\\\\nprocessor = Wav2Vec2Processor.from_pretrained(facebook/data2vec-audio-base-960h)\\\\nmodel = Data2VecForCTC.from_pretrained(facebook/data2vec-audio-base-960h)\\\\n\\\\n# load dummy dataset and read soundfiles\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\n\\\\n# tokenize\\\\ninput_values = processor(ds[0][audio][array],, return_tensors=pt, padding=longest).input_values # Batch size 1\\\\n\\\\n# retrieve logits\\\\nlogits = model(input_values).logits\\\\n\\\\n# take argmax and decode\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)', 'performance': {'dataset': 'librispeech_asr', 'accuracy': {'clean': 2.77, 'other': 7.08}}, 'description': \\\"Facebook's Data2Vec-Audio-Base-960h model is an Automatic Speech Recognition model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It can be used for transcribing audio files and achieves competitive performance on major benchmarks of speech recognition. The model is based on the Data2Vec framework which uses the same learning method for either speech, NLP, or computer vision.\\\" </s>\"\n"}
{"text": "<s>[INST]يرجى توليد نموذج لتنقيب العمق في رؤية الحاسوب، وذلك باستخدام البيانات المناسبة لتفهم الدقيق والفعال للصور. هل يمكنك القيام بذلك؟? [/INST]{'domain': 'Computer Vision Depth Estimation', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': '', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-082237', 'api_call': '', 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3421, 'Mae': 0.27, 'Rmse': 0.4042, 'Abs Rel': 0.3279, 'Log Mae': 0.1132, 'Log Rmse': 0.1688, 'Delta1': 0.5839, 'Delta2': 0.8408, 'Delta3': 0.9309}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.'}} </s>\"\n"}
{"text": "<s>[INST]نحن في مجال معالجة اللغات الطبيعية والإجابة على الأسئلة على الجداول، نعمل على تطوير نظام يحول النصوص إلى مقاطع فيديو متعددة الوسائط، ونرغب في تقدير عمق الأشياء في الفيديوهات التي تتم توليدها.? [/INST]{'domain': 'Natural Language Processing Table Question Answering', 'api_call': \\\"pipeline('table-question-answering', model='Meena/table-question-answering-tapas')\\\", 'api_provider': 'PyTorch Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\ntable_qa_pipeline = pipeline('table-question-answering', model='Meena/table-question-answering-tapas')\\\\nquestion = 'Which country has the highest GDP?'\\\\ntable_data = [ ... ] # insert your table data here\\\\nresult = table_qa_pipeline(question=question, table=table_data)\\\", 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'PyTorch Transformers', 'functionality': 'Table Question Answering', 'api_name': 'table-question-answering-tapas', 'api_call': \\\"from transformers import pipeline\\\\ntable_qa_pipeline = pipeline('table-question-answering', model='Meena/table-question-answering-tapas')\\\\nquestion = 'Which country has the highest GDP?'\\\\ntable_data = [ ... ] # insert your table data here\\\\nresult = table_qa_pipeline(question=question, table=table_data)\\\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': [{'name': 'SQA (Sequential Question Answering by Microsoft)', 'accuracy': None}, {'name': 'WTQ (Wiki Table Questions by Stanford University)', 'accuracy': None}, {'name': 'WikiSQL (by Salesforce)', 'accuracy': None}]}, 'description': 'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطبيق Natural Language Processing Fill-Mask على نص يصف مشهد للحيوانات في البرية. هل يمكنك مساعدتي في إتمام هذا التحدي؟? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. The pipeline function is used to create a fill-mask model, which helps users complete sentences by suggesting suitable words to fill in the blanks.\\\\n3. We specify the model 'distilroberta-base' to be loaded. This is a smaller, faster, and lighter version of the RoBERTa-base model, trained to understand the structure of the English language and complete sentences intelligently.\\\\n4. The created unmasker can be used to analyze a sentence containing a masked token and provide suggestions for completing the sentence.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'distilroberta-base\\\\')\\\\nresult = unmasker(\\\"Hello I\\\\'m a <mask> model.\\\")', 'api_call': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilroberta-base', 'api_call': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"unmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': 'openwebtext', 'accuracy': 'Not provided'}, 'description': \\\"DistilRoBERTa is a distilled version of the RoBERTa-base model, designed to be smaller, faster, and lighter. It is a Transformer-based language model trained on the OpenWebTextCorpus, which is a reproduction of OpenAI's WebText dataset. The model has 6 layers, 768 dimensions, and 12 heads, totaling 82M parameters. It is primarily intended for fine-tuning on downstream tasks such as sequence classification, token classification, or question answering.\\\" </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى تطبيق يستخدم تقنيات معالجة اللغة الطبيعية الدردشة لتطوير نظام يمكنه فهم وتفسير المحادثات الصوتية والنصية بشكل ذكي. يجب أن يكون النظام قادرًا على التفاعل مع المستخدم بطريقة طبيعية وفعالة.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-medium')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers library provided by Hugging Face. This includes AutoModelForCausalLM for the conversational model and AutoTokenizer for tokenizing the text.\\\\n2. We then use the from_pretrained method of the AutoModelForCausalLM class to load the pre-trained model 'microsoft/DialoGPT-medium'. This model has been trained for conversational text generation tasks, which is exactly what we need for generating responses in a customer support chatbot.\\\\n3. We also load the tokenizer to process the text and convert it into a format that the model can understand.\\\\n4. Given an input text, we can now generate conversational responses using the DialoGPT model.\\\", '<<<code>>>': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-medium')\\\\nmodel = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-medium')\\\\ninput_ids = tokenizer.encode(customer_message, return_tensors='pt')\\\\nresponse_ids = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.pad_token_id)\\\\nresponse = tokenizer.decode(response_ids[0], skip_special_tokens=True)\\\", 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-medium')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/DialoGPT-medium', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-medium')\\\", 'api_arguments': ['bot_input_ids', 'max_length', 'pad_token_id'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-medium)\\\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-medium)', 'performance': {'dataset': 'Reddit', 'accuracy': 'Comparable to human response quality under a single-turn conversation Turing test'}, 'description': 'DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.' </s>\"\n"}
{"text": "<s>[INST]تحتاج شركتنا لتطوير نظام تحويل الصوت إلى نص لمساعدة مستخدمي تطبيق الإعاقة البصرية في تقنية معالجة اللغات الطبيعية وتشابه الجمل. يجب أن يكون النظام دقيقاً وفعالاً في تحويل الكلام إلى نص باللغة العربية.? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the required libraries like SentenceTransformer and a method from sklearn to calculate cosine similarity.\\\\n2. Load the pretrained 'sentence-transformers/all-MiniLM-L6-v2' model from Hugging Face.\\\\n3. Encode the user query and the list of news article snippets using the loaded SentenceTransformer model.\\\\n4. Calculate the cosine similarity between the user query vector and the news article vectors.\\\\n5. Rank news articles based on their cosine similarity scores and display the top k results matching the user query.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\nquery = \\\"Tech companies planning major acquisitions\\\"\\\\nnews_articles = [\\\"Apple acquires new AI startup\\\", \\\"Tesla announces the opening of new factory\\\", \\\"Microsoft buys game development company\\\"]\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/all-MiniLM-L6-v2\\\\')\\\\nquery_embedding = model.encode(query)\\\\nnews_embeddings = model.encode(news_articles)\\\\nsimilarities = cosine_similarity(query_embedding.reshape(1, -1), news_embeddings)\\\\ntop_k_indices = similarities.argsort()[::-1][:3]\\\\nprint(\\\"Top 3 matching articles:\\\")\\\\nfor index in top_k_indices[0]:\\\\n    print(f\\\">>{news_articles[index]}\\\")', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/all-MiniLM-L6-v2', 'api_call': \\\"SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': '1B sentence pairs dataset', 'accuracy': 'https://seb.sbert.net'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى برنامج يقوم بتحويل النص إلى كلام لمستخدمي هذا التطبيق لاستخراج السمات المتعددة.? [/INST]'<<<domain>>>': 'Multimodal Feature Extraction', '<<<api_call>>>': \\\"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers package. This includes AutoModel for the RuBERT model and the Tokenizer for processing Russian text.\\\\n2. We then use the from_pretrained method of the AutoModel class to load the pre-trained model 'DeepPavlov/rubert-base-cased'. This model has been trained on Russian Wikipedia and news data and is optimized for processing Russian text.\\\\n3. Next, we tokenize the Russian article text and feed it to the model to extract the features from it.\\\\n4. Based on the features, we can generate a short summary of the Russian article, which the literary magazine can use for its readers.\\\", '<<<code>>>': \\\"from transformers import AutoModel, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\\\\nmodel = AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\\\\ninput_text = 'Russian article text...'\\\\ninput_tokens = tokenizer(input_text, return_tensors='pt')\\\\noutput = model(**input_tokens)\\\", 'api_call': \\\"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \\\"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>\"\n"}
{"text": "<s>[INST]بناء نظام تحويل الصور إلى صور بتقنية التعرف البصري بشكل تلقائي، حيث يستطيع تحويل الصور بدقة عالية وسرعة، ونريد تدريب النموذج على مجموعة كبيرة ومتنوعة من الصور لضمان أداء ممتاز في التحويل.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_lineart')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary Python packages, such as torch, controlnet_aux, and diffusers.\\\\n2. Load a control image (lineart image) and resize it to the desired dimensions (e.g., 512x512).\\\\n3. Preprocess the control image using LineartDetector, a pre-trained model to detect lineart images.\\\\n4. Instantiate 'ControlNetModel', a neural network that can control diffusion models by adding extra conditions.\\\\n5. Use 'StableDiffusionControlNetPipeline' to create a pipeline for the controlled diffusion, specifying the torch data type (e.g., torch.float16), and adding the required GPU support if available.\\\\n6. Use 'UniPCMultistepScheduler' to set up a multi-step scheduler for the pipeline.\\\\n7. Generate the final image by providing the artistic prompt, the input lineart control image, and specifying the number of inference steps.\\\", '<<<code>>>': 'import torch\\\\nfrom controlnet_aux import LineartDetector\\\\nfrom diffusers import (\\\\n    ControlNetModel,\\\\n    StableDiffusionControlNetPipeline,\\\\n    UniPCMultistepScheduler,\\\\n)\\\\nfrom PIL import Image\\\\n\\\\n# Load control image (lineart image) and resize\\\\ncontrol_image = Image.open(\\\"path/to/lineart_image.png\\\")\\\\ncontrol_image = control_image.resize((512, 512))\\\\n\\\\n# Preprocess control image with LineartDetector\\\\nprocessor = LineartDetector.from_pretrained(\\\"lllyasviel/Annotators\\\")\\\\ncontrol_image = processor(control_image)\\\\n\\\\n# Instantiate ControlNetModel and pipeline\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\"lllyasviel/control_v11p_sd15_lineart\\\", torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n    \\\"runwayml/stable-diffusion-v1-5\\\", controlnet=controlnet, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\n\\\\n# Generate the final image\\\\nprompt = \\\"michael jackson concert\\\"\\\\ngenerator = torch.manual_seed(0)\\\\nfinal_image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\nfinal_image.save(\\\"output/manga_scene.png\\\")', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_lineart')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/control_v11p_sd15_lineart', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_lineart')\\\", 'api_arguments': {'checkpoint': 'ControlNet-1-1-preview/control_v11p_sd15_lineart', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['pip install diffusers transformers accelerate controlnet_aux==0.3.0'], 'example_code': \\\"import torch\\\\nimport os\\\\nfrom huggingface_hub import HfApi\\\\nfrom pathlib import Path\\\\nfrom diffusers.utils import load_image\\\\nfrom PIL import Image\\\\nimport numpy as np\\\\nfrom controlnet_aux import LineartDetector\\\\nfrom diffusers import (\\\\n ControlNetModel,\\\\n StableDiffusionControlNetPipeline,\\\\n UniPCMultistepScheduler,\\\\n)\\\\ncheckpoint = ControlNet-1-1-preview/control_v11p_sd15_lineart\\\\nimage = load_image(\\\\n https://huggingface.co/ControlNet-1-1-preview/control_v11p_sd15_lineart/resolve/main/images/input.png\\\\n)\\\\nimage = image.resize((512, 512))\\\\nprompt = michael jackson concert\\\\nprocessor = LineartDetector.from_pretrained(lllyasviel/Annotators)\\\\ncontrol_image = processor(image)\\\\ncontrol_image.save(./images/control.png)\\\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(0)\\\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\nimage.save('images/image_out.png')\\\", 'performance': {'dataset': 'ControlNet-1-1-preview', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء فيديو متحرك مستوحى من وصف نصي لمشهد يصور الحيوانات في البرية بدقة عالية. يجب تضمين الحيوانات البرية المختلفة وتسليط الضوء على الروعة والتفاصيل الطبيعية. لأفضل النتائج، استخدم تقنيات معالجة اللغة الطبيعية Fill-Mask. هل تستطيع تقديم المساعدة في هذا المشروع؟? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library provided by Hugging Face. This includes the AutoModelForPreTraining class for loading the pre-trained BERT model for Portuguese, and the pipeline function for building the fill-mask pipeline.\\\\n2. Next, we use the from_pretrained() method to load the specific 'neuralmind/bert-base-portuguese-cased' BERT model for Portuguese language, which is designed for fill-mask tasks.\\\\n3. We then create a fill-mask pipeline by passing the model into the pipeline function along with the 'fill-mask' task.\\\\n4. Finally, invoke the pipeline function to complete the masked sentence provided in the input.\\\", '<<<code>>>': 'from transformers import AutoModelForPreTraining, pipeline\\\\nmodel = AutoModelForPreTraining.from_pretrained(\\\\'neuralmind/bert-base-portuguese-cased\\\\')\\\\npipe = pipeline(\\\\'fill-mask\\\\', model=model)\\\\ncompleted_sentence = pipe(\\\"Durante a nossa conversa, você mencionou estar interessado em [MASK], e queria informar que nós disponibilizamos essa solução.\\\")', 'api_call': \\\"AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'neuralmind/bert-base-portuguese-cased', 'api_call': \\\"AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'neuralmind/bert-base-portuguese-cased'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\npipe = pipeline('fill-mask', model=model, tokenizer=tokenizer)\\\\npipe('Tinha uma [MASK] no meio do caminho.')\\\", 'performance': {'dataset': 'brWaC', 'accuracy': 'state-of-the-art'}, 'description': 'BERTimbau Base is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتصميم نظم تصنيف الصوت، ونحتاج إلى تحديد نوعية المقاطع الصوتية بدقة عالية.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we need to ensure that the necessary Python packages are installed. This includes torch, torchaudio, and librosa (to process audio), transformers (for the Hugging Face models) and numpy (for numerical computations).\\\\n2. Next, we import the Wav2Vec2Model class from the transformers library to load our pre-trained model 'facebook/wav2vec2-large-xlsr-53'.\\\\n3. We pass the target Russian audio file path and the sampling rate to the 'predict' function. This function is responsible for reading the audio file and processing it accordingly.\\\\n4. The model then predicts the emotions in the Russian speech.\\\", '<<<code>>>': \\\"from transformers import Wav2Vec2Model\\\\nmodel = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')\\\\naudio_file_path = '/path/to/russian_audio_speech.wav'\\\\nsampling_rate = 16000\\\\nresult = predict(audio_file_path, sampling_rate)\\\\nprint(result)\\\", 'api_call': \\\"Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'wav2vec2-xlsr-53-russian-emotion-recognition', 'api_call': \\\"Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')\\\", 'api_arguments': {'path': '/path/to/russian_audio_speech.wav', 'sampling_rate': 16000}, 'python_environment_requirements': [['torch', 'torchaudio', 'transformers', 'librosa', 'numpy']], 'example_code': \\\"result = predict('/path/to/russian_audio_speech.wav', 16000)\\\\nprint(result)\\\", 'performance': {'dataset': 'Russian Emotional Speech Dialogs', 'accuracy': '72%'}, 'description': 'A model trained to recognize emotions in Russian speech using wav2vec2. It can classify emotions such as anger, disgust, enthusiasm, fear, happiness, neutral, and sadness.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء سيناريو تصويري متعدد الوسائط يتضمن وصفًا لصورة بدقة عالية مع توجيهات مكتوبة باللغة العربية.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='albert-base-v2')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': '1. We first import the necessary class from the transformers package. This includes the pipeline function for creating a fill-mask pipeline.\\\\n2. We then create a fill-mask pipeline using the pipeline function and the model \\\\'albert-base-v2\\\\'. This model has been trained for masked language modeling tasks, which is exactly what we need for guessing the missing word in a sentence.\\\\n3. We provide the sentence with the masked word \\\"[MASK]\\\" as an input, and the pipeline will predict the most likely word to fill in the blank.', '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'albert-base-v2\\\\')\\\\nunmasker(\\\"I want to have a [MASK] for breakfast.\\\")', 'api_call': \\\"pipeline('fill-mask', model='albert-base-v2')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'albert-base-v2', 'api_call': \\\"pipeline('fill-mask', model='albert-base-v2')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='albert-base-v2')\\\\nunmasker(Hello I'm a [MASK] model.)\\\", 'performance': {'dataset': {'SQuAD1.1': '90.2/83.2', 'SQuAD2.0': '82.1/79.3', 'MNLI': '84.6', 'SST-2': '92.9', 'RACE': '66.8'}, 'accuracy': '82.3'}, 'description': 'ALBERT Base v2 is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model, as all ALBERT models, is uncased: it does not make a difference between english and English.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بالعمل على تطوير نموذج لتصنيف النصوص بدون تدريب مسبق في مجال معالجة اللغات الطبيعية والإجابة على الأسئلة المتعلقة بالجداول، ونرغب في خلق نظام يُمكنه تصنيف النصوص الجديدة بدقة وكفاءة عالية في مجال معالجة اللغات الطبيعية والإجابة على الأسئلة بكفاءة لتكملة الثغرات اللغوية.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. The pipeline function is used to create a table-question-answering model, which is capable of answering questions based on the information provided in tabular data.\\\\n3. We specify the model 'google/tapas-large-finetuned-wtq' to be loaded. This model has been trained on a large corpus of text and can extract information from tables to answer questions.\\\\n4. The created table-question-answering pipeline can be used to query a table for specific information, given a natural language question, and will return the answer based on the table data.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nqa_pipeline = pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\\nresult = qa_pipeline(table=table, query=query)\\\", 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/tapas-large-finetuned-wtq', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\", 'api_arguments': {'model': 'google/tapas-large-finetuned-wtq', 'task': 'table-question-answering'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\nqa_pipeline = pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\\\n\\\\nresult = qa_pipeline(table=table, query=query)\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.5097}, 'description': 'TAPAS large model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).' </s>\"\n"}
{"text": "<s>[INST]عند استخدام Natural Language Processing للتصنيف الصفري، أنشئ قصة قصيرة تبدأ بـ \\\"كان ياما كان في أرض بعيدة\\\" واستخدم التصنيف الصفري لتحليل النص وتحديد موضوع القصة.? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"AutoModel.from_pretrained('typeform/squeezebert-mnli')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary library, which is 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'typeform/squeezebert-mnli'.\\\\n3. The loaded model specializes in zero-shot classification, which predicts the category of an unseen example without having matching training data.\\\\n4. Provide the input text (meeting email invitation), as well as a list of candidate categories ('operations', 'marketing', 'finance', 'technology').\\\\n5. Use the loaded model to classify the given text into the appropriate category.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'typeform/squeezebert-mnli\\\\')\\\\nemail_invitation = \\\"Dear Marketing Team, Please join us for a meeting to discuss our upcoming new marketing campaign. We will review the objectives, budget, and timeline to ensure a successful launch. Looking forward to your participation. Best, [Your Name]\\\"\\\\ncategories = [\\\\'operations\\\\', \\\\'marketing\\\\', \\\\'finance\\\\', \\\\'technology\\\\']\\\\nresult = classifier(email_invitation, categories)', 'api_call': \\\"AutoModel.from_pretrained('typeform/squeezebert-mnli')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'typeform/squeezebert-mnli', 'api_call': \\\"AutoModel.from_pretrained('typeform/squeezebert-mnli')\\\", 'api_arguments': 'text, candidate_labels, multi_label', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nnlp = pipeline('zero-shot-classification', model='typeform/squeezebert-mnli')\\\\nresult = nlp('The quick brown fox jumps over the lazy dog', candidate_labels=['sports', 'language', 'animals'])\\\", 'performance': {'dataset': 'mulit_nli', 'accuracy': 'not provided'}, 'description': 'SqueezeBERT is a transformer model designed for efficient inference on edge devices. This specific model, typeform/squeezebert-mnli, is fine-tuned on the MultiNLI dataset for zero-shot classification tasks.' </s>\"\n"}
{"text": "<s>[INST]يتعلق مجالنا بتوليد النصوص من النصوص الأخرى في مجال معالجة اللغة الطبيعية. يجب علينا تصميم نظام قادر على تحويل النصوص إلى نصوص أخرى بأساليب متعددة، بما في ذلك التحليل العميق للمحتوى النصي والتعبير عنه بوضوح ودقة.? [/INST]{'domain': 'Natural Language Processing Text2Text Generation', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-large-seq2seq')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary libraries from the transformers package: AutoTokenizer and AutoModelForSeq2SeqLM.', 'code': '', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Conversational', 'api_name': 'microsoft/GODEL-v1_1-large-seq2seq', 'api_call': '', 'api_arguments': {'instruction': 'Instruction: given a dialog context, you need to response empathically.', 'knowledge': '', 'dialog': ['Does money buy happiness?', 'It is a question. Money buys you a lot of things, but not enough to buy happiness.', 'What is the best way to buy happiness ?']}, 'python_environment_requirements': {'transformers': 'AutoTokenizer, AutoModelForSeq2SeqLM'}, 'example_code': \\\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != '':\\\\n knowledge = '[KNOWLEDGE] ' + knowledge\\\\n dialog = ' EOS '.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f'Instruction: given a dialog context, you need to response empathically.'\\\\nknowledge = ''\\\\ndialog = [\\\\n 'Does money buy happiness?',\\\\n 'It is a question. Money buys you a lot of things, but not enough to buy happiness.',\\\\n 'What is the best way to buy happiness ?'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\\\", 'performance': {'dataset': 'Reddit discussion thread, instruction and knowledge grounded dialogs', 'accuracy': 'Not provided'}, 'description': 'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد نماذج لتصنيف النصوص بدون تسميات في مجال معالجة اللغات الطبيعية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Zero-Shot Classification', '<<<api_call>>>': \\\"CrossEncoder('cross-encoder/nli-roberta-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We use the pipeline function from the transformers library (Hugging Face) to create a zero-shot classification model.\\\\n2. The model used in this case is the 'cross-encoder/nli-roberta-base', which can identify and categorize text into predefined categories even if it has not seen those categories during training.\\\\n3. The model is used to classify content from online news sources into different candidate_label categories such as 'autonomous vehicles' and 'urban planning'. \\\\n4. Based on the classification, recommendations for news sources with a focus on these two categories can be given.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'zero-shot-classification\\\\', model=\\\\'cross-encoder/nli-roberta-base\\\\')\\\\nnews_source_descriptions = [\\\\n    \\\"News source 1 description...\\\",\\\\n    \\\"News source 2 description...\\\",\\\\n    \\\"News source 3 description...\\\",\\\\n]\\\\ncandidate_labels = [\\\\'autonomous vehicles\\\\', \\\\'urban planning\\\\']\\\\nscores = []\\\\nfor description in news_source_descriptions:\\\\n    result = classifier(description, candidate_labels)\\\\n    scores.append(result)\\\\n\\\\nrecommended_news_sources = [source for source, score in zip(news_source_descriptions, scores) if max(score[\\\\'scores\\\\']) > 0.5]', 'api_call': \\\"CrossEncoder('cross-encoder/nli-roberta-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cross-encoder/nli-roberta-base', 'api_call': \\\"CrossEncoder('cross-encoder/nli-roberta-base')\\\", 'api_arguments': ['sentence1', 'sentence2'], 'python_environment_requirements': [['sentence_transformers', 'transformers']], 'example_code': \\\"from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model='cross-encoder/nli-roberta-base')\\\\nsent = Apple just announced the newest iPhone X\\\\ncandidate_labels = [technology, sports, politics]\\\\nres = classifier(sent, candidate_labels)\\\\nprint(res)\\\", 'performance': {'dataset': ['SNLI', 'MultiNLI'], 'accuracy': 'See SBERT.net - Pretrained Cross-Encoder'}, 'description': 'Cross-Encoder for Natural Language Inference trained on the SNLI and MultiNLI datasets. Outputs three scores corresponding to the labels: contradiction, entailment, neutral.' </s>\"\n"}
{"text": "<s>[INST]قم بتوليد ملخصات للغات الطبيعية بسرد محتويات النصوص بطريقة موجزة وفعالة، مع التركيز على التحليل والفهم العميق للمحتوى.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the 'transformers' library provided by Hugging Face.\\\\n2. Use the pipeline function to create a 'summarization' model.\\\\n3. Specify the model 'google/pegasus-newsroom', which has been trained to perform abstractive summarization on long news articles.\\\\n4. The created model can be used to generate a summary of the input news article.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nsummarizer = pipeline('summarization', model='google/pegasus-newsroom')\\\\nsummary = summarizer(long_news_article, min_length=50, max_length=100)[0]['summary_text']\\\", 'api_call': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/pegasus-newsroom', 'api_call': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': 'newsroom', 'accuracy': '45.98/34.20/42.18'}, 'description': 'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بترجمة نصوص في مجال تجهيز اللغة الطبيعية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the AutoTokenizer and AutoModelForSeq2SeqLM classes from the transformers library.\\\\n2. Create a tokenizer using AutoTokenizer.from_pretrained() and specifying the model path 'Helsinki-NLP/opus-mt-en-de'. This tokenizer will be used to prepare the text for translation.\\\\n3. Create a translation model using AutoModelForSeq2SeqLM.from_pretrained() with the same model path.\\\\n4. With the tokenizer and model created, you can now prepare the English text, pass it through the German translation model and receive the translated text in German.\\\\n5. Users can now read English news translated to German, which helps them understand the content better.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-de\\\\')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-de\\\\')\\\\n\\\\nenglish_text = \\\"Here is some English news text\\\"\\\\ninputs = tokenizer(english_text, return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs[\\\\'input_ids\\\\'])\\\\ngerman_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-en-de', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'Helsinki-NLP/opus-mt-en-de'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\\", 'performance': {'dataset': 'newstest2018-ende.en.de', 'accuracy': {'BLEU': 45.2, 'chr-F': 0.69}}, 'description': 'The Helsinki-NLP/opus-mt-en-de model is a translation model developed by the Language Technology Research Group at the University of Helsinki. It translates English text to German using the Hugging Face Transformers library. The model is trained on the OPUS dataset and has a BLEU score of 45.2 on the newstest2018-ende.en.de dataset.' </s>\"\n"}
{"text": "<s>[INST]قم بإعداد وثيقة نصية تتضمن نصوص وصور ومقاطع صوتية في مجال معالجة اللغة الطبيعية وإنتاج النصوص بخصوص ملخص الوثيقة. ثم أعطها للطلاب لقراءتها ومشاهدة محتواها. بعد ذلك، اطلب منهم تقديم الأسئلة المتعلقة بالوثيقة وإجابتها باستخدام أداة متعددة الوسائط.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'google/pegasus-newsroom\\\\')\\\\narticle = \\\"Article content goes here.\\\"\\\\nsummary = summarizer(article, max_length=150)[0][\\\\'summary_text\\\\']', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'google/pegasus-newsroom', 'api_call': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'google/pegasus-newsroom\\\\')\\\\narticle = \\\"Article content goes here.\\\"\\\\nsummary = summarizer(article, max_length=150)[0][\\\\'summary_text\\\\']', 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': 'newsroom', 'accuracy': '45.98/34.20/42.18'}, 'description': 'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks.'}} </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء صورة متعددة الوسائط تمثل قصة قصيرة تبدأ بجملة \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler), torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary libraries, such as 'StableDiffusionPipeline' and 'EulerDiscreteScheduler'.\\\\n2. Create a stable-diffusion model instance by loading the 'stabilityai/stable-diffusion-2' model and its scheduler.\\\\n3. Move the model to the GPU (if available) for faster processing.\\\\n4. Provide text descriptions of the clothing items you want to generate images for.\\\\n5. Execute the model on the provided text descriptions to generate images.\\\\n6. Save the generated images as desired.\\\", '<<<code>>>': \\\"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nimport torch\\\\n\\\\nmodel_id = 'stabilityai/stable-diffusion-2'\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder='scheduler')\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\n\\\\nif torch.cuda.is_available():\\\\n    pipe = pipe.to('cuda')\\\\n\\\\nad_prompt = 'A photo of a stylish red dress with a matching handbag'\\\\ngenerated_image = pipe(ad_prompt).images[0]\\\\ngenerated_image.save('stylish_red_dress_handbag.png')\\\", 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler), torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler), torch_dtype=torch.float16)\\\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors']], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>\"\n"}
{"text": "<s>[INST]استخدم تقنيات تحويل الصورة إلى صورة لتنشئ صورة تظهر بيتاً عتيقاً في منطقة ريفية خضراء.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the required libraries and functions, including ControlNetModel, StableDiffusionControlNetPipeline, and UniPCMultistepScheduler from the diffusers package and pipeline function from transformers.\\\\n2. Load both the original image and the depth estimated image obtained using 'depth_estimator' pipeline.\\\\n3. Perform pre-processing on the depth image using Sobel operators, background thresholding, normalization, and other calculations.\\\\n4. Create a control net model by loading the pretrained model 'lllyasviel/sd-controlnet-normal' with appropriate specifications.\\\\n5. Set up the image pipeline, load the appropriate scheduler, and enable memory-efficient attention and model offload.\\\\n6. Use the created pipeline to generate the normal map estimation from the input image. Save the output image as 'toy_normal_out.png'.\\\", '<<<code>>>': \\\"from diffusers import ControlNetModel, StableDiffusionControlNetPipeline, UniPCMultistepScheduler\\\\nfrom transformers import pipeline\\\\nfrom PIL import Image\\\\nimport cv2\\\\nimport numpy as np\\\\nimport torch\\\\n\\\\n# Load original_image and estimated_depth_image\\\\n# ... your_loading_and_pre-processing_code_here ...\\\\n\\\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\n\\\\nnormal_map_image = pipe(original_image, estimated_depth_image, num_inference_steps=20).images[0]\\\\nnormal_map_image.save('images/toy_normal_out.png')\\\", 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/sd-controlnet-normal', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\\\", 'api_arguments': ['image', 'num_inference_steps'], 'python_environment_requirements': [['diffusers', 'transformers', 'accelerate']], 'example_code': \\\"from PIL import Image\\\\nfrom transformers import pipeline\\\\nimport numpy as np\\\\nimport cv2\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom diffusers.utils import load_image\\\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png).convert(RGB)\\\\ndepth_estimator = pipeline(depth-estimation, model =Intel/dpt-hybrid-midas )\\\\nimage = depth_estimator(image)['predicted_depth'][0]\\\\nimage = image.numpy()\\\\nimage_depth = image.copy()\\\\nimage_depth -= np.min(image_depth)\\\\nimage_depth /= np.max(image_depth)\\\\nbg_threhold = 0.4\\\\nx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\\\\nx[image_depth &lt; bg_threhold] = 0\\\\ny = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\\\\ny[image_depth &lt; bg_threhold] = 0\\\\nz = np.ones_like(x) * np.pi * 2.0\\\\nimage = np.stack([x, y, z], axis=2)\\\\nimage /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\\\\nimage = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\\\\nimage = Image.fromarray(image)\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\n fusing/stable-diffusion-v1-5-controlnet-normal, torch_dtype=torch.float16\\\\n)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\nimage = pipe(cute toy, image, num_inference_steps=20).images[0]\\\\nimage.save('images/toy_normal_out.png')\\\", 'performance': {'dataset': 'DIODE', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير تقنية الرؤية الحاسوبية للكشف عن الكائنات، ونريد مساعدتكم على تحسين الأداء في إكتشاف وتحديد الكائنات بدقة وفعالية.? [/INST]{'domain': 'Computer Vision Object Detection', 'api_call': \\\"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries, which include transformers, PIL, and requests.', 'code': \\\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://example.com/traffic_image.jpg'  # Replace with your traffic image URL\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\\n\\\\nvehicle_classes = ['car', 'bus', 'truck', 'motorcycle', 'bicycle']\\\\nvehicle_count = {}  # Initialize an empty dictionary to store vehicle counts\\\\n\\\\nfor logits_i, bbox_i in zip(logits, bboxes):\\\\n    class_id = logits_i.argmax()\\\\n    class_label = vehicle_classes[class_id]\\\\n    \\\\n    if class_label in vehicle_count:\\\\n        vehicle_count[class_label] += 1\\\\n    else:\\\\n        vehicle_count[class_label] = 1\\\\n\\\\nprint(vehicle_count)\\\", 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-tiny', 'api_call': \\\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://example.com/traffic_image.jpg'  # Replace with your traffic image URL\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\\n\\\\nvehicle_classes = ['car', 'bus', 'truck', 'motorcycle', 'bicycle']\\\\nvehicle_count = {}  # Initialize an empty dictionary to store vehicle counts\\\\n\\\\nfor logits_i, bbox_i in zip(logits, bboxes):\\\\n    class_id = logits_i.argmax()\\\\n    class_label = vehicle_classes[class_id]\\\\n    \\\\n    if class_label in vehicle_count:\\\\n        vehicle_count[class_label] += 1\\\\n    else:\\\\n        vehicle_count[class_label] = 1\\\\n\\\\nprint(vehicle_count)\\\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \\\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\\\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '28.7 AP'}, 'description': 'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد نص يصف مشهدًا يصور الحيوانات في البرية باستخدام معالجة اللغة الطبيعية لقياس التشابه بين الجمل. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install the sentence-transformers library from Hugging Face Transformers.\\\\n2. Import the SentenceTransformer class and initialize it with the model name 'sentence-transformers/distilbert-base-nli-mean-tokens'.\\\\n3. Preprocess the articles to extract the most representative sentences.\\\\n4. Use the .encode() method of the model to convert those sentences into a 768-dimensional dense vector space (embeddings).\\\\n5. Apply clustering algorithms like K-means, DBSCAN, or hierarchical clustering on the obtained embeddings to group similar articles together.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.cluster import KMeans\\\\nimport numpy as np\\\\n\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/distilbert-base-nli-mean-tokens\\\\')\\\\n\\\\n# Preprocessed and extracted representative sentences from articles\\\\nrepresentative_sentences = [\\\\n    \\\"This is an example sentence from article 1.\\\",\\\\n    \\\"This sentence represents article 2.\\\",\\\\n    \\\"Another example from article 3.\\\",\\\\n    \\\"Sentence summarizing article 4.\\\",\\\\n]\\\\n\\\\n# Encode the sentences to generate embeddings\\\\nembeddings = model.encode(representative_sentences)\\\\n\\\\n# Perform clustering on the embeddings\\\\nkmeans = KMeans(n_clusters=2, random_state=0).fit(embeddings)\\\\n\\\\n# Assign articles to clusters\\\\narticle_clusters = kmeans.labels_', 'api_call': \\\"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \\\"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': ['pip install -U sentence-transformers'], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>\"\n"}
{"text": "<s>[INST]عميلنا يعمل في مجال معالجة اللغات الطبيعية وتطوير نماذج لتحويل النصوص إلى نصوص بيانية. نحن بحاجة إلى برنامج يستخدم تقنيات توليد النصوص لتحويل النصوص إلى نصوص جديدة بطريقة مبتكرة وفعالة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We import the necessary libraries for the task. This includes pipeline and T5ForConditionalGeneration constructors from the transformers library provided by Hugging Face.\\\\n2. We load the pretrained 'pszemraj/long-t5-tglobal-base-16384-book-summary' model using the T5ForConditionalGeneration constructor. This model is designed specifically for summarizing long inputs like books, news articles, and academic texts.\\\\n3. We utilize the loaded model to generate a summary of the given long news article. It will extract the most important information from the article and generate a concise summarized version.\\\", '<<<code>>>': \\\"from transformers import T5ForConditionalGeneration, T5Tokenizer\\\\ntokenizer = T5Tokenizer.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\\nmodel = T5ForConditionalGeneration.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\\ninputs = tokenizer(long_news_article, return_tensors='pt', max_length=2048, truncation=True)\\\\nsummary_ids = model.generate(inputs['input_ids'])\\\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\\\", 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pszemraj/long-t5-tglobal-base-16384-book-summary', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\", 'api_arguments': ['long_text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don't want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\\\", 'performance': {'dataset': 'kmfoda/booksum', 'accuracy': {'ROUGE-1': 36.408, 'ROUGE-2': 6.065, 'ROUGE-L': 16.721, 'ROUGE-LSUM': 33.34}}, 'description': 'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.' </s>\"\n"}
{"text": "<s>[INST]تأكيد التواجد بالنادي الطلابي حيث يتم اجتماع كل أسبوعين للعب كرة القدم الافتراضية. سيتم تزويدكم بأداة جديدة للعب ضد وكيل التعلم الآلي في مجال معالجة اللغة الطبيعية لتقديم الإجابات على الأسئلة.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_provider': 'Transformers', 'explanation': '1. Import the necessary libraries from the transformers package. These include AutoModelForCausalLM for the dialogue response generation model and AutoTokenizer for tokenizing the text.', 'code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-large')\\\\nmodel = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Transformers', 'functionality': 'Conversational', 'api_name': 'microsoft/DialoGPT-large', 'api_call': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-large')\\\\nmodel = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_arguments': ['bot_input_ids', 'max_length', 'pad_token_id'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-large)\\\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-large)', 'performance': {'dataset': 'Reddit discussion thread', 'accuracy': 'Comparable to human response quality under a single-turn conversation Turing test'}, 'description': 'DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multi-turn conversations. The model is trained on 147M multi-turn dialogues from Reddit discussion threads.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نموذج صور سمعي تصنيفي لتصنيف الصور التي تحتوي على حيوانات برية باستخدام تقنيات الرؤية الحاسوبية. هل يمكنك مساعدتي في تحقيق هذا؟? [/INST]{'domain': 'Computer Vision Image Classification', 'api_call': \\\"pipeline('image-classification', model='saltacc/anime-ai-detect')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', model='saltacc/anime-ai-detect')\\\\nresult = image_classifier(open('anime_artwork.jpg', 'rb'))\\\", 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'saltacc/anime-ai-detect', 'api_call': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', model='saltacc/anime-ai-detect')\\\\nresult = image_classifier(open('anime_artwork.jpg', 'rb'))\\\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'aibooru and imageboard sites', 'accuracy': '96%'}, 'description': 'A BEiT classifier to see if anime art was made by an AI or a human.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتحويل صورة إلى صورة أخرى باستخدام تقنيات الرؤية الحاسوبية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_inpaint')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import the necessary libraries, classes, and functions, including ControlNetModel from the diffusers package and load_image from diffusers.utils.\\\\n2. Load the damaged image and the mask indicating the damaged areas.\\\\n3. Define a function to create an inpaint condition, which allows us to guide the image inpainting process.\\\\n4. Create a control image using the damaged image and the mask.\\\\n5. Instantiate the ControlNetModel with the pre-trained model 'lllyasviel/control_v11p_sd15_inpaint'.\\\\n6. Create a StableDiffusionControlNetPipeline instance.\\\\n7. Set the scheduler for the pipeline and enable offloading the model to CPU.\\\\n8. Provide a random seed for the image generation process.\\\\n9. Run the inpainting process on the damaged image to produce a repaired image, and save the result to a file named 'output.png'.\\\", '<<<code>>>': 'import torch\\\\nfrom diffusers import ControlNetModel, StableDiffusionControlNetPipeline, UniPCMultistepScheduler\\\\nfrom diffusers.utils import load_image\\\\nfrom PIL import Image\\\\nimport numpy as np\\\\n\\\\n# Load the damaged painting image and the corresponding mask\\\\noriginal_image = load_image(\\\\'path/to/damaged_painting.png\\\\')\\\\nmask_image = load_image(\\\\'path/to/damaged_painting_mask.png\\\\')\\\\n\\\\n# Define make_inpaint_condition function\\\\n\\\\ndef make_inpaint_condition(image, image_mask):\\\\n    image = np.array(image.convert(\\\\'RGB\\\\')).astype(np.float32) / 255.0\\\\n    image_mask = np.array(image_mask.convert(\\\\'L\\\\'))\\\\n    assert image.shape[:2] == image_mask.shape[:2], \\\"image and image_mask must have the same image size\\\"\\\\n    image[image_mask < 128] = -1.0  # set as masked pixel\\\\n    image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\\\\n    image = torch.from_numpy(image)\\\\n    return image\\\\n\\\\ncontrol_image = make_inpaint_condition(original_image, mask_image)\\\\nprompt = \\\\'best quality\\\\'\\\\nnegative_prompt = \\\\'lowres, bad anatomy, bad hands, cropped, worst quality\\\\'\\\\n\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\'lllyasviel/control_v11p_sd15_inpaint\\\\')\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\'runwayml/stable-diffusion-v1-5\\\\', controlnet=controlnet)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(2)\\\\n\\\\nrepaired_image = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\\\nrepaired_image.save(\\\\'output.png\\\\')', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_inpaint')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/control_v11p_sd15_inpaint', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_inpaint')\\\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_inpaint', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['pip install diffusers transformers accelerate'], 'example_code': \\\"import torch\\\\nimport os\\\\nfrom diffusers.utils import load_image\\\\nfrom PIL import Image\\\\nimport numpy as np\\\\nfrom diffusers import (\\\\n ControlNetModel,\\\\n StableDiffusionControlNetPipeline,\\\\n UniPCMultistepScheduler,\\\\n)\\\\ncheckpoint = lllyasviel/control_v11p_sd15_inpaint\\\\noriginal_image = load_image(\\\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/original.png\\\\n)\\\\nmask_image = load_image(\\\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/mask.png\\\\n)\\\\ndef make_inpaint_condition(image, image_mask):\\\\n image = np.array(image.convert(RGB)).astype(np.float32) / 255.0\\\\n image_mask = np.array(image_mask.convert(L))\\\\n assert image.shape[0:1] == image_mask.shape[0:1], image and image_mask must have the same image size\\\\n image[image_mask < 128] = -1.0 # set as masked pixel \\\\n image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\\\\n image = torch.from_numpy(image)\\\\n return image\\\\ncontrol_image = make_inpaint_condition(original_image, mask_image)\\\\nprompt = best quality\\\\nnegative_prompt=lowres, bad anatomy, bad hands, cropped, worst quality\\\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\ngenerator = torch.manual_seed(2)\\\\nimage = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, \\\\n generator=generator, image=control_image).images[0]\\\\nimage.save('images/output.png')\\\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.' </s>\"\n"}
{"text": "<s>[INST]ستقوم الشركة بتطوير نموذج لتحويل الصوت إلى صوت في مجال معالجة الصوت، ونرغب في تحسين توليد الصوت لتعزيز تجربة المستخدم وجودة الصوت.? [/INST]{'domain': 'Audio Audio-to-Audio', 'api_call': \\\"separator.from_hparams(source='speechbrain/sepformer-whamr', savedir='pretrained_models/sepformer-whamr')\\\", 'api_provider': 'SpeechBrain', 'explanation': '1. We first install the required SpeechBrain library using pip.', 'code': \\\"from speechbrain.pretrained import SepformerSeparation as separator\\\\nimport torchaudio\\\\nmodel = separator.from_hparams(source='speechbrain/sepformer-whamr', savedir='pretrained_models/sepformer-whamr')\\\\nest_sources = model.separate_file(path='input_audio.wav')\\\\ntorchaudio.save('source1hat.wav', est_sources[:, :, 0].detach().cpu(), 8000)\\\\ntorchaudio.save('source2hat.wav', est_sources[:, :, 1].detach().cpu(), 8000)\\\", 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'SpeechBrain', 'functionality': 'Audio Source Separation', 'api_name': 'speechbrain/sepformer-whamr', 'api_call': \\\"from speechbrain.pretrained import SepformerSeparation as separator\\\\nimport torchaudio\\\\nmodel = separator.from_hparams(source='speechbrain/sepformer-whamr', savedir='pretrained_models/sepformer-whamr')\\\\nest_sources = model.separate_file(path='input_audio.wav')\\\\ntorchaudio.save('source1hat.wav', est_sources[:, :, 0].detach().cpu(), 8000)\\\\ntorchaudio.save('source2hat.wav', est_sources[:, :, 1].detach().cpu(), 8000)\\\", 'api_arguments': {'source': 'speechbrain/sepformer-whamr', 'savedir': 'pretrained_models/sepformer-whamr'}, 'python_environment_requirements': 'pip install speechbrain', 'example_code': \\\"from speechbrain.pretrained import SepformerSeparation as separator\\\\nimport torchaudio\\\\nmodel = separator.from_hparams(source='speechbrain/sepformer-whamr', savedir='pretrained_models/sepformer-whamr')\\\\nest_sources = model.separate_file(path='speechbrain/sepformer-wsj02mix/test_mixture.wav')\\\\ntorchaudio.save('source1hat.wav', est_sources[:, :, 0].detach().cpu(), 8000)\\\\ntorchaudio.save('source2hat.wav', est_sources[:, :, 1].detach().cpu(), 8000)\\\", 'performance': {'dataset': 'WHAMR!', 'accuracy': '13.7 dB SI-SNRi'}, 'description': 'This repository provides all the necessary tools to perform audio source separation with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAMR! dataset, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation.'}} </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى تطوير نموذج يُستخدم في نسخة تحليلية غير مشروطة لإنشاء صور بالاعتماد على مفهوم الرؤية الحاسوبية. هذا النموذج سيكون حيويًا في إنتاج صور بشكل ذكي ودقيق دون قيود مسبقة.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from diffusers import DDPMPipeline\\\\nbutterfly_generator = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\\nbutterfly_image = butterfly_generator().images[0]\\\\nbutterfly_image.show()\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'sd-class-butterflies-32', 'api_call': \\\"from diffusers import DDPMPipeline\\\\nbutterfly_generator = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\\nbutterfly_image = butterfly_generator().images[0]\\\\nbutterfly_image.show()\\\", 'api_arguments': {'model_id': 'clp/sd-class-butterflies-32'}, 'python_environment_requirements': ['diffusers'], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': None, 'accuracy': None}, 'description': 'This model is a diffusion model for unconditional image generation of cute butterflies.'}} </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير تطبيق لتصنيف الصوتيات. نحتاج إلى برنامج يمكنه تحويل النصوص المكتوبة إلى كلام لجمهور مستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries and functions such as torch, torchaudio, transformers, librosa, and numpy.\\\\n2. Load the model 'facebook/wav2vec2-large-xlsr-53' using the Wav2Vec2Model.from_pretrained method. This model is trained to recognize emotions in Russian speech.\\\\n3. Preprocess the input audio in the correct format as required by the model.\\\\n4. Use the model to classify emotions in users' speech, such as anger, disgust, enthusiasm, fear, happiness, neutral, or sadness.\\\\n5. The emotional recognition can then be utilized in the smart home system to adapt its responses accordingly.\\\", '<<<code>>>': 'import torch\\\\nimport torchaudio\\\\nfrom transformers import Wav2Vec2Model, Wav2Vec2Processor\\\\n\\\\n# Load the trained model and its processor\\\\nmodel = Wav2Vec2Model.from_pretrained(\\\\'facebook/wav2vec2-large-xlsr-53\\\\')\\\\nprocessor = Wav2Vec2Processor.from_pretrained(\\\\'facebook/wav2vec2-large-xlsr-53\\\\')\\\\n\\\\n# Load the audio file, resample, and preprocess the audio\\\\nsample_rate = 16000\\\\naudio_file = \\\\'/path/to/russian_audio_speech.wav\\\\'\\\\nspeech, _ = torchaudio.load(audio_file)\\\\nspeech = torchaudio.transforms.Resample()(speech, sample_rate)\\\\n\\\\n# Process and classify the emotion in speech\\\\ninput_data = processor(speech.numpy(), sampling_rate=sample_rate, return_tensors=\\\"pt\\\")\\\\noutput = model(input_data[\\\\'input_values\\\\'])\\\\n\\\\n# Find the most probable emotion\\\\nemotion_idx = torch.argmax(output[\\\"logits\\\"]).item()\\\\nemotion_categories = [\\\\'anger\\\\', \\\\'disgust\\\\', \\\\'enthusiasm\\\\', \\\\'fear\\\\', \\\\'happiness\\\\', \\\\'neutral\\\\', \\\\'sadness\\\\']\\\\nprediction = emotion_categories[emotion_idx]', 'api_call': \\\"Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'wav2vec2-xlsr-53-russian-emotion-recognition', 'api_call': \\\"Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')\\\", 'api_arguments': {'path': '/path/to/russian_audio_speech.wav', 'sampling_rate': 16000}, 'python_environment_requirements': [['torch', 'torchaudio', 'transformers', 'librosa', 'numpy']], 'example_code': \\\"result = predict('/path/to/russian_audio_speech.wav', 16000)\\\\nprint(result)\\\", 'performance': {'dataset': 'Russian Emotional Speech Dialogs', 'accuracy': '72%'}, 'description': 'A model trained to recognize emotions in Russian speech using wav2vec2. It can classify emotions such as anger, disgust, enthusiasm, fear, happiness, neutral, and sadness.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى برنامج يقوم بتوليد صور ثلاثية الأبعاد بدقة عالية لتخمين العمق في تقنية الرؤية الحاسوبية.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a depth estimation model.\\\\n3. Specify the model 'sayakpaul/glpn-nyu-finetuned-diode-221121-113853' to be loaded. This model is trained specifically for depth estimation tasks and can provide distance information for objects within an input image.\\\\n4. The created depth estimation model can be used to analyze the images captured by the robot's cameras in order to provide distance estimates for objects in the environment.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ndepth_estimator = pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\\\\ndepth_map = depth_estimator(image_path)\\\", 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221121-113853', 'api_call': \\\"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers==4.24.0', 'torch==1.12.1', 'tokenizers==0.13.2']], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3384, 'Mae': 0.2739, 'Rmse': 0.3959, 'Abs Rel': 0.323, 'Log Mae': 0.1148, 'Log Rmse': 0.1651, 'Delta1': 0.5576, 'Delta2': 0.8345, 'Delta3': 0.9398}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتوليد نصوص من وصف نصي لمفهوم في مجال معالجة اللغة الطبيعية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"pipeline('text-generation', model='distilgpt2')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which are 'pipeline' from transformers and 'set_seed' to control randomness.\\\\n2. Use the 'pipeline' function from transformers to create a text generation model using the 'distilgpt2' model.\\\\n3. The loaded model will be used for text generation, specifically creating a short topic summary for a news article.\\\\n4. Provide the necessary input, such as the beginning of the article or a prompt, and set the 'max_length' parameter to control the length of the generated summary.\\\\n5. The model will generate a summary based on the input provided to it.\\\", '<<<code>>>': 'from transformers import pipeline, set_seed\\\\nset_seed(42)\\\\ntext_generator = pipeline(\\\\'text-generation\\\\', model=\\\\'distilgpt2\\\\')\\\\nnews_article_prompt = \\\"Article snippet or prompt...\\\"\\\\ngenerated_summary = text_generator(news_article_prompt, max_length=50, num_return_sequences=1)\\\\nsummary = generated_summary[0][\\\\'generated_text\\\\']', 'api_call': \\\"pipeline('text-generation', model='distilgpt2')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilgpt2', 'api_call': \\\"pipeline('text-generation', model='distilgpt2')\\\", 'api_arguments': ['model'], 'python_environment_requirements': [['from transformers import pipeline, set_seed']], 'example_code': 'set_seed(42)\\\\ngenerator(Hello, I’m a language model, max_length=20, num_return_sequences=5)', 'performance': {'dataset': 'WikiText-103', 'accuracy': '21.100'}, 'description': 'DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. With 82 million parameters, it was developed using knowledge distillation and designed to be a faster, lighter version of GPT-2. It can be used for text generation, writing assistance, creative writing, entertainment, and more.' </s>\"\n"}
{"text": "<s>[INST]يقوم النموذج بتوليف نموذج تنبؤي لتشابه الجمل في مجال معالجة اللغة الطبيعية بشكل تلقائي من خلال تحويل الجملة الدخلية إلى جملة ناتجة بواسطة تدريب النموذج على مجموعة كبيرة من الجمل المتزانة.? [/INST]{'domain': 'Natural Language Processing Sentence Similarity', 'api_call': \\\"SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We start by importing SentenceTransformer from the sentence_transformers library.', 'code': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\n\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/paraphrase-multilingual-mpnet-base-v2\\\\')\\\\nreview1 = \\\"Example movie review 1.\\\"\\\\nreview2 = \\\"Example movie review 2.\\\"\\\\n\\\\n# Get embeddings\\\\nembedding1 = model.encode([review1])\\\\nembedding2 = model.encode([review2])\\\\n\\\\n# Calculate similarity\\\\nsimilarity = cosine_similarity(embedding1, embedding2)[0][0]', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Sentence Embeddings', 'api_name': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2', 'api_call': 'from sentence_transformers import SentenceTransformer\\\\nfrom sklearn.metrics.pairwise import cosine_similarity\\\\n\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/paraphrase-multilingual-mpnet-base-v2\\\\')\\\\nreview1 = \\\"Example movie review 1.\\\"\\\\nreview2 = \\\"Example movie review 2.\\\"\\\\n\\\\n# Get embeddings\\\\nembedding1 = model.encode([review1])\\\\nembedding2 = model.encode([review2])\\\\n\\\\n# Calculate similarity\\\\nsimilarity = cosine_similarity(embedding1, embedding2)[0][0]', 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Automated evaluation'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء صور ضوئية لمشاهد تصور الحيوانات في البرية استنادًا إلى النصوص المكتوبة بتقنية مرنة تقوم على دمج النصوص والصور. هل يمكنك مساعدتي في تنفيذ هذا المشروع؟? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion', torch_dtype=torch.float32)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. First, we import the necessary libraries, which include torch, autocast, and StableDiffusionPipeline.\\\\n2. We then use the from_pretrained method to load the pre-trained model 'hakurei/waifu-diffusion'. This model has been trained on high-quality anime images and is capable of generating images from text descriptions.\\\\n3. We provide a text prompt describing the desired painting, which includes 'Sunset', 'Ocean', 'Seagulls', and 'Palm Trees'.\\\\n4. We then use the generated image to inspire a painting.\\\", '<<<code>>>': \\\"import torch\\\\nfrom torch import autocast\\\\nfrom diffusers import StableDiffusionPipeline\\\\npipe = StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion', torch_dtype=torch.float32).to('cuda')\\\\nprompt = 'Sunset, Ocean, Seagulls, Palm Trees'\\\\nwith autocast(cuda):\\\\n    image = pipe(prompt, guidance_scale=6)[sample][0]\\\\nimage.save('inspired_painting.png')\\\", 'api_call': \\\"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion', torch_dtype=torch.float32)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion', torch_dtype=torch.float32)\\\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': [{'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}], 'example_code': \\\"import torch\\\\nfrom torch import autocast\\\\nfrom diffusers import StableDiffusionPipeline\\\\npipe = StableDiffusionPipeline.from_pretrained(\\\\n 'hakurei/waifu-diffusion',\\\\n torch_dtype=torch.float32\\\\n).to('cuda')\\\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\\\nwith autocast(cuda):\\\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\\\nimage.save(test.png)\\\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطبيق تقنيات معالجة اللغة الطبيعية على أنظمة الإجابة على الأسئلة. هل يمكنك مساعدتي في تطوير نموذج فعال للإجابة على الأسئلة باستخدام تقنيات المعالجة اللغوية الطبيعية؟? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. First, we import the required modules from the transformers library provided by Hugging Face: AutoModelForQuestionAnswering, AutoTokenizer, and pipeline.', 'code': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel_name = \\\\'deepset/roberta-base-squad2\\\\'\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=model_name, tokenizer=model_name)\\\\nQA_input = {\\\\n    \\\\'question\\\\': \\\"What is the capital of France?\\\",\\\\n    \\\\'context\\\\': \\\"France is a country in Western Europe, and its capital is Paris. It\\\\'s known for its cuisine, art, architecture, and cultural heritage.\\\"\\\\n}\\\\nres = nlp(QA_input)\\\\nanswer = res[\\\\'answer\\\\']', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'deepset/roberta-base-squad2', 'api_call': 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel_name = \\\\'deepset/roberta-base-squad2\\\\'\\\\nnlp = pipeline(\\\\'question-answering\\\\', model=model_name, tokenizer=model_name)\\\\nQA_input = {\\\\n    \\\\'question\\\\': \\\"What is the capital of France?\\\",\\\\n    \\\\'context\\\\': \\\"France is a country in Western Europe, and its capital is Paris. It\\\\'s known for its cuisine, art, architecture, and cultural heritage.\\\"\\\\n}\\\\nres = nlp(QA_input)\\\\nanswer = res[\\\\'answer\\\\']', 'api_arguments': {'model_name': 'deepset/roberta-base-squad2', 'tokenizer': 'deepset/roberta-base-squad2'}, 'python_environment_requirements': ['transformers'], 'example_code': {'code': \\\"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel_name = deepset/roberta-base-squad2\\\\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\\\\nQA_input = {\\\\n 'question': 'Why is model conversion important?',\\\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\\\n}\\\\nres = nlp(QA_input)\\\"}, 'performance': {'dataset': 'squad_v2', 'accuracy': {'exact': 79.87029394424324, 'f1': 82.91251169582613}}, 'description': \\\"This is the roberta-base model, fine-tuned using the SQuAD2.0 dataset for the task of Question Answering. It's been trained on question-answer pairs, including unanswerable questions.\\\"}} </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required pipeline class from the transformers package.\\\\n2. Create an instance of the text-generation pipeline using the 'sshleifer/tiny-gpt2' model. This is a small GPT-2 model specifically designed for faster inference and low-resource environments.\\\\n3. Use the generated pipeline object to create a birthday congratulation message based on the given prompt. You can customize the prompt or keep it simple like 'Happy Birthday!'.\\\\n4. The pipeline will generate a text sample with birthday congratulation text.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\\\\nresult = nlp('Happy Birthday!')\\\\nbirthday_message = result[0]['generated_text']\\\\nprint(birthday_message)\\\", 'api_call': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sshleifer/tiny-gpt2', 'api_call': \\\"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\\\", 'api_arguments': {'model': 'sshleifer/tiny-gpt2'}, 'python_environment_requirements': [{'huggingface_transformers': '>=4.0.0'}], 'example_code': \\\"from transformers import pipeline\\\\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\\\\nresult = nlp('Once upon a time')\\\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A tiny GPT-2 model for text generation, suitable for low-resource environments and faster inference. This model is part of the Hugging Face Transformers library and can be used for generating text given a prompt.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطوير نموذج لتصنيف الصور من خلال الرؤية الحاسوبية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries: PIL for image manipulation, transformers for using the ViTForImageClassification model, and requests for getting the image from a URL.\\\\n2. Load the pre-trained model 'nateraw/vit-age-classifier' using the from_pretrained method of the ViTForImageClassification class.\\\\n3. Load the ViTFeatureExtractor from the same 'nateraw/vit-age-classifier' model to preprocess and transform the input image.\\\\n4. Open the image to be classified and pass it through the feature extractor and the model.\\\\n5. The model will return an array of probabilities for each age group, which you can then use to predict the age of the visitor(s) in the image.\\\", '<<<code>>>': 'from PIL import Image\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\nimport requests\\\\nimage_url = \\\"https://path_to_image.jpg\\\" # Replace with the URL of the image containing visitor\\\\'s face\\\\nresponse = requests.get(image_url)\\\\nimage = Image.open(BytesIO(response.content))\\\\nmodel = ViTForImageClassification.from_pretrained(\\\\'nateraw/vit-age-classifier\\\\')\\\\ntransforms = ViTFeatureExtractor.from_pretrained(\\\\'nateraw/vit-age-classifier\\\\')\\\\ninputs = transforms(image, return_tensors=\\\\'pt\\\\')\\\\noutput = model(**inputs)\\\\nproba = output.logits.softmax(1)\\\\npreds = proba.argmax(1)', 'api_call': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nateraw/vit-age-classifier', 'api_call': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/vit-age-classifier'}, 'python_environment_requirements': [['requests', 'PIL', 'transformers']], 'example_code': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom io import BytesIO\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\n\\\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\\\nim = Image.open(BytesIO(r.content))\\\\n\\\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\\\n\\\\ninputs = transforms(im, return_tensors='pt')\\\\noutput = model(**inputs)\\\\n\\\\nproba = output.logits.softmax(1)\\\\npreds = proba.argmax(1)\\\", 'performance': {'dataset': 'fairface', 'accuracy': None}, 'description': \\\"A vision transformer finetuned to classify the age of a given person's face.\\\" </s>\"\n"}
{"text": "<s>[INST]تطوير تطبيق يقوم بإنشاء صور بدون شرط بشكل تلقائي بواسطة رؤية الحاسوب. يتطلب ذلك استخدام خوارزميات تقوم بتوليد صور غير مشروطة بناءً على البيانات المدخلة. يجب أن يتمكن النظام من توليد صور واقعية وبجودة عالية.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Install diffusers library required to load and use DDPMPipeline.\\\\n2. Import DDPMPipeline from the diffusers library provided by Hugging Face.\\\\n3. Use DDPMPipeline.from_pretrained method to create an instance of DDPMPipeline by loading the pretrained model 'ntrant7/sd-class-butterflies-32'.\\\\n4. Use the pipeline to generate an unconditional image of a butterfly. The resulting image can be accessed by referencing the images attribute of the generated object.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\\ngenerated_butterfly = pipeline().images[0]\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ntrant7/sd-class-butterflies-32', 'api_call': \\\"DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\", 'api_arguments': [], 'python_environment_requirements': [['diffusers']], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This model is a diffusion model for unconditional image generation of cute butterflies.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتحويل الصور المتطابقة في مجال الرؤية الحاسوبية، ونحتاج إلى توجيه النظام لتعلم تحويل الصور من نوع واحد إلى آخر.? [/INST]'<<<domain>>>': 'Computer Vision Image-to-Image', '<<<api_call>>>': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. Import necessary libraries and classes including Image from PIL, StableDiffusionControlNetPipeline, ControlNetModel, and UniPCMultistepScheduler from diffusers, torch from PyTorch, and MLSDdetector from controlnet_aux.\\\\n2. Load the M-LSD detector with the pretrained 'lllyasviel/ControlNet' checkpoint.\\\\n3. Load the original image using the provided URL or a local file, then apply the M-LSD detector to detect straight lines.\\\\n4. Load the ControlNetModel using the 'lllyasviel/sd-controlnet-mlsd' checkpoint, specifying torch.float16 as the data type.\\\\n5. Create a StableDiffusionControlNetPipeline using the 'runwayml/stable-diffusion-v1-5' checkpoint, and set the scheduler as UniPCMultistepScheduler.\\\\n6. Enable memory-efficient attention and CPU offloading for the pipeline.\\\\n7. Use the pipeline to straighten lines in the input image and save the output image.\\\", '<<<code>>>': \\\"from PIL import Image\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom controlnet_aux import MLSDdetector\\\\nfrom diffusers.utils import load_image\\\\n\\\\nmlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png')\\\\nimage = mlsd(image)\\\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd', torch_dtype=torch.float16)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\nimage = pipe('room', image, num_inference_steps=20).images[0]\\\\nimage.save('images/room_mlsd_out.png')\\\", 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lllyasviel/sd-controlnet-mlsd', 'api_call': \\\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\\\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': [{'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}], 'example_code': {'import': ['from PIL import Image', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'import torch', 'from controlnet_aux import MLSDdetector', 'from diffusers.utils import load_image'], 'setup': [\\\"mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)', 'image = mlsd(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)'], 'execution': ['pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', 'image = pipe(room, image, num_inference_steps=20).images[0]', \\\"image.save('images/room_mlsd_out.png')\\\"]}, 'performance': {'dataset': '600k edge-image, caption pairs generated from Places2', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.' </s>\"\n"}
{"text": "<s>[INST]عيِّن سؤالًا حول جدول معالجة اللغة الطبيعية باستخدام استخراج الإجابة لتوليد النص.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import necessary classes from transformers and torch packages. We will need AutoModelForCausalLM and AutoTokenizer for the model and tokenization.', 'code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-large')\\\\nmodel = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\\n\\\\ndef respond(user_input):\\\\n    input_ids = tokenizer.encode(user_input, return_tensors='pt')\\\\n    output = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.eos_token_id)\\\\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\\\\n    return response\\\", 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Transformers', 'functionality': 'Conversational', 'api_name': 'microsoft/DialoGPT-large', 'api_call': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-large')\\\\nmodel = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\\n\\\\ndef respond(user_input):\\\\n    input_ids = tokenizer.encode(user_input, return_tensors='pt')\\\\n    output = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.eos_token_id)\\\\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\\\\n    return response\\\", 'api_arguments': ['bot_input_ids', 'max_length', 'pad_token_id'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-large)\\\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-large)', 'performance': {'dataset': 'Reddit discussion thread', 'accuracy': 'Comparable to human response quality under a single-turn conversation Turing test'}, 'description': 'DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multi-turn conversations. The model is trained on 147M multi-turn dialogues from Reddit discussion threads.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتشغيل تطبيق التعرف الصوتي الآلي واستدعاء وظيفة التعرف الصوتي التلقائي لتحليل الصوت وتحويله إلى نص. حاول تحسين النطق وتوضيح الأوامر لزيادة دقة التعرف على الكلام.? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import the necessary classes and functions from the transformers library provided by Hugging Face, including the WhisperProcessor and WhisperForConditionalGeneration classes.\\\\n2. We load the pre-trained model 'openai/whisper-large-v2' using the from_pretrained method of the WhisperForConditionalGeneration class.\\\\n3. We use the WhisperProcessor to extract features from the audio sample, which is then provided as input to the pre-trained model for inference.\\\\n4. The model generates transcription predictions which are then decoded back into human-readable text using the WhisperProcessor.\\\\n5. The transcriptions can then be used for various applications such as subtitles, transcriptions, or even translations.\\\", '<<<code>>>': \\\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\\naudio_sample = YOUR_AUDIO_SAMPLE_HERE\\\\n# Replace YOUR_AUDIO_SAMPLE_HERE with the actual audio sample\\\\ninput_features = processor(audio_sample, return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.decode(predicted_ids, skip_special_tokens=True)\\\", 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai/whisper-large-v2', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", 'api_arguments': {'forced_decoder_ids': \\\"WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')\\\"}, 'python_environment_requirements': [['transformers', 'datasets']], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', \\\"processor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\\\", \\\"model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\\\", 'model.config.forced_decoder_ids = None', \\\"ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')\\\", \\\"sample = ds[0]['audio']\\\", \\\"input_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\\\", 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)'], 'performance': {'dataset': 'LibriSpeech test-clean', 'accuracy': 3.0003583080317573}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning.' </s>\"\n"}
{"text": "<s>[INST]أنا متخصص في مجال معالجة اللغات الطبيعية وتوليد النصوص. يمكنني مساعدتك في تحويل النصوص إلى تسجيل صوتي مسموع. هل ترغب في إرشادك بخطوات تنفيذ هذا العمل؟? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"pipeline('text-generation', model='facebook/opt-125m')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. Import the necessary libraries, which are 'pipeline' and 'set_seed' from transformers. \\\", 'code': 'from transformers import pipeline, set_seed\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'facebook/opt-125m\\\\')\\\\nprompt = \\\"Once upon a time in a magical forest,\\\"\\\\nset_seed(42)\\\\nstory = generator(prompt, do_sample=True, max_length=100)', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Transformers', 'functionality': 'Text Generation', 'api_name': 'facebook/opt-125m', 'api_call': 'from transformers import pipeline, set_seed\\\\ngenerator = pipeline(\\\\'text-generation\\\\', model=\\\\'facebook/opt-125m\\\\')\\\\nprompt = \\\"Once upon a time in a magical forest,\\\"\\\\nset_seed(42)\\\\nstory = generator(prompt, do_sample=True, max_length=100)', 'api_arguments': {'do_sample': 'True'}, 'python_environment_requirements': 'from transformers import pipeline, set_seed', 'example_code': \\\"generator(Hello, I'm am conscious and)\\\", 'performance': {'dataset': 'Various', 'accuracy': 'Roughly matches GPT-3 performance'}, 'description': 'OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. It was predominantly pretrained with English text, but a small amount of non-English data is present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective. OPT can be used for prompting for evaluation of downstream tasks as well as text generation.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام لتصنيف النصوص بمجال معالجة اللغة الطبيعية والوثائق المتعددة الوسائط، ونرغب في تنبؤ تصنيف الوثائق النصية بناءً على بيانات التدريب المتاحة.? [/INST]'<<<domain>>>': 'Multimodal Document Question Answer', '<<<api_call>>>': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. This includes the AutoModelForDocumentQuestionAnswering and AutoTokenizer for the pre-processing of text input.\\\\n2. Load the pre-trained model 'impira/layoutlm-invoices' using the from_pretrained method of the AutoModelForDocumentQuestionAnswering class. This model is specifically designed for question answering tasks on invoices and other documents.\\\\n3. Now, define a function for the chatbot that takes the question and the context from the invoice as input parameters.\\\\n4. Inside the function, use the tokenizer to preprocess the input text and then feed it into the model, which will generate an answer.\\\\n5. Return the answer text and incorporate it into your chatbot interface.\\\", '<<<code>>>': 'from transformers import AutoModelForDocumentQuestionAnswering, AutoTokenizer\\\\nmodel = AutoModelForDocumentQuestionAnswering.from_pretrained(\\\\'impira/layoutlm-invoices\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'impira/layoutlm-invoices\\\\')\\\\n\\\\ndef answer_invoice_question(question, context):\\\\n    inputs = tokenizer(question, context, return_tensors=\\\"pt\\\")\\\\n    outputs = model(**inputs)\\\\n    answer_start = outputs.start_logits.argmax(-1).item()\\\\n    answer_end = outputs.end_logits.argmax(-1).item() + 1\\\\n    answer = context[answer_start:answer_end]\\\\n    return answer', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'layoutlm-invoices', 'api_call': \\\"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\\\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers'], 'example_code': \\\"nlp(question='What is the total amount?', context='your_invoice_text')\\\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>\"\n"}
{"text": "<s>[INST]يرغب المستخدم في تطوير نموذج للتعرف التلقائي على الكلام الصوتي الذي يمكن استخدامه لتحويل الكلام إلى نص. هل يمكن توجيه المستخدم بشأن كيفية بدء هذا العمل في مجال تقدير الكلام الصوتي التلقائي؟? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"Wav2Vec2ForCTC.from_pretrained('vitouphy/wav2vec2-xls-r-300m-phoneme')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the Wav2Vec2ForCTC class from the transformers library provided by Hugging Face.\\\\n2. Use the from_pretrained method of the Wav2Vec2ForCTC class to load the pre-trained model 'vitouphy/wav2vec2-xls-r-300m-phoneme'. This model is designed for Automatic Speech Recognition (ASR) tasks.\\\\n3. Process the audio file to obtain the input values for the model.\\\\n4. Feed the input values into the model and obtain the output logits.\\\\n5. Decode the logits into text to obtain the transcription of the audio.\\\", '<<<code>>>': 'from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\\\nimport soundfile as sf\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\\\'vitouphy/wav2vec2-xls-r-300m-phoneme\\\\')\\\\nprocessor = Wav2Vec2Processor.from_pretrained(\\\\'vitouphy/wav2vec2-xls-r-300m-phoneme\\\\')\\\\naudio_input, _ = sf.read(\\\\'path_to_your_audio_file.wav\\\\') # Replace with your audio file path\\\\ninput_values = processor(audio_input, return_tensors=\\\"pt\\\", sampling_rate=16000).input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.decode(predicted_ids[0])', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('vitouphy/wav2vec2-xls-r-300m-phoneme')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'vitouphy/wav2vec2-xls-r-300m-phoneme', 'api_call': \\\"Wav2Vec2ForCTC.from_pretrained('vitouphy/wav2vec2-xls-r-300m-phoneme')\\\", 'api_arguments': ['input_values'], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'None', 'accuracy': {'Loss': 0.3327, 'Cer': 0.1332}}, 'description': 'This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the None dataset. It is designed for Automatic Speech Recognition tasks.' </s>\"\n"}
{"text": "<s>[INST]قم بتشغيل تسجيل صوتي لجملة قصيرة تتحدث عن كيفية استخدام تقنيات الرؤية الحاسوبية في اكتشاف الكائنات، ثم قم بتطبيق محرك الصوت الآلي لتحويل هذا التسجيل إلى تسجيل صوتي آخر يحتوي على نفس الجملة بصوت مختلف ومشابه للصوت الأصلي. قدم الصوتين للمقارنة.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary class ViltForQuestionAnswering from the transformers package.\\\\n2. We create an instance of the VILT model for Visual Question Answering by using the from_pretrained method and loading the pre-trained model 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering'.\\\\n3. The pre-trained model is designed to receive both image data and a textual question about that image as its input. It then generates an output based on the analysis of both modalities.\\\\n4. We can integrate the model into an app to describe images and answer questions related to them, helping visually impaired individuals better understand the visual content.\\\", '<<<code>>>': \\\"from transformers import ViltForQuestionAnswering\\\\nmodel = ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\\\\n# Prepare image and question inputs (image_path and question_text)\\\\nresult = model(image=image_path, question=question_text)\\\", 'api_call': \\\"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering', 'api_call': \\\"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\\\", 'api_arguments': {'image': 'path/to/image/file', 'question': 'your_question'}, 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random model for Visual Question Answering using the VILT framework.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نظام تصنيف الصوتيات لتحديد نوعية الصوتيات بدقة وفعالية. يتطلب ذلك استخدام تقنيات متقدمة في التعرف على الصوتيات وتصنيفها بناءً على خصائصها الموجودة. يجب أن يتضمن النظام قاعدة بيانات واسعة لتدريبه على تعرف الصوتيات المختلفة بدقة.? [/INST]{'domain': 'Audio Classification', 'api_call': \\\"EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the required libraries, which are 'EncoderClassifier' from speechbrain.pretrained and 'torchaudio'.\\\", 'code': \\\"import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\n\\\\nlanguage_id = EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\\nsignal = language_id.load_audio('https://omniglot.com/soundfiles/udhr/udhr_th.mp3')\\\\nprediction = language_id.classify_batch(signal)\\\\nprint(prediction)\\\", 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Spoken Language Identification', 'api_name': 'TalTechNLP/voxlingua107-epaca-tdnn', 'api_call': \\\"import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\n\\\\nlanguage_id = EncoderClassifier.from_hparams(source='TalTechNLP/voxlingua107-epaca-tdnn')\\\\nsignal = language_id.load_audio('https://omniglot.com/soundfiles/udhr/udhr_th.mp3')\\\\nprediction = language_id.classify_batch(signal)\\\\nprint(prediction)\\\", 'api_arguments': ['signal'], 'python_environment_requirements': ['speechbrain', 'torchaudio'], 'example_code': 'import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\nlanguage_id = EncoderClassifier.from_hparams(source=TalTechNLP/voxlingua107-epaca-tdnn, savedir=tmp)\\\\nsignal = language_id.load_audio(https://omniglot.com/soundfiles/udhr/udhr_th.mp3)\\\\nprediction = language_id.classify_batch(signal)\\\\nprint(prediction)', 'performance': {'dataset': 'VoxLingua107', 'accuracy': '93%'}, 'description': 'This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain. The model uses the ECAPA-TDNN architecture that has previously been used for speaker recognition. The model can classify a speech utterance according to the language spoken. It covers 107 different languages.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطبيق تقنيات معالجة اللغات الطبيعية لتصنيف النصوص. هل يمكنك مساعدتي في إنشاء نص توجيهي يصف كيفية تصنيف النصوص في مجال معالجة اللغات الطبيعية؟? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"pipeline('text-classification', model='finiteautomata/bertweet-base-sentiment-analysis')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the 'pipeline' function from the transformers package.\\\\n2. Use the 'pipeline' function to load the pre-trained sentiment analysis model 'finiteautomata/bertweet-base-sentiment-analysis'.\\\\n3. Apply the sentiment analysis model to analyze the given review text.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nnlp = pipeline(\\\\'text-classification\\\\', model=\\\\'finiteautomata/bertweet-base-sentiment-analysis\\\\')\\\\nreview_text = \\\"The room was great, hosts were friendly :) but the shared bathrooms were dirty and the Wi-Fi was unreliable.\\\"\\\\nresult = nlp(review_text)', 'api_call': \\\"pipeline('text-classification', model='finiteautomata/bertweet-base-sentiment-analysis')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'finiteautomata/bertweet-base-sentiment-analysis', 'api_call': \\\"pipeline('text-classification', model='finiteautomata/bertweet-base-sentiment-analysis')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['Transformers'], 'example_code': \\\"from transformers import pipeline\\\\nnlp = pipeline('text-classification', model='finiteautomata/bertweet-base-sentiment-analysis')\\\\nresult = nlp('I love this movie!')\\\", 'performance': {'dataset': 'SemEval 2017', 'accuracy': None}, 'description': 'Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is BERTweet, a RoBERTa model trained on English tweets. Uses POS, NEG, NEU labels.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطبيق تقنية تقسيم الصور على الصور الخاصة بالحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the required libraries including MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation from the transformers package, Image from the PIL package, and requests for retrieving the image from a URL.\\\\n2. Load the feature_extractor and model using the 'facebook/maskformer-swin-base-coco' pre-trained model. This model is trained for panoptic segmentation tasks on the COCO dataset.\\\\n3. Open the image from the provided URL using the Image.open method from the PIL package, and requests.get to download the image.\\\\n4. Extract the features from the image using the feature_extractor and provide those as inputs to the model.\\\\n5. Obtain the segmentation results from the model by post-processing the outputs using the feature_extractor.post_process_panoptic_segmentation function.\\\\n6. The panoptic segmentation map can be used to identify the boundaries and categories of objects present in the input image.\\\", '<<<code>>>': \\\"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\\nurl = 'https://example.com/image.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result['segmentation']\\\", 'api_call': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/maskformer-swin-base-coco', 'api_call': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\", 'api_arguments': ['image'], 'python_environment_requirements': [['transformers', 'PIL', 'requests']], 'example_code': \\\"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\\\n\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\noutputs = model(**inputs)\\\\n\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\n\\\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result['segmentation']\\\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.? [/INST]'<<<domain>>>': 'Multimodal Text-to-Image', '<<<api_call>>>': \\\"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion', torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face', '<<<explanation>>>': \\\"1. We start by importing the necessary packages, including StableDiffusionPipeline for the text-to-image model and torch for handling tensors.\\\\n2. We instantiate the model using the from_pretrained method with the 'nitrosocke/nitro-diffusion' model.\\\\n3. We then create a prompt describing the main character as a space pirate with a pet dragon. This prompt will be used by the model to generate an appropriate image.\\\\n4. We pass the prompt to the model and obtain an image as the output.\\\\n5. The generated image can then be saved as a file for further use in your children's book.\\\", '<<<code>>>': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = \\\\'nitrosocke/nitro-diffusion\\\\'\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\nprompt = \\\"Space pirate with a pet dragon\\\"\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(\\\"space_pirate_with_pet_dragon.png\\\")', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion', torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face', 'api_data': {'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \\\"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion', torch_dtype=torch.float16)\\\", 'api_arguments': ['prompt'], 'python_environment_requirements': [['torch', 'diffusers']], 'example_code': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = nitrosocke/nitro-diffusion\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = archer arcane style magical princess with golden hair\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتصنيف الصور في مجال رؤية الحاسوب، ونريد استخدام بعض مجموعات البيانات المتاحة لتحقيق ذلك.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which include AutoFeatureExtractor and RegNetForImageClassification from transformers, torch, and load_dataset from datasets.\\\\n2. Load a dataset containing a collection of images, which can be obtained from sources like Hugging Face's Dataset Hub.\\\\n3. Preprocess the input image data using AutoFeatureExtractor, which helps in preparing the image data to be fed to the RegNet model.\\\\n4. Load the pre-trained image classification model 'zuppif/regnet-y-040' using RegNetForImageClassification.from_pretrained(). This model is trained to categorize images based on their content.\\\\n5. Use the model to classify the images and retrieve the predicted label for each image.\\\", '<<<code>>>': \\\"from transformers import AutoFeatureExtractor, RegNetForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset('<your-dataset>')  # replace with your dataset\\\\nimage = dataset['test']['image'][0]  # sample image from the dataset\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('zuppif/regnet-y-040')\\\\nmodel = RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\\ninputs = feature_extractor(image, return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/regnet-y-008', 'api_call': \\\"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'zuppif/regnet-y-040'}, 'python_environment_requirements': [{'transformers': 'AutoFeatureExtractor, RegNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}], 'example_code': 'from transformers import AutoFeatureExtractor, RegNetForImageClassification\\\\nimport torch\\\\nfrom datasets import load_dataset\\\\ndataset = load_dataset(huggingface/cats-image)\\\\nimage = dataset[test][image][0]\\\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\\\ninputs = feature_extractor(image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n... logits = model(**inputs).logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.' </s>\"\n"}
{"text": "<s>[INST]يتبع الخطوات التالية لتطوير تطبيق في مجال معالجة اللغة الطبيعية لتوليد النصوص: ١. ابدأ بتحميل وتثبيت مكتبة معالجة اللغة الطبيعية. ٢. قم بتدريب نموذج اللغة على مجموعة بيانات واسعة. ٣. حاول توليد جمل جديدة باستخدام النموذج المدرب. ٤. قم بتحويل النص إلى كلام باستخدام برنامج تحويل النصوص إلى كلام. ٥. قم بتحسين الأداء وجودة النص المولد باستخدام تقنيات معالجة اللغة الطبيعية المتقدمة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('bigscience/bloomz-560m')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library provided by Hugging Face, including AutoModelForCausalLM and AutoTokenizer.\\\\n2. Specify the model checkpoint 'bigscience/bloomz-560m' and use it to load the tokenizer and the model.\\\\n3. Use the tokenizer to convert the input French text into tensors.\\\\n4. Use the model to generate a response given the input tensors.\\\\n5. Decode the output tensors to obtain the translated text in English.\\\\n6. This process can be performed for each message in the chat in real-time.\\\", '<<<code>>>': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ncheckpoint = \\\\'bigscience/bloomz-560m\\\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\\\n\\\\ndef translate_french_to_english(text):\\\\n    inputs = tokenizer.encode(\\\"Translate to English: \\\" + text, return_tensors=\\\\'pt\\\\')\\\\n    outputs = model.generate(inputs)\\\\n    return tokenizer.decode(outputs[0])\\\\n\\\\nenglish_text = translate_french_to_english(\\\"Je t\\\\'aime.\\\")', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('bigscience/bloomz-560m')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'bigscience/bloomz-560m', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('bigscience/bloomz-560m')\\\", 'api_arguments': {'checkpoint': 'bigscience/bloomz-560m', 'inputs': 'Translate to English: Je t’aime.'}, 'python_environment_requirements': [['transformers', 'accelerate', 'bitsandbytes']], 'example_code': {'CPU': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\ncheckpoint = bigscience/bloomz-560m\\\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\\\ninputs = tokenizer.encode(Translate to English: Je t’aime., return_tensors=pt)\\\\noutputs = model.generate(inputs)\\\\nprint(tokenizer.decode(outputs[0]))'}, 'performance': {'dataset': 'bigscience/xP3', 'accuracy': {'Winogrande XL (xl) validation set': 52.41, 'XWinograd (en) test set': 51.01, 'XWinograd (fr) test set': 51.81, 'XWinograd (jp) test set': 52.03, 'XWinograd (pt) test set': 53.99, 'XWinograd (ru) test set': 53.97, 'XWinograd (zh) test set': 54.76, 'ANLI (r1) validation set': 33.4, 'ANLI (r2) validation set': 33.4, 'ANLI (r3) validation set': 33.5}}, 'description': 'BLOOMZ & mT0 are a family of models capable of following human instructions in dozens of languages zero-shot. Finetuned on the crosslingual task mixture (xP3), these models can generalize to unseen tasks & languages. Useful for tasks expressed in natural language, such as translation, summarization, and question answering.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتطوير نماذج لتوليف صور رقمية بدون شروط في مجال رؤية الحاسوب، ونرغب في تحسين جودة الصور المُنتجة باستخدام تقنيات التوليف الصور المتقدمة.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the DDPMPipeline class from the diffusers library.\\\\n2. Use the from_pretrained method of the DDPMPipeline class to load the pre-trained model 'ocariz/universe_1400'. This model is specifically designed for generating images of the universe unconditionally.\\\\n3. Invoke the loaded model to generate an image of the universe.\\\\n4. The generated image can be used as a background for different scenes in the space-themed VR game.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\\nimage = pipeline().images[0]\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ocariz/universe_1400', 'api_call': \\\"DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\", 'api_arguments': '', 'python_environment_requirements': ['diffusers'], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ocariz/universe_1400')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is a diffusion model for unconditional image generation of the universe trained for 1400 epochs.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نادٍ لطلاب مجال معالجة اللغة الطبيعية لكتابة النصوص وتوليد النصوص. يجتمع الأعضاء كل أسبوعين لممارسة توليد النصوص الطبيعية. يمكن لأعضاء النادي أداء التحديات ومهام توليد النصوص مع توافر أدوات لمساعدتهم على اللعب ضد وكيل التعلم الآلي في تقدير العمق في الرؤية الحاسوبية.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To start, import the pipeline function from the transformers library provided by Hugging Face. \\\\n2. Then, use the pipeline function to create a text-to-text generation model with the specific model 'pszemraj/flan-t5-large-grammar-synthesis'. This is a pre-trained model for grammar correction.\\\\n3. Pass any raw text with grammar mistakes to the created model, which will detect and correct the grammatical errors in the text.\\\\n4. The corrected version of the text will be generated, which can then be used for various purposes like email templates, articles, or any other text-based content that requires correct grammar.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntext = 'I are goint to the store to by milks.'\\\\ngrammar_corrector = pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\\ncorrected_text = grammar_corrector(text)\\\", 'api_call': \\\"pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pszemraj/flan-t5-large-grammar-synthesis', 'api_call': \\\"pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\", 'api_arguments': ['raw_text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\n\\\\ncorrector = pipeline(\\\\n 'text2text-generation',\\\\n 'pszemraj/flan-t5-large-grammar-synthesis',\\\\n )\\\\n\\\\nraw_text = 'i can has cheezburger'\\\\nresults = corrector(raw_text)\\\\nprint(results)\\\", 'performance': {'dataset': 'jfleg', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of google/flan-t5-large for grammar correction on an expanded version of the JFLEG dataset.' </s>\"\n"}
{"text": "<s>[INST] يمكنني مساعدتك في تحقيق ذلك.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers package. This includes DPRQuestionEncoder and DPRQuestionEncoderTokenizer.\\\\n2. We then use the from_pretrained method to load the pre-trained model 'facebook/dpr-question_encoder-single-nq-base'. This model is designed to encode questions effectively so that it can be used for high-quality open-domain question answering tasks.\\\\n3. With this model, you can now use the tokenizer to tokenize any dog-related question into input_ids and feed those ids to the model to get embeddings for that question.\\\\n4. These embeddings can then be used to find and rank relevant documents, passages or answers for the given question.\\\", '<<<code>>>': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\\\\'facebook/dpr-question_encoder-single-nq-base\\\\')\\\\nmodel = DPRQuestionEncoder.from_pretrained(\\\\'facebook/dpr-question_encoder-single-nq-base\\\\')\\\\ninput_ids = tokenizer(\\\"What is the average lifespan of a dog?\\\", return_tensors=\\\\'pt\\\\')[\\\\'input_ids\\\\']\\\\nembeddings = model(input_ids).pooler_output', 'api_call': \\\"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \\\"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\\\", 'api_arguments': ['input_ids'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نص يصف مشهد للحيوانات في البرية باستخدام تقنيات معالجة اللغة الطبيعية. هل يمكنك مساعدتي في هذا المشروع؟? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the required libraries: AutoModelForCausalLM and AutoTokenizer from the transformers library provided by Hugging Face.\\\\n2. Load the DialoGPT-large model using the AutoModelForCausalLM.from_pretrained() function with the specified model name 'microsoft/DialoGPT-large'. This is a pretrained dialogue response generation model trained on multi-turn conversations, suitable for creating a chatbot.\\\\n3. Create a tokenizer using the AutoTokenizer.from_pretrained() function, which will convert the text into tokens that the model can understand.\\\\n4. With the loaded model and tokenizer, you can now process user messages and generate appropriate responses for your customer support chatbot.\\\", '<<<code>>>': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-large')\\\\nmodel = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/DialoGPT-large', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_arguments': ['bot_input_ids', 'max_length', 'pad_token_id'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-large)\\\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-large)', 'performance': {'dataset': 'Reddit discussion thread', 'accuracy': 'Comparable to human response quality under a single-turn conversation Turing test'}, 'description': 'DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multi-turn conversations. The model is trained on 147M multi-turn dialogues from Reddit discussion threads.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا ببناء نموذج للتصنيف الجدولي لتصنيف البيانات، ونريد التنبؤ باستخدام بعض مجموعات البيانات المتاحة في تصنيف الجداول والكشف عن الشوائب.? [/INST]{'domain': 'Tabular Tabular Regression Anomaly Detection', 'api_call': \\\"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\", 'api_provider': 'Keras', 'explanation': '1. Import the necessary libraries and components, such as TensorFlow, Keras, and the TFAutoModelForSequenceClassification class.', 'code': \\\"import tensorflow as tf\\\\nfrom tensorflow.keras import layers\\\\nfrom transformers import TFAutoModelForSequenceClassification\\\\n\\\\nmodel = TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\\n\\\\n# Prepare your data here and preprocess it as needed.\\\\nx_train, x_test = None, None  # Replace with your actual data\\\\n\\\\n# Train the model on your data\\\\nhistory = model.fit(x_train, x_train, epochs=100, batch_size=128, validation_data=(x_test, x_test))\\\\n\\\\n# Detect anomalies by computing the difference between reconstructed data and original data\\\\nreconstructed_data = model.predict(x_test)\\\\ndiff = tf.abs(x_test - reconstructed_data)\\\\n\\\\n# Set a threshold to identify anomalies\\\\nthreshold = 0.1\\\\nanomalies = diff > threshold\\\", 'api_data': {'domain': 'Tabular Tabular Regression', 'framework': 'Keras', 'functionality': 'anomaly-detection', 'api_name': 'keras-io/timeseries-anomaly-detection', 'api_call': \\\"import tensorflow as tf\\\\nfrom tensorflow.keras import layers\\\\nfrom transformers import TFAutoModelForSequenceClassification\\\\n\\\\nmodel = TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\\\\n\\\\n# Prepare your data here and preprocess it as needed.\\\\nx_train, x_test = None, None  # Replace with your actual data\\\\n\\\\n# Train the model on your data\\\\nhistory = model.fit(x_train, x_train, epochs=100, batch_size=128, validation_data=(x_test, x_test))\\\\n\\\\n# Detect anomalies by computing the difference between reconstructed data and original data\\\\nreconstructed_data = model.predict(x_test)\\\\ndiff = tf.abs(x_test - reconstructed_data)\\\\n\\\\n# Set a threshold to identify anomalies\\\\nthreshold = 0.1\\\\nanomalies = diff > threshold\\\", 'api_arguments': {'optimizer': {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'training_precision': 'float32'}, 'python_environment_requirements': ['tensorflow', 'keras'], 'example_code': '', 'performance': {'dataset': 'Numenta Anomaly Benchmark(NAB)', 'accuracy': {'Train Loss': 0.006, 'Validation Loss': 0.008}}, 'description': 'This script demonstrates how you can use a reconstruction convolutional autoencoder model to detect anomalies in timeseries data. We will use the Numenta Anomaly Benchmark(NAB) dataset. It provides artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered, timestamped, single-valued metrics.'}} </s>\"\n"}
{"text": "<s>[INST]نقوم بإنشاء تطبيق صوتي لتحويل النص إلى كلام لمساعدة مستخدمينا في تحويل النصوص إلى كلام بشكل سريع وفعال.? [/INST]'<<<domain>>>': 'Audio Text-to-Speech', '<<<api_call>>>': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', '<<<api_provider>>>': 'ESPnet', '<<<explanation>>>': '1. Install Hugging Face and ESPnet packages, as they are required for using the API.\\\\n2. In the terminal, run the provided shell script (run.sh) with the specified command-line arguments. Set skip_data_prep to false as we need to prepare the input text for TTS, and set skip_train to true because we are not training the model.\\\\n3. Download and use the \\\\'mio/amadeus\\\\' model for text-to-speech conversion. This model is trained using the amadeus recipe in ESPnet and capable of generating high-quality speech from text.\\\\n4. Provide the given text to the TTS system as the input.\\\\n<<<example_input>>>: \\\"Welcome to the grand opening of our new store! Enjoy unbeatable prices and amazing discounts.\\\"', '<<<code>>>': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus --input_text \\\"Welcome to the grand opening of our new store! Enjoy unbeatable prices and amazing discounts.\\\"', 'api_call': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'provider': 'ESPnet', 'api_data': {'domain': 'Audio Text-to-Speech', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kazusam/kt', 'api_call': './run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus', 'api_arguments': ['input_text'], 'python_environment_requirements': [['Hugging Face', 'ESPnet']], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': 'amadeus', 'accuracy': 'Not provided'}, 'description': 'An ESPnet2 TTS model trained by mio using amadeus recipe in espnet.' </s>\"\n"}
{"text": "<s>[INST]إنشاء فيديو يصور الحيوانات في البرية من وصف نصي، باستخدام تقنيات معالجة اللغات الطبيعية. هل يمكنك توفير المساعدة في تنفيذ هذه المهمة؟? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We begin by importing the necessary components from the transformers library, which includes the MarianMTModel for translation and MarianTokenizer for tokenization.\\\\n2. Next, we instantiate the model with the OPUS pre-trained Romance languages to English model 'Helsinki-NLP/opus-mt-ROMANCE-en'. This model can handle multiple source languages such as French, Spanish, Italian, etc.\\\\n3. We tokenize the input document into subwords using the tokenizer with correct source language.\\\\n4. Finally, we use the model to translate the document into English, and then decode the output into an understandable format.\\\", '<<<code>>>': \\\"from transformers import MarianMTModel, MarianTokenizer\\\\nmodel_name = 'Helsinki-NLP/opus-mt-ROMANCE-en'\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\n\\\\n# replace 'source_text' with the text in Italian, French, or Spanish\\\\nsource_text = 'Text to be translated'\\\\ntranslated = model.generate(**tokenizer(source_text, return_tensors='pt', padding=True))\\\\ndecoded_translation = tokenizer.decode(translated[0], skip_special_tokens=True)\\\", 'api_call': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-ROMANCE-en', 'api_call': \\\"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\\\", 'api_arguments': ['source languages', 'target languages'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': '', 'performance': {'dataset': 'opus', 'accuracy': {'BLEU': 62.2, 'chr-F': 0.75}}, 'description': 'A model for translating Romance languages to English, trained on the OPUS dataset. It supports multiple source languages such as French, Spanish, Portuguese, Italian, and Romanian, among others. The model is based on the transformer architecture and uses normalization and SentencePiece for pre-processing.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتطبيق تقنية الرؤية الحاسوبية لتصنيف الصور، هل يمكنك مساعدتي في توليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية؟? [/INST]'<<<domain>>>': 'Computer Vision Video Classification', '<<<api_call>>>': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, such as decord, torch, numpy, and transformers.\\\\n2. Initialize the feature extractor and the VideoMAEForVideoClassification model with a pretrained model, 'nateraw/videomae-base-finetuned-ucf101'.\\\\n3. Load the video file using the VideoReader from the decord library.\\\\n4. Define and call a function to sample frame indices from the video.\\\\n5. Use the feature extractor to process the video frames and convert them into a suitable format for the model.\\\\n6. Feed the processed frames into the pretrained VideoMAEForVideoClassification model.\\\\n7. Extract the output logits and identify the action with the highest prediction score.\\\", '<<<code>>>': \\\"from decord import VideoReader, cpu\\\\nimport torch\\\\nimport numpy as np\\\\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\\\nfrom huggingface_hub import hf_hub_download\\\\n\\\\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\\\\n    converted_len = int(clip_len * frame_sample_rate)\\\\n    end_idx = np.random.randint(converted_len, seg_len)\\\\n    start_idx = end_idx - converted_len\\\\n    indices = np.linspace(start_idx, end_idx, num=clip_len)\\\\n    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\\\\n    return indices\\\\n\\\\nfile_path = 'path/to/video_file.mp4'\\\\n\\\\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\\\\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\\\\nvideo = videoreader.get_batch(indices).asnumpy()\\\\n\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\\n\\\\ninputs = feature_extractor(list(video), return_tensors='pt')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\n\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])\\\", 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'videomae-base-finetuned-ucf101', 'api_call': \\\"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/videomae-base-finetuned-ucf101'}, 'python_environment_requirements': [['transformers', 'decord', 'huggingface_hub']], 'example_code': 'from decord import VideoReader, cpu\\\\nimport torch\\\\nimport numpy as np\\\\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\\\nfrom huggingface_hub import hf_hub_download\\\\nnp.random.seed(0)\\\\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\\\\n converted_len = int(clip_len * frame_sample_rate)\\\\n end_idx = np.random.randint(converted_len, seg_len)\\\\n start_idx = end_idx - converted_len\\\\n indices = np.linspace(start_idx, end_idx, num=clip_len)\\\\n indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\\\\n return indices\\\\nfile_path = hf_hub_download(\\\\n repo_id=nateraw/dino-clips, filename=archery.mp4, repo_type=space\\\\n)\\\\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\\\\nvideoreader.seek(0)\\\\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\\\\nvideo = videoreader.get_batch(indices).asnumpy()\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\\\\ninputs = feature_extractor(list(video), return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_label = logits.argmax(-1).item()\\\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'UCF101', 'accuracy': 0.758209764957428}, 'description': 'VideoMAE Base model fine tuned on UCF101 for Video Action Recognition' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتحديد عمق الرؤية الحاسوبية، ونرغب في تحسين أداء النموذج في تحويل الصور الواردة إلى أدق تقدير للعمق.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"etermine the depth of objects in an image for an autonomous robot to identify and navigate better in its environment.\\\\n###Output: <<<domain>>>: Computer Vision Depth Estimation\\\\n<<<api_call>>>: DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\\\\n<<<api_provider>>>: Hugging Face Transformers\\\\n<<<explanation>>>(1)We need to import 'DPTForDepthEstimation' class from transformers package.\\\\n(2)Next, we will load the pre-trained 'hf-tiny-model-private/tiny-random-DPTForDepthEstimation' model.\\\\n(3)After loading the model, we input the image file to the model. The model then outputs a depth map of the image that can help an autonomous robot to better understand the depth of the objects in its environment.\\\", '<<<code>>>': \\\"from transformers import DPTForDepthEstimation\\\\nimport cv2\\\\nmodel = DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\\\\nimage = cv2.imread('path/to/image.jpg')\\\\ndepth_map = model(image)\\\", 'api_call': \\\"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'tiny-random-DPTForDepthEstimation', 'api_call': \\\"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random DPT model for depth estimation using Hugging Face Transformers library.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نادي طلابي لعلوم معالجة اللغة الطبيعية، حيث يجتمع الأعضاء كل أسبوعين للتدريب على إنشاء نصوص بشكل توليدي. يجب تزويدهم بأداة لتوليد النصوص ضد وكيل التعلم الآلي.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-xl')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. We import the necessary classes from the transformers package. This includes T5Tokenizer for tokenizing the text input and T5ForConditionalGeneration for the text generation model.\\\\n2. We load the pre-trained model \\\\'google/flan-t5-xl\\\\', which has been fine-tuned on more than 1000 tasks covering multiple languages, making it suitable for a multilingual chatbot.\\\\n3. We then use this tokenizer and model to process and generate responses to questions and tasks. The model can handle translation, arithmetic problems, and generative storytelling tasks, among others.\\\\n4. The input_text variable will contain your chatbot\\\\'s input like \\\"translate English to German: How old are you?\\\" or \\\"calculate 4 * 4\\\". The model will decode the output and return the appropriate response.', '<<<code>>>': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(\\\\'google/flan-t5-xl\\\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'google/flan-t5-xl\\\\')\\\\ninput_text = \\\"your_multilingual_chatbot_input\\\"\\\\ninput_ids = tokenizer(input_text, return_tensors=\\\\'pt\\\\').input_ids\\\\noutputs = model.generate(input_ids)\\\\nresponse = tokenizer.decode(outputs[0])', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-xl')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/flan-t5-xl', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/flan-t5-xl')\\\", 'api_arguments': ['input_text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained('google/flan-t5-xl')\\\\nmodel = T5ForConditionalGeneration.from_pretrained('google/flan-t5-xl')\\\\ninput_text = 'translate English to German: How old are you?'\\\\ninput_ids = tokenizer(input_text, return_tensors='pt').input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))\\\", 'performance': {'dataset': [{'name': 'MMLU', 'accuracy': '75.2%'}]}, 'description': 'FLAN-T5 XL is a large-scale language model fine-tuned on more than 1000 tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot and few-shot NLP tasks, such as reasoning, question answering, and understanding the limitations of current large language models.' </s>\"\n"}
{"text": "<s>[INST]يجب تصميم نماذج اختبار صوتية متنعمة ذات دقة عالية لتحسين الأداء في التقدير عمق الرؤية الحاسوبية، يرجى توفير تقديرات عمق سوبسات الكلام لتحسين النتائج.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries – torch and transformers. In particular, we need the AutoModel class from transformers.\\\\n2. Load the pre-trained model using the from_pretrained() method with the provided model name 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'. This model has been fine-tuned for depth estimation tasks, which is useful for a robot navigating through a room.\\\\n3. Acquire the image data captured by the robot's camera, preprocess it, and convert it into a suitable format or tensor for the model.\\\\n4. Use the loaded model to predict depth information by passing the preprocessed image data as input.\\\\n5. Based on the obtained depth information, the robot can process the data and take appropriate actions, like avoiding obstacles and successfully navigating through the room.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nfrom PIL import Image\\\\nimport torch\\\\n\\\\nimage = Image.open('room_image.jpg')\\\\n# replace 'room_image.jpg' with the path to your image\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\\\\ninput_tensor = preprocess_image(image)  # preprocess_image is a custom function to preprocess image\\\\ndepth_map = model(input_tensor)\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221116-054332', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\\\", 'api_arguments': {'model_name': 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'}, 'python_environment_requirements': [{'transformers': '4.24.0', 'pytorch': '1.13.0+cu117', 'tokenizers': '0.13.2'}], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.6028, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي لطلاب تعلم اللغة الطبيعية الحاسوبية يجتمع كل أسبوعين للعب كرة القدم الإفتراضية. زودهم بأداة للعب ضد وكيل المحادثة الآلي.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('ToddGoldfarb/Cadet-Tiny', low_cpu_mem_usage=True)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries from the transformers library: AutoTokenizer and AutoModelForSeq2SeqLM. Also, import the torch library for utilizing GPU acceleration if available.\\\\n2. Define a CadetTinyAgent class to handle the conversational agent's functions like observing user input, generating model responses, and managing conversation history.\\\\n3. Load the Cadet-Tiny model trained on the SODA dataset using the from_pretrained method with low_cpu_mem_usage=True. This model is based on the t5-small pretrained model and is suitable for devices with limited resources like Raspberry Pi.\\\\n4. Implement methods like observe, set_input, generate, reset_history, run, and chat in the CadetTinyAgent class to create an interactive conversational loop between the user and the AI agent.\\\\n5. Instantiate the CadetTinyAgent object and call the run method to start the conversation.\\\", '<<<code>>>': 'import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nclass CadetTinyAgent:\\\\n    def __init__(self):\\\\n        self.device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\\\n        self.tokenizer = AutoTokenizer.from_pretrained(\\\\'t5-small\\\\', model_max_length=512)\\\\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(\\\\'ToddGoldfarb/Cadet-Tiny\\\\', low_cpu_mem_usage=True).to(self.device)\\\\n        self.conversation_history = \\\"\\\"\\\\n    # Implement other necessary methods like observe, set_input, generate, reset_history, run, and chat\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('ToddGoldfarb/Cadet-Tiny', low_cpu_mem_usage=True)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ToddGoldfarb/Cadet-Tiny', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('ToddGoldfarb/Cadet-Tiny', low_cpu_mem_usage=True)\\\", 'api_arguments': {'pretrained_model': 't5-small', 'model_max_length': 512}, 'python_environment_requirements': [{'torch': '', 'transformers': '', 'colorful': ''}], 'example_code': \\\"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style('monokai')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == '<strong>main</strong>':\\\\n main()\\\", 'performance': {'dataset': 'allenai/soda', 'accuracy': ''}, 'description': 'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.' </s>\"\n"}
{"text": "<s>[INST]نقوم بتصميم نماذج للتعلم الآلي في معالجة اللغة الطبيعية للتصنيف صفر الأحكام، ونحتاج إلى حساب قيمة العائد في كل حالة لدعم هذه النماذج.? [/INST]{'domain': 'Natural Language Processing Zero-Shot Classification', 'api_call': \\\"DebertaModel.from_pretrained('Narsil/deberta-large-mnli-zero-cls')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We import the DebertaModel class and pipeline function from the transformers library provided by Hugging Face.', 'code': \\\"from transformers import DebertaModel, pipeline\\\\nmodel = DebertaModel.from_pretrained('Narsil/deberta-large-mnli-zero-cls')\\\\nclassifier = pipeline('zero-shot-classification', model=model)\\\\nresult = classifier(user_question, candidate_labels=possible_responses)\\\", 'api_data': {'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'Narsil/deberta-large-mnli-zero-cls', 'api_call': \\\"from transformers import DebertaModel, pipeline\\\\nmodel = DebertaModel.from_pretrained('Narsil/deberta-large-mnli-zero-cls')\\\\nclassifier = pipeline('zero-shot-classification', model=model)\\\\nresult = classifier(user_question, candidate_labels=possible_responses)\\\", 'api_arguments': 'text, candidate_labels', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': {'SQuAD 1.1': {'F1': 95.5, 'EM': 90.1}, 'SQuAD 2.0': {'F1': 90.7, 'EM': 88.0}, 'MNLI-m/mm': {'Accuracy': 91.3}, 'SST-2': {'Accuracy': 96.5}, 'QNLI': {'Accuracy': 95.3}, 'CoLA': {'MCC': 69.5}, 'RTE': {'Accuracy': 91.0}, 'MRPC': {'Accuracy': 92.6}, 'QQP': {}, 'STS-B': {'P/S': 92.8}}}, 'description': 'DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on the majority of NLU tasks with 80GB training data. This is the DeBERTa large model fine-tuned with MNLI task.'}} </s>\"\n"}
{"text": "<s>[INST]سوف تعمل الشركة على تطوير نظام يتعامل مع الوثائق الصوتية، وتحتاج إلى تصميم خوارزميات تستطيع التصنيف الصوتي لهذه الوثائق.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"pipeline('audio-classification', model=Wav2Vec2ForCTC.from_pretrained('anton-l/wav2vec2-random-tiny-classifier'))\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required packages and classes such as 'pipeline' from transformers and 'Wav2Vec2ForCTC' from the transformers.models.wav2vec2 module.\\\\n2. Instantiate the audio classification pipeline by providing the 'audio-classification' task and the pre-trained model 'anton-l/wav2vec2-random-tiny-classifier', which is based on the Wav2Vec2 architecture.\\\\n3. You can now pass an audio file into the pipeline, which will classify the input as one of the predefined categories: speech, music, or background noise.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nfrom transformers.models.wav2vec2 import Wav2Vec2ForCTC\\\\naudio_classifier = pipeline('audio-classification', model=Wav2Vec2ForCTC.from_pretrained('anton-l/wav2vec2-random-tiny-classifier'))\\\\nclassification_result = audio_classifier(audio_file)\\\", 'api_call': \\\"pipeline('audio-classification', model=Wav2Vec2ForCTC.from_pretrained('anton-l/wav2vec2-random-tiny-classifier'))\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'wav2vec2-random-tiny-classifier', 'api_call': \\\"pipeline('audio-classification', model=Wav2Vec2ForCTC.from_pretrained('anton-l/wav2vec2-random-tiny-classifier'))\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'An audio classification model based on wav2vec2.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير منتج في مجال معالجة اللغة الطبيعية والإجابة على الأسئلة يتيح تطوير نماذج للتنبؤ بالإجابات على أسئلة بناءً على النصوص، ونريد تطبيق التنبؤ على النصوص لتحسين أداء نماذج الإجابة الآلية.? [/INST]{'domain': 'Natural Language Processing Question Answering', 'api_call': \\\"pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Using the Hugging Face Transformers library, we will import the pipeline function.', 'code': \\\"from transformers import pipeline\\\\n\\\\nmultilingual_qa = pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\\n\\\\nresult = multilingual_qa({\\\\n    'context': 'Manuel Romero lavora duramente nel repository hugginface/transformers di recente.',\\\\n    'question': 'Chi ha lavorato duramente per hugginface/transformers di recente?'})\\\", 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Multilingual Question Answering', 'api_name': 'mrm8488/bert-multi-cased-finetuned-xquadv1', 'api_call': \\\"from transformers import pipeline\\\\n\\\\nmultilingual_qa = pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\\\\n\\\\nresult = multilingual_qa({\\\\n    'context': 'Manuel Romero lavora duramente nel repository hugginface/transformers di recente.',\\\\n    'question': 'Chi ha lavorato duramente per hugginface/transformers di recente?'})\\\", 'api_arguments': {'context': 'string', 'question': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"qa_pipeline({\\\\n 'context': Manuel Romero has been working hardly in the repository hugginface/transformers lately,\\\\n 'question': Who has been working hard for hugginface/transformers lately?\\\\n})\\\", 'performance': {'dataset': 'XQuAD', 'accuracy': 'Not provided'}, 'description': 'This model is a BERT (base-multilingual-cased) fine-tuned for multilingual Question Answering on 11 different languages using the XQuAD dataset and additional data augmentation techniques.'}} </s>\"\n"}
{"text": "<s>[INST]يمكن لفريق معالجة اللغة الطبيعية توليد نصوص تلقائية تقاومية. جهز الفريق بأداة للتفاعل مع نماذج اللغة الصناعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries and classes from the transformers and torch packages.\\\\n2. Load the pre-trained transformer model 'facebook/opt-66b' using the AutoModelForCausalLM class. This is a powerful text generation model aimed at matching GPT-3's performance.\\\\n3. Set-up a tokenizer for the model, using the AutoTokenizer class from transformers.\\\\n4. You can now feed the model with a prompt or seed text that will be used as a starting point for generating creative headlines.\\\\n5. Use the generate() function to produce multiple possible headlines by adjusting the sampling parameters such as 'do_sample', 'num_return_sequences', and 'max_length'.\\\\n6. Finally, use the tokenizer's batch_decode() method to convert the generated tokens back into text form.\\\", '<<<code>>>': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\n\\\\nmodel = AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained('facebook/opt-66b', use_fast=False)\\\\n\\\\nprompt = 'Introducing our new product: '\\\\ninput_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\\\\nset_seed(32)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\\\\nheadlines = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\\\", 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/opt-66b', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16)\\\", 'api_arguments': ['input_ids', 'do_sample', 'num_return_sequences', 'max_length'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': 'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-66b, torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-66b, use_fast=False)\\\\nprompt = Hello, I am conscious and\\\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\\\nset_seed(32)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\\\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)', 'performance': {'dataset': 'GPT-3', 'accuracy': 'roughly matched'}, 'description': 'OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. OPT models are trained to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج تصنيف صور غير مشروط بواسطة تقنيات الرؤية الحاسوبية.? [/INST]{'domain': 'Computer Vision Unconditional Image Generation', 'api_call': \\\"DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the DDPMPipeline class from the diffusers package.', 'code': \\\"from diffusers import DDPMPipeline\\\\nbutterfly_pipeline = DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\\nbutterfly_image = butterfly_pipeline().images[0]\\\\nbutterfly_image.save('butterfly_background.png')\\\", 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'ntrant7/sd-class-butterflies-32', 'api_call': \\\"from diffusers import DDPMPipeline\\\\nbutterfly_pipeline = DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\\nbutterfly_image = butterfly_pipeline().images[0]\\\\nbutterfly_image.save('butterfly_background.png')\\\", 'api_arguments': [], 'python_environment_requirements': ['diffusers'], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\\\nimage = pipeline().images[0]\\\\nimage\\\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This model is a diffusion model for unconditional image generation of cute butterflies.'}} </s>\"\n"}
{"text": "<s>[INST]يعمل العميل على تطوير نموذج \\\"Natural Language Processing Text Classification\\\" لتصنيف النصوص بدون توجيه مسبق. الحاجة ماسّة إلى تطبيق قادر على تحليل النصوص وتصنيفها تلقائياً دون الحاجة لبيانات تدريبية مسبقة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, we import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Then, we use the pipeline function to create a sentiment analysis model, and we specify the model 'ProsusAI/finbert' to be loaded. This model has been fine-tuned for financial sentiment classification tasks, making it suitable for analyzing the sentiment of a cryptocurrency news article.\\\\n3. We can now use the loaded model to analyze the sentiment of the given news article. The model will classify the sentiment as 'positive', 'negative', or 'neutral'.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'sentiment-analysis\\\\', model=\\\\'ProsusAI/finbert\\\\')\\\\nresult = classifier(\\\"Bitcoin has recently seen an impressive surge in value, leading to a renewed interest in cryptocurrencies. This has created a lot of excitement among investors, with many predicting a profitable future. However, the market remains volatile, and it would be wise for investors to approach with caution, as sudden fluctuations can result in significant losses.\\\")', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ProsusAI/finbert', 'api_call': \\\"AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline; classifier = pipeline('sentiment-analysis', model='ProsusAI/finbert'); classifier('your_text_here')\\\", 'performance': {'dataset': 'Financial PhraseBank', 'accuracy': 'Not provided'}, 'description': 'FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.' </s>\"\n"}
{"text": "<s>[INST]يجب على الباحثين تشجيع تطوير وتقييم نماذج طبق الصناعة في مجال التعلم التعزيزي للتصنيف الذاتي للصور بما فيها من تدريب مسبق في خوارزميات التعلم. يرجى تقديم أداة للعب ضد وكيل التعلم الآلي خلال جلسة النادي الطلابي لتعزيز التفاعل والابتكار.? [/INST]{'domain': 'Reinforcement Learning', 'api_call': \\\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\\\", 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'edbeeching/decision-transformer-gym-walker2d-expert', 'api_call': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\\\", 'api_arguments': {'mean': [1.2384834, 0.19578537, -0.10475016, -0.18579608, 0.23003316, 0.022800924, -0.37383768, 0.337791, 3.925096, -0.0047428459, 0.025267061, -0.0039287535, -0.01736751, -0.48212224, 0.00035432147, -0.0037124525, 0.0026285544], 'std': [0.06664903, 0.16980624, 0.17309439, 0.21843709, 0.74599105, 0.02410989, 0.3729872, 0.6226182, 0.9708009, 0.72936815, 1.504065, 2.495893, 3.511518, 5.3656907, 0.79503316, 4.317483, 6.1784487]}, 'python_environment_requirements': ['transformers'], 'example_code': 'See our Blog Post, Colab notebook or Example Script for usage.', 'performance': {'dataset': 'Gym Walker2d environment', 'accuracy': 'Not provided'}, 'description': 'Decision Transformer model trained on expert trajectories sampled from the Gym Walker2d environment.'}} </s>\"\n"}
{"text": "<s>[INST]يمكنك استخدام تقنيات معالجة اللغة الطبيعية لتوليد ملف صوتي من نص مكتوب. يمكنك البحث عن نماذج متاحة أو استخدام الذكاء الصناعي لهذا الغرض. هل تحتاج لمزيد من التفاصيل؟? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"T5ForConditionalGeneration.from_pretrained('google/byt5-small')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library, which include T5ForConditionalGeneration for the text-to-text generation model and AutoTokenizer for tokenizing the input and output texts.\\\\n2. Load the pre-trained model 'google/byt5-small' using T5ForConditionalGeneration.\\\\n3. Load the tokenizer from the same model name.\\\\n4. Tokenize the input French text using the tokenizer and create input tensors for the model.\\\\n5. Use the translation model to generate the translated English text.\\\\n6. Decode the model output to get the translation as a readable string.\\\", '<<<code>>>': 'from transformers import T5ForConditionalGeneration, AutoTokenizer\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\\'google/byt5-small\\\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'google/byt5-small\\\\')\\\\ninput_text = \\\"Bonjour, comment ça va?\\\"\\\\ninput_ids = tokenizer(input_text, return_tensors=\\\\'pt\\\\').input_ids\\\\noutput_ids = model.generate(input_ids)\\\\ntranslation = tokenizer.decode(output_ids[0], skip_special_tokens=True)', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/byt5-small')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/byt5-small', 'api_call': \\\"T5ForConditionalGeneration.from_pretrained('google/byt5-small')\\\", 'api_arguments': ['input_ids', 'labels'], 'python_environment_requirements': [['transformers', 'torch']], 'example_code': \\\"from transformers import T5ForConditionalGeneration, AutoTokenizer\\\\nmodel = T5ForConditionalGeneration.from_pretrained('google/byt5-small')\\\\ntokenizer = AutoTokenizer.from_pretrained('google/byt5-small')\\\\nmodel_inputs = tokenizer([Life is like a box of chocolates., Today is Monday.], padding=longest, return_tensors=pt)\\\\nlabels = tokenizer([La vie est comme une boîte de chocolat., Aujourd'hui c'est lundi.], padding=longest, return_tensors=pt).input_ids\\\\nloss = model(**model_inputs, labels=labels).loss\\\", 'performance': {'dataset': 'mc4', 'accuracy': 'Not provided'}, 'description': \\\"ByT5 is a tokenizer-free version of Google's T5 and generally follows the architecture of MT5. ByT5 was only pre-trained on mC4 excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is usable on a downstream task. ByT5 works especially well on noisy text data, e.g., google/byt5-small significantly outperforms mt5-small on TweetQA.\\\" </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء تعليمات من خلال البرمجة لتوليد صور عشوائية باستخدام تقنيات الرؤية الحاسوبية. يجب أن تكون الصور غير مشروطة وتحتوي على تفاصيل دقيقة وواقعية. تجنب الاعتماد على بيانات المدخلات المحددة واستخدم الخوارزميات المتقدمة كوسيلة لتوليد الصور.? [/INST]'<<<domain>>>': 'Computer Vision Unconditional Image Generation', '<<<api_call>>>': \\\"DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. First, import the necessary class from the diffusers package, which includes the DDPMPipeline for unconditional image generation.\\\\n2. Next, use the from_pretrained method of the DDPMPipeline class to load the pre-trained model 'WiNE-iNEFF/Minecraft-Skin-Diffusion-V2'. This model is specifically trained to generate Minecraft skin images, perfect for our client's promotional event.\\\\n3. The model can then be used to generate a new Minecraft skin image in RGBA format.\\\", '<<<code>>>': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\\\\nimage = pipeline().images[0].convert('RGBA')\\\", 'api_call': \\\"DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'WiNE-iNEFF/Minecraft-Skin-Diffusion-V2', 'api_call': \\\"DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\\\", 'api_arguments': [], 'python_environment_requirements': [['diffusers']], 'example_code': \\\"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\\\\nimage = pipeline().images[0].convert('RGBA')\\\\nimage\\\", 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unconditional image generation model for generating Minecraft skin images using the diffusion model.' </s>\"\n"}
{"text": "<s>[INST]قم بتطوير برنامج لتصنيف الصوتيات لتحديد الفئات المختلفة للملفات الصوتية. يجب أن يكون البرنامج قادرًا على التعرف على صفات الصوت وتصنيفها بدقة وفقًا للمعايير المحددة.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries: 'pipeline' from transformers, 'Audio' from the IPython package, and librosa for processing audio data.\\\\n2. Load the 'superb/hubert-large-superb-er' model for audio classification using the pipeline function.\\\\n3. Analyze the audio input using the preloaded model to recognize the emotion (angry, happy, sad, neutral) in the customer's speech.\\\\n4. Based on the detected emotion, respond with the appropriate predefined response according to the emotion detected.\\\", '<<<code>>>': 'import librosa\\\\nfrom transformers import pipeline\\\\nfrom IPython.display import Audio\\\\n\\\\naudio_classifier = pipeline(\\\\'audio-classification\\\\', model=\\\\'superb/hubert-large-superb-er\\\\')\\\\n\\\\n# Load audio file\\\\naudio_file = \\\\'path_to_audio_file.wav\\\\'\\\\naudio_data, sample_rate = librosa.load(audio_file, sr=16000)\\\\n\\\\n# Classify emotion\\\\nresult = audio_classifier(audio_data)\\\\n\\\\n# Extract the detected emotion\\\\nemotion = result[0][\\\\'label\\\\']\\\\n\\\\n# Predefined responses\\\\nresponses = {\\\\n    \\\\'angry\\\\': \\\\'I understand that you are upset. Let us find a solution together.\\\\',\\\\n    \\\\'happy\\\\': \\\\'Glad to hear you are happy! Is there anything else we can help you with?\\\\',\\\\n    \\\\'sad\\\\': \\\\'I am sorry to hear that. Let us know how we can support you.\\\\',\\\\n    \\\\'neutral\\\\': \\\\'Thank you for your input. How may I assist you further?\\\\'\\\\n}\\\\n\\\\n# Select the appropriate response\\\\nresponse = responses.get(emotion, \\\\'I am unable to detect your emotion. Can you please clarify your issue?\\\\')\\\\n\\\\nprint(f\\\"User emotion detected: {emotion}\\\")\\\\nprint(f\\\"Response: {response}\\\")', 'api_call': \\\"pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'superb/hubert-large-superb-er', 'api_call': \\\"pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\", 'api_arguments': ['model'], 'python_environment_requirements': [['transformers', 'datasets', 'librosa']], 'example_code': \\\"from datasets import load_dataset\\\\nfrom transformers import pipeline\\\\ndataset = load_dataset('anton-l/superb_demo', 'er', split='session1')\\\\nclassifier = pipeline('audio-classification', model='superb/hubert-large-superb-er')\\\\nlabels = classifier(dataset[0]['file'], top_k=5)\\\", 'performance': {'dataset': 'IEMOCAP', 'accuracy': 0.6762}, 'description': \\\"This is a ported version of S3PRL's Hubert for the SUPERB Emotion Recognition task. The base model is hubert-large-ll60k, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.\\\" </s>\"\n"}
{"text": "<s>[INST]يمكن لمطوري معالجة اللغة الطبيعية في تصنيف النصوص الاجتماع في النادي الطلابي كل أسبوعين ليحسنوا مهاراتهم في تطوير نماذج التعلم الآلي. يُنصح بتوفير أداة لعب القدم الافتراضية للمشاركين من أجل تحقيق التحدي والتعلم.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"pipeline('sentiment-analysis', model='lvwerra/distilbert-imdb')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary components from the transformers package, specifically the 'pipeline' function.\\\\n2. Use the 'pipeline' function to load the pre-trained model, 'lvwerra/distilbert-imdb', which has been fine-tuned on the IMDb dataset for sentiment analysis of movie reviews.\\\\n3. With this model, we can classify movie reviews into positive or negative sentiment categories.\\\\n4. The model will determine the sentiment behind the movie review, giving insight into audience perception of the movie.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nclassifier = pipeline(\\\\'sentiment-analysis\\\\', model=\\\\'lvwerra/distilbert-imdb\\\\')\\\\nreview = \\\"I absolutely loved this movie, the acting was fantastic\\\"\\\\nresult = classifier(review)', 'api_call': \\\"pipeline('sentiment-analysis', model='lvwerra/distilbert-imdb')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lvwerra/distilbert-imdb', 'api_call': \\\"pipeline('sentiment-analysis', model='lvwerra/distilbert-imdb')\\\", 'api_arguments': [], 'python_environment_requirements': [['transformers', 'pytorch']], 'example_code': \\\"classifier('I love this movie!')\\\", 'performance': {'dataset': 'imdb', 'accuracy': 0.928}, 'description': 'This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set.' </s>\"\n"}
{"text": "<s>[INST]نحن بحاجة إلى تطوير تطبيق لمعالجة اللغة الطبيعية يستخدم طريقة ملأ الفجوة. عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحتاج إلى برنامج يقوم بتحويل النص إلى كلام لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='roberta-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the 'pipeline' function to create an unmasker object that takes a masked text as input and predicts the missing word(s).\\\\n3. Specify the 'roberta-base' model for the 'fill-mask' task in the pipeline.\\\\n4. You can now use the unmasker object to process user input and generate predictions for the missing word in the given sentence.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'roberta-base\\\\')\\\\nmasked_sentence = \\\"The adventurer entered the <mask> room.\\\"\\\\npredicted_words = unmasker(masked_sentence)', 'api_call': \\\"pipeline('fill-mask', model='roberta-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'roberta-base', 'api_call': \\\"pipeline('fill-mask', model='roberta-base')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='roberta-base')\\\\nunmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': [{'name': 'MNLI', 'accuracy': 87.6}, {'name': 'QQP', 'accuracy': 91.9}, {'name': 'QNLI', 'accuracy': 92.8}, {'name': 'SST-2', 'accuracy': 94.8}, {'name': 'CoLA', 'accuracy': 63.6}, {'name': 'STS-B', 'accuracy': 91.2}, {'name': 'MRPC', 'accuracy': 90.2}, {'name': 'RTE', 'accuracy': 78.7}]}, 'description': 'RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using the Masked language modeling (MLM) objective. This model is case-sensitive and can be fine-tuned on a downstream task.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام للإجابة على الأسئلة بمعالجة اللغة الطبيعية، ونحتاج إلى تدريب نموذج لتوقع الإجابات الصحيحة على أسئلة معينة.? [/INST]'<<<domain>>>': 'Natural Language Processing Question Answering', '<<<api_call>>>': \\\"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. First, we import the necessary classes from the transformers package, which includes DPRQuestionEncoder for the question encoding model and DPRQuestionEncoderTokenizer for tokenizing the input text.\\\\n2. We use the from_pretrained method of the DPRQuestionEncoder and DPRQuestionEncoderTokenizer classes to load the pre-trained model 'facebook/dpr-question_encoder-single-nq-base'.\\\\n3. This model has been trained for question answering tasks, and it can convert a given question into a dense vector representation.\\\\n4. The input question is tokenized, and the tokens are passed to the model, returning the dense vector representation, which can be used for further processing in the question-answering pipeline.\\\", '<<<code>>>': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\\\\'facebook/dpr-question_encoder-single-nq-base\\\\')\\\\nmodel = DPRQuestionEncoder.from_pretrained(\\\\'facebook/dpr-question_encoder-single-nq-base\\\\')\\\\nquestion = \\\"Hello, is my dog cute?\\\"\\\\ninput_ids = tokenizer(question, return_tensors=\\\\'pt\\\\')[\\\\'input_ids\\\\']\\\\nembeddings = model(input_ids).pooler_output', 'api_call': \\\"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \\\"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\\\", 'api_arguments': ['input_ids'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى برنامج يستخدم تقنيات معالجة اللغة الطبيعية لتحويل النصوص إلى كلام يمكن فهمه من قبل مستخدمي تطبيقنا لذوي الإعاقة البصرية.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a translation model for English to Arabic translation using the pre-trained model 'Helsinki-NLP/opus-mt-en-ar'.\\\\n3. In order to provide an automated financial report in Arabic, the English text data from the financial report should be passed through the created translation pipeline. This will result in a translated version of the report in Arabic.\\\\n4. Share the translated report with the Middle East clients.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\ntranslator = pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\\narabic_financial_report = translator(english_financial_report_text)\\\", 'api_call': \\\"pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-en-ar', 'api_call': \\\"pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\\\ntranslated_text = translation('Hello World')\\\", 'performance': {'dataset': 'Tatoeba-test.eng.ara', 'accuracy': {'BLEU': 14.0, 'chr-F': 0.437}}, 'description': \\\"A Hugging Face Transformers model for English to Arabic translation, trained on the Tatoeba dataset. It uses a transformer architecture and requires a sentence initial language token in the form of '>>id<<' (id = valid target language ID).\\\" </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج لتوليد النصوص في مجال معالجة اللغة الطبيعية.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"pipeline('text-generation', model='distilgpt2')\\\", 'api_provider': 'Transformers', 'explanation': \\\"1. Import the required libraries 'pipeline' and 'set_seed' from the transformers library.\\\", 'code': \\\"from transformers import pipeline, set_seed\\\\ngenerator = pipeline('text-generation', model='distilgpt2')\\\\nset_seed(42)\\\\nprompt = 'In a world of advanced technology,'\\\\nideas = generator(prompt, max_length=50, num_return_sequences=5)\\\", 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Transformers', 'functionality': 'Text Generation', 'api_name': 'distilgpt2', 'api_call': \\\"from transformers import pipeline, set_seed\\\\ngenerator = pipeline('text-generation', model='distilgpt2')\\\\nset_seed(42)\\\\nprompt = 'In a world of advanced technology,'\\\\nideas = generator(prompt, max_length=50, num_return_sequences=5)\\\", 'api_arguments': ['model'], 'python_environment_requirements': ['from transformers import pipeline, set_seed'], 'example_code': 'set_seed(42)\\\\ngenerator(Hello, I’m a language model, max_length=20, num_return_sequences=5)', 'performance': {'dataset': 'WikiText-103', 'accuracy': '21.100'}, 'description': 'DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. With 82 million parameters, it was developed using knowledge distillation and designed to be a faster, lighter version of GPT-2. It can be used for text generation, writing assistance, creative writing, entertainment, and more.'}} </s>\"\n"}
{"text": "<s>[INST]أنشئ تعليمة صوتية لنموذج اكتشاف الأشياء في الرؤية الحاسوبية تبدأ بجملة \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8m-nlf-head-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries which are 'YOLO' and 'render_result' from ultralyticsplus.\\\\n2. Use the YOLO class to load the pre-trained model 'keremberke/yolov8m-nlf-head-detection'. This model has been trained for object detection tasks, particularly for detecting helmets in American football footage.\\\\n3. Set the appropriate parameter overrides, such as confidence, Intersection over Union (IOU), and maximum number of detections.\\\\n4. Load the image or video frame from the football game.\\\\n5. Use the model's 'predict' method to detect helmets in the footage.\\\\n6. Optionally, use the 'render_result' function to visualize the detected helmets by overlaying bounding boxes on the image or video frame.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO('keremberke/yolov8m-nlf-head-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage_url = 'http://url_to_image_or_video_frame'\\\\n\\\\nresults = model.predict(image_url)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image_url, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8m-nlf-head-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8m-nlf-head-detection', 'api_call': \\\"YOLO('keremberke/yolov8m-nlf-head-detection')\\\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.24 ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \\\"model = YOLO('keremberke/yolov8m-nlf-head-detection')\\\", \\\"model.overrides['conf'] = 0.25\\\", \\\"model.overrides['iou'] = 0.45\\\", \\\"model.overrides['agnostic_nms'] = False\\\", \\\"model.overrides['max_det'] = 1000\\\", \\\"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'nfl-object-detection', 'accuracy': 0.287}, 'description': 'A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.' </s>\"\n"}
{"text": "<s>[INST]يحتاج عميلنا في مجال تطوير تطبيقات معالجة اللغة الطبيعية إلى برنامج يمكنه الإجابة على أسئلة جدولية. يجب أن يكون البرنامج قادرًا على تحويل النص إلى كلام لمستخدمي هذا التطبيق.? [/INST]'<<<domain>>>': 'Natural Language Processing Table Question Answering', '<<<api_call>>>': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. First, import the necessary libraries, which include 'pipeline' from transformers.\\\\n2. Use the 'pipeline' function from transformers to load the model, which in this case is 'google/tapas-medium-finetuned-wtq'.\\\\n3. The loaded model will be used for table question answering, which is designed to answer questions based on a table with data in it.\\\\n4. Convert your school grade report into tabular data format and use the model to ask questions and analyze your academic performance.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ntable_qa = pipeline(\\\\'table-question-answering\\\\', model=\\\\'google/tapas-medium-finetuned-wtq\\\\')\\\\n# Define table data\\\\ntable = [{\\\\n            \\\"Subject\\\": \\\"Math\\\",\\\\n            \\\"Score\\\": 85,\\\\n            \\\"Grade\\\": \\\"B\\\",\\\\n        },{\\\\n            \\\"Subject\\\": \\\"History\\\",\\\\n            \\\"Score\\\": 70,\\\\n            \\\"Grade\\\": \\\"C\\\",\\\\n        },{\\\\n            \\\"Subject\\\": \\\"Physics\\\",\\\\n            \\\"Score\\\": 91,\\\\n            \\\"Grade\\\": \\\"A\\\",\\\\n    }]\\\\nquery = \\\"What is my average score?\\\"\\\\nanswer = table_qa(table=table, query=query)', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Table Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/tapas-medium-finetuned-wtq', 'api_call': \\\"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\", 'api_arguments': 'table, query', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntable_qa = pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\\\n\\\\n# Define the table and the query\\\\ntable = {...}\\\\nquery = '...'\\\\n\\\\n# Get the answer\\\\nanswer = table_qa(table=table, query=query)\\\", 'performance': {'dataset': 'wikitablequestions', 'accuracy': 0.4324}, 'description': 'TAPAS medium model fine-tuned on WikiTable Questions (WTQ). This model is pretrained on a large corpus of English data from Wikipedia and is used for answering questions related to a table.' </s>\"\n"}
{"text": "<s>[INST]نحن نعمل على تصنيف البيانات الجدولية وإنشاء نماذج لتحقيق دقة عالية في تصنيف تلك البيانات، ونحتاج إلى إنشاء تعليمات تفاعلية لتحقيق أفضل نتائج في هذا المجال.? [/INST]'<<<domain>>>': 'Tabular Tabular Classification', '<<<api_call>>>': \\\"joblib.load('model.joblib')\\\", '<<<api_provider>>>': 'Joblib', '<<<explanation>>>': \\\"1. Import the required libraries: joblib, pandas, and json.\\\\n2. Load the saved model using joblib.load function and the provided model path ('model.joblib').\\\\n3. Load the configuration file containing the selected features using json.load and open functions.\\\\n4. Read the input tabular data as a pandas DataFrame.\\\\n5. Filter the DataFrame to include only the features specified in the configuration file.\\\\n6. Rename the DataFrame columns to match the format expected by the model.\\\\n7. Use the trained model to make predictions on the input data and obtain results as a numpy array.\\\", '<<<code>>>': \\\"import joblib\\\\nimport pandas as pd\\\\nimport json\\\\n\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\n\\\\ndata = pd.read_csv('data.csv')\\\\ndata = data[features]\\\\ndata.columns = [feat_ + str(col) for col in data.columns]\\\\n\\\\npredictions = model.predict(data)\\\", 'api_call': \\\"joblib.load('model.joblib')\\\", 'provider': 'Joblib', 'api_data': {'domain': 'Tabular Tabular Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pachi107/autotrain-in-class-test-1780161764', 'api_call': \\\"joblib.load('model.joblib')\\\", 'api_arguments': ['data.csv'], 'python_environment_requirements': [['json', 'joblib', 'pandas']], 'example_code': \\\"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load('model.joblib')\\\\nconfig = json.load(open('config.json'))\\\\nfeatures = config['features']\\\\ndata = pd.read_csv(data.csv)\\\\ndata = data[features]\\\\ndata.columns = [feat_ + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\\\", 'performance': {'dataset': 'Validation Metrics', 'accuracy': 0.974}, 'description': 'A binary classification model for predicting CO2 emissions based on tabular data. Trained using AutoTrain with a model ID of 1780161764.' </s>\"\n"}
{"text": "<s>[INST]نحتاج إلى استخدام تقنيات تقطيع الصور لتقسيم الصورة التصويرية الغير مصنفة لحيوانات البرية إلى مناطق مختلفة. هل تستطيع مساعدتنا في ذلك؟? [/INST]'<<<domain>>>': 'Computer Vision Image Segmentation', '<<<api_call>>>': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the necessary classes from the transformers and PIL packages. This includes MaskFormerFeatureExtractor and MaskFormerForInstanceSegmentation for the image segmentation model and Image for processing image data.\\\\n2. We load an image from a URL or a file (replace with the path to a satellite image of a city).\\\\n3. Import MaskFormerFeatureExtractor.from_pretrained method and create the feature_extractor, using the pre-trained model 'facebook/maskformer-swin-base-ade'. This model has been trained for image segmentation tasks.\\\\n4. This model can then be used to analyze a satellite image and segment it into distinct land-use categories.\\\", '<<<code>>>': \\\"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\\\nfrom PIL import Image\\\\n\\\\nimage = Image.open('satellite_image_path.jpg')\\\\n# replace 'satellite_image_path.jpg' with path to your satellite image \\\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-ade')\\\\ninputs = feature_extractor(images=image, return_tensors='pt')\\\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\\\\noutputs = model(**inputs)\\\\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\", 'api_call': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/maskformer-swin-base-ade', 'api_call': \\\"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\\\", 'api_arguments': {'from_pretrained': 'facebook/maskformer-swin-base-ade'}, 'python_environment_requirements': [{'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}], 'example_code': 'from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\\\\noutputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.' </s>\"\n"}
{"text": "<s>[INST]عندما تبدأ في استخدام تقنيات معالجة اللغة الطبيعية لإنشاء نصوص، احرص على تطبيق النماذج المختلفة لضمان توليد نصوص طبيعية واقعية. قم بتوليد جمل وفقرات تعتمد على الإحصاء والتعلم الآلي لزيادة دقة الناتج النهائي.? [/INST]{'domain': 'Natural Language Processing Text Generation', 'api_call': \\\"pipeline('text-generation', model='Pi3141/DialoGPT-medium-elon-3')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. We will begin by importing the pipeline function from the transformers library.', 'code': \\\"from transformers import pipeline\\\\nelon_chatbot = pipeline('text-generation', model='Pi3141/DialoGPT-medium-elon-3')\\\\n# Replace 'your_question_here' with your actual question or conversation starter\\\\ninput_text = 'your_question_here'\\\\ngenerated_text = elon_chatbot(input_text)[0]['generated_text']\\\", 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Conversational', 'api_name': 'Pi3141/DialoGPT-medium-elon-3', 'api_call': \\\"from transformers import pipeline\\\\nelon_chatbot = pipeline('text-generation', model='Pi3141/DialoGPT-medium-elon-3')\\\\n# Replace 'your_question_here' with your actual question or conversation starter\\\\ninput_text = 'your_question_here'\\\\ngenerated_text = elon_chatbot(input_text)[0]['generated_text']\\\", 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.', 'performance': {'dataset': 'Twitter tweets by Elon Musk', 'accuracy': 'N/A'}, 'description': 'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير تقنية لترجمة اللغة الطبيعية، ونرغب في تحري الاحتمال الذي تقدم عليه الوظيفة بناءً على بعض مجموعات البيانات المتاحة.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fi-en')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. To translate the Finnish paragraph to English, we need to import the relevant classes from the transformers library. Import AutoTokenizer and AutoModelForSeq2SeqLM.\\\\n2. Create a tokenizer to tokenize the Finnish input text using the AutoTokenizer.from_pretrained() method and specifying the pre-trained model 'Helsinki-NLP/opus-mt-fi-en'.\\\\n3. Load the pre-trained translation model from 'Helsinki-NLP/opus-mt-fi-en' using the AutoModelForSeq2SeqLM.from_pretrained() method.\\\\n4. Tokenize the Finnish input text and pass it to the loaded translation model.\\\\n5. Decode the output tensor from the model and obtain the translated English text.\\\", '<<<code>>>': \\\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-fi-en')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fi-en')\\\\ninput_text = 'Tämä on esimerkki tekstistä.'\\\\ninputs = tokenizer.encode(input_text, return_tensors='pt')\\\\noutputs = model.generate(inputs)\\\\ntranslation = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\", 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fi-en')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Helsinki-NLP/opus-mt-fi-en', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fi-en')\\\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-fi-en')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fi-en')\\\\ninput_text = 'Tämä on esimerkki tekstistä.'\\\\ninputs = tokenizer.encode(input_text, return_tensors='pt')\\\\noutputs = model.generate(inputs)\\\\ntranslation = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\", 'performance': {'dataset': [{'name': 'newsdev2015-enfi-fineng.fin.eng', 'accuracy': 'BLEU: 25.3, chr-F: 0.536'}, {'name': 'newstest2015-enfi-fineng.fin.eng', 'accuracy': 'BLEU: 26.9, chr-F: 0.547'}, {'name': 'newstest2016-enfi-fineng.fin.eng', 'accuracy': 'BLEU: 29.0, chr-F: 0.571'}, {'name': 'newstest2017-enfi-fineng.fin.eng', 'accuracy': 'BLEU: 32.3, chr-F: 0.594'}, {'name': 'newstest2018-enfi-fineng.fin.eng', 'accuracy': 'BLEU: 23.8, chr-F: 0.517'}, {'name': 'newstest2019-fien-fineng.fin.eng', 'accuracy': 'BLEU: 29.0, chr-F: 0.565'}, {'name': 'newstestB2016-enfi-fineng.fin.eng', 'accuracy': 'BLEU: 24.5, chr-F: 0.527'}, {'name': 'newstestB2017-enfi-fineng.fin.eng', 'accuracy': 'BLEU: 27.4, chr-F: 0.557'}, {'name': 'newstestB2017-fien-fineng.fin.eng', 'accuracy': 'BLEU: 27.4, chr-F: 0.557'}, {'name': 'Tatoeba-test.fin.eng', 'accuracy': 'BLEU: 53.4, chr-F: 0.697'}]}, 'description': 'Helsinki-NLP/opus-mt-fi-en is a machine translation model for translating Finnish text to English text. It is trained on the OPUS dataset and can be used with the Hugging Face Transformers library.' </s>\"\n"}
{"text": "<s>[INST]نستطيع تنظيم برنامج تدريبي دوري لملخص معالجة اللغة الطبيعية باستخدام تقنيات الرؤية الحاسوبية. يجب على المشاركين تطبيق نماذج الملخص على مجموعة متنوعة من النصوص لتحسين الأداء. كمكون إضافي، يجب اقتراح مسابقة لملخص النصوص التي تتم اختيارها بشكل عشوائي لتعزيز التعلم التفاعلي.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. First, import the appropriate model (BarthezModel) from the Transformers library provided by Hugging Face.', 'code': 'from transformers import BarthezModel, BarthezTokenizer\\\\ntokenizer = BarthezTokenizer.from_pretrained(\\\\'moussaKam/barthez-orangesum-abstract\\\\')\\\\nmodel = BarthezModel.from_pretrained(\\\\'moussaKam/barthez-orangesum-abstract\\\\')\\\\n\\\\narticle = \\\"Les ventes de voitures électriques ont connu une hausse significative tout au long de l\\\\'année dernière en France. Selon les données récentes, le nombre de voitures électriques vendues a doublé par rapport à l\\\\'année précédente.\\\"\\\\n\\\\ninputs = tokenizer(article, return_tensors=\\\\'pt\\\\', max_length=512, truncation=True)\\\\nsummary_ids = model.generate(inputs[\\\\'input_ids\\\\'], num_beams=4, max_length=100, early_stopping=True)\\\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'moussaKam/barthez-orangesum-abstract', 'api_call': 'from transformers import BarthezModel, BarthezTokenizer\\\\ntokenizer = BarthezTokenizer.from_pretrained(\\\\'moussaKam/barthez-orangesum-abstract\\\\')\\\\nmodel = BarthezModel.from_pretrained(\\\\'moussaKam/barthez-orangesum-abstract\\\\')\\\\n\\\\narticle = \\\"Les ventes de voitures électriques ont connu une hausse significative tout au long de l\\\\'année dernière en France. Selon les données récentes, le nombre de voitures électriques vendues a doublé par rapport à l\\\\'année précédente.\\\"\\\\n\\\\ninputs = tokenizer(article, return_tensors=\\\\'pt\\\\', max_length=512, truncation=True)\\\\nsummary_ids = model.generate(inputs[\\\\'input_ids\\\\'], num_beams=4, max_length=100, early_stopping=True)\\\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)', 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'orangeSum', 'accuracy': ''}, 'description': 'Barthez model finetuned on orangeSum for abstract generation in French language'}} </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج لتصنيف النصوص في مجال معالجة اللغة الطبيعية، ونحتاج إلى تحويل النصوص الواردة إلى تصنيفات محددة مثل الأخبار، الرياضة، والتكنولوجيا.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Classification', '<<<api_call>>>': \\\"pipeline('text-classification', model='roberta-base-openai-detector')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a text classification model, which will detect whether the input text was likely generated by an AI model.\\\\n3. Specify the model 'roberta-base-openai-detector' to be loaded. This model is specifically designed for detecting GPT-2 generated text.\\\\n4. Pass the text in question to the classifier to receive the likelihood score that the text is AI-generated or not.\\\", '<<<code>>>': 'from transformers import pipeline\\\\ndetector = pipeline(\\\"text-classification\\\", model=\\\\'roberta-base-openai-detector\\\\')\\\\ntext = \\\"The importance of maintaining a balanced lifestyle cannot be overstated. Without balance in our lives, we can quickly become overwhelmed by life\\\\'s challenges and setbacks. A healthy balance enables us to better manage stress, maintain a positive mindset, and achieve our goals. Prioritizing self-care and personal growth can lead to more satisfying and fulfilling experiences, both in our personal and professional lives.\\\"\\\\nresult = detector(text)', 'api_call': \\\"pipeline('text-classification', model='roberta-base-openai-detector')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'roberta-base-openai-detector', 'api_call': \\\"pipeline('text-classification', model='roberta-base-openai-detector')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': 'from transformers import pipeline\\\\npipe = pipeline(text-classification, model=roberta-base-openai-detector)\\\\nprint(pipe(Hello world! Is this content AI-generated?))', 'performance': {'dataset': 'WebText', 'accuracy': '95%'}, 'description': 'RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نظام لمعالجة اللغة الطبيعية لتلخيص المحتوى الصوتي إلى نصوص موجزة، ونحن نريد استخدام تقنيات التعلم الآلي لتحقيق ذلك في مجال معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"pipeline('summarization', model='Samuel-Fipps/t5-efficient-large-nl36_fine_tune_sum_V2')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '', '<<<code>>>': '', 'api_call': \\\"pipeline('summarization', model='Samuel-Fipps/t5-efficient-large-nl36_fine_tune_sum_V2')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 't5-efficient-large-nl36_fine_tune_sum_V2', 'api_call': \\\"pipeline('summarization', model='Samuel-Fipps/t5-efficient-large-nl36_fine_tune_sum_V2')\\\", 'api_arguments': 'text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': [{'name': 'samsum', 'accuracy': {'ROUGE-1': 54.933, 'ROUGE-2': 31.797, 'ROUGE-L': 47.006, 'ROUGE-LSUM': 51.203, 'loss': 1.131, 'gen_len': 23.799}}, {'name': 'cnn_dailymail', 'accuracy': {'ROUGE-1': 34.406, 'ROUGE-2': 14.127, 'ROUGE-L': 24.335, 'ROUGE-LSUM': 31.658, 'loss': 2.446, 'gen_len': 45.928}}]}, 'description': \\\"A T5-based summarization model trained on the Samsum dataset. This model can be used for text-to-text generation tasks such as summarization without adding 'summarize' to the start of the input string. It has been fine-tuned for 10K steps with a batch size of 10.\\\" </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء برنامج يستخدم الهندسة اللغوية لقياس التشابه بين الجمل. هل تستطيع مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Natural Language Processing Sentence Similarity', '<<<api_call>>>': \\\"SentenceTransformer('sentence-transformers/gtr-t5-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the SentenceTransformer class from the sentence-transformers library provided by Hugging Face.\\\\n2. Initialize a SentenceTransformer instance by specifying the model 'sentence-transformers/gtr-t5-base' to be loaded. This is a model that has been trained on a large corpus of text, specifically for the task of semantic search.\\\\n3. Use the model's encode method to convert your list of sentences into a high-dimensional vector space.\\\\n4. Once the sentences are encoded into their corresponding vectors in the high-dimensional space, similarity between sentences can be calculated using metrics such as cosine similarity or euclidean distance. High similarity indicates similar content in the sentences.\\\", '<<<code>>>': 'from sentence_transformers import SentenceTransformer\\\\nsentences = [\\\"This product is amazing\\\", \\\"I love the quality of this item\\\", \\\"The delivery service was fast\\\"]\\\\nmodel = SentenceTransformer(\\\\'sentence-transformers/gtr-t5-base\\\\')\\\\nembeddings = model.encode(sentences)', 'api_call': \\\"SentenceTransformer('sentence-transformers/gtr-t5-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/gtr-t5-base', 'api_call': \\\"SentenceTransformer('sentence-transformers/gtr-t5-base')\\\", 'api_arguments': ['sentences'], 'python_environment_requirements': [['sentence-transformers>=2.2.0']], 'example_code': \\\"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer('sentence-transformers/gtr-t5-base')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\\\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'N/A'}, 'description': 'This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of semantic search.' </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذجًا لتوليد نصوص في مجال معالجة اللغة الطبيعية، يبدأ الناتج بجملة \\\"كان ياما كان في أرض بعيدة\\\".? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries for text generation, which are 'AutoTokenizer' and 'AutoModelForCausalLM' from the transformers library.\\\\n2. Load the 'Salesforce' code generation model, 'codegen-2B-multi', pretrained to generate programming code.\\\\n3. Provide a text prompt describing the desired Python program.\\\\n4. Use the tokenizer to convert the text prompt into tokens suitable for the model. Generate a Python program based on these tokens.\\\\n5. Run the generated Python code for a demonstration purpose.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Salesforce/codegen-2B-multi\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'Salesforce/codegen-2B-multi\\\\')\\\\ntext_prompt = \\\"Write a Python program to check if a number is even or odd.\\\"\\\\ninput_ids = tokenizer(text_prompt, return_tensors=\\\"pt\\\").input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\ngenerated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\\\\nprint(generated_code)', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Salesforce/codegen-2B-multi', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi')\\\", 'api_arguments': {'input_ids': 'input_ids', 'max_length': 128}, 'python_environment_requirements': [{'transformers': 'AutoTokenizer, AutoModelForCausalLM'}], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-2B-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-2B-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval, MTPB'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B). The checkpoint included in this repository is denoted as CodeGen-Multi 2B, where Multi means the model is initialized with CodeGen-NL 2B and further pre-trained on a dataset of multiple programming languages, and 2B refers to the number of trainable parameters.' </s>\"\n"}
{"text": "<s>[INST]لإنشاء نموذج لتصنيف الصوت، اجمع مجموعة متنوعة من عينات الصوت المصنفة. ثم قم بتقسيمها إلى فئات مختلفة باستخدام أدوات تصنيف الصوت المتاحة. قم بتدريب النموذج باستخدام البيانات العلامة وقيم أداؤه بدقة واستجابته لمختلف الأصوات. وأخيراً، قم بتحسين أداء النموذج باستخدام تقنيات التعلم الآلي ومعالجة الصوت لتحقيق أفضل نتائج.? [/INST]'<<<domain>>>': 'Audio Audio Classification', '<<<api_call>>>': \\\"foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\", '<<<api_provider>>>': 'SpeechBrain', '<<<explanation>>>': \\\"1. Start by importing the foreign_class from the speechbrain.pretrained.interfaces package.\\\\n2. Use the foreign_class function to load the pretrained model 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP'. This will create a CustomEncoderWav2vec2Classifier instance, which can detect emotions in speech audio.\\\\n3. Use the classify_file method of this classifier to analyze a user's spoken audio. This function will return the emotion probabilities, scores, indexes, and text labels for the detected emotions.\\\\n4. You can use the obtained text labels and other relevant information to react appropriately to the user's emotional state while delivering counseling services.\\\", '<<<code>>>': \\\"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\\nout_prob, score, index, text_lab = classifier.classify_file(user_audio_file_path)\\\\nprint(text_lab)\\\", 'api_call': \\\"foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\", 'provider': 'SpeechBrain', 'api_data': {'domain': 'Audio Audio Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP', 'api_call': \\\"foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\", 'api_arguments': ['file_path'], 'python_environment_requirements': ['pip install speechbrain'], 'example_code': \\\"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source='speechbrain/emotion-recognition-wav2vec2-IEMOCAP', pymodule_file='custom_interface.py', classname='CustomEncoderWav2vec2Classifier')\\\\nout_prob, score, index, text_lab = classifier.classify_file('speechbrain/emotion-recognition-wav2vec2-IEMOCAP/anger.wav')\\\\nprint(text_lab)\\\", 'performance': {'dataset': 'IEMOCAP', 'accuracy': '78.7%'}, 'description': 'This repository provides all the necessary tools to perform emotion recognition with a fine-tuned wav2vec2 (base) model using SpeechBrain. It is trained on IEMOCAP training data.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لتصنيف الصور غير المراقبة في مجال رؤية الحاسوب.? [/INST]'<<<domain>>>': 'Computer Vision Zero-Shot Image Classification', '<<<api_call>>>': \\\"pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the pipeline class from transformers which is provided by Hugging Face.\\\\n2. Create a zero-shot-image-classification pipeline with the model 'Bingsu/clip-vit-base-patch32-ko'.\\\\n3. The provided model is trained for Korean CLIP and can be used for zero-shot image classification tasks without prior training data.\\\\n4. The user provides a URL of the image, and the model will return the plant genus based on the provided candidate_labels.\\\\n5. Add the plant genera you want to classify as candidate_labels.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nplant_classifier = pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\\\\nimage_url = 'https://example.com/user_plant_image.jpg'\\\\ncandidate_labels = ['acer', 'quercus', 'pinus', 'betula', 'fagus']\\\\nplant_genus = plant_classifier(images=image_url, candidate_labels=candidate_labels, hypothesis_template='{}')\\\", 'api_call': \\\"pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Zero-Shot Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'clip-vit-base-patch32-ko', 'api_call': \\\"pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\\\", 'api_arguments': {'images': 'url', 'candidate_labels': 'Array of strings', 'hypothesis_template': 'String'}, 'python_environment_requirements': [['transformers', 'torch', 'PIL', 'requests']], 'example_code': \\\"from transformers import pipeline\\\\nrepo = 'Bingsu/clip-vit-base-patch32-ko'\\\\npipe = pipeline('zero-shot-image-classification', model=repo)\\\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\\\nresult = pipe(images=url, candidate_labels=[], hypothesis_template='{}')\\\\nresult\\\", 'performance': {'dataset': 'AIHUB', 'accuracy': 'Not provided'}, 'description': 'Korean CLIP model trained by Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. It is a zero-shot image classification model that can be used to classify images without any training data.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير نموذج للكشف عن الأشياء في صور لتطبيقات الرؤية الحاسوبية، حيث نقوم بتحديد وتحليل الأشياء الموجودة في الصور لتحسين جودة الكشف.? [/INST]'<<<domain>>>': 'Computer Vision Object Detection', '<<<api_call>>>': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries, which in this case include the YOLOv8 class from ultralyticsplus.\\\\n2. Initialize the YOLOv8 object detection model by specifying the model name 'keremberke/yolov8s-csgo-player-detection'.\\\\n3. Modify appropriate configurations like confidence threshold, IOU threshold, agnostic NMS, and max_det to fine-tune the object detection process.\\\\n4. Once the model is loaded, use the 'predict' method to analyze the input image and detect players in the image.\\\\n5. The detected players' coordinates can be extracted from the 'boxes' attribute in the returned results.\\\\n6. Optionally, render the result by overlaying the detected boxes onto the input image using the 'render_result' function.\\\", '<<<code>>>': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage_url = 'https://example.com/csgo_gameplay_image.jpg'\\\\nresults = model.predict(image_url)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image_url, result=results[0])\\\\nrender.show()\\\", 'api_call': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \\\"YOLO('keremberke/yolov8s-csgo-player-detection')\\\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': [['ultralyticsplus==0.0.23', 'ultralytics==8.0.21']], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \\\"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\\\" </s>\"\n"}
{"text": "<s>[INST]أنشئ نموذج استرجاع للإجابة باستخدام تقنيات رؤية الحاسوب لكشف الكائنات.? [/INST]{'domain': 'Computer Vision Object Detection', 'api_call': \\\"YOLO('keremberke/yolov8m-table-extraction')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. First, we import the YOLO class and the render_result function from the ultralyticsplus package.', 'code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-table-extraction')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path_to_scanned_document.jpg'\\\\n# replace 'path_to_scanned_document.jpg' with the path to your scanned document\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\", 'api_data': {'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8m-table-extraction', 'api_call': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-table-extraction')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'path_to_scanned_document.jpg'\\\\n# replace 'path_to_scanned_document.jpg' with the path to your scanned document\\\\nresults = model.predict(image)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \\\"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO('keremberke/yolov8m-table-extraction')\\\\nmodel.overrides['conf'] = 0.25\\\\nmodel.overrides['iou'] = 0.45\\\\nmodel.overrides['agnostic_nms'] = False\\\\nmodel.overrides['max_det'] = 1000\\\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\\\", 'performance': {'dataset': 'table-extraction', 'accuracy': 0.952}, 'description': 'A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset.'}} </s>\"\n"}
{"text": "<s>[INST]يجب عليك توفير ملفات صوتية للإدخال والإخراج، وتحديد الصوت الذي تود استخراجه ونسخته الصوتية.? [/INST]'<<<domain>>>': 'Audio Audio-to-Audio', '<<<api_call>>>': \\\"pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first import the pipeline function from the transformers library provided by Hugging Face.\\\\n2. The pipeline function is used to create an audio-source-separation model, which is capable of separating individual voices from a mixture of audio signals. This model, 'Awais/Audio_Source_Separation', has been trained on the Libri2Mix dataset.\\\\n3. We then provide the podcast audio file as input to the model, which will perform the source separation.\\\\n4. The resulting separated audio sources can then be individually saved and analyzed as required.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nseparator = pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\\\\nseparated_audio_sources = separator(create_audio_patch(file_path))\\\\n# Replace 'file_path' with the path to your audio file\\\", 'api_call': \\\"pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Audio-to-Audio', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'Awais/Audio_Source_Separation', 'api_call': \\\"pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\\\", 'api_arguments': 'audio', 'python_environment_requirements': ['Asteroid'], 'example_code': '', 'performance': {'dataset': 'Libri2Mix', 'accuracy': {'si_sdr': 14.764543634468069, 'si_sdr_imp': 14.764029375607246, 'sdr': 15.29337970745095, 'sdr_imp': 15.114146605113111, 'sir': 24.092904661115366, 'sir_imp': 23.913669683141528, 'sar': 16.06055906916849, 'sar_imp': -51.980784441287454, 'stoi': 0.9311142440593033, 'stoi_imp': 0.21817376142710482}}, 'description': 'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.' </s>\"\n"}
{"text": "<s>[INST]نقدم شركتنا نموذجًا لمعالجة اللغة الطبيعية مع إمكانية ملء الفجوة، ونود تقييم قدرة النموذج على توليد نصوص طبيعية ومفهومة بدقة وسلاسة.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='roberta-large')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. We will first import the necessary library from the transformers package, which includes the pipeline function for creating a model pipeline.\\\\n2. Then, we use the pipeline function to create a fill-mask model using the 'roberta-large' pre-trained model. This model is designed to predict suitable words for masked positions in a given sentence.\\\\n3. The created unmasker can be used to suggest appropriate words for the missing positions in the input sentence.\\\\n4. Users can provide their sentences with '<mask>' symbol representing the missing word, and the trained model will generate candidates for the masked positions.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'roberta-large\\\\')\\\\nsentence = \\\"The weather today is <mask> and beautiful.\\\"\\\\nsuggestions = unmasker(sentence)', 'api_call': \\\"pipeline('fill-mask', model='roberta-large')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'roberta-large', 'api_call': \\\"pipeline('fill-mask', model='roberta-large')\\\", 'api_arguments': {'model': 'roberta-large'}, 'python_environment_requirements': ['transformers'], 'example_code': \\\"from transformers import pipeline\\\\nunmasker = pipeline('fill-mask', model='roberta-large')\\\\nunmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': 'GLUE', 'accuracy': {'MNLI': 90.2, 'QQP': 92.2, 'QNLI': 94.7, 'SST-2': 96.4, 'CoLA': 68.0, 'STS-B': 96.4, 'MRPC': 90.9, 'RTE': 86.6}}, 'description': 'RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using the Masked language modeling (MLM) objective. It can be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering.' </s>\"\n"}
{"text": "<s>[INST]نطاقنا يتعلق بالتعامل مع النصوص وتجميع المعلومات من النصوص لإنتاج ملخصات بواسطة الحوسبة، وسيتم إتقان تصميم نماذج لمعالجة اللغة الطبيعية بواسطة الذكاء الاصطناعي، ويجب تحديد الخصائص الأساسية والتفاصيل في النصوص لاستخدامها في تلك النماذج.? [/INST]'<<<domain>>>': 'Natural Language Processing Summarization', '<<<api_call>>>': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. To perform automatic summarization of articles, you will need to import the 'pipeline' function from the transformers package.\\\\n2. Create an instance of the summarization pipeline using the pre-trained 'google/pegasus-newsroom' model, which has been specifically trained for summarization tasks on news-related datasets.\\\\n3. To use the pipeline for summarization, simply call it with the text of an article, and it will return a summarized version.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nsummarizer = pipeline(\\\\'summarization\\\\', model=\\\\'google/pegasus-newsroom\\\\')\\\\narticle_text = \\\"Insert the full article text here...\\\"\\\\nsummary = summarizer(article_text, max_length=100, min_length=10, do_sample=False)[0][\\\\'summary_text\\\\']', 'api_call': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/pegasus-newsroom', 'api_call': \\\"pipeline('summarization', model='google/pegasus-newsroom')\\\", 'api_arguments': '', 'python_environment_requirements': ['transformers'], 'example_code': 'This model can be loaded on the Inference API on-demand.', 'performance': {'dataset': 'newsroom', 'accuracy': '45.98/34.20/42.18'}, 'description': 'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks.' </s>\"\n"}
{"text": "<s>[INST]تقوم شركتنا بتطوير تقنيات توليد النصوص من النصوص في مجال معالجة اللغات الطبيعية باستخدام الذكاء الاصطناعي، نحتاج إلى توليد نصوص جديدة بناءً على النصوص المعطاة.? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. We import the necessary function (pipeline) from the transformers library provided by Hugging Face.\\\\n2. We use the pipeline function, specifying the task as \\\\'text2text-generation\\\\' and the model \\\\'pszemraj/flan-t5-large-grammar-synthesis\\\\'. This model has been fine-tuned for grammar correction tasks, providing improved language quality.\\\\n3. We pass the raw input text with incorrect grammar/spelling to the pipeline(\\\"text2text-generation\\\", \\\"pszemraj/flan-t5-large-grammar-synthesis\\\") function to generate corrected, more professional text.', '<<<code>>>': \\\"from transformers import pipeline\\\\ncorrector = pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\\nraw_text = 'I haz some truble with mi code. I waz workin on a projct n I kep gitin errorz. Can u pls luk at it and hlp mi fix it? Thanx in advanc.'\\\\nresult = corrector(raw_text)\\\\ncorrected_text = result[0]['generated_text']\\\", 'api_call': \\\"pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pszemraj/flan-t5-large-grammar-synthesis', 'api_call': \\\"pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\\\", 'api_arguments': ['raw_text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import pipeline\\\\n\\\\ncorrector = pipeline(\\\\n 'text2text-generation',\\\\n 'pszemraj/flan-t5-large-grammar-synthesis',\\\\n )\\\\n\\\\nraw_text = 'i can has cheezburger'\\\\nresults = corrector(raw_text)\\\\nprint(results)\\\", 'performance': {'dataset': 'jfleg', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of google/flan-t5-large for grammar correction on an expanded version of the JFLEG dataset.' </s>\"\n"}
{"text": "<s>[INST]يجب تصميم نموذج لمعالجة اللغة الطبيعية لقياس التشابه بين مقاطع الفيديو في تصنيف الفيديوهات في مجال الرؤية الحاسوبية، وهو أمر حاسم لتحسين الأداء ودقة النظام.? [/INST]{'domain': 'Computer Vision Video Classification', 'api_call': \\\"AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': \\\"1. Import the necessary libraries, including 'AutoModelForVideoClassification' from transformers and 'torch' library for handling video tensors.\\\", 'code': 'import torch\\\\nfrom transformers import AutoModelForVideoClassification\\\\nfrom preprocessing import preprocess_video\\\\n\\\\nmodel = AutoModelForVideoClassification.from_pretrained(\\\\'lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2\\\\')\\\\nvideo_path = \\\"path/to/your/video.mp4\\\"\\\\nprocessed_video = preprocess_video(video_path)\\\\npredictions = model(processed_video)\\\\n\\\\n# Analyze the output to determine the action being performed in the video.', 'api_data': {'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Video Classification', 'api_name': 'lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2', 'api_call': 'import torch\\\\nfrom transformers import AutoModelForVideoClassification\\\\nfrom preprocessing import preprocess_video\\\\n\\\\nmodel = AutoModelForVideoClassification.from_pretrained(\\\\'lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2\\\\')\\\\nvideo_path = \\\"path/to/your/video.mp4\\\"\\\\nprocessed_video = preprocess_video(video_path)\\\\npredictions = model(processed_video)\\\\n\\\\n# Analyze the output to determine the action being performed in the video.', 'api_arguments': 'video_path', 'python_environment_requirements': 'transformers==4.27.4, torch==2.0.0+cu117, datasets==2.11.0, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': 0.7212}, 'description': 'This model is a fine-tuned version of MCG-NJU/videomae-large-finetuned-kinetics on an unknown dataset.'}} </s>\"\n"}
{"text": "<s>[INST]تحتاج تقنية معالجة اللغة الطبيعية التي نستخدمها في الحوارات إلى معرفة كيفية فهم وتحليل اللغة الطبيعية، وتطوير القدرة على التعرف على الأسئلة والاستجابة بشكل منطقي ومفهوم.? [/INST]'<<<domain>>>': 'Natural Language Processing Conversational', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the AutoModelForCausalLM and AutoTokenizer classes from the transformers library.\\\\n2. Load the tokenizer and the DialoGPT-large model using the from_pretrained method of the respective classes.\\\\n3. Encode user-input questions using the tokenizer.\\\\n4. Use the model's generate method to generate chatbot responses based on the user inputs by providing the bot_input_ids.\\\\n5. Decode the chatbot responses and print them out.\\\", '<<<code>>>': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'microsoft/DialoGPT-large\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'microsoft/DialoGPT-large\\\\')\\\\n\\\\nuser_input = \\\"Can you tell me about your company\\\\'s services?\\\"\\\\ninput_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\\\\'pt\\\\')\\\\ngenerated_response_ids = model.generate(input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\nresponse = tokenizer.decode(generated_response_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\\\\nprint(response)', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Conversational', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DialoGPT-large', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\\\", 'api_arguments': ['bot_input_ids', 'max_length', 'pad_token_id'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-large)\\\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-large)\\\\n\\\\nfor step in range(5):\\\\n new_user_input_ids = tokenizer.encode(input(&gt;&gt; User:) + tokenizer.eos_token, return_tensors='pt')\\\\n bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step &gt; 0 else new_user_input_ids\\\\n chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n print(DialoGPT: {}.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\\\", 'performance': {'dataset': 'Reddit discussion thread', 'accuracy': 'Comparable to human response quality under a single-turn conversation Turing test'}, 'description': 'DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.' </s>\"\n"}
{"text": "<s>[INST]يتطلب بناء نظام استجابة متعدد الوسائط للاستفسارات البصرية تحليل اللغة الطبيعية القدرة على التعرف على السياق وتحليل وتقدير عمق المعنى في النصوص المدخلة لمعالجة جدول أسئلة اللغة الطبيعية.? [/INST]{'domain': 'Multimodal Visual Question Answering', 'api_call': \\\"pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')\\\", 'api_provider': 'Hugging Face', 'explanation': \\\"1. We first import the required libraries, which mainly include the 'pipeline' function from the 'transformers' package provided by Hugging Face.\\\", 'code': 'from transformers import pipeline\\\\nvqa = pipeline(\\\\'visual-question-answering\\\\', model=\\\\'JosephusCheung/GuanacoVQAOnConsumerHardware\\\\')\\\\nimage_path = \\\"path/to/street_scene_image.jpg\\\"\\\\nquestion = \\\"Is there a zebra crossing in the image?\\\"\\\\nanswer = vqa(image_path, question)', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'api_call': 'from transformers import pipeline\\\\nvqa = pipeline(\\\\'visual-question-answering\\\\', model=\\\\'JosephusCheung/GuanacoVQAOnConsumerHardware\\\\')\\\\nimage_path = \\\"path/to/street_scene_image.jpg\\\"\\\\nquestion = \\\"Is there a zebra crossing in the image?\\\"\\\\nanswer = vqa(image_path, question)', 'api_arguments': {'model': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'tokenizer': 'JosephusCheung/GuanacoVQAOnConsumerHardware'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': 'vqa(image_path, question)', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'unknown'}, 'description': 'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.'}} </s>\"\n"}
{"text": "<s>[INST]أنا متخصص في توجيه النماذج عبر الوسائط المتعددة، وأود أن أعلم كيف يمكن توليد وصف نصي دقيق لصورة ثلاثية الأبعاد تم إنشاؤها باستخدام تقنية الرؤية الحاسوبية من صورة ثنائية الأبعاد تصور مشهد للحيوانات في البرية. هل يمكنك مساعدتي في تحقيق هذا؟? [/INST]{'domain': 'Multimodal Image-to-Text', 'api_call': \\\"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '', 'code': \\\"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://example.com/image.jpg'\\\\n# replace with the URL of the image you want to analyze\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\\\\nprocessor = Pix2StructProcessor.from_pretrained('google/pix2struct-textcaps-base')\\\\ninputs = processor(images=image, return_tensors='pt')\\\\npredictions = model.generate(**inputs)\\\\n\\\\nsummary = processor.decode(predictions[0], skip_special_tokens=True)\\\", 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \\\"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\\\nfrom PIL import Image\\\\nimport requests\\\\n\\\\nurl = 'https://example.com/image.jpg'\\\\n# replace with the URL of the image you want to analyze\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\\\\nprocessor = Pix2StructProcessor.from_pretrained('google/pix2struct-textcaps-base')\\\\ninputs = processor(images=image, return_tensors='pt')\\\\npredictions = model.generate(**inputs)\\\\n\\\\nsummary = processor.decode(predictions[0], skip_special_tokens=True)\\\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتحليل الصوت لاكتشاف نشاط الصوت في الفيديو. هل بإمكانك مساعدتي في تطوير نموذج للكشف عن الأصوات في الحيوانات في البرية؟? [/INST]'<<<domain>>>': 'Audio Voice Activity Detection', '<<<api_call>>>': \\\"VoiceActivityDetection(segmentation='pyannote/segmentation')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the pyannote.audio.pipelines package. Specifically, VoiceActivityDetection is required for detecting voice activity.\\\\n2. Create a VoiceActivityDetection pipeline and use the 'pyannote/segmentation' model as the segmentation parameter.\\\\n3. Set the desired hyperparameters for onset, offset, minimum duration on, and minimum duration off.\\\\n4. Instantiate the pipeline with the hyperparameters using the instantiate() method.\\\\n5. Process the audio file using the pipeline to get voice activity detection results.\\\", '<<<code>>>': \\\"from pyannote.audio.pipelines import VoiceActivityDetection\\\\npipeline = VoiceActivityDetection(segmentation='pyannote/segmentation')\\\\nHYPER_PARAMETERS = {\\\\n  'onset': 0.5,\\\\n  'offset': 0.5,\\\\n  'min_duration_on': 0.0,\\\\n  'min_duration_off': 0.0\\\\n}\\\\npipeline.instantiate(HYPER_PARAMETERS)\\\\nvad_results = pipeline('audio_path.wav')\\\\n# Replace 'audio_path.wav' with the path to your audio file\\\", 'api_call': \\\"VoiceActivityDetection(segmentation='pyannote/segmentation')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Voice Activity Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'pyannote/segmentation', 'api_call': \\\"VoiceActivityDetection(segmentation='pyannote/segmentation')\\\", 'api_arguments': {'onset': 0.5, 'offset': 0.5, 'min_duration_on': 0.0, 'min_duration_off': 0.0}, 'python_environment_requirements': ['pyannote.audio 2.0'], 'example_code': 'from pyannote.audio.pipelines import VoiceActivityDetection\\\\npipeline = VoiceActivityDetection(segmentation=pyannote/segmentation)\\\\nHYPER_PARAMETERS = {\\\\n  onset: 0.5, offset: 0.5,\\\\n  min_duration_on: 0.0,\\\\n  min_duration_off: 0.0\\\\n}\\\\npipeline.instantiate(HYPER_PARAMETERS)\\\\nvad = pipeline(audio.wav)', 'performance': {'dataset': {'ami': {'accuracy': {'onset': 0.684, 'offset': 0.577, 'min_duration_on': 0.181, 'min_duration_off': 0.037}}, 'dihard': {'accuracy': {'onset': 0.767, 'offset': 0.377, 'min_duration_on': 0.136, 'min_duration_off': 0.067}}, 'voxconverse': {'accuracy': {'onset': 0.767, 'offset': 0.713, 'min_duration_on': 0.182, 'min_duration_off': 0.501}}}}, 'description': 'Model from End-to-end speaker segmentation for overlap-aware resegmentation, by Hervé Bredin and Antoine Laurent. It provides voice activity detection, overlapped speech detection, and resegmentation functionalities.' </s>\"\n"}
{"text": "<s>[INST]قم بتطبيق خوارزمية التعلم التعزيزي على بيئة معينة وقم بتدريب النموذج على تحسين أداءه بمرور الوقت.? [/INST]'<<<domain>>>': 'Reinforcement Learning', '<<<api_call>>>': \\\"load_from_hub(repo_id='sb3/ppo-BreakoutNoFrameskip-v4',filename='{MODEL FILENAME}.zip',)\\\", '<<<api_provider>>>': 'Stable-Baselines3', '<<<explanation>>>': \\\"1. Install the 'RL Zoo', 'stable-baselines3', and 'stable-baselines3-contrib' packages.\\\\n2. Use the 'load_from_hub' function from 'rl_zoo3' package to load the pre-trained model 'sb3/ppo-BreakoutNoFrameskip-v4', which is a Proximal Policy Optimization (PPO) model trained to play the 'BreakoutNoFrameskip-v4' game environment.\\\\n3. Save the downloaded model in a zip file named '{MODEL FILENAME}.zip'.\\\\n4. Use the model to create an AI player for the Breakout game in the children's museum by integrating it into the game code.\\\", '<<<code>>>': \\\"from rl_zoo3 import load_from_hub\\\\nmodel = load_from_hub(repo_id='sb3/ppo-BreakoutNoFrameskip-v4', filename='{MODEL FILENAME}.zip',)\\\", 'api_call': \\\"load_from_hub(repo_id='sb3/ppo-BreakoutNoFrameskip-v4',filename='{MODEL FILENAME}.zip',)\\\", 'provider': 'Stable-Baselines3', 'api_data': {'domain': 'Reinforcement Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ppo-BreakoutNoFrameskip-v4', 'api_call': \\\"load_from_hub(repo_id='sb3/ppo-BreakoutNoFrameskip-v4',filename='{MODEL FILENAME}.zip',)\\\", 'api_arguments': ['algo', 'env', 'orga', 'f'], 'python_environment_requirements': [['rl_zoo3', 'stable-baselines3', 'stable-baselines3-contrib']], 'example_code': ['python -m rl_zoo3.load_from_hub --algo ppo --env BreakoutNoFrameskip-v4 -orga sb3 -f logs/', 'python enjoy.py --algo ppo --env BreakoutNoFrameskip-v4 -f logs/', 'python train.py --algo ppo --env BreakoutNoFrameskip-v4 -f logs/', 'python -m rl_zoo3.push_to_hub --algo ppo --env BreakoutNoFrameskip-v4 -f logs/ -orga sb3'], 'performance': {'dataset': 'BreakoutNoFrameskip-v4', 'accuracy': '398.00 +/- 16.30'}, 'description': 'This is a trained model of a PPO agent playing BreakoutNoFrameskip-v4 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نظام تصنيف الرموز بمجال معالجة اللغة الطبيعية يقوم بتحويل البيانات الصوتية إلى نص مكتوب بشكل تلقائي ودقيق، وقم بتدريب النموذج باستخدام مجموعة كبيرة ومتنوعة من البيانات الصوتية لضمان أداءه المتميز.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes, such as AutoTokenizer and AutoModelForTokenClassification, from the transformers library provided by Hugging Face.\\\\n2. Load the pre-trained model 'd4data/biomedical-ner-all', which is capable of recognizing 107 bio-medical entities from text, using AutoModelForTokenClassification.\\\\n3. Load the corresponding tokenizer for the model using AutoTokenizer.\\\\n4. Create a pipeline using the loaded model, tokenizer, and the pipeline function from the transformers library.\\\\n5. Use the created pipeline to analyze the biomedical paper and extract bio-medical entities from the text.\\\", '<<<code>>>': \\\"from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\\\\nmodel = AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\\ntokenizer = AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\\\\nbiomedical_ner = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy='simple')\\\\nentities = biomedical_ner(biomedical_paper_text)\\\", 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'd4data/biomedical-ner-all', 'api_call': \\\"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\\\", 'api_arguments': {'model': 'AutoModelForTokenClassification.from_pretrained(d4data/biomedical-ner-all)', 'tokenizer': 'AutoTokenizer.from_pretrained(d4data/biomedical-ner-all)', 'aggregation_strategy': 'simple'}, 'python_environment_requirements': [{'transformers': 'from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification'}], 'example_code': 'pipe(The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.)', 'performance': {'dataset': 'Maccrobat', 'accuracy': 'Not provided'}, 'description': 'An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased.' </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بإنشاء نموذج تقدير العمق باستخدام تقنيات الرؤية الحاسوبية. هل يمكنك مساعدتي في تنفيذ هذا المشروع؟? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required libraries, which include 'AutoModel' from the transformers package, provided by Hugging Face.\\\\n2. Use 'AutoModel.from_pretrained()' function to load the pre-trained depth estimation model 'sayakpaul/glpn-nyu-finetuned-diode-221215-095508'.\\\\n3. The loaded model is based on the GLPN architecture and fine-tuned on the DIODE dataset, which is a large dataset for depth estimation.\\\\n4. Use this model to perform depth estimation on input images. By processing these depth maps, the augmented reality app can render the environment accurately.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\ndepth_model = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\\\\ndepth_estimation = depth_model(image_input)\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221215-095508', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\\\", 'api_arguments': None, 'python_environment_requirements': [['transformers']], 'example_code': None, 'performance': {'dataset': 'DIODE', 'accuracy': None}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.' </s>\"\n"}
{"text": "<s>[INST]العميل يعمل على تطوير تقنية تقدير عمق الرؤية الحاسوبية. نحتاج إلى برنامج يستطيع تحويل الصور إلى بيانات عمق لتحسين الدقة والفعالية في تطبيقاتنا.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We first need to import the necessary class from the transformers package. This includes AutoModel for loading the pre-trained model of interest.\\\\n2. We load the pre-trained depth estimation model 'sayakpaul/glpn-nyu-finetuned-diode-221116-062619'.\\\\n3. We then need to preprocess the input image using the appropriate transforms, such as resizing and normalization.\\\\n4. Pass the preprocessed image as input to the model to estimate the depth map.\\\\n5. The depth map generated can then be used to measure the depth of various objects in the image.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nimport torch\\\\nimport torchvision.transforms as transforms\\\\nfrom PIL import Image\\\\n\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\\\\nimage = Image.open('input_image.jpg')\\\\n\\\\ntransform = transforms.Compose([\\\\n    transforms.Resize((640, 480)), \\\\n    transforms.ToTensor(), \\\\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\\\\n\\\\npreprocessed_image = transform(image).unsqueeze(0)\\\\noutput = model(preprocessed_image)\\\\ndepth_map = output.cpu().squeeze().numpy()\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221116-062619', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\\\", 'api_arguments': 'None', 'python_environment_requirements': ['Transformers 4.24.0, Pytorch 1.13.0+cu117, Tokenizers 0.13.2'], 'example_code': 'None', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.548, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي في مجال تصنيف الصور بالرؤية الحاسوبية.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. To create this tool, we will use the pre-trained model `martinezomg/vit-base-patch16-224-diabetic-retinopathy`, which is specifically fine-tuned for diabetic retinopathy detection in retinal images.\\\\n2. First, import the `pipeline` function from the transformers library.\\\\n3. Use the `pipeline()` function to create an image classification model, specifying the `martinezomg/vit-base-patch16-224-diabetic-retinopathy` model to be loaded.\\\\n4. Once the model is loaded, it can be used to classify retinal images, returning the probability of diabetic retinopathy.\\\\n5. The created classifier can be used to evaluate any retinal images for the presence of diabetic retinopathy.', '<<<code>>>': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\\nresult = image_classifier('path/to/retinal_image.jpg')\\\", 'api_call': \\\"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy', 'api_call': \\\"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\", 'api_arguments': {'model_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy'}, 'python_environment_requirements': [{'transformers': '4.28.1', 'pytorch': '2.0.0+cu118', 'datasets': '2.11.0', 'tokenizers': '0.13.3'}], 'example_code': \\\"from transformers import pipeline\\\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\\\nresult = image_classifier('path/to/image.jpg')\\\", 'performance': {'dataset': 'None', 'accuracy': 0.7744}, 'description': 'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.' </s>\"\n"}
{"text": "<s>[INST]نقترح عليك اختيار مكتبة معالجة اللغة الطبيعية المناسبة لتوليد ملخص قصير ومفيد للنصوص. يمكنك استخدام تقنيات مثل تحليل التعبيرات المفتاحية وتجزئة الجمل لإنشاء ملخص متكامل يعكس المحتوى الرئيسي للنص. جرب تطبيق الخوارزميات المختلفة المتاحة في هذا المجال وقم بضبط البارامترات والخصائص وفقاً لاحتياجاتك الخاصة.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('human-centered-summarization/financial-summarization-pegasus')\\\", 'api_provider': 'Transformers', 'explanation': '1. We use the PegasusForConditionalGeneration and the PegasusTokenizer classes from the transformers library.', 'code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration\\\\nmodel_name = \\\\'human-centered-summarization/financial-summarization-pegasus\\\\'\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\n\\\\ntext_to_summarize = \\\"Your financial news article text here\\\"\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=\\\\'pt\\\\').input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nsummary = tokenizer.decode(output[0], skip_special_tokens=True)', 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Transformers', 'functionality': 'text2text-generation', 'api_name': 'financial-summarization-pegasus', 'api_call': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration\\\\nmodel_name = \\\\'human-centered-summarization/financial-summarization-pegasus\\\\'\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\n\\\\ntext_to_summarize = \\\"Your financial news article text here\\\"\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=\\\\'pt\\\\').input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nsummary = tokenizer.decode(output[0], skip_special_tokens=True)', 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabia’s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba’s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region’s third-largest lender. The entity’s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East’s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))', 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 35.206, 'ROUGE-2': 16.569, 'ROUGE-L': 30.128, 'ROUGE-LSUM': 30.171}}, 'description': 'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.'}} </s>\"\n"}
{"text": "<s>[INST]قم بتطوير برنامج يقوم بتحويل النص إلى كلام لمستخدمي تطبيق لمساعدة أشخاص ذوي الإعاقة البصرية باستخدام تقنيات معالجة اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Text Generation', '<<<api_call>>>': \\\"AutoModelForCausalLM.from_pretrained('af1tang/personaGPT')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the transformers library, such as AutoTokenizer and AutoModelForCausalLM.\\\\n2. Use the from_pretrained method to load the 'af1tang/personaGPT' model. This model is designed for generating personalized responses based on input personality facts and incorporating turn-level goals in its responses.\\\\n3. If a GPU is available, move the model to the GPU.\\\\n4. Prepare the input context by providing personality facts and previous dialog history.\\\\n5. Use the model to generate a response for a given user message. The generated response can then be used to find common preferences and discuss common interests with the user.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\nimport torch\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'af1tang/personaGPT\\\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\\'af1tang/personaGPT\\\\')\\\\n\\\\nif torch.cuda.is_available():\\\\n    model = model.cuda()\\\\n\\\\n# Provide the input context, e.g., personality facts and previous dialog history, and generate a response\\\\ninput_context = \\\"your input here\\\"\\\\ndialog_hx = [input_context]\\\\n\\\\nbot_input_ids = tokenizer.encode(dialog_hx, return_tensors=\\\\'pt\\\\')\\\\nresponse_ids = model.generate(bot_input_ids, do_sample=True, top_k=10, top_p=0.92, max_length=1000)\\\\nresponse_text = tokenizer.decode(response_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('af1tang/personaGPT')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'af1tang/personaGPT', 'api_call': \\\"AutoModelForCausalLM.from_pretrained('af1tang/personaGPT')\\\", 'api_arguments': {'do_sample': 'True', 'top_k': '10', 'top_p': '.92', 'max_length': '1000', 'pad_token': 'tokenizer.eos_token_id'}, 'python_environment_requirements': [{'transformers': 'GPT2Tokenizer, GPT2LMHeadModel', 'torch': 'torch'}], 'example_code': {'load_model': 'tokenizer = AutoTokenizer.from_pretrained(af1tang/personaGPT)\\\\nmodel = AutoModelForCausalLM.from_pretrained(af1tang/personaGPT)\\\\nif torch.cuda.is_available():\\\\n model = model.cuda()', 'generate_response': 'bot_input_ids = to_var([personas + flatten(dialog_hx)]).long()\\\\nmsg = generate_next(bot_input_ids)\\\\ndialog_hx.append(msg)\\\\nprint(Bot: {}.format(tokenizer.decode(msg, skip_special_tokens=True)))'}, 'performance': {'dataset': 'Persona-Chat', 'accuracy': 'Not provided'}, 'description': 'PersonaGPT is an open-domain conversational agent designed to do 2 tasks: decoding personalized responses based on input personality facts (the persona profile of the bot) and incorporating turn-level goals into its responses through action codes (e.g., talk about work, ask about favorite music). It builds on the DialoGPT-medium pretrained model based on the GPT-2 architecture.' </s>\"\n"}
{"text": "<s>[INST]قم بتجميع مجموعة من الصور مع توصيلها بشكل نصي وصوتي. قم بتحليل الصور واستخراج النصوص المتعلقة بها، ثم قم بتوليف النصوص مع الصوتيات المقابلة لتحقيق تجربة متعددة الوسائط.? [/INST]'<<<domain>>>': 'Multimodal Image-to-Text', '<<<api_call>>>': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary pipeline function from the transformers library.\\\\n2. Use the pipeline function to instantiate the 'kha-white/manga-ocr-base' model, a pre-trained OCR model specialized in Japanese manga text recognition.\\\\n3. This model uses the Vision Encoder Decoder framework and is able to recognize both vertical and horizontal text, text with furigana, text overlaid on images, and various fonts and font styles typically used in manga.\\\\n4. Pass the manga image (either as a file path or an image object) as an argument to run the OCR model and get the extracted text.\\\\n5. The extracted text can now be used as subtitles for foreign viewers.\\\", '<<<code>>>': \\\"from transformers import pipeline\\\\nmanga_ocr = pipeline('ocr', model='kha-white/manga-ocr-base')\\\\nimage_path = 'path/to/your/manga/image.jpg'\\\\nextracted_text = manga_ocr(image_path)\\\", 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \\\"pipeline('ocr', model='kha-white/manga-ocr-base')\\\", 'api_arguments': 'image', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>\"\n"}
{"text": "<s>[INST]يقوم فريقنا بتطوير نموذج لمعالجة اللغة الطبيعية لتلخيص النصوص بفعالية، ونهدف إلى تطوير نظام يختصر المحتوى اللغوي بشكل دقيق ومناسب باستخدام التقنيات الحديثة في هذا المجال.? [/INST]{'domain': 'Natural Language Processing Summarization', 'api_call': \\\"PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the necessary class (PegasusForConditionalGeneration) and tokenizer (PegasusTokenizer) from transformers.', 'code': \\\"from transformers import PegasusForConditionalGeneration, PegasusTokenizer\\\\nmodel = PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntokenizer = PegasusTokenizer.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntext = '长篇新闻文章...'\\\\ninputs = tokenizer(text, max_length=1024, return_tensors='pt')\\\\nsummary_ids = model.generate(inputs['input_ids'])\\\\nsummary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\\\", 'api_data': {'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Text Summarization', 'api_name': 'Randeng-Pegasus-238M-Summary-Chinese', 'api_call': \\\"from transformers import PegasusForConditionalGeneration, PegasusTokenizer\\\\nmodel = PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntokenizer = PegasusTokenizer.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntext = '长篇新闻文章...'\\\\ninputs = tokenizer(text, max_length=1024, return_tensors='pt')\\\\nsummary_ids = model.generate(inputs['input_ids'])\\\\nsummary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\\\", 'api_arguments': {'text': 'string', 'max_length': 'integer'}, 'python_environment_requirements': ['transformers', 'tokenizers_pegasus.py', 'data_utils.py'], 'example_code': \\\"from transformers import PegasusForConditionalGeneration\\\\nfrom tokenizers_pegasus import PegasusTokenizer\\\\nmodel = PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntokenizer = PegasusTokenizer.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\\\ntext = '1'\\\\ninputs = tokenizer(text, max_length=1024, return_tensors='pt')\\\\nsummary_ids = model.generate(inputs['input_ids'])\\\\ntokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\\\", 'performance': {'dataset': 'LCSTS', 'accuracy': {'rouge-1': 43.46, 'rouge-2': 29.59, 'rouge-L': 39.76}}, 'description': 'Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus. It is fine-tuned on 7 Chinese text summarization datasets including education, new2016zh, nlpcc, shence, sohu, thucnews, and weibo. The model can be used to generate summaries for Chinese text inputs.'}} </s>\"\n"}
{"text": "<s>[INST]قم ببناء نموذج تصنيف الصور باستخدام مبادئ الرؤية الحاسوبية. يقوم النموذج بتصنيف الصور المدخلة بناءً على المحتوى الموجود فيها. يجب أن يكون لديك مجموعة تدريبية من الصور المصنفة بشكل صحيح لتدريب النموذج. استخدم تقنيات مثل الشبكات العصبية الاصطناعية وتقنيات التعلم العميق لتحسين أداء النموذج في تصنيف الصور.? [/INST]'<<<domain>>>': 'Computer Vision Image Classification', '<<<api_call>>>': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries such as requests, PIL (Python Imaging Library), and transformers.\\\\n2. Utilize the Vision Transformer (ViT) for image classification by loading the 'nateraw/vit-age-classifier' pretrained model.\\\\n3. To preprocess the input image, obtain the ViTFeatureExtractor object from the same pretrained model.\\\\n4. Apply the feature extractor (transforms) to the input image to create tensors that will be passed into the ViT model.\\\\n5. Obtain the model output, which includes logits representing age categories.\\\\n6. Apply softmax to the logits for obtaining probabilities and find the age class with the highest probability as the final prediction.\\\", '<<<code>>>': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom io import BytesIO\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\n\\\\nurl = 'https://example.com/image.jpg'\\\\nresponse = requests.get(url)\\\\nimage = Image.open(BytesIO(response.content))\\\\n\\\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\\\n\\\\ninputs = transforms(image, return_tensors='pt')\\\\noutput = model(**inputs)\\\\n\\\\nproba = output.logits.softmax(1)\\\\npredicted_age_class = proba.argmax(1)\\\", 'api_call': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'nateraw/vit-age-classifier', 'api_call': \\\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/vit-age-classifier'}, 'python_environment_requirements': [['requests', 'PIL', 'transformers']], 'example_code': \\\"import requests\\\\nfrom PIL import Image\\\\nfrom io import BytesIO\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\n\\\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\\\nim = Image.open(BytesIO(r.content))\\\\n\\\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\\\n\\\\ninputs = transforms(im, return_tensors='pt')\\\\noutput = model(**inputs)\\\\n\\\\nproba = output.logits.softmax(1)\\\\npreds = proba.argmax(1)\\\", 'performance': {'dataset': 'fairface', 'accuracy': None}, 'description': \\\"A vision transformer finetuned to classify the age of a given person's face.\\\" </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لتحويل النص المكتوب إلى نص مكتوب بشكل تلقائي باستخدام تقنيات معالجة اللغة الطبيعية (NLP) للجيل النصوص (Text2Text Generation).? [/INST]'<<<domain>>>': 'Natural Language Processing Text2Text Generation', '<<<api_call>>>': \\\"AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-question-generation-ap')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': '1. First, import the AutoTokenizer and AutoModelWithLMHead classes from the Hugging Face transformers package.\\\\n2. Load the pre-trained T5 model \\\"mrm8488/t5-base-finetuned-question-generation-ap\\\" using the AutoTokenizer and AutoModelWithLMHead methods.\\\\n3. To generate a question from the provided context and answer, combine the answer and context into a single input string.\\\\n4. Pass the input string into the tokenizer, and then feed the tokenized input into the pre-trained T5 model.\\\\n5. The model returns a question generated from the provided context and answer.', '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'mrm8488/t5-base-finetuned-question-generation-ap\\\\')\\\\nmodel = AutoModelWithLMHead.from_pretrained(\\\\'mrm8488/t5-base-finetuned-question-generation-ap\\\\')\\\\n\\\\ndef get_question(answer, context):\\\\n    input_string = f\\\"answer: {answer} context: {context}\\\"\\\\n    tokenized_input = tokenizer(input_string, return_tensors=\\\"pt\\\")\\\\n    outputs = model.generate(**tokenized_input)\\\\n    question = tokenizer.decode(outputs[0])\\\\n    return question\\\\n\\\\ncontext = \\\"In 1705, the mathematician and physicist Isaac Newton published his theories on the laws of motion and universal gravitation in a book called \\\\'Philosophiæ Naturalis Principia Mathematica\\\\'.\\\"\\\\nanswer = \\\"Newton\\\"\\\\ngenerated_question = get_question(answer, context)', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-question-generation-ap')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'mrm8488/t5-base-finetuned-question-generation-ap', 'api_call': \\\"AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-question-generation-ap')\\\", 'api_arguments': {'answer': 'string', 'context': 'string', 'max_length': 'integer (optional, default=64)'}, 'python_environment_requirements': [{'transformers': 'from source'}], 'example_code': {'tokenizer': 'AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-question-generation-ap)', 'model': 'AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-question-generation-ap)', 'get_question': 'get_question(answer, context)'}, 'performance': {'dataset': 'SQuAD', 'accuracy': 'Not provided'}, 'description': \\\"Google's T5 model fine-tuned on SQuAD v1.1 for Question Generation by prepending the answer to the context.\\\" </s>\"\n"}
{"text": "<s>[INST]يمكنك توليد صورة من وصف نصي لتصنيف الرموز في مجال معالجة اللغة الطبيعية. يمكنك البحث عن الحيوانات في البرية لصور تصويرية غير مصنفة وتقديم مساعدة في هذا العمل.? [/INST]'<<<domain>>>': 'Natural Language Processing Token Classification', '<<<api_call>>>': \\\"SequenceTagger.load('flair/ner-german')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary classes from the flair package. This includes Sentence for processing textual data and SequenceTagger for the named entity recognition model.\\\\n2. We then use the load method of the SequenceTagger class to load the pre-trained model 'flair/ner-german'. This model has been trained for four-class named entity recognition in the German language.\\\\n3. We create a Sentence object containing the input text that we want to analyze.\\\\n4. The tagger.predict function is called on the sentence object. This function uses the pre-trained model to identify named entities in the text.\\\\n5. Lastly, we iterate over the recognized entities and display them.\\\", '<<<code>>>': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\ntagger = SequenceTagger.load('flair/ner-german')\\\\nsentence = Sentence('Berlins Bürgermeister Michael Müller und Frankfurts Oberbürgermeister Peter Feldmann besuchten das Reichstagsgebäude.')\\\\ntagger.predict(sentence)\\\\n\\\\nfor entity in sentence.get_spans('ner'):\\\\n    print(entity)\\\", 'api_call': \\\"SequenceTagger.load('flair/ner-german')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'flair/ner-german', 'api_call': \\\"SequenceTagger.load('flair/ner-german')\\\", 'api_arguments': ['Sentence'], 'python_environment_requirements': [['flair']], 'example_code': \\\"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\n# load tagger\\\\ntagger = SequenceTagger.load('flair/ner-german')\\\\n\\\\n# make example sentence\\\\nsentence = Sentence('George Washington ging nach Washington')\\\\n\\\\n# predict NER tags\\\\ntagger.predict(sentence)\\\\n\\\\n# print sentence\\\\nprint(sentence)\\\\n\\\\n# print predicted NER spans\\\\nprint('The following NER tags are found:')\\\\n\\\\n# iterate over entities and print\\\\nfor entity in sentence.get_spans('ner'):\\\\n    print(entity)\\\", 'performance': {'dataset': 'conll2003', 'accuracy': '87.94'}, 'description': 'This is the standard 4-class NER model for German that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF.' </s>\"\n"}
{"text": "<s>[INST]أنشئ مجموعة لعلوم معالجة اللغة الطبيعية، حيث يجتمع الأعضاء كل أسبوعين للتدريب على ملء الفجوات. يجب تزويدهم بأداة لملء الفجوات باستخدام اللغة الطبيعية.? [/INST]'<<<domain>>>': 'Natural Language Processing Fill-Mask', '<<<api_call>>>': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", '<<<api_provider>>>': 'Transformers', '<<<explanation>>>': \\\"1. To assist with automatically completing sentences, we will import the pipeline function from the Hugging Face transformers library.\\\\n2. We'll instantiate the fill-mask pipeline with the model 'distilroberta-base'. This model is a distilled version of RoBERTa specifically designed for the task of filling in the blanks (mask tokens) in sentences.\\\\n3. By passing the partially completed sentence with a mask token (mask) in place of the missing word(s), the model can predict the most likely words to fill in the blank to make the sentence more fluent.\\\", '<<<code>>>': 'from transformers import pipeline\\\\n\\\\nunmasker = pipeline(\\\\'fill-mask\\\\', model=\\\\'distilroberta-base\\\\')\\\\ncompleted_sentence = unmasker(\\\"The software allows users to <mask> files between devices.\\\")', 'api_call': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", 'provider': 'Transformers', 'api_data': {'domain': 'Natural Language Processing Fill-Mask', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'distilroberta-base', 'api_call': \\\"pipeline('fill-mask', model='distilroberta-base')\\\", 'api_arguments': ['text'], 'python_environment_requirements': [['transformers']], 'example_code': \\\"unmasker(Hello I'm a <mask> model.)\\\", 'performance': {'dataset': 'openwebtext', 'accuracy': 'Not provided'}, 'description': \\\"DistilRoBERTa is a distilled version of the RoBERTa-base model, designed to be smaller, faster, and lighter. It is a Transformer-based language model trained on the OpenWebTextCorpus, which is a reproduction of OpenAI's WebText dataset. The model has 6 layers, 768 dimensions, and 12 heads, totaling 82M parameters. It is primarily intended for fine-tuning on downstream tasks such as sequence classification, token classification, or question answering.\\\" </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج للإجابة على الأسئلة المرئية متعددة الوسائط.? [/INST]'<<<domain>>>': 'Multimodal Visual Question Answering', '<<<api_call>>>': \\\"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the required pipeline function from the transformers library provided by Hugging Face.\\\\n2. Use the pipeline function to create a visual question answering model by specifying the 'visual-question-answering' task and the model to be loaded ('ivelin/donut-refexp-combined-v1').\\\\n3. Provide the image file and the visitor's question as inputs to the created model.\\\\n4. The model will analyze the image and provide an answer to the visitor's question based on the visual content and the context of the question.\\\", '<<<code>>>': 'from transformers import pipeline\\\\nvqa = pipeline(\\\\'visual-question-answering\\\\', model=\\\\'ivelin/donut-refexp-combined-v1\\\\')\\\\nimage_path = \\\\'path/to/image.jpg\\\\'\\\\nquestion = \\\"What is the color of the object?\\\"\\\\nanswer = vqa(image=image_path, question=question)', 'api_call': \\\"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'ivelin/donut-refexp-combined-v1', 'api_call': \\\"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\\\", 'api_arguments': 'image, question', 'python_environment_requirements': ['transformers'], 'example_code': \\\"vqa(image='path/to/image.jpg', question='What is the color of the object?')\\\", 'performance': {'dataset': 'ivelin/donut-refexp-combined-v1', 'accuracy': 'N/A'}, 'description': 'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.' </s>\"\n"}
{"text": "<s>[INST]أكتب برنامج لتقدير العمق في الرؤية الحواسبية باستخدام خوارزمية معينة.? [/INST]'<<<domain>>>': 'Computer Vision Depth Estimation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import the necessary libraries AutoModel from transformers package.\\\\n2. Load the pre-trained model 'sayakpaul/glpn-nyu-finetuned-diode-221228-072509'. This model specializes in depth estimation for images, which will help the robot's navigation in the 3D printing facility.\\\\n3. The robot's camera can be used to capture images of the environment in real-time, which will then be processed by the model to estimate depth information.\\\\n4. This estimated depth information can be integrated into the robot's navigation system to enhance its ability to avoid obstacles and move safely through the 3D printing facility.\\\", '<<<code>>>': \\\"from transformers import AutoModel\\\\nmodel = AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\\\\n# Load image from the robot's camera\\\\n# Process the image to get depth estimation\\\", 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'glpn-nyu-finetuned-diode-221228-072509', 'api_call': \\\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\\\", 'api_arguments': '', 'python_environment_requirements': ['Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4012, 'Mae': 0.403, 'Rmse': 0.6173, 'Abs Rel': 0.3487, 'Log Mae': 0.1574, 'Log Rmse': 0.211, 'Delta1': 0.4308, 'Delta2': 0.6997, 'Delta3': 0.8249}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>\"\n"}
{"text": "<s>[INST]إلى كل خبير في ترجمة معالجة اللغة الطبيعية: لتحسين جودة تصنيف النص، يُقترح إضافة أداة جديدة في نوادي الطلاب الافتراضية. يتم اجتماع النادي كل أسبوعين للعب ضد وكيل التعلم الآلي، مهمته تحسين تقنيات تصنيف النص بمعالجة اللغة الطبيعية.? [/INST]{'domain': 'Natural Language Processing Translation', 'api_call': \\\"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\\\", 'api_provider': 'Hugging Face Transformers', 'explanation': '1. Import the required libraries and classes, including pipeline and AutoModelForSeq2SeqLM from the transformers package.', 'code': 'from transformers import pipeline, AutoModelForSeq2SeqLM\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-ru\\\\')\\\\ntranslation_pipeline = pipeline(\\\\'translation_en_to_ru\\\\', model=model)\\\\ndocument = \\\"<your document in English>\\\"\\\\ntranslated_text = translation_pipeline(document)\\\\n# Save the translated text as desired', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Translation', 'api_name': 'opus-mt-en-ru', 'api_call': 'from transformers import pipeline, AutoModelForSeq2SeqLM\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'Helsinki-NLP/opus-mt-en-ru\\\\')\\\\ntranslation_pipeline = pipeline(\\\\'translation_en_to_ru\\\\', model=model)\\\\ndocument = \\\"<your document in English>\\\"\\\\ntranslated_text = translation_pipeline(document)\\\\n# Save the translated text as desired', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers', 'torch', 'tensorflow'], 'example_code': \\\"from transformers import pipeline\\\\ntranslation = pipeline('translation_en_to_ru', model='Helsinki-NLP/opus-mt-en-ru')\\\\ntranslated_text = translation('Hello, how are you?')\\\", 'performance': {'dataset': 'newstest2019-enru', 'accuracy': '27.1'}, 'description': 'Helsinki-NLP/opus-mt-en-ru is a translation model trained on the OPUS dataset, which translates English text to Russian. It is based on the Marian NMT framework and can be used with Hugging Face Transformers.'}} </s>\"\n"}
{"text": "<s>[INST]أنا مهتم بتحويل الصوت إلى نص لتطبيق الشبابيك التلقائية. هل يمكنك مساعدتي في ذلك؟? [/INST]'<<<domain>>>': 'Audio Automatic Speech Recognition', '<<<api_call>>>': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. We import required classes from the transformers library to load the speech recognition model and processor.\\\\n2. We load the WhisperProcessor and WhisperForConditionalGeneration from the pre-trained model 'openai/whisper-tiny.en'. This model is trained to transcribe human speech to text.\\\\n3. For recording transcription, we first prepare the audio file containing the meeting recordings.\\\\n4. The processor is used to preprocess the audio file and generate input features that can be fed to the model for performing automatic speech recognition (ASR).\\\\n5. We then feed the input features to the ASR model, which generates a sequence of predicted token IDs.\\\\n6. The predicted token IDs are then decoded using the WhisperProcessor to obtain the final transcription of the recorded meeting.\\\", '<<<code>>>': \\\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\n\\\\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-tiny.en')\\\\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\\\\n\\\\n# replace 'audio_file.wav' with path to your audio file\\\\nsample = {'array': 'audio_file.wav', 'sampling_rate': 16000}\\\\ninput_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\\", 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'openai/whisper-tiny.en', 'api_call': \\\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')\\\", 'api_arguments': {'model_name': 'openai/whisper-tiny.en'}, 'python_environment_requirements': [['transformers', 'datasets', 'torch']], 'example_code': ['from transformers import WhisperProcessor, WhisperForConditionalGeneration', 'from datasets import load_dataset', 'processor = WhisperProcessor.from_pretrained(openai/whisper-tiny.en)', 'model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-tiny.en)', 'ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)', 'sample = ds[0][audio]', 'input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features', 'predicted_ids = model.generate(input_features)', 'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)'], 'performance': {'dataset': 'LibriSpeech (clean)', 'accuracy': 8.437}, 'description': 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.' </s>\"\n"}
{"text": "<s>[INST]قم بإنشاء نموذج لترجمة اللغة الطبيعية، حيث يقوم النموذج بتحويل النص من لغة مصدرية إلى لغة هدفية باستخدام تقنيات معالجة اللغة الطبيعية. يمكنك استخدام مجموعة البيانات المترجمة لتدريب النموذج وتقييم أدائه.? [/INST]'<<<domain>>>': 'Natural Language Processing Translation', '<<<api_call>>>': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", '<<<api_provider>>>': 'Hugging Face Transformers', '<<<explanation>>>': \\\"1. Import necessary classes from the transformers library provided by Hugging Face, which include: AutoTokenizer and AutoModelForSeq2SeqLM.\\\\n2. Instantiate the tokenizer and the model using the pre-trained Swedish-to-English translation model 'Helsinki-NLP/opus-mt-sv-en'.\\\\n3. Tokenize the input Swedish text and pass the tokenized input to the model for translation.\\\\n4. Decode the model's output to obtain the translated English text.\\\", '<<<code>>>': 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\\\'Helsinki-NLP/opus-mt-sv-en\\\\')\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\\'Helsinki-NLP/opus-mt-sv-en\\\\')\\\\n\\\\nswedish_text = \\\"Ditt exempel på svenska här\\\"\\\\ninputs = tokenizer.encode(swedish_text, return_tensors=\\\"pt\\\")\\\\noutputs = model.generate(inputs)\\\\nenglish_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)', 'api_call': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", 'provider': 'Hugging Face Transformers', 'api_data': {'domain': 'Natural Language Processing Translation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'opus-mt-sv-en', 'api_call': \\\"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\\\", 'api_arguments': ['inputs'], 'python_environment_requirements': [['transformers']], 'example_code': '', 'performance': {'dataset': 'Tatoeba.sv.en', 'accuracy': 'BLEU: 64.5, chr-F: 0.763'}, 'description': 'A Swedish to English translation model trained on the OPUS dataset using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece.' </s>\"\n"}

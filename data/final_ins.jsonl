{"text": "<s>[INST]ابتكر تطبيقًا يمكنه تحليل الموسيقى وتوصية المستخدمين بالأغاني المناسبة لذوقهم الفني.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"}
{"text": "<s>[INST] كتب أداة تساعد المستخدمين في تحويل النصوص إلى لغة برمجة معينة بشكل تلقائي.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"}
{"text": "<s>[INST] اكتب برنامجًا يساعد في تحليل تصرفات المستخدمين عبر مواقع التواصل الاجتماعي.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"}
{"text": "<s>[INST] قم بتطوير منصة لتحليل المقالات الطبية وتحديد المعلومات الهامة والإحصائيات الحيوية.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"}
{"text": "<s>[INST] عيّن نظام تصنيف آلي لفئات الأفلام بناءً على محتواها وتصنيفات المستخدمين.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"}
{"text": "<s>[INST] قم بتصميم وتنفيذ بوت ذكاء اصطناعي لخدمة عملاء شركتك عبر الدردشة.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"}
{"text": "<s>[INST] إنشاء مكتبة تفاعلية لتصدير البيانات الجغرافية إلى تنسيقات مختلفة لتحليل البواقع.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"}
{"text": "<s>[INST] بناء نظام توصية للمستخدمين لتحديد الأزياء الأنيقة استنادًا إلى تفضيلاتهم الشخصية.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"}
{"text": "<s>[INST] بناء نظام ذكاء اصطناعي يستخدم تقنيات الرؤية الحاسوبية لتحليل صور الأشخاص والكشف عن المزاج والمعالم الوجهية.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"}
{"text": "<s>[INST] طور برنامجًا يستخدم تقنيات التعلم الآلي للتنبؤ بأداء الأسهم وتوجيه المستثمرين.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"}
{"text": "<s>[INST]نحن نرغب في تطوير نظام يتيح لنا تحليل مدى اتساع النصوص التي تُدخل إلى منصتنا عبر الإنترنت. هل يمكنك تقديم ال API المناسب لهذه الغاية؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"}
{"text": "<s>[INST] نود تطوير نظام يساعدنا في تحديد الأماكن الجذابة للزوار في مناطق السياحة. هل يمكنك اقتراح API لاستخراج المعلومات اللازمة لذلك؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"}
{"text": "<s>[INST] لدينا مشروع لتطوير تطبيق يسهل عملية الترجمة الآلية للنصوص بدقة وسلاسة. ما هي الAPI التي تنصح باستخدامها لهذا الغرض؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"}
{"text": "<s>[INST] نرغب في تطوير نظام يساعدنا في تصنيف المستندات الطبية تلقائياً. هل يمكنك تقديم API مناسبة لهذا الغرض؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"}
{"text": "<s>[INST] عميلنا يرغب في تحليل كتبه الرقمية لفهم المواضيع التي يتناولها. هل يمكنك توجيهنا إلى API مناسب لتحقيق هذا الهدف؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"}
{"text": "<s>[INST] تحتاج شركتنا إلى تقدير جودة الترجمة الآلية المستخدمة في تطبيقنا الحالي. هل تعرف API تقدم هذه الخدمة بكفاءة؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"}
{"text": "<s>[INST] نود تطوير نظام يستطيع تحليل الأخبار واستخراج المعلومات الرئيسية منها. هل يمكنك توجيهنا إلى API يساعدنا في ذلك؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"}
{"text": "<s>[INST] يهدف مشروعنا إلى تحسين خدمة العملاء عبر تحليل محتوى مكالمات الهاتف. هل يمكنك توجيهنا إلى أداة تحليل نصوص تناسب هذا الغرض؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"}
{"text": "<s>[INST] نرغب في تحسين أداء نظام التصنيف الضروري لمشروعنا. هل يمكنك تقديم API تعزز مهارات التصنيف النصي؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"}
{"text": "<s>[INST] نود تطوير أداة تحليل نصوص بشكل تفاعلي وسريع. هل يمكنك توجيهنا إلى API يدعم هذا الهدف؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"}
{"text": "<s>[INST]ترغب مدرستنا في تحسين تعلم اللغة الإنجليزية للطلاب الصغار عبر تطبيق تفاعلي. نحتاج إلى أداة تتيح التعرف على نطق الكلمات بدقة.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"}
{"text": "<s>[INST] تخطط عيادتنا لتحسين خدمة العملاء من خلال تطبيق الدردشة الآلية. نحتاج إلى واجهة برمجية تمكن التفاعل مع العملاء بشكل ذكي.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"}
{"text": "<s>[INST] نريد تحسين تجربة التسوق عبر الإنترنت لعملائنا من خلال تصميم نظام توصيات شخصية. ماهي الطريقة المثلى لتحقيق ذلك؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"}
{"text": "<s>[INST] لدى منصة تعليمية ترغب في تحسين تجربة المستخدمين في اختباراتها الإلكترونية. نرغب في استخدام تقنية التصنيف الآلي لتقديم تقييم سريع ودقيق.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"}
{"text": "<s>[INST] نرغب في إنشاء تطبيق لتعليم اللغات الأجنبية يستند إلى الذكاء الاصطناعي. ما هي أفضل واجهة برمجية لتقديم دروس تفاعلية للمستخدمين؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"}
{"text": "<s>[INST] تسعى منصتنا الإلكترونية إلى تقديم تجربة تسوق مخصصة لكل مستخدم بناءً على تفضيلاته الشخصية. ما هو أفضل اسم لواجهة برمجية تحقق هذا الهدف؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"}
{"text": "<s>[INST] لدينا تطبيق يهدف إلى تعزيز اللياقة البدنية للمستخدمين. نرغب في تضمين نظام تتبّع تدريبات اللياقة البدنية. ما هو أفضل اسم واجهة برمجية لهذه الخدمة؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"}
{"text": "<s>[INST] تريد شركتنا تحسين عمليات التسويق الرقمي من خلال تخصيص العروض لكل عميل بناءً على سلوكه التفاعلي. ما هو أفضل اسم لواجهة برمجية تفعل هذا؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"}
{"text": "<s>[INST] تقوم شبكة المدارس بتنفيذ نظام تعليم عن بُعد للطلاب. نحتاج إلى تكامل نظام إلكتروني لتقديم الاختبارات وتقييم أداء الطلاب. ما هي أفضل واجهة برمجية لهذا الغرض؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"}
{"text": "<s>[INST] نهدف إلى تحسين تجربة تسوق التجزئة عبر استخدام تكنولوجيا الواقع المعزز. نحتاج إلى دمج واجهة برمجية لتوفير تجربة تسوق مبتكرة للعملاء.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"}
{"text": "<s>[INST]تطوير تطبيق لتحليل مستويات الإجهاد والتوتر للمستخدمين. يرجى استخدام API التالي لاستخراج الميزات اللازمة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"}
{"text": "<s>[INST] إنشاء منصة للتعلم عن بسبب المتداولات بالبورصة. استخدم API التالي لاستخراج المعلومات الضرورية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"}
{"text": "<s>[INST] تصميم تطبيق للمراقبة الصحية الشخصية. قم باستخدام API التالي لاستخراج البيانات اللازمة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"}
{"text": "<s>[INST] تطوير نظام توصية الموسيقى الشخصية. استخدم API التالي لاستخراج التفاصيل اللازمة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"}
{"text": "<s>[INST] تنفيذ نظام تنبؤ بالطقس بناءً على بيانات خاصة بالموقع الجغرافي. استخدم API التالي للحصول على المعلومات الجوية الحالية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"}
{"text": "<s>[INST] تصميم تطبيق يقدم مشورة تغذية صحية عبر الهاتف المحمول. استخدم API التالي للحصول على المعلومات الغذائية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"}
{"text": "<s>[INST] إنشاء نظام لتحليل المشاعر الموجودة في التعليقات على وسائل التواصل الاجتماعي. استخدم الAPI التالي لاستعراض المشاركات.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"}
{"text": "<s>[INST] تنفيذ نظام توصية الأفلام المخصص. قم باستخدام API التالي لاستخراج تفاصيل الأفلام.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"}
{"text": "<s>[INST] تطوير تطبيق يحسن عمليات التعلم عن بُعد. استخدم API التالي لاستخراج المعلومات التعليمية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"}
{"text": "<s>[INST] إعداد منصة لمتابعة التدفقات الحية لمباريات الرياضة. استخدم API التالي للحصول على معلومات الأحداث الرياضية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"}
{"text": "<s>[INST]تحتاج منظمتنا لنظام لتحليل محتوى وسائل التواصل الاجتماعي لفهم تفاعل الجمهور مع منشوراتنا.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] يحتاج فريق التسويق لدينا إلى واجهة برمجة تطبيقات تتيح لهم إجراء تحليل لغوي لعناوين الإعلانات.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] في مشروعنا الحالي، نود تحويل سجلات الأصوات إلى نص لتحسين عملية توثيق المحادثات.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] عندنا مجموعة كبيرة من النصوص الطبية ونرغب في تصنيفها تلقائياً حسب الموضوعات الرئيسية.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] تحتاج مجموعتنا لتوليد نصوص توجيهية ذات تنسيق محدد لتعليم نماذج الذكاء الاصطناعي.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] نحتاج إلى واجهة أمامية للتفاعل مع نموذج تعلم آلي يتنبأ بالأعطال المحتملة في أجهزة الكمبيوتر.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] تريد شركتنا تطوير نظام يستخدم النصوص التقنية لتوليد أسئلة امتحانات للتدريب على الموضوعات الهندسية.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] نعمل على تطوير نظام يحسن التكامل بين القواميس اللغوية لتحسين جودة الترجمة التلقائية.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] تحتاج شركتنا إلى تحليل النصوص القانونية لاستخراج المعلومات الرئيسية منها لتسهيل البحث والتصفح.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] تهدف شركتنا إلى تصميم نظام يستخدم النماذج اللغوية لتحسين توجيه المحادثات الصوتية التفاعلية.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST]تفضل بإنشاء تطبيق يقوم بتحويل النصوص العربية إلى نصوص انجليزية باستخدام تقنيات توليد النصوص.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] يرغب العميل في تطوير تطبيق لتوليد نصوص دقيقة وواضحة من خلال تحويل اللغة العربية إلى لغة انجليزية. هل يمكنك استخدام الأدوات المتاحة في api للمساعدة في هذا الأمر؟? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] لدينا مشروع لتحويل الصور إلى نصوص. هل يمكنك استخدام الأدوات المعرفية المتوفرة لتحويل بيانات الصور إلى نصوص واضحة؟? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] يرغب العميل في إنشاء تطبيق لتحويل النصوص الدينية من العربية إلى اللغات الأخرى. هل يمكنك استخدام التكنولوجيا المعرفية المتاحة لدعم المشروع؟? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] يطلب العميل تصميم تطبيق يعتمد على التعلم الآلي لتشخيص الأمراض الجلدية من الصور. يرجى استخدام تقنيات توليد النصوص لتقديم وصف دقيق لنتائج التشخيص.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] أنت مطلوب لتصميم خدمة تقوم بتحليل اللغة النصية لردود فعل العملاء على المنتجات. استخدم التكنولوجيا المعرفية لتبسيط هذه المهمة وتوليد تقارير مفصلة.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] ترغب مؤسسة خيرية في تطوير نظام لتوليد تقارير توضح القضايا الاجتماعية بشكل دوري. اسخدم الأدوات الذكية لإنشاء نصوص إحصائية توضح المعلومات بشكل دقيق وفعال.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] يحتاج المشروع العلمي لتوليد تقارير توضح النتائج البحثية بشكل ملائم ودقيق. استفد من التكنولوجيا المعرفية لتحسين جودة النصوص وإتقان التقارير البحثية.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] نظام خدمة العملاء بحاجة إلى تحسين في توليد الردود التلقائية عن استفسارات العملاء. يرجى استخدام التكنولوجيا المعرفية لتوجيهنا في كيفية تنفيذ هذا التطوير بكفاءة.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST] يرغب عميلنا في دمج خدمة الترجمة التلقائية في تطبيقه، يرجى اعتماد الحلول المعرفية لتطوير نظام يسهل ترجمة النصوص بفاعلية.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"}
{"text": "<s>[INST]نود تطبيقًا يتمكن من تحليل نصوص التويتر لاستخراج المشاعر والمواضيع الرئيسية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"}
{"text": "<s>[INST] لدى مشروع يتطلب تحويل النصوص الطبية إلى تشخيصات طبية. يرجى تقديم API مناسب لتحقيق ذلك.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"}
{"text": "<s>[INST] تحتاج منصتنا إلى إنشاء نظام يقوم بفهم الإشارات الصوتية وتحويلها إلى نص. هل يمكن توجيهنا إلى API مناسب لهذه المهمة؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"}
{"text": "<s>[INST] نحن نبحث عن حلاً يمكنه تحليل سلوك مستخدمي تطبيقات الهاتف المحمول. هل لديكم API مناسب للقيام بذلك؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"}
{"text": "<s>[INST] نريد بناء نظام للتعرف على أنماط الاحتيال في عمليات الدفع الإلكتروني. هل يمكنكم تقديم API للقيام بهذه المهمة؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"}
{"text": "<s>[INST] نحن بحاجة إلى تطبيق لتوصيف الصور الطبية وتحديد الأمراض المعروضة فيها. يرجى تقديم API للقيام بهذه المهمة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"}
{"text": "<s>[INST] نود تطبيقًا يقوم بتحديد مواقف السيارات الشاغرة في الطرق. هل يمكنكم توجيهنا إلى API مناسب لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"}
{"text": "<s>[INST] عميلنا يطلب نظامًا يمكنه تتبع تطور الأسعار في السوق المالية. هل تستطيعون توفير API لهذه الوظيفة؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"}
{"text": "<s>[INST] لدينا مشروع يتعلق بكشف الإساءة والتنمر عبر الإنترنت. هل لديكم API يمكن استخدامه لتحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"}
{"text": "<s>[INST] نحن نبحث عن تكنولوجيا لتحليل الرأي العام حول منتجاتنا من خلال تقييمات المستخدمين. هل يمكنكم توجيهنا إلى API مناسب لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"}
{"text": "<s>[INST]يتعين علينا تحديد المواقع التي تظهر فيها الازدحام المروري وتوفير تقارير دورية حولها.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"}
{"text": "<s>[INST] يجب علينا تحليل مشكلة التصنيف الآلي للنصوص لتحسين تجربة المستخدمين في تصفح موقعنا.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"}
{"text": "<s>[INST] تطوير تطبيق يتيح للمستخدمين تعلم لغة جديدة من خلال تمارين تفاعلية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"}
{"text": "<s>[INST] ضع خطة تنفيذية لتحسين تصنيف مقالات الأخبار بناءً على تفاعل القراء ومشاركتهم.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"}
{"text": "<s>[INST] كتابة سيناريوات تفاعلية تساعد في تحسين تجربة المستخدم على منصة التجارة الإلكترونية الخاصة بك.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"}
{"text": "<s>[INST] تطبيق نظام توصية لتحسين تجربة المستخدمين عن طريق توجيههم للمنتجات المناسبة بناءً على تفضيلاتهم الشخصية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"}
{"text": "<s>[INST] تصميم واجهة مستخدم جذابة لتعزيز تفاعل المستخدمين مع تطبيق الألعاب الجديد الخاص بك.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"}
{"text": "<s>[INST] تطوير نظام ذكاء اصطناعي يقدم توقعات دقيقة حول السوق المالية والاستثمارات.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"}
{"text": "<s>[INST] تعزيز الأمان لتطبيق الويب الخاص بك من خلال تطبيق أحدث تقنيات التشفير والمراقبة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"}
{"text": "<s>[INST] إنشاء نموذج تفاعلي لتصور أفكار جديدة لتطوير تكنولوجيا جديدة في شركتك.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"}
{"text": "<s>[INST]أنا أبحث عن طريقة لتحليل ردود فعل العملاء على منتجاتنا من خلال تقنيات معالجة لغوية طبيعية. ما هو أفضل شيء يمكنني استخدامه؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"}
{"text": "<s>[INST] أحتاج إلى نموذج يستطيع تحديد موضوعات النصوص تلقائيًا. ما هي أفضل تقنية لهذا الغرض؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"}
{"text": "<s>[INST] أريد بناء نموذج يستطيع توليد نصوص إبداعية مثل قصائد أو نصوص فنية. كيف يمكنني البدء في ذلك؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"}
{"text": "<s>[INST] أود إضافة ترجمة تلقائية لموقعي عبر الإنترنت. ما هي أفضل تقنية لتحقيق ذلك؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"}
{"text": "<s>[INST] أريد إنشاء نموذج لتحليل النصوص الطبية واستخراج المعلومات الحساسة. من فضلك، ما هو أداة تحليل النصوص المناسبة؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"}
{"text": "<s>[INST] أبحث عن طريقة لتلخيص النصوص الطويلة تلقائيًا. هل يمكنني استخدام تحويلات بايتيد لهذا الغرض؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"}
{"text": "<s>[INST] أنا أبحث عن حلاً لتصنيف النصوص القانونية بشكل دقيق. ما هي أفضل تقنية تحليل نصوص يمكنني استخدامها؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"}
{"text": "<s>[INST] أريد إنشاء نموذج لتحليل شعور المستخدمين من تعليقاتهم على منصة التواصل الاجتماعي. ما هي أداة التحليل النصي الأنسب لذلك؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"}
{"text": "<s>[INST] أريد تنفيذ تحليل نصوص سريع للتعرف على المواضيع الرئيسية. أي نموذج يمكنني استخدامه لهذا الغرض؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"}
{"text": "<s>[INST] أنا بحاجة إلى نموذج يستطيع تعلم التفاصيل الدقيقة في النصوص الصحفية. ما هو أفضل نموذج لتحقيق ذلك؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"}
{"text": "<s>[INST]يرغب مدرب كرة السلة في تحسين أداء فريقه من خلال تحليل أداء لاعبيه في المباريات السابقة. هل بإمكانك مساعدته باستخدام التعلم الآلي؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"}
{"text": "<s>[INST] تريد شركة النقل العام توفير تطبيق لتحسين تجربة المستخدمين من خلال تقديم تنبؤات دقيقة لأوقات الوصول. كيف يمكن تطبيق التعلم الآلي لهذا الغرض؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"}
{"text": "<s>[INST] تحتاج شركة السيارات لتطوير نظام تنبؤ بأعطال المركبات للحد من التوقفات غير المتوقعة. كيف يمكن تطبيق تقنيات التعلم الآلي لهذا الغرض؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"}
{"text": "<s>[INST] تريد شركة الصحة الرقمية تطوير نظام توصيف آلي لتحسين التشخيص الطبي. كيف يمكن استخدام تعلم الآلي لتحقيق هذا الهدف؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"}
{"text": "<s>[INST] تريد شركة البحث العلمي تحسين عملية فهم النصوص الطبية الكبيرة. هل يمكنك توجيههم إلى كيفية تعتمد التعلم الآلي في هذا المجال؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"}
{"text": "<s>[INST] يرغب المتحدثون العامون في تطوير تطبيق لتحليل التغريدات وتصنيفها إلى مجموعات مختلفة. كيف يمكن هؤلاء الأفراد استخدام تقنيات التعلم الآلي؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"}
{"text": "<s>[INST] تريد شركة الأمن السيبراني تحسين أداء نظام الكشف عن التهديدات عبر تحليل سلوك المستخدمين. كيف يمكن استخدام تعلم الآلي في هذا السياق؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"}
{"text": "<s>[INST] ترغب شركة التجارة الإلكترونية في توفير تجربة تسوق أكثر شخصنة للعملاء من خلال توصيات المنتجات الأكثر توافقًا مع ذوق كل فرد. كيف يمكن تحسين هذه العملية باستخدام التعلم الآلي؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"}
{"text": "<s>[INST] ترغب شركة الترجمة في تعزيز دقة الترجمة التلقائية باستخدام تقنيات التعلم الآلي. كيف يمكن استخدام هذه التقنيات لتحسين جودة الترجمة؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"}
{"text": "<s>[INST] تريد مؤسسة إعلامية تطوير نموذج للتحليل التلقائي للصور لفهم محتواها بشكل أفضل. كيف يمكن أن يساعد التعلم الآلي في تحقيق هذا الهدف؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"}
{"text": "<s>[INST]\"تعمل شركتنا على تطوير روبوت محادثة جديد لخدمة العملاء، ونحن بحاجة إلى وظيفة لتحليل المشاعر في رسائل العملاء.\" \"يتضمن مشروعنا تصنيف صور سلالات الكلاب المختلفة.\" نحن نبحث عن واجهة برمجة التطبيقات (API) التي يمكنها تحديد سلالة الكلاب بدقة في الصور.، أنا أعمل على مشروع يتضمن إنشاء ملخصات نصية للمقالات الطويلة. هل هناك واجهة برمجة تطبيقات يمكن أن تساعد في هذه المهمة؟، \"يعمل فريقنا على تطوير نظام التعرف على الكلام لتطبيق الرعاية الصحية. نحتاج إلى واجهة برمجة التطبيقات (API) لتدوين المصطلحات الطبية المنطوقة بدقة.، أنا أعمل على مشروع يتضمن اكتشاف النشاط الاحتيالي في المعاملات المالية. هل لديك واجهة برمجة التطبيقات (API) التي يمكنها المساعدة في هذه المهمة؟، يتضمن مشروعنا معالجة كميات كبيرة من البيانات النصية بلغات متعددة. نحن بحاجة إلى واجهة برمجة تطبيقات (API) يمكنها إجراء اكتشاف اللغة بدقة.، نحن نعمل على تطوير نظام توصية للأفلام بناءً على تفضيلات المستخدم. هل هناك واجهة برمجة تطبيقات (API) يمكنها تقديم توصيات بشأن الأفلام؟ \"، \"يعمل فريقنا على مشروع يتضمن استخراج المعلومات الأساسية من المستندات القانونية. هل هناك واجهة برمجة تطبيقات يمكنها المساعدة في استخراج الكيانات؟\"، \"أنا أبحث عن واجهة برمجة تطبيقات يمكنها إنشاء وجوه بشرية واقعية بناءً على أوصاف نصية لأداة إنشاء الشخصية في اللعبة.\"، \"يتضمن مشروعنا تحليل المشاعر على وسائل التواصل الاجتماعي المشاركات بلغات مختلفة. نحن نبحث عن واجهة برمجة التطبيقات (API) التي يمكنها التعامل مع تحليل المشاعر متعدد اللغات.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/codebert-base', 'api_call': \"AutoModel.from_pretrained('microsoft/codebert-base')\", 'api_arguments': 'n/a', 'python_environment_requirements': ['transformers'], 'example_code': 'n/a', 'performance': {'dataset': 'CodeSearchNet', 'accuracy': 'n/a'}, 'description': 'Pretrained weights for CodeBERT: A Pre-Trained Model for Programming and Natural Languages. The model is trained on bi-modal data (documents & code) of CodeSearchNet. This model is initialized with Roberta-base and trained with MLM+RTD objective.' </s>"}
{"text": "<s>[INST]لدينا مشروع لتصميم تطبيق يسهل على المستخدمين البحث عن وصفات طعام مبتكرة. نحتاج إلى استخدام نموذج استخراج الميزات لتصنيف الوصفات.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"}
{"text": "<s>[INST] نود تحسين خوارزمية البحث الخاصة بموقعنا لزيادة دقة النتائج. يرجى استخدام نموذج استخراج الميزات لتحليل تسلسلات البحث.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"}
{"text": "<s>[INST] تهدف شركتنا إلى تطوير نظام تسويقي يعتمد على تحليل المشاعر. نحتاج إلى استخدام نموذج لاستخراج ملامح النصوص لتحديد المشاعر المتضمنة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"}
{"text": "<s>[INST] نحن نخطط لإطلاق تطبيق لتعليم اللغات الأجنبية ونريد استخدام نموذج استخراج الميزات لتحليل التراكيب اللغوية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"}
{"text": "<s>[INST] تواجه شركتنا تحديات في تصنيف الصور بدقة عالية. نريد تطبيق نموذج استخراج الميزات لتحديد سمات الصور بشكل دقيق.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"}
{"text": "<s>[INST] نريد تطوير نظام لتحليل النصوص الطبية ونحتاج إلى نموذج يستخرج الميزات لتصنيف الكلمات المرتبطة بالطب.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"}
{"text": "<s>[INST] نود تصميم نظام لتحديد الأخبار الزائفة ونريد استخدام نموذج استخراج الميزات لتحليل هياكل الجمل.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"}
{"text": "<s>[INST] لدينا مشروع يتضمن تقنيات التعلم الآلي للتنبؤ بالأسهم. نحتاج إلى استخدام نموذج استخراج الميزات لتحليل أنماط البيانات المالية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"}
{"text": "<s>[INST] شركتنا تسعى لتحسين تجربة المستخدم في تطبيقات الويب. نحتاج إلى استخدام نموذج استخراج الميزات لتحليل تفاعلات المستخدمين.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"}
{"text": "<s>[INST] نحن نعمل على تطوير نظام لفحص الوثائق القانونية وتصنيفها. نريد استخدام نموذج استخراج الميزات لتحليل هياكل النصوص القانونية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"}
{"text": "<s>[INST]نريد تطوير نظام يمكنه تحليل مشاكل الصوت في البيئات الصاخبة. هل يمكنك اقتراح الخطوات اللازمة لتحقيق ذلك؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"}
{"text": "<s>[INST] نحن نبحث عن طريقة لتحسين تعرف الكلام لروبوتنا الذي يستخدمه الأشخاص ذوو الاحتياجات الخاصة. هل يتوفر API يمكن استخدامه لهذا الغرض؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"}
{"text": "<s>[INST] نرغب في تحسين نظام الاستشعار لسياراتنا الذاتية القيادة. هل يمكن استخدام API معين لتحقيق ذلك؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"}
{"text": "<s>[INST] هل يمكن توظيف تكنولوجيا الذكاء الاصطناعي لتعزيز تصميماتنا المعمارية؟ نريد اقتراح API يمكن استخدامه لتطوير هذا المجال.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"}
{"text": "<s>[INST] أرغب في إنشاء نظام لتشخيص الأمراض بالصور الطبية. هل يوجد API مناسب يمكن استخدامه لهذا الغرض؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"}
{"text": "<s>[INST] نحتاج إلى تحسين تعرف الكلام في تطبيقنا الصوتي. هل يمكن استخدام API معين لتحقيق هذا الهدف؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"}
{"text": "<s>[INST] هل يمكن إنشاء تطبيق لتحليل الشخصيات في الرسوم المتحركة باستخدام تقنيات الذكاء الاصطناعي؟ أريد اقتراح API يمكن الاعتماد عليه لهذا الغرض.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"}
{"text": "<s>[INST] أحتاج إلى استخدام تقنيات التعلم الآلي لتعزيز نظام التشخيص الطبي الخاص بشركتنا. هل يتوفر API يمكن استخدامه لذلك؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"}
{"text": "<s>[INST] كيف يمكننا استخدام التحليل الصوتي باستخدام API لتحسين استجابة تطبيقنا الصوتي؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"}
{"text": "<s>[INST] نرغب في توفير تجربة مستخدم متعددة اللغات في تطبيقنا الجديد. هل توجد واجهة برمجية يمكننا استخدامها لتحقيق ذلك؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"}
{"text": "<s>[INST]لدينا مشروع لتطوير تطبيق يقوم بتحليل محتوى النصوص الطبية واستخراج المعلومات الرئيسية منها، نحتاج إلى استخدام خدمة استخراج الميزات المدمجة في Transformers.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"}
{"text": "<s>[INST] مطورو التطبيقات يرغبون في استخدام تقنيات تعلم الآلة لتعريب النصوص التي يتعاملون معها من مختلف اللغات، يمكنهم استخدام خدمة استخراج الميزات المدربة مسبقًا في Transformers.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"}
{"text": "<s>[INST] تخطط الشركة لبناء نظام توصيات يعتمد على التحليل النصي لتفهم اهتمامات العملاء، يمكنك استخدام API استخراج الميزات في Hugging Face Transformers لتحويل النص إلى تمثيل يمكن تحليله.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"}
{"text": "<s>[INST] تحتاج شركتنا لوحة تحكم تعتمد على النصوص لتلخيص تقارير العمل، يمكنك استخدام خدمة استخراج الميزات في Transformers لاستخراج المعلومات الرئيسية من النصوص بشكل دقيق.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"}
{"text": "<s>[INST] نحتاج لنظام يقوم بتصنيف النصوص القانونية بناءً على محتواها، يمكنك استخدام API استخراج الميزات في Transformers لخدمة هذا الغرض بفعالية ودقة.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"}
{"text": "<s>[INST] عميلنا يطلب تطبيقًا يقوم بتحليل تعليقات المستخدمين على منصات التواصل الاجتماعي لفهم ردود فعلهم، يمكنك استخدام API استخراج الميزات في Hugging Face Transformers لاستخلاص المعلومات الرئيسية من النصوص بكفاءة.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"}
{"text": "<s>[INST] نود تطوير نظام يستخدم تقنيات معالجة اللغة الطبيعية لفهم مقاطع الفيديو الحركية، يمكنك استخدام API استخراج الميزات في Hugging Face Transformers لاستخلاص المعلومات الرئيسية من النصوص المرتبطة بالفيديو.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"}
{"text": "<s>[INST] شركتنا تبحث عن حل يستخدم تقنيات تعلم الآلة لفهم النوبات الصرعية من النصوص الطبية، يمكنك استخدام API استخراج الميزات في Hugging Face Transformers لاستخراج البيانات الأساسية بدقة وسرعة.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"}
{"text": "<s>[INST] يهتم عملاؤنا بفهم مدى تأثير حملاتهم التسويقية من خلال تحليل مواد التسويق، يمكنك استخدام API استخراج الميزات في Hugging Face Transformers لاستخدام تقنيات التعلم الآلي في تحليل النصوص الدعائية.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"}
{"text": "<s>[INST] تطلب شركتنا تقديم خدمات تحليل النصوص لفهم تفاعل العملاء مع منتجاتها، يمكنك استخدام API استخراج الميزات في Transformers لاستخلاص المعلومات الرئيسية من التعليقات والمراجعات بكفاءة عالية.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"}
{"text": "<s>[INST]نريد تطوير تطبيق يقوم بتحويل الصور إلى نصوص باللغة العربية. استخدم الـ API التالي كمرجع:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"}
{"text": "<s>[INST] عميلنا يرغب في بشراء برنامج لتحليل المشاعر من النصوص العربية في وسائل التواصل الاجتماعي. يمكنك استخدام الـ API التالي:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"}
{"text": "<s>[INST] بناءً على طلب الشركة، نحتاج إلى تطوير برنامج يقوم بتحويل الكتب الإلكترونية من النصوص إلى ملفات صوتية. استخدم الـ API التالي:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"}
{"text": "<s>[INST] نرغب في تطوير تطبيق يقوم بترجمة النصوص الطبية إلى العربية. يمكنك استخدام الـ API التالي كمرجع:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"}
{"text": "<s>[INST] لدينا حاجة لتطوير برنامج يستخدم تقنية OCR لاستخراج النصوص من الصور باللغة العربية. قم باستخدام الـ API التالي:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"}
{"text": "<s>[INST] عميلنا يرغب في تحليل الكلمات الرئيسية في نصوص عربية. يمكنك استخدام الـ API التالي لتحقيق ذلك:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"}
{"text": "<s>[INST] نريد تطوير برنامج يقوم بتوليد خلاصات للمقالات الطبية باللغة العربية. يمكنك استخدام الـ API التالي للمساعدة:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"}
{"text": "<s>[INST] نحن بحاجة إلى برنامج يستخدم تقنية الـ NER لاستخراج أسماء الشركات من النصوص باللغة العربية. يمكنك الاعتماد على الـ API التالي:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"}
{"text": "<s>[INST] تهدف شركتنا لتطوير نظام يعتمد على تقنية الـ POS tagging للنصوص القانونية باللغة العربية. يمكنك استخدام الـ API التالي كمرجع:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"}
{"text": "<s>[INST] عميلنا يتطلع إلى تطبيق يقوم بتحليل اللهجات في النصوص العربية. يمكنك استخدام الـ API التالي لهذا الغرض:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"}
{"text": "<s>[INST]إنشاء نظام تحليل الصوت لتقدير مشاعر المتحدثين.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"}
{"text": "<s>[INST] كيفية توليد نص قصير بناءً على صورة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"}
{"text": "<s>[INST] بناء نظام تعرف على الكلام وتحويل الصوت إلى نص.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"}
{"text": "<s>[INST] استخدام تقنية تحليل اللغة الطبيعية لتصنيف النصوص الطبية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"}
{"text": "<s>[INST] بناء نظام ترجمة تلقائية بين اللغات المختلفة باستخدام نماذج تعلم آلي.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"}
{"text": "<s>[INST] تطوير نظام للتعرف على الكائنات في الصور باستخدام تقنيات الرؤية الحاسوبية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"}
{"text": "<s>[INST] تصميم نظام لتحليل المشاهد في مقاطع الفيديو وتحديد الأنماط الحركية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"}
{"text": "<s>[INST] تنفيذ تحليل شبكات التواصل الاجتماعي لفهم سلوك المستخدمين واكتشاف الاتجاهات.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"}
{"text": "<s>[INST] تطوير نظام لتعبئة النماذج الطبية تلقائياً باستخدام تقنيات تعلم الآلة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"}
{"text": "<s>[INST] استخدام تقنيات التعلم النقلي لتحسين أداء نماذج التعلم الآلي في مجال جديد.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"}
{"text": "<s>[INST]نحن نبني تطبيقًا لتسجيل المعلومات الصوتية ونريد تحويل هذه المعلومات إلى نص. يمكنك استخدام واجهة برمجة التطبيقات التالية لتحقيق ذلك.? [/INST]'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/wavlm-large', 'api_call': \"Wav2Vec2Model.from_pretrained('microsoft/wavlm-large')\", 'api_arguments': 'speech input', 'python_environment_requirements': 'transformers', 'example_code': 'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.', 'performance': {'dataset': 'SUPERB benchmark', 'accuracy': 'state-of-the-art performance'}, 'description': 'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.' </s>"}
{"text": "<s>[INST] ترغب شركتنا في تحليل المكالمات الهاتفية للعملاء وتحويلها إلى نص قابل للتحليل. يمكنك استخدام API التالي لتحقيق هذا الهدف.? [/INST]'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/wavlm-large', 'api_call': \"Wav2Vec2Model.from_pretrained('microsoft/wavlm-large')\", 'api_arguments': 'speech input', 'python_environment_requirements': 'transformers', 'example_code': 'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.', 'performance': {'dataset': 'SUPERB benchmark', 'accuracy': 'state-of-the-art performance'}, 'description': 'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.' </s>"}
{"text": "<s>[INST] تطلب عمليات البحث والإنقاذ أداة لتحويل النص الصوتي من استغاثات الضحايا إلى نص مفهوم. يمكنك استخدام الAPI التالي لتلبية هذه الاحتياجات.? [/INST]'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/wavlm-large', 'api_call': \"Wav2Vec2Model.from_pretrained('microsoft/wavlm-large')\", 'api_arguments': 'speech input', 'python_environment_requirements': 'transformers', 'example_code': 'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.', 'performance': {'dataset': 'SUPERB benchmark', 'accuracy': 'state-of-the-art performance'}, 'description': 'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.' </s>"}
{"text": "<s>[INST] نود تحسين تجربة تعلم اللغات لطلابنا من خلال توفير أداة لتحويل النص إلى كلام بأداء عالٍ. يمكنك استخدام الAPI التالي لتحقيق هذا الهدف.? [/INST]'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/wavlm-large', 'api_call': \"Wav2Vec2Model.from_pretrained('microsoft/wavlm-large')\", 'api_arguments': 'speech input', 'python_environment_requirements': 'transformers', 'example_code': 'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.', 'performance': {'dataset': 'SUPERB benchmark', 'accuracy': 'state-of-the-art performance'}, 'description': 'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.' </s>"}
{"text": "<s>[INST] تريد منظمتنا تحسين عمليات الإثراء التعليمي من خلال تطوير أداة لتحويل المقالات التعليمية إلى مواد صوتية. يمكنك الوصول إلى ذلك باستخدام الAPI التالي.? [/INST]'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/wavlm-large', 'api_call': \"Wav2Vec2Model.from_pretrained('microsoft/wavlm-large')\", 'api_arguments': 'speech input', 'python_environment_requirements': 'transformers', 'example_code': 'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.', 'performance': {'dataset': 'SUPERB benchmark', 'accuracy': 'state-of-the-art performance'}, 'description': 'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.' </s>"}
{"text": "<s>[INST]تريدين إضافة تقنية التعرف على الوجوه إلى تطبيقك. يمكنك استخدام خدمة Hugging Face Transformers للقيام بذلك.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"}
{"text": "<s>[INST] قم بتطوير تطبيق لتحليل مشاعر الوجوه في الصور. يمكنك استخدام خدمة Hugging Face Transformers لتحقيق هذا الهدف.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"}
{"text": "<s>[INST] تهدف إلى تحسين تقنية تمييز الألوان في الصور. يمكنك استخدام خدمة Hugging Face Transformers لاستخراج المزيد من المعلومات حول الألوان في الصور.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"}
{"text": "<s>[INST] تحتاج إلى تصنيف الإشارات المرورية في الصور. استخدم Hugging Face Transformers لتنفيذ هذه المهمة بكفاءة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"}
{"text": "<s>[INST] قم بتطبيق تقنية تعرف أوتوماتيكي للنصوص المكتوبة في الصور. يوفر Hugging Face Transformers حلاً فعالاً لهذه الغرض.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"}
{"text": "<s>[INST] ترغب في تطوير تطبيق يمكنه تحليل تاريخ الصور المقدمة. يمكنك استخدام Hugging Face Transformers لتحديد التواريخ والأحداث الموجودة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"}
{"text": "<s>[INST] تحتاج إلى تحسين دقة التعرف على المناظر الطبيعية في الصور. يتيح Hugging Face Transformers إمكانية تحسين وتدريب نماذج التعرف على الصور.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"}
{"text": "<s>[INST] تخطط لتطوير تطبيق لتحديد وتصنيف الكائنات الفردية في الصور. تعتبر Hugging Face Transformers خيارًا ممتازًا لتحقيق هذا الهدف.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"}
{"text": "<s>[INST] تريد تطبيقًا يعمل على تحليل وتصنيف الأطعمة في الصور. يمكنك استخدام خدمة Hugging Face Transformers لاستخراج معلومات حول أنواع الطعام المعروضة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"}
{"text": "<s>[INST] تريد تحسين تصنيف المناظر العامة في الصور التي يلتقطها تطبيقك. يمكنك تحقيق ذلك عن طريق استخدام خدمة Hugging Face Transformers لتصنيف المناظر العامة بدقة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"}
{"text": "<s>[INST]قم بتوليد خطوط توجيه لشبكة الطرق في مدينة معينة باستخدام البيانات الجغرافية. هل يمكنك مساعدتي في استخدام UniXcoder لهذا الغرض؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"}
{"text": "<s>[INST] اكتب مقطعاً شعرياً قصيراً يحتوي على كلمة \"حرية\" باستخدام خوارزمية UniXcoder لفهم اللغة العربية بشكل أفضل.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"}
{"text": "<s>[INST] استخدم UniXcoder لتوليد نموذج تنبؤي لدرجة الحرارة في مدينة معينة استناداً إلى بيانات الطقس السابقة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"}
{"text": "<s>[INST] أنشىء واجهة مستخدم تفاعلية تسمح بتحويل الصور إلى رسومات فنية باستخدام تقنيات UniXcoder لتحليل المحتوى البصري.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"}
{"text": "<s>[INST] توليد توصيف لأغنية موسيقية معينة باستخدام UniXcoder لفهم الشعور والمضمون الناتج.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"}
{"text": "<s>[INST] قم بتوليد تعليمات تشغيل مختصرة لجهاز إلكتروني معين باستخدام UniXcoder لتوليد النصوص التقنية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"}
{"text": "<s>[INST] أنشىء نص تعليمي متميز حول موضوع معين باستخدام UniXcoder لضمان جودة التحليل اللغوي والتوجيه.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"}
{"text": "<s>[INST] قم بتوليد تحليل شخصية لشخصية أدبية معينة باستخدام UniXcoder لفهم السلوك والعوامل النفسية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"}
{"text": "<s>[INST] أنشىء ملخصاً تنفيذياً لأحداث تاريخية هامة باستخدام أسلوب الكتابة الأكاديمي ودقة UniXcoder.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"}
{"text": "<s>[INST] قم بتوليد توصيف مشهد طبيعي باستخدام UniXcoder لتعزيز التفاصيل والإحساس بالفضاء والطبيعة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"}
{"text": "<s>[INST]قم بإنشاء تطبيق يقوم بتطبيق تقنية التعلم الآلي لتحليل مشاعر التغريدات على وسائل التواصل الاجتماعي.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"}
{"text": "<s>[INST] كتب برنامج يستخدم تقنية تعلم عميق لتوليد نصوص شعرية جديدة.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"}
{"text": "<s>[INST] قم بإنشاء واجهة تطبيق تستخدم API للترجمة الآلية بين اللغات المختلفة.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"}
{"text": "<s>[INST] انشئ تطبيق يستخدم API لتحليل الصور واستخراج الكائنات والمشاهد الموجودة في الصورة.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"}
{"text": "<s>[INST] قم بتطوير تطبيق يستخدم تقنية التعلم العميق لتصنيف نصوص البريد الإلكتروني إلى فئات مختلفة.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"}
{"text": "<s>[INST] أنشئ تطبيق يستخدم تقنية البصمة الصوتية لتحقق الهوية الصوتية لمستخدميه.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"}
{"text": "<s>[INST] كتب برنامج يستخدم API لتحليل نغمة الصوت والتعرف على المقطع الموسيقي.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"}
{"text": "<s>[INST] قم ببناء تطبيق يستخدم API لتحويل النص إلى كلام باللغة العربية.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"}
{"text": "<s>[INST] أنشئ تطبيق يستخدم API لتحليل الشبكات الاجتماعية واستخراج التقارير والإحصائيات.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"}
{"text": "<s>[INST] كتب برنامج يستخدم تقنية التعلم الآلي لتحسين أداء نظام توصية المنتجات في تطبيق التسوق الإلكتروني.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"}
{"text": "<s>[INST]تحتاج شركتك إلى تحليل محتوى صور لاكتشاف الأماكن العامة في صور الشوارع.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"}
{"text": "<s>[INST] ترغب شركتك في دمج خدمة معرفة الكلام الذكي في تطبيقك الحالي.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"}
{"text": "<s>[INST] يجب على فريقك تطوير نموذج للتعرف على الأشخاص في أشرطة الفيديو.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"}
{"text": "<s>[INST] عليك تصميم واجهة مستخدم جديدة لتطبيق الهاتف المحمول الخاص بشركتك.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"}
{"text": "<s>[INST] نحن بحاجة إلى تطوير أداة لتحليل نغمات الموسيقى وتوصيف الأغاني بناءً على بيانات الصوت.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"}
{"text": "<s>[INST] قم بتطوير نموذج للتنبؤ بأداء الأسهم في البورصة باستخدام بيانات السوق التاريخية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"}
{"text": "<s>[INST] يجب على فريق التطوير تكامل واجهات برمجة التطبيقات (APIs) لعدة خدمات خارجية في تطبيقك الجديد.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"}
{"text": "<s>[INST] تهدف شركتك إلى تحسين تصنيف نماذج التعلم الآلي لنشاطك التجاري.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"}
{"text": "<s>[INST] ابتكر تقنية لتتبع مراحل الإنتاج في خط الإنتاج باستخدام تكنولوجيا الواقع المعزز.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"}
{"text": "<s>[INST] يجب على فريق التطوير تنفيذ نظام سجلات مركزية لتتبع عمليات الوصول إلى البيانات داخل التطبيق.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"}
{"text": "<s>[INST]قم بتوليف موسيقى مميزة تعبر عن موقفك في الحياة الحالي.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"}
{"text": "<s>[INST] صمم شخصية كرتونية تعبر عن شعورك بالإبداع والتحدي.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"}
{"text": "<s>[INST] قم بإنشاء تحليل احصائي لنتائج استبيانك الأخير لتحديد الاتجاهات والتوجهات لديك.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"}
{"text": "<s>[INST] ضع خطة لتحسين أداء موقع الويب الخاص بشركتك بالنسبة لمستخدمي الهواتف المحمولة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"}
{"text": "<s>[INST] أنشىء روبوت يقوم بتحديد مكان الأشياء في الغرفة وتصنيفها.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"}
{"text": "<s>[INST] قم بتصميم نموذج تعلم آلي لتنبؤ بأداء الأسهم في السوق المالية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"}
{"text": "<s>[INST] أنشىء تطبيقًا يقدم نصائح يومية لتعزيز الصحة النفسية والرفاهية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"}
{"text": "<s>[INST] قم بتطوير برنامج سطح المكتب الذي يتيح للمستخدمين تتبع أوقات العمل وإدارة المهام.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"}
{"text": "<s>[INST] أنشىء موقع ويب يوفر دليلًا للمسافرين لاستكشاف أفضل الوجهات السياحية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"}
{"text": "<s>[INST] قم بإعداد برنامج تدريبي لتعليم البرمجة للأطفال باستخدام أساليب تفاعلية وألعاب تعليمية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"}
{"text": "<s>[INST]ترغب في تقديم توصية لمقهى تستخدم خدمة تجميع المعلومات لفهم تفضيلات العملاء. هل يمكن لواجهة برمجة التطبيقات (API) أن تقدم هذه الخدمة؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"}
{"text": "<s>[INST] تريد بناء تطبيق لمساعدة المستخدمين في تعلم اللغة الإنجليزية من خلال توليد نصوص قصيرة مع شرح توضيحي. هل تتوفر واجهة برمجة التطبيقات (API) لمثل هذه الغرض؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"}
{"text": "<s>[INST] تريد إنشاء نظام لتحديد تصنيف المفاهيم في الوقت الحقيقي. هل توفر واجهة برمجة التطبيقات (API) تلك الوظيفة؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"}
{"text": "<s>[INST] تريد تحليل المحتوى النصي لرسائل البريد الإلكتروني لفهم الانطباعات والمواضيع الرئيسية. هل يمكن لواجهة برمجة التطبيقات (API) تلبية هذه الاحتياجات؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"}
{"text": "<s>[INST] تود استخدام واجهة برمجة التطبيقات (API) لتوليد أسماء الشركات بناءً على النصوص الخاصة بالمجال الذي تعمل به. هل توجد واجهة برمجة التطبيقات (API) تقدم تلك الخدمة؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"}
{"text": "<s>[INST] تسعى لتطوير نظام المحادثات الذكية وترغب في استخدام واجهة برمجة التطبيقات (API) لتحليل نغمة الصوت للتعرف على المشاعر. هل تتوفر واجهة برمجة التطبيقات (API) تقدم هذه الخدمة؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"}
{"text": "<s>[INST] تخطط لإنشاء منصة لتحليل مقاطع نصية للأفلام والتوصل إلى استنتاجات حول مشاعر الشخصيات ومراحل القصة. هل يمكن تحقيق ذلك باستخدام واجهة برمجة التطبيقات (API)؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"}
{"text": "<s>[INST] تبحث عن طريقة لتحديد المزيد من التفاصيل حول كتب معينة في التخصصات الأكاديمية من خلال استخدام واجهة برمجة التطبيقات (API). هل تتوفر واجهة برمجة التطبيقات (API) لدعم هذا النوع من العمل؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"}
{"text": "<s>[INST] تحتاج إلى استخدام واجهة برمجة التطبيقات (API) لتحديد أكثر الكلمات شيوعاً في نصوص أدبية. هل يمكن أن توفر واجهة برمجة التطبيقات (API) هذه الخدمة؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"}
{"text": "<s>[INST] تبحث عن واجهة برمجة التطبيقات (API) تعمل على تحليل محتوى النصوص الصوتية لفهم الأفكار والمضامين بشكل شامل. هل تقدم أي من الواجهات البرمجية هذه الخدمة؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"}
{"text": "<s>[INST]استخدم الـ API لتحويل نص قصير إلى كلام مسموع لاستخدامه في تطبيقك الذي يساعد ذوي الإعاقة البصرية.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"}
{"text": "<s>[INST] قم بتوليد قصة عن الطبيعة باستخدام الـ API المقدم، ثم قم بتطبيق عملية تحويل النص إلى كلام.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"}
{"text": "<s>[INST] قم بتحليل عنصر معين في المشهد الذي تلتقطه كاميرات مركبات القيادة الذاتية باستخدام API المقدم.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"}
{"text": "<s>[INST] استخدم الـ API الخاص بتوليد التضاريس لإنشاء مشهد طبيعي مفصل.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"}
{"text": "<s>[INST] أنشئ تطبيقًا يقوم بتصنيف الصور الطبيعية باستخدام الـ API المقدم.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"}
{"text": "<s>[INST] قم بتطبيق تقنية التعرف على الكلام لتحويل الأوامر الصوتية إلى نص مفهوم.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"}
{"text": "<s>[INST] استخدم الـ API لتحويل مقالات علمية إلى نصوص محكمة تخصصية.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"}
{"text": "<s>[INST] قم بتصميم تطبيق لتوليد رسوم بيانية تفاعلية باستخدام الـ API.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"}
{"text": "<s>[INST] أنشئ تطبيقًا يستخدم الـ API للكشف عن اللهجات الصوتية في النصوص الصوتية.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"}
{"text": "<s>[INST] استخدم الـ API لتحويل الصور الثنائية الملونة إلى أبيض وأسود.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"}
{"text": "<s>[INST]ترغب شركتنا في تطوير تطبيق لتحليل مشاهد الفيديو لرصد الحيوانات في البرية. يمكنك استخدام واجهة برمجة التطبيقات (API) التالية لاستخراج الميزات اللازمة:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"}
{"text": "<s>[INST] لدينا تطبيق لتعلم اللغات ونحتاج إلى خاصية توليد النصوص التوضيحية للكلمات. يمكنك استخدام الAPI التالي لتوليد النصوص بناءً على الكلمات:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"}
{"text": "<s>[INST] تخطط شركتنا لتطوير تطبيق للتعرف على الأشياء في الصور، ونريد استخدام مكتبة تعلم الآلة متعددة الاستخدامات لتحقيق هذه الخاصية. يمكنك استخدام الواجهة البرمجية التالية لتحقيق ذلك:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"}
{"text": "<s>[INST] لدينا تطبيق للذكاء الاصطناعي ونحتاج لنموذج يمكنه توقع سلوك المستخدمين استنادًا إلى سجلات النشاط. يمكنك استخدام الواجهة البرمجية التالية لإجراء هذا التوقع:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"}
{"text": "<s>[INST] تقوم شركتنا بتطوير تطبيق للترجمة التلقائية وترغب في استخدام نموذج قادر على فهم النصوص بلغة معينة لتحقيق أفضلية داخل المجال. يمكنك استخدام الواجهة البرمجية التالية للترجمة:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"}
{"text": "<s>[INST] نحن في حاجة إلى تطوير نموذج لتصنيف الموسيقى في تطبيقنا ونفضل استخدام نموذج مسبق التدريب لذلك. يمكنك استخدام الواجهة البرمجية التالية لإنشاء نموذج تصنيف الموسيقى:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"}
{"text": "<s>[INST] تطويرنا تطبيقًا لتعلم الآلة ونريد تنفيذ خوارزمية التصنيف الرؤوسي باستخدام إطار عمل مناسب. يمكنك استخدام الواجهة البرمجية التالية لتنفيذ الخوارزمية:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"}
{"text": "<s>[INST] لدينا تطبيق لتصنيف الأفلام ونريد استخدام تقنيات تحليل النصوص لتقديم تصنيفات دقيقة للمحتوى النصي. يمكنك استخدام الواجهة البرمجية التالية لتنفيذ تحليل النصوص:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"}
{"text": "<s>[INST] شركتنا تعمل على تطبيق لتحليل البيانات ونحب أن يتضمن هذا التطبيق تقنية استكشاف البيانات المكثفة. يمكنك استخدام الواجهة البرمجية التالية لاستخراج الميزات بشكل مكثف:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"}
{"text": "<s>[INST] ترغب شركتنا في تطوير نظام لتصنيف النصوص الطبية ونحب استخدام نموذج متعدد المهام لذلك. يمكنك استخدام الواجهة البرمجية التالية لاستخدام نموذج التصنيف:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"}
{"text": "<s>[INST]نبحث عن طريقة لتحسين توقعاتنا للمخاطر المالية المحتملة في قطاع البنوك.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"}
{"text": "<s>[INST] تخطط شركتنا لإنشاء نظام لتحديد مكان الجوال لموظفينا بدقة عالية.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"}
{"text": "<s>[INST] تحتاج شركتنا لواجهة برمجة استخراج بيانات لتقديم تقارير دورية عن أداء الموظفين.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"}
{"text": "<s>[INST] ترغب شركتنا في تطوير نظام لتحليل معاني النصوص واكتشاف المواضيع الرئيسية في المقالات الصحفية.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"}
{"text": "<s>[INST] تهدف شركتنا لتطوير نظام لتنقيب البيانات واستخراج الأنماط الاستهلاكية للمستهلكين.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"}
{"text": "<s>[INST] ترغب شركتنا في تصميم نظام لاستخراج الموسيقى من مقاطع الفيديو الخاصة بالأفلام.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"}
{"text": "<s>[INST] تحتاج شركتنا إلى تطوير أداة لتصنيف الصور الطبية بناءً على أنواع الأمراض.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"}
{"text": "<s>[INST] نحن بصدد تطوير تطبيق لتحليل السوق ومتابعة أداء الأسهم لمستثمرينا.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"}
{"text": "<s>[INST] نحتاج لإنشاء أداة لتحليل ردود الفعل على المنتجات وتصنيفها بمستويات الرضا.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"}
{"text": "<s>[INST] ترغب شركتنا في تقديم أداة لقياس أثر التسويق الرقمي على زيادة المبيعات.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"}
{"text": "<s>[INST]إنشاء نظام للتوصيات السياحية يستند إلى تفسيرات المستخدم.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"}
{"text": "<s>[INST] كتابة برنامج يستخدم تقنية المعالجة اللغوية الطبيعية لفهم الانفعالات في النصوص.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"}
{"text": "<s>[INST] بناء نظام استشاري يقدم نصائح قانونية عبر تحليل النصوص القانونية.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"}
{"text": "<s>[INST] إنشاء نظام يقوم بتوليد أسماء تجارية جديدة بناءً على استخدام تحليل اللغة الطبيعية.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"}
{"text": "<s>[INST] تنفيذ نظام يقوم بتحويل الخطوط اليدوية إلى نصوص مكتوبة بالحاسوب.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"}
{"text": "<s>[INST] بناء نظام لتحليل الشخصيات في الروايات واستخلاص سماتها الرئيسية باستخدام تقنيات معالجة اللغة الطبيعية.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"}
{"text": "<s>[INST] تطوير نظام لتوليد شعر جديد باستخدام تقنيات الذكاء الاصطناعي.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"}
{"text": "<s>[INST] استخدام تحليل اللغة الطبيعية لتقديم تحليلات سريعة للنصوص الطبية المعقدة.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"}
{"text": "<s>[INST] بناء نظام لتحليل المقالات الصحفية واستخراج أهم الموضوعات المتداولة.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"}
{"text": "<s>[INST] تنفيذ نظام يقوم بتحويل الخطوط العربية اليدوية إلى نصوص مكتوبة بالحاسوب.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"}
{"text": "<s>[INST]تريد شركتنا تطوير تطبيق خاص بتحليل نغمات الصوت واستخراج معلومات دقيقة منها.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"}
{"text": "<s>[INST] ترغب شركتنا في تطوير تطبيق للتعرف التلقائي على وجوه الأشخاص في الصور.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"}
{"text": "<s>[INST] تخطط شركتنا لتطوير نظام لتحليل وتصنيف النصوص التلقائي.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"}
{"text": "<s>[INST] تهتم شركتنا بتطوير نظام ذكاء اصطناعي لتحليل مشاهد الفيديو والتعرف على الأشياء والإجراءات المحددة في الفيديو.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"}
{"text": "<s>[INST] ترغب شركتنا في تطوير نظام لتوليد نصوص تلقائياً بناءً على الصور.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"}
{"text": "<s>[INST] تنوي شركتنا تطوير تطبيق للتعرف على النصوص في الصور واستخراج المعلومات منها.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"}
{"text": "<s>[INST] تخطط شركتنا لتطوير نظام يمكنه تحويل النص إلى كلام بشكل تلقائي.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"}
{"text": "<s>[INST] أنا بحاجة إلى تطوير تطبيق يمكنه تعريف شخصيات الأفلام والتسميات في الصور بشكل تلقائي.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"}
{"text": "<s>[INST] تريد شركتنا تطوير تطبيق يمكنه تنبؤ بموازين الموسيقى استنادًا إلى النغمات الصوتية الحالية.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"}
{"text": "<s>[INST] تحتاج شركتنا إلى تطوير نظام لتحليل المشاهد الفيديو وكشف الأنشطة الغير مرغوب فيها.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"}
{"text": "<s>[INST]نود بناء نموذج تفاعلي يتمكن من تحليل مشاكل الرياضيات. هل يوجد API يمكن استخدامه لإنشاء ذلك؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"}
{"text": "<s>[INST] نبحث عن API يمكن استخدامه لتحويل النصوص الطبية إلى صور توضيحية للأعضاء في الجسم. هل يمكنك توجيهنا إلى واحدة مثل هذه؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"}
{"text": "<s>[INST] نريد إنشاء نظام يساعد المشترين عبر الإنترنت على تجربة المنتجات الرقمية قبل الشراء. هل هناك API يمكن استخدامه لهذا الغرض؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"}
{"text": "<s>[INST] نحتاج إلى تطبيق يستطيع تحليل المشاعر في النصوص وتأكيد صحة المعلومات المنقولة. هل هناك API ملائم لهذه الوظيفة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"}
{"text": "<s>[INST] نبحث عن وسيلة لضبط صور معتمة لتحسين جودتها وجعلها أكثر وضوحاً. هل يوجد API يتناسب مع ذلك؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"}
{"text": "<s>[INST] ننوي تطوير تطبيق استكشافي يقوم بعرض المعالم السياحية الشهيرة على خريطة ثلاثية الأبعاد. هل هناك API يمكننا استخدامه لهذا الغرض؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"}
{"text": "<s>[INST] نريد تصميم نظام لتحويل الصور الثنائية إلى الصور الألوان. هل يمكن أن تساعدنا في العثور على API مناسب؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"}
{"text": "<s>[INST] نحتاج إلى API قادرة على تحويل النص إلى رموز باركود لتسهيل عمليات المتجر الإلكتروني. هل يمكنك توجيهنا إلى واحدة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"}
{"text": "<s>[INST] نود تطوير تطبيق يتيح للمستخدمين تحويل النصوص إلى ملفات صوتية بصيغ مختلفة. هل يتوفر API يمكن استخدامه لهذا الغرض؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"}
{"text": "<s>[INST] نريد إنشاء نظام يقدم توصيات مخصصة للمستخدمين استنادًا إلى تفاعلاتهم السابقة. هل هناك API يمكن أن نستخدمه لهذا الهدف؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"}
{"text": "<s>[INST]تحتاج منصتنا التعليمية إلى نظام يتيح تحويل النصوص إلى كلام بصوت طبيعي لطلابنا. هل يمكنك استخدام API من Hugging Face Transformers لتحقيق هذا؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"}
{"text": "<s>[INST] نود تطوير نظام لتوليد وصف صوتي للصور المرفقة في تطبيقنا الجديد. هل يمكنك استعمال API من Hugging Face Transformers لتحقيق هذه المهمة الحساسة؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"}
{"text": "<s>[INST] شركتنا تحتاج إلى نظام يحدد الأشياء في الصور ويترجمها إلى نصوص مفهومة. هل يمكنك استخدام API من Hugging Face Transformers لتحقيق هذا الهدف الهام؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"}
{"text": "<s>[INST] نريد تطوير أداة تستخدم الذكاء الاصطناعي لتأليف نصوص أدبية فائقة الجودة. هل يمكنك استعمال API من Hugging Face Transformers لمساعدتنا في إنجاز هذه المهمة الإبداعية؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"}
{"text": "<s>[INST] نحتاج إلى تطوير نظام يستخدم التعلم الآلي لتحليل الصور واقتراح وصف دقيق لها. هل يمكنك استخدام API من Hugging Face Transformers في هذا المجال التقني؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"}
{"text": "<s>[INST] نريد تصميم تطبيق يستخدم الذكاء الاصطناعي لتحويل الخطوط اليدوية إلى نصوص طباعية قابلة للتحرير. هل يمكنك استعمال API من Hugging Face Transformers لتحقيق هذا الهدف المبتكر؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"}
{"text": "<s>[INST] تطبيقنا يحتاج إلى نظام يمكنه ترجمة النصوص الإنجليزية إلى لغة الإشارة لفئة المعوقين سمعياً. هل يمكنك استخدام API من Hugging Face Transformers لتدريب هذا النظام الخاص؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"}
{"text": "<s>[INST] نحن بحاجة إلى أداة تستخدم الذكاء الاصطناعي لفهم محتوى الصوتيات التي يقدمها موظفونا. هل يمكنك استعمال API من Hugging Face Transformers لبناء نظام كتابة نصوص اعتماداً على المحتوى الصوتي؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"}
{"text": "<s>[INST] يتم استخدام تطبيقنا في مساعدة المحامين في البحث عن نقاط قانونية محددة. نريد تنفيذ نظام يحول النصوص القانونية إلى كلام يتم فهمه بسهولة. هل يمكنك استخدام API من Hugging Face Transformers لإنشاء هذا النظام؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"}
{"text": "<s>[INST] نود تطوير تطبيق لتوليد وصف بصري دقيق للصور الطبية. هل يمكنك مساعدتنا في استخدام API من Hugging Face Transformers لتحقيق هذا الهدف الحيوي؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"}
{"text": "<s>[INST]نريد إنشاء تطبيق يستخدم تقنية التعلم الآلي لتحليل صور الأطعمة وتقديم معلومات تغذوية دقيقة حولها. هل يمكنك مساعدتنا في تنفيذ هذا؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"}
{"text": "<s>[INST] يجب علينا تطوير تطبيق لتحديد مواقع الأمان العامة في مدينة معينة باستخدام تكنولوجيا خرائط الذكاء الاصطناعي. ما هي الخطوات اللازمة للبدء في هذا المشروع؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"}
{"text": "<s>[INST] نرغب في إنشاء نظام لتحليل بيانات الصوت وتحديد نغمات موسيقية محددة. كيف يمكننا استخدام تكنولوجيا التعلم الآلي لتحقيق هذا الهدف؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"}
{"text": "<s>[INST] لدينا مشروع لتطوير تطبيق يستخدم تقنيات التعلم الآلي للتعرف على وجوه الأشخاص في الفيديوهات. ما هي الأدوات والخطوات اللازمة لتحقيق ذلك؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"}
{"text": "<s>[INST] نحن بحاجة إلى تطوير خدمة ويب تعتمد على تحليل اللغة الطبيعية لتحويل النص إلى لغة برمجة. كيف يمكن استخدام تقنيات التعلم الآلي لتنفيذ هذه الوظيفة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"}
{"text": "<s>[INST] نود إنشاء تطبيق يستخدم تكنولوجيا التعلم الآلي لتوصيات الموسيقى. ما هي الخطوات الأساسية لبناء نموذج تنبؤي لتوصيات الموسيقى؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"}
{"text": "<s>[INST] نريد تطوير تطبيق يستخدم تعلم الآلة لتوليد نصوص إبداعية وشعرية. كيف يمكن استخدام تكنولوجيا التوليف النصي لتحقيق هذا الهدف؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"}
{"text": "<s>[INST] نحتاج إلى برنامج يستخدم تعلم الآلة لتصنيف أفلام السينما تلقائيا استنادا إلى محتواها. كيف يمكن تنفيذ هذا النوع من التحليل باستخدام تقنيات التعلم الآلي؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"}
{"text": "<s>[INST] نريد تطوير تطبيق يمكنه تحليل الرسائل النصية وتحديد المشاعر المعبر عنها. ما هي الأدوات البرمجية الأساسية التي نحتاج إليها لبناء هذا النوع من التطبيقات؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"}
{"text": "<s>[INST] يتطلب مشروعنا إنشاء نظام يستخدم تعلم الآلة لتحليل بيانات السلوك على الإنترنت والتنبؤ بالسلوك المستقبلي. كيف يمكن تطوير نموذج تنبؤي لهذه الغاية باستخدام تقنيات التعلم الآلي؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"}
{"text": "<s>[INST]تحتاج شركة تصميم أزياء إلى طريقة لتحويل رسومات قديمة إلى تصاميم حديثة بواسطة الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"}
{"text": "<s>[INST] نريد تطوير نظام يتغلب على مشكلة التشويش في الصور الطبية ويزيل الضوضاء للحصول على صور واضحة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"}
{"text": "<s>[INST] تطوير تطبيق لتحليل وتصنيف الفيديوهات الرياضية. ابحث عن API لاستخدام الذكاء الاصطناعي في تحديد نوع الرياضة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"}
{"text": "<s>[INST] تحتاج شركة تأمين لبناء نظام للكشف التلقائي عن حوادث السيارات من الصور. ابحث عن API لتحليل الصور وتحديد الأضرار.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"}
{"text": "<s>[INST] مطلوب تطوير نظام يقوم بتحويل النصوص العربية إلى كلام بصوت واضح وجودة عالية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"}
{"text": "<s>[INST] نريد تطبيقًا يمكنه تحليل شهادات التقارير الطبية واستخراج المعلومات الرئيسية. ابحث عن API لـ OCR التخصصي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"}
{"text": "<s>[INST] تحتاج وكالة العقارات إلى عرض ثلاثي الأبعاد للعقارات المتاحة للبيع. ابحث عن API يحول الصور إلى نماذج ثلاثية الأبعاد.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"}
{"text": "<s>[INST] لدينا تطبيق يقوم بتحليل نغمات الموسيقى الكلاسيكية. نحتاج إلى API يساعدنا في تقدير عصر الموسيقى استنادًا إلى النغمات.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"}
{"text": "<s>[INST] مطلوب تطوير نظام يقوم بفحص الأفلام وتحليل المشاهد المؤثرة. ابحث عن API تقوم بتحديد المشاهد الرئيسية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"}
{"text": "<s>[INST] تطوير نظام توصية للمطاعم يستند إلى تحليل صور الطعام. ابحث عن API لتحليل الأطعمة في الصور.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"}
{"text": "<s>[INST]أضف تأثيرات بصرية مذهلة على فيديو لتحسين جودته وإضفاء رونق عليه.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"}
{"text": "<s>[INST] تحتاج منصة التجارة الإلكترونية الخاصة بك إلى عرض صور مُحسنة تلقائيًا للمنتج عندما يتم تحميله. استخدم هذا الاستدعاء لتوليد صور ملائمة تلقائيًا.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"}
{"text": "<s>[INST] أنشئ خوارزمية لتحويل الصور الرقمية إلى لوحات فنية تشبه أسلوب الرسامين الشهيرين باستخدام هذا الاستدعاء.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"}
{"text": "<s>[INST] قم بتطوير نظام تعرف على الوجوه في الصور باستخدام التعلم العميق لمساعدة عميلك في مشروع الأمان والمراقبة الذي يعمل عليه. توثيق النظام الذي قمت بتطويره.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"}
{"text": "<s>[INST] أنشئ نظامًا يقوم بتنبؤ درجة تحمل الإجهاد للمستخدمين باستخدام إشارات الوجوه في الصور. اعتمد على هذا الاستدعاء لتحقيق ذلك.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"}
{"text": "<s>[INST] استخدم تقنية التعلم العميق لتوليد نص خلاق بالعربية استنادًا إلى مدخلات ثنائية، مثل التحليل اللغوي الضوئي. استنادًا إلى الاستدعاء المرفق، أنشئ نظام لإنتاج نصوص خلاقة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"}
{"text": "<s>[INST] قم بتطوير نظام ترجمة آلية للنصوص بين العربية والإنجليزية يعتمد على شبكات الترجمة العصبية. استخدم هذا الاستدعاء لتحقيق ترجمة نصوص فعالة بين اللغتين.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"}
{"text": "<s>[INST] أنشئ تطبيقًا يوفر تحليلًا شاملًا للنصوص باللغة العربية باستخدام تقنيات التعلم الآلي لفهم المحتوى واستخراج الأفكار الرئيسية. تعتمد على هذا الاستدعاء لبناء نظام تحليل نصوص.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"}
{"text": "<s>[INST] تطوير نظام استدعاء لتوليد تلقائي لخط العربية يستند على التعلم العميق لمقدمي الخدمات اللغوية. استخدم هذا الاستدعاء لتنفيذ تحويل النص إلى خط كتابي بالعربية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"}
{"text": "<s>[INST] أحتاج إلى تطوير نظام مدمج لترجمة النصوص إلى كلام بالعربية وتنفيذه. استخدم هذا الاستدعاء لتحقيق الهدف.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"}
{"text": "<s>[INST]عُرضنا فكرة لبرنامج تدريبي للموظفين لتعلم كيفية التعامل مع العملاء الصعبين. يرجى استخدام الـ API التالي لتطوير محتوى تدريبي مصاحب بصور توضيحية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"}
{"text": "<s>[INST] تطوير بوت للذكاء الاصطناعي لمساعدة الطلاب في فحص وتقديم التدريب لامتحانات اللغة الإنجليزية. استخدم الـ API أدناه لتوليد صور تعليمية تساعد الطلاب في فهم المفاهيم بشكل أفضل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"}
{"text": "<s>[INST] نحن بصدد تطوير منصة تعليمية تفاعلية لتعليم الرياضيات للأطفال. يُطلب منك استخدام الـ API المُبين أدناه لإنشاء صور توضيحية تعليمية لمساعدة الأطفال في فهم المفاهيم الرياضية بشكل أفضل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"}
{"text": "<s>[INST] لدينا مشروع لتصميم دليل سياحي رقمي لمدينة القاهرة. نرغب في إضافة صور تفاعلية توضح معالم الجذب السياحي في القاهرة. مستخدم المُعلومات المقدمة أدناه لتطوير الصور المناسبة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"}
{"text": "<s>[INST] نقوم بتطوير تطبيق لمشاركة الوصفات الغذائية. يرجى استخدام الـ API التالي لتوليد صور توضيحية تكشف عن عملية تحضير وصفة معينة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"}
{"text": "<s>[INST] عُزّز منصتك الرقمية للتعليم عن بُعد بصور تفاعلية تُظهر عملية تفكيك الجزيئات الكيميائية. يُطلب منك استخدام الـ API المُوجود أدناه لتصميم الصور المناسبة للمحتوى التعليمي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"}
{"text": "<s>[INST] نريد توفير مرجع تدريبي رقمي لتعليم البرمجة للمبتدئين. يرجى استخدام الـ API المذكور أدناه لإنشاء صور توضيحية تساعد الطلاب في فهم مفاهيم البرمجة بشكل أفضل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"}
{"text": "<s>[INST] نحن بصدد تطوير لعبة تعليمية للأطفال لتحسين مهارات القراءة والكتابة. يُرجى التعرف على الـ API المُوجود أدناه لتوليد صور توضيحية مميزة لمساعدة الأطفال في تحقيق أهداف التعلم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"}
{"text": "<s>[INST] لدى شركتنا مشروع لتصميم برنامج تدريبي لتعليم اللغة الإسبانية. يُطلب منك استخدام الـ API المُعرف أدناه لإعداد صور تعليمية تُعزز فهم الطلاب للمفاهيم اللغوية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"}
{"text": "<s>[INST]كتابة قصيدة تعبر عن فصل الخريف وجمال الألوان في هذا الوقت من العام.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"}
{"text": "<s>[INST] أكتب خطوات بسيطة لتحضير وجبة إفطار صحية وشهية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"}
{"text": "<s>[INST] إنشاء صفحة ويب بسيطة تعرض معلومات عن فنان مشهور وأعماله الفنية البارزة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"}
{"text": "<s>[INST] كتابة مقال تعليمي حول أساليب تحسين الذاكرة وزيادة التركيز.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"}
{"text": "<s>[INST] إعداد جدول بيانات يحتوي على معلومات حول أنواع الزهور وألوانها المختلفة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"}
{"text": "<s>[INST] إنشاء تطبيق يقدم نصائح يومية لتحسين الصحة العامة ونوعية الحياة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"}
{"text": "<s>[INST] تصميم بوستر إعلاني لفعالية ثقافية تحتوي على الفن والموسيقى.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"}
{"text": "<s>[INST] تحليل بيانات مبيعات شهرية وإنشاء تقرير يوضح الاتجاهات والإحصائيات الهامة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"}
{"text": "<s>[INST] تطوير تطبيق لإدارة المهام الشخصية وإضافة مهام جديدة بالأولويات.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"}
{"text": "<s>[INST] كتابة نص ترويجي جذاب لمنتج جديد يعتمد على التقنية الحديثة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"}
{"text": "<s>[INST]نحن نقوم بتطوير منصة تعليمية للأطفال الذين يعانون من صعوبات في التعلم، نحتاج إلى واجهة برمجية تحويل النص إلى صوت لتعزيز تفاعل الأطفال مع المحتوى.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"}
{"text": "<s>[INST] مطعمنا الجديد يقدم قائمة طعام متعددة اللغات، نحتاج إلى واجهة برمجية تقوم بترجمة النصوص إلى لغات متعددة لتلبية احتياجات زبائننا الدوليين.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"}
{"text": "<s>[INST] نود تطوير تطبيق ترجمة فورية لمساعدة الأشخاص في التواصل عبر اللغات المختلفة، نأمل الحصول على واجهة برمجية تقوم بهذه المهمة بدقة وسرعة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"}
{"text": "<s>[INST] شركتنا ترغب في تنفيذ نظام يقوم بتحليل محتوى وسائل التواصل الاجتماعي لفهم مشاعر العملاء بشكل أفضل، نرجو منكم تزويدنا بواجهة برمجية لتحليل النصوص واستخراج المشاعر.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"}
{"text": "<s>[INST] لقد وفرنا تطبيقًا للسياح لاستكشاف معالم المدينة، نريد واجهة برمجية تحسّن جودة ودقّة الوصف المرئي لهذه المعالم لزيادة جاذبيتها.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"}
{"text": "<s>[INST] نريد إضافة ميزة جديدة لتطبيق الروبوت الخاص بنا لتقديم نصائح تداول مالية، نحتاج إلى واجهة برمجية تحليل النصوص المالية وتقديم تقارير شاملة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"}
{"text": "<s>[INST] تطبيقنا يُعنى بتعليم الطهي، نريد تكامل واجهة برمجية تتمكن من تحليل نصوص الوصف للوصفات واقتراح تعديلات وتحسينات لزيادة جاذبيتها للمستخدمين.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"}
{"text": "<s>[INST] نحن نعمل على تطبيق لتعليم اللغات الأجنبية، نحتاج إلى واجهة برمجية تحليل النصوص وتقديم تمارين لتحسين مفردات المستخدمين بشكل فعّال.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"}
{"text": "<s>[INST] تطبيقنا يُقدم خدمات القراءة الصوتية للكتب، نرغب في تنزيل واجهة برمجية تحليل النصوص لتقديم روايات بتأثيرات صوتية مثيرة لاكتساب اهتمام أكبر من القرّاء.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"}
{"text": "<s>[INST] نحن نقدم منصة تعليم عبر الإنترنت، نود دمج واجهة برمجية ترجمة نصوص الدورات التعليمية لتوفير دورات متعددة اللغات للطلاب الدوليين.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"}
{"text": "<s>[INST]يرغب مطورو التطبيقات في إنشاء صورة لعروس ترتدي فستانًا أبيض.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"}
{"text": "<s>[INST] تحتاج شركة التسويق لخلق صورة تُظهر منظرًا لشلال في غابة استوائية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"}
{"text": "<s>[INST] يطلب مهندس البرمجيات إنشاء صورة لشخص يقود دراجة نارية في الصحراء.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"}
{"text": "<s>[INST] ترغب شر\\n\\n5. instruction: العملاء في إنشاء صورة توضح حفل زفاف في الهواء الطلق.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"}
{"text": "<s>[INST] تحتاج شركة الأزياء إلى إنشاء صورة لشخصية مرتدية فستان أزرق على سجادة حمراء.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"}
{"text": "<s>[INST] يريد مدير الإعلانات إنشاء صورة لعبوة منتج تتميز بالتصميم الجذاب.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"}
{"text": "<s>[INST] تقوم شركة السياحة بالحاجة إلى إنشاء صورة لمشهد خلاب لغروب الشمس على الشاطئ.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"}
{"text": "<s>[INST] تطلب شركة التصميم إنشاء صورة لمطبخ حديث بتصميم أنيق.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"}
{"text": "<s>[INST] ترغب وكالة الفضاء في إنشاء صورة لمذنب يجتاح السماء الليلية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"}
{"text": "<s>[INST]تريد شركتنا إنشاء تطبيق يسمح لمستخدميه بتحويل النصوص إلى صور ملهمة وإبداعية. استخدم الخدمة التالية لتحقيق ذلك.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"}
{"text": "<s>[INST] تريد شركتنا تطوير نظام لتوليد رسومات فنية احترافية استنادًا إلى النصوص المدخلة. استخدم الخدمة التالية لتحقيق ذلك.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"}
{"text": "<s>[INST] نحتاج في شركتنا لتطبيق يمكّن المستخدمين من تحويل أفكارهم ومشاعرهم إلى صور فنية. ساعدنا في استخدام الخدمة التالية لتحقيق هذا.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"}
{"text": "<s>[INST] تسعى شركتنا لتطوير منتج يمكّن المستخدمين من إنشاء صور فريدة بناءً على وصف نصي. استخدم الخدمة التالية لتحقيق هدفنا.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"}
{"text": "<s>[INST] تحتاج شركتنا لحلول تقنية تساعد في تحويل النصوص إلى صور ملهمة وواقعية. تعاونا مع الخدمة التالية لتحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"}
{"text": "<s>[INST] نود في شركتنا استخدام تقنية تحويل النصوص إلى صور لتطوير منتج فني مبتكر. حدد الخدمة التالية للمساعدة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"}
{"text": "<s>[INST] تبحث شركتنا عن حلاً يُمكّن من توليد صور فنية عالية الجودة بناءً على وصف نصي. اعتمد على الخدمة التالية لتحقيق هدفنا.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"}
{"text": "<s>[INST] تريد شركتنا تطوير تطبيق يدعم إنشاء صور فنية إبداعية بالاستناد إلى الوصف النصي. استخدم الخدمة التالية لتحقيق ذلك الهدف.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"}
{"text": "<s>[INST] في إطار تطوير منتجات جديدة، تهدف شركتنا لإنشاء تطبيق يستخدم تقنية تحويل النصوص إلى صور فنية. استفد من الخدمة التالية لتحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"}
{"text": "<s>[INST] تحتاج شركتنا لتكامل تقنية تحويل النصوص إلى صور لتحقيق عمليات التصميم الإبداعية. ساهم بالخدمة التالية لإكمال هذه العملية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"}
{"text": "<s>[INST]تريد مدير المدرسة إنشاء نظام لإدارة المحتوى التعليمي الرقمي. اقترح استخدام API لتوليد وتعديل صور تعليمية بناءً على النصوص.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"}
{"text": "<s>[INST] لكتابة تطبيق للتعرف الصوتي للكلام والتحويله إلى نص، نحتاج إلى API لتوليد توجيهات يمكن تفسيرها بصرياً.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"}
{"text": "<s>[INST] تطوير تطبيق لتعليم الرسم للأطفال. اقترح استخدام API لتجسيد رسومات توضيحية استناداً إلى وصفات نصية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"}
{"text": "<s>[INST] يحتاج فريق التصميم إلى أداة تسهل تحويل تصورات المستخدمين إلى صور قابلة للتحريك. اقترح استخدام API لتوليد وتعديل صور متحركة استناداً إلى الوصفات النصية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"}
{"text": "<s>[INST] تهدف مؤسسة تعليمية إلى تنمية منصة تفاعلية تساعد الطلاب في فهم المفاهيم الصعبة بشكل أسهل. اقترح أداة تستخدم API لتوليد صور توضيحية استناداً إلى النصوص.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"}
{"text": "<s>[INST] يُطلب منك تطوير أداة تقوم بتوليد صور فنية مستقبلية بناءً على الرؤى الإبداعية. اقترح استخدام API لهذا الغرض.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"}
{"text": "<s>[INST] لتوسيع قاعدة مستخدمي تطبيقنا التعليمي، نريد إضافة صور توضيحية تعليمية. اقترح API لتحويل الوصفات التعليمية إلى صور.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"}
{"text": "<s>[INST] تتطلب تجربة التعلم عن بُعد لمنصتك التعليمية وجود عناصر بصرية تعزز التحفيز للطلاب. اختر API يمكنه تحويل مفاهيم التحفيز إلى صور ملهمة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"}
{"text": "<s>[INST] تطوير منصة تسويقية تسعى لانتقاء الصور الفنية المناسبة لحسابات عملائك. قم باختيار API يمكنه توليد وتعديل الصور الفنية بناءً على الوصفات.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"}
{"text": "<s>[INST] لدعم التعليم الذاتي على منصة التعلم الإلكتروني لنادي البرمجة، نريد أداة لتحويل تفاصيل الأكواد إلى رسومات تفهمها الطلاب بسهولة. اقترح API لتوليد صور تعليمية للأكواد.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"}
{"text": "<s>[INST]لدى فريقنا مشروع لتطوير تطبيق للتعليم عن برامج الذكاء الاصطناعي، ونحتاج إلى توليد صور توضيحية للخوارزميات.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"}
{"text": "<s>[INST] نريد تصميم تطبيق للهواتف الذكية للبساتين والحدائق الذكية، نحتاج إلى توليد صور للنباتات والزهور.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"}
{"text": "<s>[INST] يجري فريقنا البحث عن تصميم تفاعلي لمعرض فني رقمي، ونحتاج إلى إنشاء صور لوحات فنية وتحف معروضة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"}
{"text": "<s>[INST] نريد تطوير تطبيق يقدم جولات افتراضية للمتاحف الشهيرة، نحتاج إلى إنشاء صور لمحتويات المعارض.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"}
{"text": "<s>[INST] لدينا مشروع لإنشاء تطبيق للمحافظة على البيئة، ونحتاج إلى توليد صور توعوية عن التدوير وحماية الحياة البرية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"}
{"text": "<s>[INST] يجري فريق البحث في شركتنا تطوير منصة تعليمية للأطفال، نريد إنشاء صور توضيحية للمفاهيم التعليمية المعقدة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"}
{"text": "<s>[INST] عندنا فكرة لمشروع تطويري لتطبيق نشر قصص قصيرة، نحتاج إلى توليد صور توضيحية للقصص المختارة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"}
{"text": "<s>[INST] نريد إنشاء تطبيق للسفر يتيح للمستخدمين استكشاف الأماكن السياحية، نحتاج إلى صور توضيحية للوجهات السياحية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"}
{"text": "<s>[INST] تعمل شركتنا على تطوير تطبيق لألعاب الفيديو، نحتاج إلى توليد رسومات للشخصيات والمناظر في اللعبة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"}
{"text": "<s>[INST] لدينا فريق يبني تطبيقًا للسفر بالزمن، ونريد توليد صور توضيحية للتاريخ والحقبات الزمنية المختلفة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"}
{"text": "<s>[INST]تطلب شركتنا تصميم لوحة فنية تحتوي على صورة لمدينة futuristica.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"}
{"text": "<s>[INST] تريد عميلنا تصميم لوحة فنية تجمع بين الطبيعة والرقص.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"}
{"text": "<s>[INST] تحتاج شركتنا إلى تصميم تصميم جرافيكي يحتوي على صورة لسماء مليئة بالنجوم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"}
{"text": "<s>[INST] أنشئ رسمًا لمشهد طبيعي يحتوي على شلال.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"}
{"text": "<s>[INST] طلب عميلنا تصميم شعار لشركته يعكس التكنولوجيا والابتكار.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"}
{"text": "<s>[INST] نحتاج إلى تصميم رسم توضيحي لمدينة ذكية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"}
{"text": "<s>[INST] اختر عميلنا تصميم دعوة زفاف مميزة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"}
{"text": "<s>[INST] تطلب شركة عميلة تصميم لوحة فنية تجسد الحياة البرية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"}
{"text": "<s>[INST] أنشئ رسماً يصوّر منظراً بحرياً جميلًا.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"}
{"text": "<s>[INST] ترغب شركة العميل في تصميم لوحة فنية تجسد الهدوء والاسترخاء.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"}
{"text": "<s>[INST]نريد تطوير تطبيق يتيح للمستخدمين إنشاء قصص تفاعلية مصورة. هل يمكنك استخدام واجهة برمجة التطبيقات لتحويل النصوص إلى صور؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] لدينا حدث ترويجي قادم ونريد إنتاج بوسترات إعلانية مبتكرة. هل يوجد واجهة برمجة تقوم بتحويل النصوص الترويجية إلى صور جذابة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] نحن بصدد تطوير تطبيق لإنشاء بطاقات معايدة مخصصة. هل هناك واجهة برمجة يمكن استخدامها لتحويل النصوص إلى صور بطاقات جذابة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] تحتاج مدرستنا إلى وسيلة لإنشاء صور توضيحية لدروس العلوم. هل يمكنك استخدام واجهة برمجة لتحويل النصوص التوضيحية إلى صور معبرة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] ترغب شركتنا في تصميم قائمة طعام بصرية جذابة ومتميزة. هل يمكن استخدام واجهة برمجة لتحويل قائمة الطعام إلى صور إبداعية؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] نود إنشاء صور فنية تشجيعية لفريق كرة القدم المدرسي. هل توجد واجهة برمجة تقوم بتحويل النصوص التحفيزية إلى صور ملونة وجذابة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] نرغب في تصميم شعار جديد لشركتنا. هل توجد واجهة برمجة لتحويل النصوص إلى صور تعبيرية تستخدم لإنشاء الشعارات؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] نود إنتاج صور رمزية لمنتجاتنا الجديدة لاستخدامها في التسويق عبر الإنترنت. هل يمكن استخدام واجهة برمجة لتحويل النصوص إلى صور رمزية فريدة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] نرغب في تصميم بوسترات تعريفية لورش عملنا القادمة. هل يمكن استخدام واجهة برمجة لتحويل النصوص الورشة إلى صور جذابة وإبداعية؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] يجّمع مشروعنا القادم بين الابتكار والتكنولوجيا، ونريد إنشاء صور توضيحية تعبر عن رؤيتنا. هل يمكن استخدام واجهة برمجة لتحويل النصوص التعبيرية إلى صور إبداعية؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST]قم بإنشاء صورة تصويرية تمثل مشهدًا ليليًا لمدينة حديثة متلألئة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"}
{"text": "<s>[INST] ساعدنا في إنشاء صورة تصويرية تعبر عن فصل الشتاء بألوانه الباردة والثلوج الناعمة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"}
{"text": "<s>[INST] قم بتوليد صورة توضح مشهدًا طبيعيًا ساحرًا بجبال شاهقة ونهر جارٍ بينها.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"}
{"text": "<s>[INST] أكتب نصًا يصف أحد أكثر الأزقة التاريخية شهرةً وألقًا في العالم العربي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"}
{"text": "<s>[INST] باستخدام النص المعطى، أنشئ صورة رقمية تعبيرية عن الشجاعة والقوة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"}
{"text": "<s>[INST] ابتكر صورة توضح فن تحضير الأطعمة العربية التقليدية بألوانها ونكهاتها الفريدة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"}
{"text": "<s>[INST] كتبت نصًا يصف جلسة صلاة هادئة في مسجد تاريخي عتيق. اصنع صورة تعبيرية تجسد هذا الموقف بأقصى قدر من الواقعية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"}
{"text": "<s>[INST] أنشئ صورة توضح جمال شاطئ البحر بالمناظر الذهنية الرائعة للبحر الزرقاء الخلابة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"}
{"text": "<s>[INST] استخدم النص المُعطى لتوليد صورة تعبيرية تمثل حفل زفاف ساحر في قصر تاريخي مهيب.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"}
{"text": "<s>[INST] قم بكتابة وصف خيالي لشخصية بطل خيالية قوية وقم بتوليد صورتها التوضيحية باستخدام النص المعطى.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"}
{"text": "<s>[INST]تحتاج شركتنا إلى إنشاء نظام للكشف عن احتمالات الإصابة بالأمراض النفسية من النصوص الطبية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"}
{"text": "<s>[INST] نريد تطوير تطبيق للتعرف على أشكال الوجوه والتعبيرات الوجهية. نحتاج إلى واجهة برمجة تطبيقات لتحقيق ذلك.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"}
{"text": "<s>[INST] يريد فريق التسويق إنشاء نموذج للتنبؤ بالسلوك على وسائل التواصل الاجتماعي من خلال تحليل النصوص.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"}
{"text": "<s>[INST] نحن بحاجة إلى تقديم أداة لتوليد نصوص توجيهية لتدريب العملاء على استخدام تطبيقنا الجديد.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"}
{"text": "<s>[INST] تبحث شركتنا عن طريقة لتحليل بيانات الصوت والتمييز بين الأصوات المختلفة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"}
{"text": "<s>[INST] تحتاج شركتنا إلى تصميم نظام لتحسين جودة الصور الطبية وتوحيد التنسيق.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"}
{"text": "<s>[INST] يرغب قسم البحث والتطوير في تطوير نظام لتحليل النصوص الطبية لاكتشاف الاختلالات اللغوية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"}
{"text": "<s>[INST] نحن بحاجة إلى تطوير أداة لاستخراج المعلومات الهامة من النصوص الطبية لمساعدة الأطباء في صنع القرارات.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"}
{"text": "<s>[INST] تريد شركتنا تطوير أداة لتحويل النصوص إلى بيانات جدولية لتسهيل التحليل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"}
{"text": "<s>[INST] تبحث شركتنا عن حل لتحسين جودة البيانات الصوتية وتنقيتها من الضوضاء.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"}
{"text": "<s>[INST]كتب طالب أولاد صالح مقالة عن أهمية التعليم في المجتمع. استخدم الآي بي ذو القوة العالية \"stabilityai/stable-diffusion-2-depth\" لتحويل النص إلى صورة تمثل فكرة المقالة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] قررت شركة تحليل البيانات تحليل بيانات العملاء لتحديد احتياجاتهم. أستخدم آي بي \"stabilityai/stable-diffusion-2-depth\" لتوليد صورة تعبر عن نتائج التحليل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] أعدت السلطات المحلية حملة للتوعية بأهمية فصل النفايات. استخدم الآي بي لتوليد صورة توضح أهمية الحملة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] تقررت الشركة تحسين تجربة المستخدم لتطبيق الهواتف الذكية. استخدم الآي بي لإنشاء صورة تعبر عن تصميم جديد للتطبيق.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] تواجه شركة تسويق تحدي في تحديد الشعار المناسب لعميل جديد. قم بإستخدام الآي بي لإنتاج صورة تعبر عن الشعور المطلوب للعلامة التجارية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] يعمل الباحثون على تطوير أداة لتحويل الكتب الورقية إلى نصوص رقمية. إستخدم الآي بي لتوليد صورة توضح عملية التحويل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] أنشىء مطوّر تطبيق لعبة تعليمية للأطفال. إستخدم الآي بي لإنشاء صورة تعبر عن فكرة اللعبة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] تسعى منظمة غير ربحية لزيادة الوعي بأهمية الطاقة المتجددة. إستخدم الآي بي لإنتاج صورة توضح فوائد الاعتماد على الطاقة المتجددة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] يعتزم المطورون تطوير تطبيق للأشخاص الذين يعانون من الاكتئاب. استخدم الآي بي لإنشاء صورة تعبر عن أهمية الدعم النفسي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] يرغب الناشر في تصميم غلاف كتاب جديد بشكل ملهم. أستخدم الآي بي لإنتاج صورة تلخص موضوع الكتاب.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"}
{"text": "<s>[INST]يتم تطوير تطبيق لمساعدة الطلاب في تعلم الرياضيات. يُطلب منك تنفيذ واجهة رسومية تحفز الطلاب على حل المسائل الرياضية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"}
{"text": "<s>[INST] تريد شركة الطاقة الشمسية عرض معلومات استهلاك الطاقة الشمسية على شكل رسوم بيانية ديناميكية على موقع الويب الخاص بها.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"}
{"text": "<s>[INST] مطور تطبيق يبحث عن طريقة لإدراج خريطة تظهر مواقع المستخدمين في التطبيق.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"}
{"text": "<s>[INST] مالك متجر على الإنترنت يرغب في تحسين تجربة المستخدم في البحث عن المنتجات. يجب عليك توفير خاصية البحث الذكي باستخدام تقنيات الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"}
{"text": "<s>[INST] تطبيق إدارة المهام يحتاج إلى قدرة على إنشاء تذكيرات فورية للمهام المهمة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"}
{"text": "<s>[INST] مطور تطبيق يحتاج إلى إضافة خاصية مكالمات صوتية وفيديو بجودة عالية إلى تطبيق التواصل الحالي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"}
{"text": "<s>[INST] فريق تطوير يرغب في إضافة خاصية تعريف الوجه إلى تطبيق الهاتف الذكي الخاص بهم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"}
{"text": "<s>[INST] شركة تقدم خدمات التوصيل ترغب في تحسين نظام التوصيل الخاص بها من خلال رصد حركة المركبات وتقديم تقارير عن الأداء.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"}
{"text": "<s>[INST] مالك موقع ويب يريد تحسين تجربة المستخدم من خلال تقديم محتوى ديناميكي معروض بناءً على سلوك المستخدمين.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"}
{"text": "<s>[INST] مبرمج يبحث عن طريقة لتطوير تطبيق للعب ألعاب الفيديو على محاكي متقدم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"}
{"text": "<s>[INST]لدينا مشروع لتطوير تطبيق للتعرف على الكلاب من الصور. نحتاج إلى إنشاء صور لكلب وتحديد سلالته من الصورة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] نحن بصدد إنشاء تطبيق للتعرف على النباتات من الصور. نود إنشاء صور لنبات محدد وتحديد اسمه العلمي من الصورة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] ترغب شركتنا في بناء نظام لتحليل انحناءات الطرق من الصور الجوية. نريد إنشاء صور توضح درجة انحناء الطريق بدقة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] نحتاج إلى تصميم نظام يستطيع تحديد أنواع السحب من الصور الجوية. يجب إنشاء صور لسحب مختلفة وتصنيفها بناءً على النوع.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] لدينا حاجة للكشف عن العناصر الغريبة في صور المدينة الفلكية. نأمل في إنشاء صور تظهر العناصر الغريبة بوضوح.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] نتطلع إلى تصميم نظام لتحليل بيئات البحر الأحمر من الصور الجوية. نريد إنتاج صور توضح مواطن الاستقرار البيئي في المنطقة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] نحتاج إلى توليد صور لتحديد نقاط الاصطدام في البيئات المختلفة. يتوجب إنشاء صور لنقاط الاصطدام بدقة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] أنا بحاجة إلى توليد صور توضح تأثير تغير المناخ على الغابات. يجب إنشاء صور لتغيرات في البيئة الغابية بدقة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] لدينا مشروع لتطوير نظام لتحليل مواقع البناء من الصور الجوية. نريد إنشاء صور توضح هياكل البناء بدقة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"}
{"text": "<s>[INST] نود تصميم نظام لتحليل تأثير الزلازل على المباني من الصور الجوية. يطلب منك إنشاء صور تظهر تأثير الزلازل بوضوح.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"}
{"text": "<s>[INST]تحتاج إلى إنشاء تصميم شخصية كرتونية ذات ملامح أنيقة وجريئة باستخدام تقنية Text-to-Image. طلب من الآلة رسم شخصية تبدو كسيدة من عصر النهضة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"}
{"text": "<s>[INST] قم بإنشاء صورة فنية تمثل منظرًا طبيعيًا هادئًا، مع استخدام النموذج Text-to-Image. استخدم وصف \"غروب الشمس على بحيرة هادئة\" كبداية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"}
{"text": "<s>[INST] تابع خطوات إعداد نموذج Text-to-Image لرسم صورة تجسد مفهوم الخيال العلمي بواسطة نموذج Nitro Diffusion. استخدم جملة \"روبوتات فضائية متقدمة تحتل المجرة\" كمدخل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"}
{"text": "<s>[INST] ابدأ بتوليف نموذج Text-to-Image لرسم صورة تعبر عن فكرة حلم المسافرين بالفضاء الخارجي. ضع الجملة \"رحلة إلى المريخ وإطلالة من صنوف الكوكب\" لتكوين الصورة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"}
{"text": "<s>[INST] قم بإعداد طلب لإنشاء صورة فنية تمثل منطقة ساحرة معمارية قديمة باستخدام خوارزمية Text-to-Image. استخدم العبارة \"مدينة قديمة بأبراج شاهقة وأزقة ضيقة\" للتوجيه.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"}
{"text": "<s>[INST] استخدم خوارزمية Text-to-Image لخلق صورة تؤكد على روعة الطبيعة مع وجود نباتات غريبة. ابدأ بالعبارة \"غابة ساحرة مليئة بالنباتات الخلابة\" كدليل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"}
{"text": "<s>[INST] تحتاج إلى توليف النموذج Text-to-Image لرسم صورة تعكس جمال وأناقة الزهور الاستوائية. استعمل مقطع \"ورود استوائية ملونة وساحرة على شاطئ البحر\" كمدخل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"}
{"text": "<s>[INST] أنشئ نموذج Text-to-Image لرسم صورة تجسد منظر الغسق في الصحراء بأشعة الشمس الأخيرة. استخدم النص \"تضاءلت النجوم وانبعثت ألوان الغروب\" للإشارة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"}
{"text": "<s>[INST] قم بدمج النموذج Text-to-Image لخلق صورة تصويرية لجسر عتيق فوق نهر هادئ. استخدم جملة \"جسر قديم يعبر نهرًا هادئًا في غسق اليوم\" لتوجيه الرسم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"}
{"text": "<s>[INST] اطلب من خوارزمية Text-to-Image رسم صورة تمثل هدوء المناظر الطبيعية القطبية. استخدم عبارة \"صمت قطبي وسحر الشمس القطبية\" لتوجيه التصميم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"}
{"text": "<s>[INST]أنشئ قصة خيالية قصيرة تبدأ بعبارة \"في عالم بعيد جداً\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"}
{"text": "<s>[INST] عميلك يبحث عن تطبيق يقوم بتحويل نص إلى صورة افتراضية لمعلومات المالية. توضح له كيف نستخدم الـ API لتحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"}
{"text": "<s>[INST] أنشئ قصيدة شعرية من 10 أبيات تستخدم فيها كلمة \"حب\". استخدم الـ API لتحويل النص إلى صورة تعبر عن المشهد الذي توصفه القصيدة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"}
{"text": "<s>[INST] اكتب سكريبت لفيلم قصير يحكي عن مغامرة في الفضاء. استخدم النص الناتج لتوليد صور توضح مشاهد الفيلم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"}
{"text": "<s>[INST] تطلب عميلك إنتاج صورة توضح مفهوم السلام لديه. قم بكتابة نص يصف هذا المفهوم واستخدم الـ API لتحويله إلى صورة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"}
{"text": "<s>[INST] أنت بحاجة إلى إضافة صورة رمزية جديدة لملفك الشخصي. اكتب نبذة قصيرة تصف تلك الصورة ثم استخدم الـ API لتوليدها من النص.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"}
{"text": "<s>[INST] كتابة قصة خيالية تتحدث عن عالم ساحر. اشرح للكاتب كيفية استخدام الـ API لإنشاء صورة تجسد هذا العالم الخيالي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"}
{"text": "<s>[INST] طلب عميلك إضافة صورة لمقال يتحدث عن الطبيعة وجمالها. استخدم الـ API لتحويل فقرة من المقال إلى صورة تعبر عن هذا المحتوى.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"}
{"text": "<s>[INST] كتابة رواية قصيرة تبدأ بـ \"في قرية ساحرة عاشت أسرار عظيمة\". قم باستخدام الـ API لتحويل هذه الجملة إلى صورة تعكس بداية القصة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"}
{"text": "<s>[INST] عميلك يريد تصميم شعار جديد لشركته. اكتب جملة تعبر عن رؤية الشركة ثم استخدم الـ API لتوليد صورة تعكس هذه الرؤية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"}
{"text": "<s>[INST]قم بإنشاء تصميم لوجو جديد لشركتك يحتوي على عنصر الرقمية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"}
{"text": "<s>[INST] اكتب خطة عمل لإطلاق منتج جديد يستهدف فئة الشباب باللغة العربية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"}
{"text": "<s>[INST] تحتاج إلى كتابة مقالة حول تأثير التكنولوجيا على التعليم الجامعي. استخدم الأمثلة والأبحاث الحديثة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"}
{"text": "<s>[INST] أنشئ مجموعة من الإعلانات المبوبة عبر الإنترنت لترويج لخدماتك الجديدة في مدينتك.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"}
{"text": "<s>[INST] قم بإعداد خطة تسويقية لمنتجك الحالي لزيادة مبيعاته خلال الربع القادم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"}
{"text": "<s>[INST] عندك مشروع بحثي تحتاج إلى تحليل بيانات تفاعلات الأعمدة. استخدم أداة تحليل البيانات لمساعدتك في ذلك.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"}
{"text": "<s>[INST] أدخل برنامج تدريبي لتطوير مهاراتك القيادية بالتعاون مع فريقك.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"}
{"text": "<s>[INST] أكتب نص إعلاني جذاب لجذب عملاء جدد لخدماتك الاستشارية في مجال الأعمال.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"}
{"text": "<s>[INST] قم بتصميم مجموعة من الرسوم التوضيحية لتوضيح عملية التسوق عبر الإنترنت لمتجرك الإلكتروني.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"}
{"text": "<s>[INST] اكتب مقالًا تقنيًا يشرح نظرة عامة عن تقنية الواقع المعزز وتطبيقاتها المستقبلية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"}
{"text": "<s>[INST]قم بتصميم شخصية خيالية بشعر متوهج وعيون لامعة، مرتدية ملابس مبتكرة وتحمل سيفًا سحريًا.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"}
{"text": "<s>[INST] أنشئ لوحة فنية تصور مشهدًا طبيعيًا خريفيًا مليئًا بأشجار القيقب والألوان الدافئة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"}
{"text": "<s>[INST] قم بإنشاء لوحة فنية تصور مدينة مستقبلية تتوهج بالأضواء والتكنولوجيا الحديثة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"}
{"text": "<s>[INST] ابتكر شخصية خيالية لعالم الخيال العلمي، تجمع بين عناصر الآليين والمخلوقات الخرافية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"}
{"text": "<s>[INST] ابتكر تصميمًا لشخصية خيالية تعبر عن القوة والحكمة في عالم الخيال السحري.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"}
{"text": "<s>[INST] أنشئ صورة فنية تمثل مشهدًا هادئًا في الريف، مع وجود مروج خضراء وسماء زرقاء صافية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"}
{"text": "<s>[INST] اصنع لوحة فنية تصوّر شخصية خيالية تتحول إلى حيوان خرافي بألوانه البرّاقة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"}
{"text": "<s>[INST] قم بإنشاء تصميم يتضمن منظرًا فضائيًا لكوكب غريب مع أقمار صناعية تحوم حوله.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"}
{"text": "<s>[INST] ابتكر تصميمًا لمخلوق بحري خيالي يجمع بين خصائص الأسماك والمخلوقات البحرية الأخرى.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"}
{"text": "<s>[INST] أنشئ صورة فنية تجسد مشهدًا من عصور ملحمية مع وجود معارك ملحمية ومحاربين قويين.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"}
{"text": "<s>[INST]? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-anime-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-anime-1.0')\", 'api_arguments': ['prompt', 'negative_prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': \"from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\nnegative_prompt = 'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry'\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\nimage.save(./result.jpg)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\" </s>"}
{"text": "<s>[INST]يرغب الباحثون في تطوير نظام يحول الكتب الإلكترونية إلى نص مسموع للأشخاص ذوي الإعاقة البصرية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"}
{"text": "<s>[INST] قم بتحويل سلسلة نصية إلى تنسيق صوتي باستخدام خدمة المحولات.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"}
{"text": "<s>[INST] قم بإنشاء واجهة مستخدم تفاعلية تعرض تصنيفات فنية لأعمال الفنانين باستخدام واجهة برمجة تطبيقات تحليل الصور.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"}
{"text": "<s>[INST] يرغب الباحثون في تطوير نظام يقوم بتحليل المحتوى الصوتي لمقاطع الفيديو وتحويله إلى نص.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"}
{"text": "<s>[INST] قم بتطوير نظام يقوم بتحليل الصوت وتحديد العوامل التي قد تؤثر على جودة التسجيل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"}
{"text": "<s>[INST] أنشئ تطبيقًا يستخدم تقنية التعلم العميق لتحديد سلالة الكلب من الصور.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"}
{"text": "<s>[INST] قم بتطوير نظام يقوم بتحليل النصوص القانونية وتحديد المفاهيم الرئيسية والكلمات المفتاحية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"}
{"text": "<s>[INST] أنشئ تطبيقًا يستخدم خدمة تفريق الألوان لتحويل الصور إلى رسوم فنية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"}
{"text": "<s>[INST] يرغب العلماء في تحليل الموجات الصوتية وتحديد الترددات العالية والمنخفضة الموجودة بها.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"}
{"text": "<s>[INST] قم بتحليل المشاهد الفيديو وتحديد الأشياء المتحركة فيها باستخدام نموذج التعرف على الأشياء.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"}
{"text": "<s>[INST]أريد إنشاء صورة لمجموعة من الأزهار متنوعة وجميلة. هل يمكنك توجيهي لذلك؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"}
{"text": "<s>[INST] كتبت قصيدة عن الشتاء، وأرغب في رؤية صورة تعكس أجواء الشعر. ما هو الخطوات التي يجب اتباعها باستخدام ال API؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"}
{"text": "<s>[INST] أرغب في تصميم غلاف كتاب فني يتناسب مع قصتي. هل يمكنك توجيهي لاستخدام ال API؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"}
{"text": "<s>[INST] لدي قصة قصيرة عن مغامرةٍ سحرية، وأود رؤية صورة تجسد أحداثها. كيف يمكنني الاستفادة من هذا ال API؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"}
{"text": "<s>[INST] أريد إنشاء صورة تجسد الهدوء والاسترخاء. هل يمكن ال API مساعدتي في ذلك؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"}
{"text": "<s>[INST] كتبت قصة خيالية عن عالم ساحر، وأرغب في إنشاء صورة تعكس خيال القصة. هل يمكنني استخدام هذا ال API للحصول على ذلك؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"}
{"text": "<s>[INST] أريد رسمًا لمشهد صيفي زرعًا وخضرة. هل يمكن ال API مساعدتي في تحقيق ذلك؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"}
{"text": "<s>[INST] أود رؤية صورة لمدينة خيالية متقدمة تعكس التطور التكنولوجي والفضاء. هل يمكن ال API توليد هذا النوع من الصور؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"}
{"text": "<s>[INST] أحب الطيور وأود رؤية صورة لمجموعة متنوعة من الطيور الجميلة. هل يمكن استخدام ال API لهذا الغرض؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"}
{"text": "<s>[INST] لدي قصة خيالية تستند على عالم الفانتازيا وأريد رؤية لوحة فنية تجسد خيال القصة. هل ال API مناسب لهذا الغرض؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"}
{"text": "<s>[INST]كتابة قصيدة شعرية تبدأ بسطر \"في ليلة ساحرة من زمان\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"}
{"text": "<s>[INST] توليد صورة تصويرية تصف مناظر طبيعية على شاطئ البحر بإستخدام جملة \"في هدوء مطلق يمتد البحر بلا نهاية\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"}
{"text": "<s>[INST] إنشاء لقطة فنية تعبر عن الحب والحنان باستخدام جملة \"عندما يلتقي القلبان، تتفتح زهرة جديدة بجمال لا يصدق\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"}
{"text": "<s>[INST] ابتكار صورة توضح مفهوم الحرية والطيران بجملة \"عندما تتحرر روحك، تعانق السماء بأجنحة من ضوء\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"}
{"text": "<s>[INST] إخراج صورة فنية تصور السلام والهدوء بعبارة \"عندما تعود الطبيعة لسكونها، ينعم العالم بالسلام\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"}
{"text": "<s>[INST] عمل صورة تعبيرية عن الخيال والإبداع مع جملة \"عندما تطير الأفكار إلى عوالم غير مألوفة\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"}
{"text": "<s>[INST] تحويل حكاية قديمة إلى صورة فنية مع بداية \"في زمن بعيد وبلاد بعيدة، حيث\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"}
{"text": "<s>[INST] إبداع صورة فنية تستعرض تضاريس الحياة بجملة \"في متاهات الوقت تنسجم ذكريات الأمس مع أحلام الغد\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"}
{"text": "<s>[INST] عرض حكاية خيالية بصورة فنية بابتداء جملة \"في سراديب الزمن وأفق الخيال، استيقظت كائنات الظلام\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"}
{"text": "<s>[INST] توليد صورة فنية تمثل التلاقي بين الطبيعة والتكنولوجيا بعبارة \"عندما تتحد الأشجار بأسلاك التكنولوجيا\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"}
{"text": "<s>[INST]يمكننا تخصيص نظام تعلم آلي لتوقع أداء الأسهم على المدى القصير باستخدام البيانات التاريخية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"}
{"text": "<s>[INST] نود تطوير نظام توليد نصوص إبداعية باستخدام خوارزمية توليد اللغة الطبيعية الحديثة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"}
{"text": "<s>[INST] نحتاج إلى بناء نظام لتصنيف الصور الطبية بدقة عالية لتسهيل عملية تشخيص الأمراض.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"}
{"text": "<s>[INST] نرغب في إنشاء نظام ترجمة فورية للنصوص بين لغات متعددة باستخدام تقنيات الترجمة الآلية الحديثة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"}
{"text": "<s>[INST] يتطلب لدينا تطبيق إنترنت سريع وموثوق به لمشروعنا الجديد.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"}
{"text": "<s>[INST] تحتاج شركتنا إلى نظام تحليل بيانات فعال لفهم اتجاهات العملاء وتحسين تجربتهم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"}
{"text": "<s>[INST] تبحث منظمتنا عن تطوير أداة لتحليل المخاطر المتعلقة بالتداول عبر الإنترنت.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"}
{"text": "<s>[INST] تهدف شركتنا إلى تطوير تطبيق ذكي يسهل عملية التسوق عبر الإنترنت للمستخدمين.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"}
{"text": "<s>[INST] نحتاج إلى نظام لتحليل بيانات الأداء الرياضي وتوقع نتائج المباريات باستخدام الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"}
{"text": "<s>[INST] نود إنشاء أداة لتحويل النصوص إلى كلام واقعي بواسطة تقنيات توليد الصوت الحديثة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"}
{"text": "<s>[INST]نحن بحاجة إلى تطوير نظام لتحليل سلوك المستهلكين استنادًا إلى بياناتهم الشخصية وتوقع احتياجاتهم المستقبلية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"}
{"text": "<s>[INST] تخطط شركتنا لتطوير تطبيق للتعرف على الأشخاص في الصور وتوفير تفاصيل حولهم. نريد استخدام تقنيات الذكاء الاصطناعي لهذا الغرض.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"}
{"text": "<s>[INST] نريد إنشاء نموذج للكشف عن التجميعات الجيولوجية في الصور الجوية، وتحديد أماكنها بدقة عالية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"}
{"text": "<s>[INST] نحن بهدف تطوير نظام توصيات لتحسين تجربة التداول في السوق المالية. نرغب في استخدام التحليل التقني لتوليد هذه التوصيات.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"}
{"text": "<s>[INST] نريد إنشاء أداة لتحليل وتصنيف السيارات في الصور الفوتوغرافية استنادًا إلى موديلاتها وسنوات الصنع.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"}
{"text": "<s>[INST] ترغب شركتنا في تطوير نظام لتقدير أعداد السكان في مناطق محددة باستخدام تقنيات تحليل الصور الجوية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"}
{"text": "<s>[INST] نود تحسين موديل توقع الإقبال على شراء المنتجات الغذائية باستخدام إشارات الدماغ. هل يمكنكم مساعدتنا في تصميم ذلك؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"}
{"text": "<s>[INST] نريد تطوير نظام للتعرف على أنماط التصميم في الملابس والأزياء باستخدام الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"}
{"text": "<s>[INST] نحتاج إلى تصميم أداة لقياس وتحليل نوعية الهواء باستخدام الصور الفضائية. هل يمكنكم توجيهنا لاستخدام هذه التقنية؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"}
{"text": "<s>[INST] ترغب شركتنا في تطبيق تقنيات الذكاء الاصطناعي لتمييز وتصنيف النباتات في الصور الطبيعية. كيف يمكننا البدء في هذا المشروع؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"}
{"text": "<s>[INST]لدينا مشروع لإنشاء منصة تعليمية عبر الإنترنت، نحتاج لدمج خاصية تحويل النص إلى صوت لتسهيل تجربة التعلم للطلاب.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] نريد تطوير تطبيق تسوق عبر الإنترنت يعرض منتجات معينة، ونحتاج لخاصية توجيه المستخدمين إلى المنتجات عن طريق وصف صوتي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] نحتاج إلى نظام يمكنه تحويل النصوص العلمية complex إلى تصوير ثلاثي الأبعاد.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] نود تطوير تطبيق لتعليم اللغات الأجنبية يساعد الطلاب على تحسين مهارات النطق، نحتاج إلى API لتحويل النصوص إلى تسجيلات صوتية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] نحتاج إلى تقنية تسمح لنا بتحويل تعليمات الصوت إلى دلائل تفاعلية للألعاب التعليمية التفاعلية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] نريد إضافة وظيفة تحويل الروايات النصية إلى صور رقمية ملونة عالية الدقة في تطبيقنا الحالي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] تطوير منصة تفاعلية لتعليم الرياضيات للأطفال الصغار، نحتاج إلى API لتحويل المعادلات الرياضية إلى رسوم بيانية ملونة وسهلة الفهم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] نحتاج إلى تقنية تمكننا من تحويل وصف الفيديوهات إلى توضيح صوتي مع وضع تسجيلات صوتية للإضافة على الفيديو.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] تطوير نظام روبوتي لتقديم محاضرات تفاعلية في المؤتمرات الافتراضية، نريد API لتحويل النصوص التقنية إلى رسوم توضيحية ثلاثية الأبعاد.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST] تصميم تطبيق لتعليم القراءة للأطفال الصغار، نحتاج API لتحويل النصوص القصصية إلى رسوم توضيحية متحركة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"}
{"text": "<s>[INST]قم بتطوير تطبيق لتحويل النص إلى كلام لمستخدمين متحدين بصرياً.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"}
{"text": "<s>[INST] بناء تطبيق للتعرف على الأشياء من الصور لفئة ذوي الاحتياجات الخاصة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"}
{"text": "<s>[INST] قم ببناء تطبيق لتحليل المشاهد من ملفات الفيديو لمساعدة أشخاص ذوي إعاقة بصرية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"}
{"text": "<s>[INST] إنشاء تطبيق لترجمة النصوص من لغة إلى أخرى لتحسين تجربة المستخدمين الذين لا يجيدون لغة معينة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"}
{"text": "<s>[INST] تطوير تطبيق لتحليل المشاهد الطبيعية من الصور لمساعدة الأشخاص الذين يعانون من ضعف البصر.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"}
{"text": "<s>[INST] برمجة تطبيق لتحديد المواقع الجغرافية من الصور لأفراد ذوي الاحتياجات الخاصة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"}
{"text": "<s>[INST] تنفيذ تطبيق لتحليل التعابير الوجهية من الصور لتوفير تجربة أفضل لفئة الصم والبكم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"}
{"text": "<s>[INST] إنشاء تطبيق لتوصيف الفيديوهات باللغة العربية من أجل تعزيز الوصولية للمحتوى.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"}
{"text": "<s>[INST] تطوير تطبيق لتعريف العناصر في الصور بالأصوات لدعم فهم البيئة لذوي الاحتياجات الخاصة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"}
{"text": "<s>[INST] بناء تطبيق لتحليل الصورة وتقديم وصف صوتي لمساعدة الأشخاص ذوي الإعاقة البصرية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"}
{"text": "<s>[INST]نود إنشاء نماذج ثلاثية الأبعاد لمسيرات في شوارع المدينة. ما هو الخطوات اللازمة باستخدام API المشار إليه؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"}
{"text": "<s>[INST] نحن بصدد تطوير تطبيق للألعاب المحمولة تحتاج إلى إنشاء صور جذابة للشخصيات الخيالية. كيف يمكننا استخدام API المشار إليه؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"}
{"text": "<s>[INST] لدينا مشروع لتصميم منشآت فضائية متقدمة، ونحتاج إلى توليد صور مفصلة للمحطات. كيف يمكننا استخدام API المعني في هذا السياق؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"}
{"text": "<s>[INST] أود إنتاج لوحة فنية تجسد شروق الشمس على البحر. هل بإمكانك مساعدتي في استخدام الAPI المشار إليه؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"}
{"text": "<s>[INST] تطوير شركتنا تطبيق للتعليم الطبي و نريد إنشاء رسوم متحركة توضح عمليات جراحية معقدة. كيف يمكن استخدام الـ API في هذا السياق؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"}
{"text": "<s>[INST] بنيتُ روبوتاً للإسعافات الأولية وأرغب في توليد صور توضيحية لوظائفه. كيف يمكنني استخدام الAPI المشار إليه؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"}
{"text": "<s>[INST] أريد تصميم منزل ذكي متكامل، هل يمكن استخدام الـ API لتوليد صور توضيحية للمنزل؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"}
{"text": "<s>[INST] نريد تصميم لعبة تربيوهية تعليمية تتضمن معالم التاريخ العالمي. كيف يمكننا استخدام الـ API لإنشاء الصور اللازمة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"}
{"text": "<s>[INST] لدينا مشروع لإنتاج سلسلة كتب للأطفال باللغة العربية، ونحتاج لصور م? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"}
{"text": "<s>[INST]قم بتحليل تطبيق للجهاز اللوحي يحتوي على عدة تقنيات تعليمية متعددة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"}
{"text": "<s>[INST] قم بتصميم نظام حجز لمطعم يعتمد على تقنيات الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"}
{"text": "<s>[INST] أنشئ تطبيق للمواعدة يعتمد على تحليل النصوص والمحادثات لتوفير توصيات دقيقة للمستخدمين.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"}
{"text": "<s>[INST] قم بتنفيذ نظام مراقبة للمرافق العامة يستخدم تقنيات التعرف على الوجوه.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"}
{"text": "<s>[INST] قم بتطوير تطبيق توصيل يعتمد على تقنيات تحليل البيانات الجغرافية لتحديد أفضل الطرق والوقت المثالي للتوصيل.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"}
{"text": "<s>[INST] أنشئ منصة للتدريب عن بعد توفر محتوى تعليمي شامل وتفاعلي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"}
{"text": "<s>[INST] قم بتطوير تطبيق للتحليل الوبائي الذي يستخدم تقنيات تعلم الآلة للتنبؤ بانتشار الأمراض.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"}
{"text": "<s>[INST] أنشئ نظام إدارة المخازن الذكي الذي يعتمد على تقنيات الإنترنت الأشياء.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"}
{"text": "<s>[INST] قم بتصميم تطبيق لتتبع اللياقة البدنية يستخدم تقنيات تحليل البيانات الحيوية لمستخدميه.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"}
{"text": "<s>[INST] ابتكر منصة للتعليم الإلكتروني تعتمد على تقنيات التفاعل الصوتي والردود التلقائية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"}
{"text": "<s>[INST]كتابة قصيدة تحمل موضوع الشجاعة والتحدي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"}
{"text": "<s>[INST] تحليل محتوى نصي لفهم المشاعر التي يحملها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"}
{"text": "<s>[INST] إنشاء مقالة تقنية حول آخر التطورات في مجال الذكاء الصناعي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"}
{"text": "<s>[INST] تحويل صورة إلى رسم كاريكاتوري مضحك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"}
{"text": "<s>[INST] تصميم مخطط بياني يعرض توزيع الإنفاق الشهري على النفقات المختلفة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"}
{"text": "<s>[INST] إنشاء تطبيق ويب بسيط يعرض معلومات عن الطقس باستخدام API الجوية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"}
{"text": "<s>[INST] نظّم ورشة عمل عبر الإنترنت حول أسس تطوير تطبيقات الهاتف.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"}
{"text": "<s>[INST] تحويل نص إلى لغة برمجية باستخدام تقنية توليد النصوص.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"}
{"text": "<s>[INST] تصميم قالب موقع ويب احترافي لشركة تكنولوجيا.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"}
{"text": "<s>[INST] تطوير تطبيق محمول يتيح للمستخدمين تصوير الطعام والحصول على وصف لذلك الطعام تلقائياً.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"}
{"text": "<s>[INST]يجب على فريقنا تطوير تطبيق لتعليم اللغة الإنجليزية للأطفال، ونحتاج إلى واجهة برمجية تقوم بتحليل صور الألعاب التعليمية لنقدم تعليقات نصية على النشاطات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"}
{"text": "<s>[INST] نحتاج إلى تطوير منصة لتسويق الملابس عبر الإنترنت، ونريد استخدام واجهة برمجية تقوم بتحليل الصور لتوفير توصيات نصية بناء على أسلوب اللباس.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"}
{"text": "<s>[INST] تتطلب متجرنا الإلكتروني إمكانية تحليل الصور لتسهيل عملية تصنيف المنتجات، يرجى توفير إرشادات حول الاستخدام الصحيح لواجهة برمجية تحليل الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"}
{"text": "<s>[INST] نريد إضافة ميزة لتطبيقنا الطبي لتحويل الوصف النصي للأعراض إلى تشخيص طبي مبني على الصور، هل يمكنك مساعدتنا في الحصول على واجهة برمجية مناسبة؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"}
{"text": "<s>[INST] يرغب مشروعنا البحثي في استخدام تقنية التعلم العميق لتحليل صور الأفلام القديمة واستخراج وصف قصير لكل صورة، هل هناك واجهة برمجية توفر هذه الإمكانية؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"}
{"text": "<s>[INST] نحتاج إلى تطبيق يستطيع تحليل الصور المأخوذة من الأقمار الصناعية لتقديم تقارير عن التغيرات البيئية، ما الواجهة البرمجية المناسبة لذلك؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"}
{"text": "<s>[INST] تحتاج مشروعنا الجديد إلى إضافة خاصية تعريف الأشخاص من الصور التي يتم تحميلها، هل يمكنكم توجيهنا لاستخدام واجهة برمجية تقوم بتعريف ووصف الأشخاص بدقة؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"}
{"text": "<s>[INST] نريد تطوير تطبيق مساعدة للمعاقين من ذوي الاحتياجات الخاصة، يجب أن يكون للتطبيق خاصية تحليل الصور لتمكين المستخدمين من تعرف على الأشياء من حولهم، هل هناك واجهة برمجية توفر هذه الخاصية؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"}
{"text": "<s>[INST] يتطلب مشروعنا الإبداعي إمكانية تحويل الصور إلى رسوم بيانية، نحتاج استخدام واجهة برمجية تساعدنا في تحويل العناصر البصرية إلى تمثيل بياني يمكن استخدامه في تصاميمنا.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"}
{"text": "<s>[INST] أرغب في تطوير تطبيق يقوم بتحويل النصوص الشعرية إلى صور فنية تعبر عن المضمون، ما هي الواجهة البرمجية التي يمكن استخدامها لهذا الغرض؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"}
{"text": "<s>[INST]لدينا مشروع لتطوير تطبيق يتعلق بالصور الطبية، ونريد استخدام تقنيات التحويل لتحليل الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] نتاج شركتنا يتضمن مجموعة كبيرة من النصوص الإبداعية، ونود تصنيف هذه النصوص بشكل تلقائي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] نظرًا لتنوع المنتجات التي نقوم بتصميمها، نحتاج إلى API يمكنه تحويل الصور إلى نصوص لإضافة تسميات توضيحية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تعمل شركتنا على تطوير تطبيق لمقارنة الأسعار من متاجر مختلفة، نحتاج إلى API لتحليل نصوص التقييمات لتقديم ملخصات للمستخدمين.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] لدينا مشروع لتطوير منصة للتعليم عن بعد، نحتاج إلى تقنية تساعدنا في تحويل النصوص المكتوبة يدويًا إلى نصوص صوتية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] نريد تحسين تجربة مستخدمي تطبيقنا الذكي باستخدام تقنيات تعلم الآلة، لذا نحتاج إلى API يقوم بتوصيف نمط استخدام التطبيق.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] نحتاج إلى آلية تعمل على فحص الوثائق القانونية والتحقق من صحتها بشكل آلي، يمكنكم توجيهنا إلى الAPI المناسب؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] لدينا منصة لمشاركة الصور بين المستخدمين، ونريد دمج تقنية التعرف على الأشياء في الصور لتوفير وظيفة البحث بالصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تعمل شركتنا على منصة لتنظيم الأحداث والمؤتمرات، نرغب في دمج خاصية تحويل التواريخ المكتوبة بالنصوص إلى تواريخ قابلة للتحليل.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] نحن نعمل على تطبيق يجمع بين اللياقة البدنية والتغذية الصحية، ونريد إضافة ميزة تقديم وصفات طعام استنادًا إلى الصور التي يقوم المستخدم بالتقاطها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST]تريد شركتك إنتاج صور فنية من مجموعة نصوص شعرية. هل يمكنك توجيهنا إلى الواجهة البرمجية المناسبة لذلك؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] لديكم تطبيق لمشاركة الصور وترغبون في إضافة خاصية تحليل المحتوى النصي داخل الصور. هل تتوفر لديكم واجهة برمجية لهذا الغرض؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] أرغب في إنشاء تطبيق يمكنه تحليل اللوحات الفنية واستخراج المعلومات منها. هل يمكنك إيجاد واجهة برمجية تدعم ذلك؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] ترغب في تطوير تطبيق لتحليل الإعلانات التجارية واستخراج المعلومات الخاصة بها من خلال الصور. هل يمكنك الإرشاد إلى الواجهة البرمجية المناسبة؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] تحتاج إلى إضافة خاصية لتحليل الأزياء في الصور لتطبيق الأزياء الخاص بشركتك. هل توفر خدمة تطبيق برمجي لهذا الغرض؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] تريد تطوير نظام لتصنيف الطعام من الصور، هل يمكنك توجيهنا إلى الواجهة البرمجية المناسبة لهذا الغرض؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] تبحث عن واجهة برمجية لتحليل الإعلانات السيارات من الصور، هل تستطيع مساعدتي في العثور على الحل؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] تحتاج إلى إنشاء تطبيق يمكنه تحليل مشاهد الطبيعة واستخراج المعلومات المتعلقة بها. هل تتيح واجهة برمجية لهذه المهمة؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] تريد تطوير تطبيق لتحليل المناظر الطبيعية واستخراج تفاصيلها من الصور، هل يمكنك إرشادي إلى الواجهة البرمجية المناسبة؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] تبحث عن واجهة برمجية لتحليل الفنون التشكيلية في الصور، هل يمكنك توجيهنا إلى الأداة المناسبة لذلك؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST]تحتاج إلى تحديد نوع الزهور الموجودة في الصورة والتعرف عليها، ابحث عن نموذج يستخدم شبكات تحويلية للقيام بذلك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] تريد تحليل محتوى الفيديو الخاص بك للكشف عن الأشياء المعروضة فيه، استخدم نموذج يعتمد على الذكاء الاصطناعي لتحقيق ذلك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] تحتاج إلى تحليل الصور الطبية لتحديد الأورام الموجودة فيها، استخدم نموذج يستخدم تقنيات Deep Learning لهذا الغرض.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] ترغب في تحسين دقة تحديد النص في الصور، استخدم نموذج يعتمد على شبكات تحويلية محسنة لهذه المهمة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] تخطط لتطوير تطبيق يتيح للمستخدمين تعديل الصور باستخدام تقنيات الذكاء الاصطناعي، استخدم أحد النماذج الجاهزة لتحقيق ذلك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] تحتاج إلى تحويل الصور الى نص، ابحث عن نموذج يمكن استخدامه لهذا الغرض بشكل فعال.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] ترغب في معرفة التفاصيل عن مكونات الصورة وتحليلها، استخدم نموذج يستخدم تقنيات Deep Learning لاستخراج المعلومات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] تريد تقديم خدمة لتصنيف الصور بناءً على المحتوى الظاهر فيها، استخدم نموذج يعتمد على الشبكات العصبية لتحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] تبحث عن طريقة لتحويل النصوص اليدوية في الصور إلى نصوص رقمية، استخدم نموذج يتميز بالدقة في التعرف على النصوص باللغة العربية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] تود تحويل النصوص الموجودة في الصور إلى كود برمجي، ابحث عن نموذج يستخدم تقنيات Deep Learning لتحقيق ذلك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"}
{"text": "<s>[INST]كتابة نص إعلاني جذاب يروج لمنتجنا الجديد على وسائل التواصل الاجتماعي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"}
{"text": "<s>[INST] تطوير تطبيق جوال يتيح للمستخدمين تقديم طلبات الجلوس في المطاعم المحلية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"}
{"text": "<s>[INST] إعداد تقرير شهري يحتوي على نتائج أداء مبيعات منتجاتنا على الموقع الإلكتروني.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"}
{"text": "<s>[INST] تحليل أداء حملة البريد الإلكتروني الأخيرة لزيادة معدل الانتقال إلى صفحة المنتج.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"}
{"text": "<s>[INST] إنشاء نموذج توقعي للتنبؤ بالطلب على منتجاتنا باستخدام الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"}
{"text": "<s>[INST] تصميم واجهة مستخدم جذابة لتطبيق الهاتف الذكي الخاص بشركتنا.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"}
{"text": "<s>[INST] تنفيذ اختبار وحدات لضمان جودة تطبيقنا الجديد قبل إطلاقه للجمهور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"}
{"text": "<s>[INST] تحسين أداء موقعنا على الويب من خلال تحسين سرعة التحميل وتجربة المستخدم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"}
{"text": "<s>[INST] إنشاء تطبيق ويب يوفر خدمات الحجز لعملائنا في مجال الفنادق والسفر.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"}
{"text": "<s>[INST] تنفيذ تحليل استراتيجي للسوق لفهم احتياجات العملاء وتحديد فرص جديدة للنمو.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"}
{"text": "<s>[INST]عميلك يرغب في إنشاء تطبيق لتعليم اللغة الإنجليزية للأطفال. يحتاج الى واجهة تفاعلية تقدم أسئلة عن الصور لتحفيز تعلمهم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"}
{"text": "<s>[INST] مطورك يرغب في إنشاء تطبيق للتعرف التلقائي على النباتات والحيوانات عن طريق استخدام الصور. احتاج باستخدام نموذج لتحديد أنواع الكائنات في الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"}
{"text": "<s>[INST] تريد شركتك إنشاء تطبيق لتحليل الشخصيات في الأفلام عن طريق الصور. يتطلب تحديد الشخصيات الرئيسية وتقديم تحليلات حول أدوارهم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"}
{"text": "<s>[INST] صديقك يريد تطوير تطبيق لمساعدة الطلاب في دراستهم عن بسهولة بتقديم اسئلة مرئية تجذب انتباههم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"}
{"text": "<s>[INST] تخطط لإنشاء منصة لفهم نوع العملاء المستهدفين عبر تحليل صورهم. تريد ان تجمع بين تقنيات تحليل الصور وتحليل النص لتكوين فهم شامل.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"}
{"text": "<s>[INST] عميلك يخطط لإنشاء تطبيق لتحفيز مستخدميه على ممارسة التمارين الرياضية بواسطة عرض صور تحفيزية. يحتاج الى نموذج يحدد نوع التمرينات المعروضة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"}
{"text": "<s>[INST] تود إنشاء تطبيق يساعد السياح في فهم تاريخ المواقع السياحية عن طريق استخدام الصور. يدعم التطبيق تسلسل الأحداث التاريخية بالنص والصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"}
{"text": "<s>[INST] تحتاج الى تطبيق يمكنه تحليل أعماق البحار من خلال الصور التي تلتقطها غواصاتك. يحتاج الى نموذج يحدد الكائنات البحرية في الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"}
{"text": "<s>[INST] مطورك يريد إنشاء تطبيق لتسهيل فهم الرسوم البيانية والبيانات المعقدة عبر عرضها بشكل بصري مفهوم. يحتاج الى نموذج يحدد المعلومات الرئيسية في الرسوم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"}
{"text": "<s>[INST] تحتاج الى تطبيق يمكنه تقديم توصيات لمطعمك عن طريق تحليل الصور التي التقطها العملاء للأطعمة. يحتاج الى نموذج يحدد الأطعمة ويقدم التوصيات المناسبة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"}
{"text": "<s>[INST]نبحث عن طريقة لتوليد نصوص تفسيرية لصور تاريخية. هل يمكنك تقديم اقتراح حول كيفية استخدام Hugging Face Transformers لهذا الغرض؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"}
{"text": "<s>[INST] نحتاج إلى توليد أسماء شخصيات خيالية لقصصنا القصيرة. هل يمكنك إرشادنا حول كيفية استخدام Hugging Face Transformers لإنشاء هذه الأسماء؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"}
{"text": "<s>[INST] نرغب في تحويل تواريخ التاريخ إلى نصوص قصيرة وسهلة الفهم. هل بإمكانك استخدام Hugging Face Transformers من أجل ذلك؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"}
{"text": "<s>[INST] نود توليد تسميات معبرة للمناظر الطبيعية التي نصورها. هل يوجد واجهة برمجية مناسبة تعمل بتكنولوجيا Hugging Face Transformers؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"}
{"text": "<s>[INST] نبحث عن طريقة لتوليد تعليقات ساخرة للصور الكوميدية. هل يمكن Hugging Face Transformers مساعدتنا في هذا الأمر؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"}
{"text": "<s>[INST] يتعذر علينا إيجاد أفكار لعناوين مدوناتنا. هل يمكن استخدام Hugging Face Transformers لتوليد عناوين مثيرة وملهمة؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"}
{"text": "<s>[INST] نحتاج إلى إنشاء وصف قصير للأطعمة التي نبيعها في المتجر. هل يوجد API يستند إلى Hugging Face Transformers لمساعدتنا في هذا؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"}
{"text": "<s>[INST] نود توليد عبارات تعبيرية تناسب منتجاتنا الجديدة. هل يوجد API يستند إلى Hugging Face Transformers يمكن استخدامه لهذا الغرض؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"}
{"text": "<s>[INST] نبحث عن طريقة لإنشاء نصوص تعليمية تشرح استخدام منتجاتنا. هل يمكن Hugging Face Transformers مساعدتنا في ذلك؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"}
{"text": "<s>[INST] نود توليد تعليقات إيجابية للصور التي نشاركها على وسائل التواصل الاجتماعي. هل بإمكانك اقتراح API يعتمد على Hugging Face Transformers من أجل ذلك؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"}
{"text": "<s>[INST]يرغب فريقنا في تطوير نظام توصيات للمستخدمين بناءً على تاريخ تصفحهم على الموقع. اقترح استخدام تحليل البيانات لهذا الغرض.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"}
{"text": "<s>[INST] نريد تطوير نظام تحليل تعليقات المستخدمين على منتجاتنا لاستخراج الرأي العام. اقترح استخدام تحليل المشاعر لتحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"}
{"text": "<s>[INST] نحتاج إلى تحديث نظام البحث في موقعنا ليشمل البحث عن الكتب الصوتية. يرجى توضيح كيفية تضمين هذه الخاصية باستخدام Elasticsearch API.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"}
{"text": "<s>[INST] يجب علينا تطوير نظام تصنيف للأفلام الجديدة وتصنيفها في فئات محددة. يمكنك الاقتراح على استخدام تقنيات تعلم الآلة لهذا الغرض.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"}
{"text": "<s>[INST] نريد إنشاء نظام يحسب معدل إنجاز الموظفين في الشركة بناءً على بيانات الحضور والمغادرة الخاصة بهم. كيف يمكن تحقيق ذلك باستخدام Python API؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"}
{"text": "<s>[INST] نريد تنفيذ نظام تتبع للمستندات الواردة إلى الشركة وتصنيفها تلقائيًا حسب نوعها. هل يمكن استخدام خوارزمية تقاطع الانتقاء لتحقيق ذلك؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"}
{"text": "<s>[INST] يرغب فريق الإحصائيات في تحليل بيانات العملاء لتوقع سلوكهم المستقبلي. استخدم خوارزمية \"Random Forest\" لتحليل البيانات واقترح كيفية تنفيذها باستخدام Python.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"}
{"text": "<s>[INST] يرغب فريق التسويق في تحليل أداء إعلاناتنا على وسائل التواصل الاجتماعي. استخدم تقنية تحليل الشبكات العصبية لفهم الاستجابة للإعلانات وقم بتقديم توجيهات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"}
{"text": "<s>[INST] يرغب فريق الأبحاث في تطوير نظام لتحليل المقالات العلمية واستخراج المعلومات الهامة منها. استخدم خوارزمية \"NLP\" القائمة على الشبكات العصبية لتحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"}
{"text": "<s>[INST] نريد تطوير نظام لتحليل تقارير المبيعات الشهرية وتوقع الإيرادات المستقبلية. اقترح استخدام طرق التحليل الزمني لذلك وتوضيح كيفية تنفيذها باستخدام Python.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"}
{"text": "<s>[INST]قم بإنشاء شخصية خيالية تحمل إجابة لغز مثل \"ما هو الشيء الذي يمشي بأربعة أرجل في الصباح وبثلاثة في المساء؟\".? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] قم بتطوير نموذج يستطيع تحليل مشهد يحتوي على تشويه بصري وتقديم توصيات لإصلاحه.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] أنشئ قصيدة شعرية قصيرة تعبر عن جمال الطبيعة في فصل الربيع.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] اكتب مقالًا علميًا يشرح كيفية تأثير التكنولوجيا على المجتمع الحديث.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] أكتب نص تعليمي يشرح كيفية تنظيم المعلومات باستخدام الرسوم البيانية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] أنشئ كتابة سينمائية تصف مواجهة بين بطل وشرير في قصة خيالية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] كتابة مقال ساخر يعبر عن تجارب مضحكة في الحياة اليومية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] أنشئ تقريرًا علميًا يشرح أهمية الحفاظ على التنوع البيولوجي في العالم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] كتابة رسالة شكر تعبر عن امتنانك لزملاء العمل في فريقك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] أنشئ نص قصير يعبر عن رؤيتك للعالم بعد عقود من الآن.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST]كتب مقالة تحدث عن أهمية الذكاء الاصطناعي في تحسين الخدمات الصحية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] ابحث عن أفضل طريقة لتحسين أداء نموذج التعلم العميق الذي تستخدمه في تصنيف الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] قم بتطوير نظام توصيل طلبات الطعام عبر الإنترنت باستخدام خرائط العالم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] أكتب برنامج باستخدام نموذج معالجة اللغة الطبيعية لتحليل رأي العملاء حول منتجات شركتك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] ابنِ نظام توصيل مستندات آلي يقوم بتسليم الملفات بين الموظفين في الشركة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] عرّف نظام مصادقة ذكي يستخدم تقنيات التعرف على الوجوه للدخول إلى مبنى الشركة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] أنشئ برنامج يستخدم خوارزمية تعلّم آلي لتحديد الكائنات في الصور الطبية لتسهيل تشخيص الأمراض.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] انشئ تطبيق يستخدم خرائط جوجل لعرض مواقع الشركات في المنطقة باستخدام واجهة برمجة التطبيقات لخرائط جوجل.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] قم ببناء نظام إدارة المشاريع الخاص بشركتك باستخدام خوارزميات التعلم الآلي لتحسين التخطيط وتوزيع الموارد.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] أنشئ برمجية تستخدم تقنيات الذكاء الاصطناعي لتحليل البيانات المالية وتقديم توصيات لأداء الاستثمارات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST]تحتاج شركتك إلى تحديد محتوى النص في الصور بشكل تلقائي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] عندك حاجة لمعالجة النصوص اليدوية بشكل تلقائي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] لديك حاجة لتحويل الخطوط المكتوبة بخط اليد إلى نصوص مكتوبة على الحاسوب.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تريد تصميم نظام يتعرف على الأشخاص ويعبر عنهم بشكل نصي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تود القدرة على تحليل المشاهد السينمائية واستخراج المشاعر والأحاسيس منها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تحتاج إلى نظام يمكنه تحويل الصور الطبية إلى تقارير نصية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تحتاج لتطوير نظام لتحليل التعبيرات الوجهية واستخدامها في فهم ردود الأفعال.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] ترغب في تصميم نظام يمكنه تحليل الصور الفضائية واستخراج المعلومات النصية منها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تريد إنشاء نظام يلخص المحتوى التلقائي من الصور الفوتوغرافية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تحتاج إلى تطوير نظام قادر على فهم الرسومات التوضيحية واستخلاص النصوص المتعلقة بها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST]قم بتطبيق نموذج لتوليد تعليقات على صور لفيديوهات درجة الوضوح المرتفعة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"}
{"text": "<s>[INST] استخدم النموذج الخاص بـ \"ydshieh/vit-gpt2-coco-en\" لتحليل صورة واستخراج محتواها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"}
{"text": "<s>[INST] قم باستخدام النموذج \"ydshieh/vit-gpt2-coco-en\" لتوليد نص توضيحي لصورة من إخراج الكاميرا.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"}
{"text": "<s>[INST] أنشئ تعليقًا لصورة معينة باستخدام نموذج التشفير والفك \"ydshieh/vit-gpt2-coco-en\".? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"}
{"text": "<s>[INST] قم بتحليل صورة وإنشاء وصف لمحتواها باستخدام واجهة \"ydshieh/vit-gpt2-coco-en\".? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"}
{"text": "<s>[INST] اكتب كودًا يستخدم النموذج \"ydshieh/vit-gpt2-coco-en\" لتحويل صورة معينة إلى نص وصفي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"}
{"text": "<s>[INST] اكتب دالة تستخدم النموذج \"ydshieh/vit-gpt2-coco-en\" لتحليل صورة وإنشاء سياق تصويري لها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"}
{"text": "<s>[INST] استخدم API \"ydshieh/vit-gpt2-coco-en\" لتحويل تسلسل صور إلى نصوص توضيحية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"}
{"text": "<s>[INST] أنشئ تعليقاً على صورة باستخدام وظيفة \"ydshieh/vit-gpt2-coco-en\" لتحليل الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"}
{"text": "<s>[INST] ابتكر تعليقاً توضيحياً لصورة معينة باستخدام نموذج \"ydshieh/vit-gpt2-coco-en\" لتوليد الوصف.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"}
{"text": "<s>[INST]نريد تطوير أداة تساعد المتعاملين باللغة العربية على ترجمة النصوص إلى لغات أخرى بشكل فعال.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] تحتاج منظمة خيرية إلى أداة لتحليل مشاريعها وقياس تأثيرها على المجتمع.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] نبحث عن برنامج يقوم بتصنيف الأفلام استنادًا إلى تفاصيل المؤامرة الخاصة بها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] لدينا تطبيق للتعلم عن بسلك، ونحتاج إلى أداة لتوليد وظائف اللغة الطبيعية بجودة عالية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] نشرك يرغب في بناء نظام إدارة مخزون يستند إلى تحليلات التوقع الذكي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] يريد مطعمنا تحسين خدمة الزبائن من خلال تحليل تقييماتهم عبر وسائل التواصل الاجتماعي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] تريد شركتنا تطوير نظام يقوم بالتعرف على النصوص غير المرغوب فيها عبر البريد الإلكتروني.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] تحلم منظمتنا بإنشاء مساعد صوتي يستجيب لأسئلة المستخدمين بصورة ذكية وسلسة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] نود تطوير أداة تحليل الصور الطبية وتقديم تقارير دقيقة للأطباء عن التشخيص.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST] تريد منظمتنا تطوير نظام يحسن ترتيب نتائج البحث الداخلي للموقع الإلكتروني.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"}
{"text": "<s>[INST]تخطط لتطوير تطبيق للتعليم عن بعد يستخدم التعلم الآلي لتوفير مراجع تعليمية للطلاب. يمكنك استخدام API التالي لدمج خوارزميات التعلم الآلي في التطبيق.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تريد إنشاء نظام توصية لتحسين تجربة التسوق عبر الإنترنت لزبائنك. يمكنك استخدام ال API التالي لتطوير نموذج توصية قائم على الاهتمامات السابقة للمستخدم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تريد تحسين تجربة المستخدم على موقع الويب الخاص بشركتك من خلال توفير إجابات أكثر دقة في قسم الدعم الفني. يمكنك استخدام ال API التالي لتدريب نموذج لتحليل لغة الطلبات الواردة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تريد تنفيذ نظام للكشف عن الغش في الامتحانات الإلكترونية. يمكنك استخدام ال API التالي لتطوير نموذج لتحليل الصور الرقمية لإكتشاف الغش.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تهدف لبناء نظام توصية مخصص لتحسين تجربة المستخدم على منصة التداول الإلكترونية لشركتك. يمكنك استخدام API التالي لتطوير نموذج توصية معتمد على الفهم العميق لسلوكيات التداول.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تخطط لتطوير تطبيق لمساعدة الطلاب على تحسين مهارات اللغة الإنجليزية الخاصة بهم. يمكنك استخدام API التالي لدمج نموذج تعلم الآلة لتصحيح نطق الكلمات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تريد أن تظهر نتائج تصنيف الصور بطريقة تفاعلية لمستخدمي موقع الويب الخاص بك. يمكنك استخدام ال API التالي لتطوير واجهة رسومية تعرض تفاصيل نتائج تصنيف الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تهدف إلى تعزيز تواجد شركتك على وسائل التواصل الاجتماعي من خلال إنشاء نصوص واضحة وجذابة. يمكنك استخدام ال API التالي لتطوير نموذج توليد النصوص.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تريد تحسين استجابة النظام الخاص بشركتك لطلبات العملاء من خلال فهم نغمة الكلام والشعور من خلال النصوص. يمكنك استخدام API التالي لتطوير نموذج لتحليل المشاعر.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST] تريد تكامل نظام الدفع الخاص بك مع خدمة تقديم التوصيات للعملاء استنادًا إلى عادات الشراء السابقة. يمكنك استخدام API التالي لتطوير جزء من النظام الذي يعرض التوصيات للمستخدمين.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"}
{"text": "<s>[INST]مطلوب تطبيق يمكنه تحديد نوع الأشجار من الصور التي يلتقطها المستخدم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"}
{"text": "<s>[INST] يتطلب تطوير تطبيق للمراقبة الصحية تقنيات تصنيف الصور الطبية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"}
{"text": "<s>[INST] لدينا مشروع لتطوير تطبيق يمكنه تحليل الصور الفضائية لتساعد في توقع الكوارث الطبيعية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"}
{"text": "<s>[INST] يطلب عميلنا إنشاء تطبيق يمكنه تحليل الصور لتقديم توصيات غذائية مبنية على فحص الوجبات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"}
{"text": "<s>[INST] يحتاج مشروعنا إلى تكامل نظام تعليم آلي يمكنه فهم المواقف الاجتماعية من خلال تحليل الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"}
{"text": "<s>[INST] نحتاج إلى تطبيق يستخدم تقنيات التعلم الآلي لتحليل الصور الطبية وتقديم تشخيص دقيق.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"}
{"text": "<s>[INST] تطوير تطبيق يمكنه تحديد الأعطال في معدات الشركة من خلال تحليل صورها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"}
{"text": "<s>[INST] يطلب عميلنا إنشاء أداة لتحليل الصور الأمنية والكشف عن أنماط غير معتادة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"}
{"text": "<s>[INST] تطوير تطبيق لتقديم توصيات فنية للمصممين بناءً على تحليل صور الأزياء.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"}
{"text": "<s>[INST] تنفيذ تطبيق لتحليل الصور الفنية واقتراح أعمال فنية متشابهة للفنانين.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"}
{"text": "<s>[INST]تنظم مدرستك مسابقة لأفضل رسمة تصف فصل الشتاء. استخدم API لتوليد نص قصير يصف الرسمة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] كتبت قصيدة عن الطبيعة وترغب في تحويلها إلى صورة فنية. استخدم API لتحويل النص إلى صورة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] قم بإعداد عرض تقديمي حول تطوير تطبيقات الهاتف الذكي. استخدم API لإنشاء شرائح تقديمية توضح العناصر الرئيسية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] تخطيط لرحلة برية إلى الصحراء وترغب في كتابة قصيدة توضح جمال الطبيعة هناك. استخدم API لتحول الفكرة إلى نص شعري.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] تخيلت عرضًا تفاعليًا حول تاريخ العلوم وتريد توليد نص توضيحي للأفكار الرئيسية. استخدم API لإنشاء النص.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] أردت كتابة مقالة عن التنمية المستدامة وتحتاج إلى تحسين الجودة. استخدم ال API المرتبط لتوليد نص محسن للمقالة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] تود كتابة كتاب خيال علمي وتحتاج إلى مساعدة في بناء العالم المخيل للقصة. استخدم API لإنشاء وصف مفصل للعوالم خيالية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] تخيلت فلمًا قصيرًا يحكي قصة حب رومانسية وتحتاج إلى كتابة نص السيناريو. استخدم ال API لتوليد نص السيناريو.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] تخيلت تطبيقًا جديدًا لمنصة التواصل الاجتماعي وتريد توليد وصف مفصل لطريقة عمل التطبيق. استخدم API لإنشاء وصف دقيق.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"}
{"text": "<s>[INST] تريد تحسين خطابك العام في المؤتمر القادم وتبحث عن طريقة لجعل الخطاب أكثر جاذبية. استخدم API لتحسين نص الخطاب.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"}
{"text": "<s>[INST]احتفظ بعينات ماء الأمطار في حديقتك واستخدمها لري النباتات.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] أنشئ قائمة بالمهام اليومية التي تريد إنجازها وحدد أهمية كل مهمة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] ابحث عن مطعم يقدم وجبات صحية وشهية في منطقتك واستكشف قائمتهم.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] قم بتصوير منظر طبيعي جميل وشاركه مع أصدقائك على وسائل التواصل الاجتماعي.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] اختر مسلسل تلفزيوني جديد لمشاهدته واستعرض تقييماته عبر منصة مراجعات الأفلام.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] ابحث عن أحدث أخبار التكنولوجيا وتقنيات الذكاء الاصطناعي عن طريق استخدام موقع الأخبار المتخصصة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] قم بتطبيق تمارين الرياضة الأساسية في منزلك لتعزيز قوة العضلات والمرونة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] اطلب وجبة مأكولات بحرية لذيذة من أحد مطاعم السوشي المحلية واستمتع بتجربة فريدة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] ابدأ يومك بتناول وجبة فطور صحية تحتوي على فواكه وحبوب كاملة لزيادة طاقتك وتحفيز نشاطك.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] استخدم تطبيق مراقبة النوم على هاتفك لتتبع جودة نومك وتحسين عادات نومك.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST]أنشئ تطبيقًا يقوم بتحليل مشاهد الفيديو وتتبّع جسم الإنسان.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] قم بتدريب نموذج لتقدير عمر الأشخاص في الصور باستخدام بيانات تدريب موجودة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] ابن شاتبوت المتكلم لمساعدة المستخدمين في المراقبة الصوتية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] تحقق من استخدام أبل الشهير للذكاء الاصطناعي في برنامج Siri الصوتي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] قم بإنشاء تطبيق يستخدم الروبوتات الذكية لتقديم الطلبات وخدمة العملاء.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] قم بتحليل المشهد الكبير من خلال تصوير جوي للزراعة داخل المزارع.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] أنشئ تطبيقًا يستخدم تقنية التعلم العميق لتحليل المستندات اليدوية بوضوح.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] تطوّر نظام دعم القرارات الذكي للمساعدة في تحليل البيانات واتخاذ القرارات الاستراتيجية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] أنشئ تطبيق يُشفر الرسائل النصية لضمان الخصوصية والأمان.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST] إنشاء تطبيق يحلل نثر الكتب والنصوص لاستخراج المعلومات الأساسية والإحصائيات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"}
{"text": "<s>[INST]قم بتحليل محتوى الصورة واستخراج نصوص متعلقة بها باستخدام النموذج الجديد.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"}
{"text": "<s>[INST] إنشاء برنامج يقوم بتحويل نص إنجليزي إلى لغة أخرى باستخدام النموذج المعتمد.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"}
{"text": "<s>[INST] قم بتصميم نظام لمعالجة اللغة الطبيعية لتحديد المشاعر في النصوص المدخلة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"}
{"text": "<s>[INST] تنفيذ نظام لتوليد الرموز الموسيقية من النصوص الشعرية بواسطة النموذج المحدد.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"}
{"text": "<s>[INST] استخدام النموذج الجديد لتحليل الصور واستخراج الكائنات والمواضيع المعروضة فيها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"}
{"text": "<s>[INST] تطوير تطبيق للتعرف التلقائي على مضمون الصور باستخدام النموذج المستخدم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"}
{"text": "<s>[INST] إنشاء نظام لتوليد نصوص شعرية مستوحاة من الرسومات الفنية باستخدام النموذج الجديد.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"}
{"text": "<s>[INST] بناء تطبيق يستخدم النموذج الخاص لتحليل مفهوم الصور ثم إنشاء نص يصفه.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"}
{"text": "<s>[INST] تصميم نظام لتحويل النصوص الطبيعية إلى تعبيرات فنية بصرية باستخدام النموذج المحدد.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"}
{"text": "<s>[INST] تنفيذ برنامج لتحليل اللغات القديمة وترجمة النصوص إلى لغات حديثة باستخدام النموذج المطلوب.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"}
{"text": "<s>[INST]كتابة قصيدة شعرية تبدأ بسطر \"في ليلة من ليالي الشتاء الباردة\".? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"}
{"text": "<s>[INST] أنشئ درسًا تفاعليًا يشرح كيفية بناء تطبيق ويب باستخدام React.js وNode.js.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"}
{"text": "<s>[INST] أنا بحاجة إلى إعداد مقالة بحثية عن تأثير الذكاء الاصطناعي في صناعة التصميم الجرافيكي. يمكنك مساعدتي في استخدام بيانات ومعادلات حول هذا الموضوع؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"}
{"text": "<s>[INST] قم بتوليد قصة خيالية تدور حول رحلة في الزمن لعالم موازٍ.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"}
{"text": "<s>[INST] إنشاء تطبيق ديناميكي يوفر قائمة تشغيل موسيقى استنادًا إلى تفضيلات المستخدم باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"}
{"text": "<s>[INST] أنشئ تطبيقًا يقوم بتحليل وتصنيف الصور لتقديم تفاصيل دقيقة عن المحتوى البصري.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"}
{"text": "<s>[INST] تطوير نظام استشعار ذكي يتيح للمستخدمين مراقبة مستويات درجة حرارة المنزل عن بُعد وتلقي تنبيهات في حالة وجود تغييرات كبيرة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"}
{"text": "<s>[INST] إعداد تطبيق يقوم بتحليل نمط الكتابة اليدوية لتحديد الهوية الشخصية باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"}
{"text": "<s>[INST] إنشاء موقع ويب للتعليم عن بُعد يوفر محتوى تعليمي تفاعلي لطلاب مختلف المراحل الدراسية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"}
{"text": "<s>[INST] إنشاء نظام توصية للأفلام يستند إلى تحليل تفضيلات المستخدم وتقييمات الأفلام باستخدام خوارزميات تعلم الآلة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"}

"<s>[INST]ابتكر تطبيقًا يمكنه تحليل الموسيقى وتوصية المستخدمين بالأغاني المناسبة لذوقهم الفني.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"
"<s>[INST] كتب أداة تساعد المستخدمين في تحويل النصوص إلى لغة برمجة معينة بشكل تلقائي.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"
"<s>[INST] اكتب برنامجًا يساعد في تحليل تصرفات المستخدمين عبر مواقع التواصل الاجتماعي.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"
"<s>[INST] قم بتطوير منصة لتحليل المقالات الطبية وتحديد المعلومات الهامة والإحصائيات الحيوية.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"
"<s>[INST] عيّن نظام تصنيف آلي لفئات الأفلام بناءً على محتواها وتصنيفات المستخدمين.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"
"<s>[INST] قم بتصميم وتنفيذ بوت ذكاء اصطناعي لخدمة عملاء شركتك عبر الدردشة.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"
"<s>[INST] إنشاء مكتبة تفاعلية لتصدير البيانات الجغرافية إلى تنسيقات مختلفة لتحليل البواقع.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"
"<s>[INST] بناء نظام توصية للمستخدمين لتحديد الأزياء الأنيقة استنادًا إلى تفضيلاتهم الشخصية.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"
"<s>[INST] بناء نظام ذكاء اصطناعي يستخدم تقنيات الرؤية الحاسوبية لتحليل صور الأشخاص والكشف عن المزاج والمعالم الوجهية.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"
"<s>[INST] طور برنامجًا يستخدم تقنيات التعلم الآلي للتنبؤ بأداء الأسهم وتوجيه المستثمرين.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'YituTech/conv-bert-base', 'api_call': \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.' </s>"
"<s>[INST]نحن نرغب في تطوير نظام يتيح لنا تحليل مدى اتساع النصوص التي تُدخل إلى منصتنا عبر الإنترنت. هل يمكنك تقديم ال API المناسب لهذه الغاية؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"
"<s>[INST] نود تطوير نظام يساعدنا في تحديد الأماكن الجذابة للزوار في مناطق السياحة. هل يمكنك اقتراح API لاستخراج المعلومات اللازمة لذلك؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"
"<s>[INST] لدينا مشروع لتطوير تطبيق يسهل عملية الترجمة الآلية للنصوص بدقة وسلاسة. ما هي الAPI التي تنصح باستخدامها لهذا الغرض؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"
"<s>[INST] نرغب في تطوير نظام يساعدنا في تصنيف المستندات الطبية تلقائياً. هل يمكنك تقديم API مناسبة لهذا الغرض؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"
"<s>[INST] عميلنا يرغب في تحليل كتبه الرقمية لفهم المواضيع التي يتناولها. هل يمكنك توجيهنا إلى API مناسب لتحقيق هذا الهدف؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"
"<s>[INST] تحتاج شركتنا إلى تقدير جودة الترجمة الآلية المستخدمة في تطبيقنا الحالي. هل تعرف API تقدم هذه الخدمة بكفاءة؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"
"<s>[INST] نود تطوير نظام يستطيع تحليل الأخبار واستخراج المعلومات الرئيسية منها. هل يمكنك توجيهنا إلى API يساعدنا في ذلك؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"
"<s>[INST] يهدف مشروعنا إلى تحسين خدمة العملاء عبر تحليل محتوى مكالمات الهاتف. هل يمكنك توجيهنا إلى أداة تحليل نصوص تناسب هذا الغرض؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"
"<s>[INST] نرغب في تحسين أداء نظام التصنيف الضروري لمشروعنا. هل يمكنك تقديم API تعزز مهارات التصنيف النصي؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"
"<s>[INST] نود تطوير أداة تحليل نصوص بشكل تفاعلي وسريع. هل يمكنك توجيهنا إلى API يدعم هذا الهدف؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'dmis-lab/biobert-v1.1', 'api_call': \"AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.' </s>"
"<s>[INST]ترغب مدرستنا في تحسين تعلم اللغة الإنجليزية للطلاب الصغار عبر تطبيق تفاعلي. نحتاج إلى أداة تتيح التعرف على نطق الكلمات بدقة.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"
"<s>[INST] تخطط عيادتنا لتحسين خدمة العملاء من خلال تطبيق الدردشة الآلية. نحتاج إلى واجهة برمجية تمكن التفاعل مع العملاء بشكل ذكي.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"
"<s>[INST] نريد تحسين تجربة التسوق عبر الإنترنت لعملائنا من خلال تصميم نظام توصيات شخصية. ماهي الطريقة المثلى لتحقيق ذلك؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"
"<s>[INST] لدى منصة تعليمية ترغب في تحسين تجربة المستخدمين في اختباراتها الإلكترونية. نرغب في استخدام تقنية التصنيف الآلي لتقديم تقييم سريع ودقيق.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"
"<s>[INST] نرغب في إنشاء تطبيق لتعليم اللغات الأجنبية يستند إلى الذكاء الاصطناعي. ما هي أفضل واجهة برمجية لتقديم دروس تفاعلية للمستخدمين؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"
"<s>[INST] تسعى منصتنا الإلكترونية إلى تقديم تجربة تسوق مخصصة لكل مستخدم بناءً على تفضيلاته الشخصية. ما هو أفضل اسم لواجهة برمجية تحقق هذا الهدف؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"
"<s>[INST] لدينا تطبيق يهدف إلى تعزيز اللياقة البدنية للمستخدمين. نرغب في تضمين نظام تتبّع تدريبات اللياقة البدنية. ما هو أفضل اسم واجهة برمجية لهذه الخدمة؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"
"<s>[INST] تريد شركتنا تحسين عمليات التسويق الرقمي من خلال تخصيص العروض لكل عميل بناءً على سلوكه التفاعلي. ما هو أفضل اسم لواجهة برمجية تفعل هذا؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"
"<s>[INST] تقوم شبكة المدارس بتنفيذ نظام تعليم عن بُعد للطلاب. نحتاج إلى تكامل نظام إلكتروني لتقديم الاختبارات وتقييم أداء الطلاب. ما هي أفضل واجهة برمجية لهذا الغرض؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"
"<s>[INST] نهدف إلى تحسين تجربة تسوق التجزئة عبر استخدام تكنولوجيا الواقع المعزز. نحتاج إلى دمج واجهة برمجية لتوفير تجربة تسوق مبتكرة للعملاء.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'princeton-nlp/unsup-simcse-roberta-base', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': None, 'accuracy': None}, 'description': 'An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.' </s>"
"<s>[INST]تطوير تطبيق لتحليل مستويات الإجهاد والتوتر للمستخدمين. يرجى استخدام API التالي لاستخراج الميزات اللازمة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"
"<s>[INST] إنشاء منصة للتعلم عن بسبب المتداولات بالبورصة. استخدم API التالي لاستخراج المعلومات الضرورية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"
"<s>[INST] تصميم تطبيق للمراقبة الصحية الشخصية. قم باستخدام API التالي لاستخراج البيانات اللازمة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"
"<s>[INST] تطوير نظام توصية الموسيقى الشخصية. استخدم API التالي لاستخراج التفاصيل اللازمة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"
"<s>[INST] تنفيذ نظام تنبؤ بالطقس بناءً على بيانات خاصة بالموقع الجغرافي. استخدم API التالي للحصول على المعلومات الجوية الحالية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"
"<s>[INST] تصميم تطبيق يقدم مشورة تغذية صحية عبر الهاتف المحمول. استخدم API التالي للحصول على المعلومات الغذائية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"
"<s>[INST] إنشاء نظام لتحليل المشاعر الموجودة في التعليقات على وسائل التواصل الاجتماعي. استخدم الAPI التالي لاستعراض المشاركات.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"
"<s>[INST] تنفيذ نظام توصية الأفلام المخصص. قم باستخدام API التالي لاستخراج تفاصيل الأفلام.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"
"<s>[INST] تطوير تطبيق يحسن عمليات التعلم عن بُعد. استخدم API التالي لاستخراج المعلومات التعليمية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"
"<s>[INST] إعداد منصة لمتابعة التدفقات الحية لمباريات الرياضة. استخدم API التالي للحصول على معلومات الأحداث الرياضية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'api_call': \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", 'api_arguments': 'input_ids, attention_mask', 'python_environment_requirements': 'transformers', 'example_code': \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'UMLS', 'accuracy': 'N/A'}, 'description': 'SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.' </s>"
"<s>[INST]تحتاج منظمتنا لنظام لتحليل محتوى وسائل التواصل الاجتماعي لفهم تفاعل الجمهور مع منشوراتنا.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] يحتاج فريق التسويق لدينا إلى واجهة برمجة تطبيقات تتيح لهم إجراء تحليل لغوي لعناوين الإعلانات.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] في مشروعنا الحالي، نود تحويل سجلات الأصوات إلى نص لتحسين عملية توثيق المحادثات.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] عندنا مجموعة كبيرة من النصوص الطبية ونرغب في تصنيفها تلقائياً حسب الموضوعات الرئيسية.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] تحتاج مجموعتنا لتوليد نصوص توجيهية ذات تنسيق محدد لتعليم نماذج الذكاء الاصطناعي.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] نحتاج إلى واجهة أمامية للتفاعل مع نموذج تعلم آلي يتنبأ بالأعطال المحتملة في أجهزة الكمبيوتر.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] تريد شركتنا تطوير نظام يستخدم النصوص التقنية لتوليد أسئلة امتحانات للتدريب على الموضوعات الهندسية.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] نعمل على تطوير نظام يحسن التكامل بين القواميس اللغوية لتحسين جودة الترجمة التلقائية.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] تحتاج شركتنا إلى تحليل النصوص القانونية لاستخراج المعلومات الرئيسية منها لتسهيل البحث والتصفح.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] تهدف شركتنا إلى تصميم نظام يستخدم النماذج اللغوية لتحسين توجيه المحادثات الصوتية التفاعلية.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-base', 'api_call': \"BartModel.from_pretrained('facebook/bart-base')\", 'api_arguments': ['inputs'], 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST]تفضل بإنشاء تطبيق يقوم بتحويل النصوص العربية إلى نصوص انجليزية باستخدام تقنيات توليد النصوص.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] يرغب العميل في تطوير تطبيق لتوليد نصوص دقيقة وواضحة من خلال تحويل اللغة العربية إلى لغة انجليزية. هل يمكنك استخدام الأدوات المتاحة في api للمساعدة في هذا الأمر؟? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] لدينا مشروع لتحويل الصور إلى نصوص. هل يمكنك استخدام الأدوات المعرفية المتوفرة لتحويل بيانات الصور إلى نصوص واضحة؟? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] يرغب العميل في إنشاء تطبيق لتحويل النصوص الدينية من العربية إلى اللغات الأخرى. هل يمكنك استخدام التكنولوجيا المعرفية المتاحة لدعم المشروع؟? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] يطلب العميل تصميم تطبيق يعتمد على التعلم الآلي لتشخيص الأمراض الجلدية من الصور. يرجى استخدام تقنيات توليد النصوص لتقديم وصف دقيق لنتائج التشخيص.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] أنت مطلوب لتصميم خدمة تقوم بتحليل اللغة النصية لردود فعل العملاء على المنتجات. استخدم التكنولوجيا المعرفية لتبسيط هذه المهمة وتوليد تقارير مفصلة.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] ترغب مؤسسة خيرية في تطوير نظام لتوليد تقارير توضح القضايا الاجتماعية بشكل دوري. اسخدم الأدوات الذكية لإنشاء نصوص إحصائية توضح المعلومات بشكل دقيق وفعال.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] يحتاج المشروع العلمي لتوليد تقارير توضح النتائج البحثية بشكل ملائم ودقيق. استفد من التكنولوجيا المعرفية لتحسين جودة النصوص وإتقان التقارير البحثية.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] نظام خدمة العملاء بحاجة إلى تحسين في توليد الردود التلقائية عن استفسارات العملاء. يرجى استخدام التكنولوجيا المعرفية لتوجيهنا في كيفية تنفيذ هذا التطوير بكفاءة.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST] يرغب عميلنا في دمج خدمة الترجمة التلقائية في تطبيقه، يرجى اعتماد الحلول المعرفية لتطوير نظام يسهل ترجمة النصوص بفاعلية.? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/bart-large', 'api_call': \"BartModel.from_pretrained('facebook/bart-large')\", 'api_arguments': {'pretrained_model_name': 'facebook/bart-large'}, 'python_environment_requirements': {'library': 'transformers', 'version': 'latest'}, 'example_code': \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'arxiv', 'accuracy': 'Not provided'}, 'description': 'BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).' </s>"
"<s>[INST]نود تطبيقًا يتمكن من تحليل نصوص التويتر لاستخراج المشاعر والمواضيع الرئيسية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"
"<s>[INST] لدى مشروع يتطلب تحويل النصوص الطبية إلى تشخيصات طبية. يرجى تقديم API مناسب لتحقيق ذلك.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"
"<s>[INST] تحتاج منصتنا إلى إنشاء نظام يقوم بفهم الإشارات الصوتية وتحويلها إلى نص. هل يمكن توجيهنا إلى API مناسب لهذه المهمة؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"
"<s>[INST] نحن نبحث عن حلاً يمكنه تحليل سلوك مستخدمي تطبيقات الهاتف المحمول. هل لديكم API مناسب للقيام بذلك؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"
"<s>[INST] نريد بناء نظام للتعرف على أنماط الاحتيال في عمليات الدفع الإلكتروني. هل يمكنكم تقديم API للقيام بهذه المهمة؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"
"<s>[INST] نحن بحاجة إلى تطبيق لتوصيف الصور الطبية وتحديد الأمراض المعروضة فيها. يرجى تقديم API للقيام بهذه المهمة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"
"<s>[INST] نود تطبيقًا يقوم بتحديد مواقف السيارات الشاغرة في الطرق. هل يمكنكم توجيهنا إلى API مناسب لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"
"<s>[INST] عميلنا يطلب نظامًا يمكنه تتبع تطور الأسعار في السوق المالية. هل تستطيعون توفير API لهذه الوظيفة؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"
"<s>[INST] لدينا مشروع يتعلق بكشف الإساءة والتنمر عبر الإنترنت. هل لديكم API يمكن استخدامه لتحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"
"<s>[INST] نحن نبحث عن تكنولوجيا لتحليل الرأي العام حول منتجاتنا من خلال تقييمات المستخدمين. هل يمكنكم توجيهنا إلى API مناسب لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vits8', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vits8')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.' </s>"
"<s>[INST]يتعين علينا تحديد المواقع التي تظهر فيها الازدحام المروري وتوفير تقارير دورية حولها.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"
"<s>[INST] يجب علينا تحليل مشكلة التصنيف الآلي للنصوص لتحسين تجربة المستخدمين في تصفح موقعنا.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"
"<s>[INST] تطوير تطبيق يتيح للمستخدمين تعلم لغة جديدة من خلال تمارين تفاعلية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"
"<s>[INST] ضع خطة تنفيذية لتحسين تصنيف مقالات الأخبار بناءً على تفاعل القراء ومشاركتهم.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"
"<s>[INST] كتابة سيناريوات تفاعلية تساعد في تحسين تجربة المستخدم على منصة التجارة الإلكترونية الخاصة بك.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"
"<s>[INST] تطبيق نظام توصية لتحسين تجربة المستخدمين عن طريق توجيههم للمنتجات المناسبة بناءً على تفضيلاتهم الشخصية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"
"<s>[INST] تصميم واجهة مستخدم جذابة لتعزيز تفاعل المستخدمين مع تطبيق الألعاب الجديد الخاص بك.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"
"<s>[INST] تطوير نظام ذكاء اصطناعي يقدم توقعات دقيقة حول السوق المالية والاستثمارات.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"
"<s>[INST] تعزيز الأمان لتطبيق الويب الخاص بك من خلال تطبيق أحدث تقنيات التشفير والمراقبة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"
"<s>[INST] إنشاء نموذج تفاعلي لتصور أفكار جديدة لتطوير تكنولوجيا جديدة في شركتك.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dino-vitb16', 'api_call': \"ViTModel.from_pretrained('facebook/dino-vitb16')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/dino-vitb16'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.' </s>"
"<s>[INST]أنا أبحث عن طريقة لتحليل ردود فعل العملاء على منتجاتنا من خلال تقنيات معالجة لغوية طبيعية. ما هو أفضل شيء يمكنني استخدامه؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"
"<s>[INST] أحتاج إلى نموذج يستطيع تحديد موضوعات النصوص تلقائيًا. ما هي أفضل تقنية لهذا الغرض؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"
"<s>[INST] أريد بناء نموذج يستطيع توليد نصوص إبداعية مثل قصائد أو نصوص فنية. كيف يمكنني البدء في ذلك؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"
"<s>[INST] أود إضافة ترجمة تلقائية لموقعي عبر الإنترنت. ما هي أفضل تقنية لتحقيق ذلك؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"
"<s>[INST] أريد إنشاء نموذج لتحليل النصوص الطبية واستخراج المعلومات الحساسة. من فضلك، ما هو أداة تحليل النصوص المناسبة؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"
"<s>[INST] أبحث عن طريقة لتلخيص النصوص الطويلة تلقائيًا. هل يمكنني استخدام تحويلات بايتيد لهذا الغرض؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"
"<s>[INST] أنا أبحث عن حلاً لتصنيف النصوص القانونية بشكل دقيق. ما هي أفضل تقنية تحليل نصوص يمكنني استخدامها؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"
"<s>[INST] أريد إنشاء نموذج لتحليل شعور المستخدمين من تعليقاتهم على منصة التواصل الاجتماعي. ما هي أداة التحليل النصي الأنسب لذلك؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"
"<s>[INST] أريد تنفيذ تحليل نصوص سريع للتعرف على المواضيع الرئيسية. أي نموذج يمكنني استخدامه لهذا الغرض؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"
"<s>[INST] أنا بحاجة إلى نموذج يستطيع تعلم التفاصيل الدقيقة في النصوص الصحفية. ما هو أفضل نموذج لتحقيق ذلك؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'PyTorch Transformers', 'functionality': 'Feature Extraction', 'api_name': 'kobart-base-v2', 'api_call': \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'api_arguments': {'tokenizer': \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, 'python_environment_requirements': {'transformers': 'latest', 'tokenizers': 'latest'}, 'example_code': \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", 'performance': {'dataset': 'NSMC', 'accuracy': 0.901}, 'description': 'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.' </s>"
"<s>[INST]يرغب مدرب كرة السلة في تحسين أداء فريقه من خلال تحليل أداء لاعبيه في المباريات السابقة. هل بإمكانك مساعدته باستخدام التعلم الآلي؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"
"<s>[INST] تريد شركة النقل العام توفير تطبيق لتحسين تجربة المستخدمين من خلال تقديم تنبؤات دقيقة لأوقات الوصول. كيف يمكن تطبيق التعلم الآلي لهذا الغرض؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"
"<s>[INST] تحتاج شركة السيارات لتطوير نظام تنبؤ بأعطال المركبات للحد من التوقفات غير المتوقعة. كيف يمكن تطبيق تقنيات التعلم الآلي لهذا الغرض؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"
"<s>[INST] تريد شركة الصحة الرقمية تطوير نظام توصيف آلي لتحسين التشخيص الطبي. كيف يمكن استخدام تعلم الآلي لتحقيق هذا الهدف؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"
"<s>[INST] تريد شركة البحث العلمي تحسين عملية فهم النصوص الطبية الكبيرة. هل يمكنك توجيههم إلى كيفية تعتمد التعلم الآلي في هذا المجال؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"
"<s>[INST] يرغب المتحدثون العامون في تطوير تطبيق لتحليل التغريدات وتصنيفها إلى مجموعات مختلفة. كيف يمكن هؤلاء الأفراد استخدام تقنيات التعلم الآلي؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"
"<s>[INST] تريد شركة الأمن السيبراني تحسين أداء نظام الكشف عن التهديدات عبر تحليل سلوك المستخدمين. كيف يمكن استخدام تعلم الآلي في هذا السياق؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"
"<s>[INST] ترغب شركة التجارة الإلكترونية في توفير تجربة تسوق أكثر شخصنة للعملاء من خلال توصيات المنتجات الأكثر توافقًا مع ذوق كل فرد. كيف يمكن تحسين هذه العملية باستخدام التعلم الآلي؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"
"<s>[INST] ترغب شركة الترجمة في تعزيز دقة الترجمة التلقائية باستخدام تقنيات التعلم الآلي. كيف يمكن استخدام هذه التقنيات لتحسين جودة الترجمة؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"
"<s>[INST] تريد مؤسسة إعلامية تطوير نموذج للتحليل التلقائي للصور لفهم محتواها بشكل أفضل. كيف يمكن أن يساعد التعلم الآلي في تحقيق هذا الهدف؟? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Contextual Representation', 'api_name': 'indobenchmark/indobert-base-p1', 'api_call': \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", 'api_arguments': ['BertTokenizer', 'AutoModel', 'tokenizer.encode', 'torch.LongTensor', 'model(x)[0].sum()'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", 'performance': {'dataset': 'Indo4B', 'accuracy': '23.43 GB of text'}, 'description': 'IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.' </s>"
"<s>[INST]\"تعمل شركتنا على تطوير روبوت محادثة جديد لخدمة العملاء، ونحن بحاجة إلى وظيفة لتحليل المشاعر في رسائل العملاء.\" \"يتضمن مشروعنا تصنيف صور سلالات الكلاب المختلفة.\" نحن نبحث عن واجهة برمجة التطبيقات (API) التي يمكنها تحديد سلالة الكلاب بدقة في الصور.، أنا أعمل على مشروع يتضمن إنشاء ملخصات نصية للمقالات الطويلة. هل هناك واجهة برمجة تطبيقات يمكن أن تساعد في هذه المهمة؟، \"يعمل فريقنا على تطوير نظام التعرف على الكلام لتطبيق الرعاية الصحية. نحتاج إلى واجهة برمجة التطبيقات (API) لتدوين المصطلحات الطبية المنطوقة بدقة.، أنا أعمل على مشروع يتضمن اكتشاف النشاط الاحتيالي في المعاملات المالية. هل لديك واجهة برمجة التطبيقات (API) التي يمكنها المساعدة في هذه المهمة؟، يتضمن مشروعنا معالجة كميات كبيرة من البيانات النصية بلغات متعددة. نحن بحاجة إلى واجهة برمجة تطبيقات (API) يمكنها إجراء اكتشاف اللغة بدقة.، نحن نعمل على تطوير نظام توصية للأفلام بناءً على تفضيلات المستخدم. هل هناك واجهة برمجة تطبيقات (API) يمكنها تقديم توصيات بشأن الأفلام؟ \"، \"يعمل فريقنا على مشروع يتضمن استخراج المعلومات الأساسية من المستندات القانونية. هل هناك واجهة برمجة تطبيقات يمكنها المساعدة في استخراج الكيانات؟\"، \"أنا أبحث عن واجهة برمجة تطبيقات يمكنها إنشاء وجوه بشرية واقعية بناءً على أوصاف نصية لأداة إنشاء الشخصية في اللعبة.\"، \"يتضمن مشروعنا تحليل المشاعر على وسائل التواصل الاجتماعي المشاركات بلغات مختلفة. نحن نبحث عن واجهة برمجة التطبيقات (API) التي يمكنها التعامل مع تحليل المشاعر متعدد اللغات.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/codebert-base', 'api_call': \"AutoModel.from_pretrained('microsoft/codebert-base')\", 'api_arguments': 'n/a', 'python_environment_requirements': ['transformers'], 'example_code': 'n/a', 'performance': {'dataset': 'CodeSearchNet', 'accuracy': 'n/a'}, 'description': 'Pretrained weights for CodeBERT: A Pre-Trained Model for Programming and Natural Languages. The model is trained on bi-modal data (documents & code) of CodeSearchNet. This model is initialized with Roberta-base and trained with MLM+RTD objective.' </s>"
"<s>[INST]لدينا مشروع لتصميم تطبيق يسهل على المستخدمين البحث عن وصفات طعام مبتكرة. نحتاج إلى استخدام نموذج استخراج الميزات لتصنيف الوصفات.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"
"<s>[INST] نود تحسين خوارزمية البحث الخاصة بموقعنا لزيادة دقة النتائج. يرجى استخدام نموذج استخراج الميزات لتحليل تسلسلات البحث.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"
"<s>[INST] تهدف شركتنا إلى تطوير نظام تسويقي يعتمد على تحليل المشاعر. نحتاج إلى استخدام نموذج لاستخراج ملامح النصوص لتحديد المشاعر المتضمنة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"
"<s>[INST] نحن نخطط لإطلاق تطبيق لتعليم اللغات الأجنبية ونريد استخدام نموذج استخراج الميزات لتحليل التراكيب اللغوية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"
"<s>[INST] تواجه شركتنا تحديات في تصنيف الصور بدقة عالية. نريد تطبيق نموذج استخراج الميزات لتحديد سمات الصور بشكل دقيق.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"
"<s>[INST] نريد تطوير نظام لتحليل النصوص الطبية ونحتاج إلى نموذج يستخرج الميزات لتصنيف الكلمات المرتبطة بالطب.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"
"<s>[INST] نود تصميم نظام لتحديد الأخبار الزائفة ونريد استخدام نموذج استخراج الميزات لتحليل هياكل الجمل.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"
"<s>[INST] لدينا مشروع يتضمن تقنيات التعلم الآلي للتنبؤ بالأسهم. نحتاج إلى استخدام نموذج استخراج الميزات لتحليل أنماط البيانات المالية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"
"<s>[INST] شركتنا تسعى لتحسين تجربة المستخدم في تطبيقات الويب. نحتاج إلى استخدام نموذج استخراج الميزات لتحليل تفاعلات المستخدمين.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"
"<s>[INST] نحن نعمل على تطوير نظام لفحص الوثائق القانونية وتصنيفها. نريد استخدام نموذج استخراج الميزات لتحليل هياكل النصوص القانونية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'GanjinZero/UMLSBert_ENG', 'api_call': \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER' </s>"
"<s>[INST]نريد تطوير نظام يمكنه تحليل مشاكل الصوت في البيئات الصاخبة. هل يمكنك اقتراح الخطوات اللازمة لتحقيق ذلك؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"
"<s>[INST] نحن نبحث عن طريقة لتحسين تعرف الكلام لروبوتنا الذي يستخدمه الأشخاص ذوو الاحتياجات الخاصة. هل يتوفر API يمكن استخدامه لهذا الغرض؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"
"<s>[INST] نرغب في تحسين نظام الاستشعار لسياراتنا الذاتية القيادة. هل يمكن استخدام API معين لتحقيق ذلك؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"
"<s>[INST] هل يمكن توظيف تكنولوجيا الذكاء الاصطناعي لتعزيز تصميماتنا المعمارية؟ نريد اقتراح API يمكن استخدامه لتطوير هذا المجال.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"
"<s>[INST] أرغب في إنشاء نظام لتشخيص الأمراض بالصور الطبية. هل يوجد API مناسب يمكن استخدامه لهذا الغرض؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"
"<s>[INST] نحتاج إلى تحسين تعرف الكلام في تطبيقنا الصوتي. هل يمكن استخدام API معين لتحقيق هذا الهدف؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"
"<s>[INST] هل يمكن إنشاء تطبيق لتحليل الشخصيات في الرسوم المتحركة باستخدام تقنيات الذكاء الاصطناعي؟ أريد اقتراح API يمكن الاعتماد عليه لهذا الغرض.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"
"<s>[INST] أحتاج إلى استخدام تقنيات التعلم الآلي لتعزيز نظام التشخيص الطبي الخاص بشركتنا. هل يتوفر API يمكن استخدامه لذلك؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"
"<s>[INST] كيف يمكننا استخدام التحليل الصوتي باستخدام API لتحسين استجابة تطبيقنا الصوتي؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"
"<s>[INST] نرغب في توفير تجربة مستخدم متعددة اللغات في تطبيقنا الجديد. هل توجد واجهة برمجية يمكننا استخدامها لتحقيق ذلك؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'hubert-large-ll60k', 'api_call': \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'api_arguments': 'pretrained model name', 'python_environment_requirements': 'transformers', 'example_code': \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", 'performance': {'dataset': 'Libri-Light', 'accuracy': 'matches or improves upon the state-of-the-art wav2vec 2.0 performance'}, 'description': 'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.' </s>"
"<s>[INST]لدينا مشروع لتطوير تطبيق يقوم بتحليل محتوى النصوص الطبية واستخراج المعلومات الرئيسية منها، نحتاج إلى استخدام خدمة استخراج الميزات المدمجة في Transformers.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"
"<s>[INST] مطورو التطبيقات يرغبون في استخدام تقنيات تعلم الآلة لتعريب النصوص التي يتعاملون معها من مختلف اللغات، يمكنهم استخدام خدمة استخراج الميزات المدربة مسبقًا في Transformers.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"
"<s>[INST] تخطط الشركة لبناء نظام توصيات يعتمد على التحليل النصي لتفهم اهتمامات العملاء، يمكنك استخدام API استخراج الميزات في Hugging Face Transformers لتحويل النص إلى تمثيل يمكن تحليله.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"
"<s>[INST] تحتاج شركتنا لوحة تحكم تعتمد على النصوص لتلخيص تقارير العمل، يمكنك استخدام خدمة استخراج الميزات في Transformers لاستخراج المعلومات الرئيسية من النصوص بشكل دقيق.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"
"<s>[INST] نحتاج لنظام يقوم بتصنيف النصوص القانونية بناءً على محتواها، يمكنك استخدام API استخراج الميزات في Transformers لخدمة هذا الغرض بفعالية ودقة.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"
"<s>[INST] عميلنا يطلب تطبيقًا يقوم بتحليل تعليقات المستخدمين على منصات التواصل الاجتماعي لفهم ردود فعلهم، يمكنك استخدام API استخراج الميزات في Hugging Face Transformers لاستخلاص المعلومات الرئيسية من النصوص بكفاءة.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"
"<s>[INST] نود تطوير نظام يستخدم تقنيات معالجة اللغة الطبيعية لفهم مقاطع الفيديو الحركية، يمكنك استخدام API استخراج الميزات في Hugging Face Transformers لاستخلاص المعلومات الرئيسية من النصوص المرتبطة بالفيديو.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"
"<s>[INST] شركتنا تبحث عن حل يستخدم تقنيات تعلم الآلة لفهم النوبات الصرعية من النصوص الطبية، يمكنك استخدام API استخراج الميزات في Hugging Face Transformers لاستخراج البيانات الأساسية بدقة وسرعة.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"
"<s>[INST] يهتم عملاؤنا بفهم مدى تأثير حملاتهم التسويقية من خلال تحليل مواد التسويق، يمكنك استخدام API استخراج الميزات في Hugging Face Transformers لاستخدام تقنيات التعلم الآلي في تحليل النصوص الدعائية.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"
"<s>[INST] تطلب شركتنا تقديم خدمات تحليل النصوص لفهم تفاعل العملاء مع منتجاتها، يمكنك استخدام API استخراج الميزات في Transformers لاستخلاص المعلومات الرئيسية من التعليقات والمراجعات بكفاءة عالية.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sup-simcse-roberta-large', 'api_call': \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", 'api_arguments': ['AutoTokenizer', 'AutoModel'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)', 'performance': {'dataset': 'STS tasks', 'accuracy': \"Spearman's correlation (See associated paper Appendix B)\"}, 'description': 'A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.' </s>"
"<s>[INST]نريد تطوير تطبيق يقوم بتحويل الصور إلى نصوص باللغة العربية. استخدم الـ API التالي كمرجع:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"
"<s>[INST] عميلنا يرغب في بشراء برنامج لتحليل المشاعر من النصوص العربية في وسائل التواصل الاجتماعي. يمكنك استخدام الـ API التالي:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"
"<s>[INST] بناءً على طلب الشركة، نحتاج إلى تطوير برنامج يقوم بتحويل الكتب الإلكترونية من النصوص إلى ملفات صوتية. استخدم الـ API التالي:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"
"<s>[INST] نرغب في تطوير تطبيق يقوم بترجمة النصوص الطبية إلى العربية. يمكنك استخدام الـ API التالي كمرجع:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"
"<s>[INST] لدينا حاجة لتطوير برنامج يستخدم تقنية OCR لاستخراج النصوص من الصور باللغة العربية. قم باستخدام الـ API التالي:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"
"<s>[INST] عميلنا يرغب في تحليل الكلمات الرئيسية في نصوص عربية. يمكنك استخدام الـ API التالي لتحقيق ذلك:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"
"<s>[INST] نريد تطوير برنامج يقوم بتوليد خلاصات للمقالات الطبية باللغة العربية. يمكنك استخدام الـ API التالي للمساعدة:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"
"<s>[INST] نحن بحاجة إلى برنامج يستخدم تقنية الـ NER لاستخراج أسماء الشركات من النصوص باللغة العربية. يمكنك الاعتماد على الـ API التالي:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"
"<s>[INST] تهدف شركتنا لتطوير نظام يعتمد على تقنية الـ POS tagging للنصوص القانونية باللغة العربية. يمكنك استخدام الـ API التالي كمرجع:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"
"<s>[INST] عميلنا يتطلع إلى تطبيق يقوم بتحليل اللهجات في النصوص العربية. يمكنك استخدام الـ API التالي لهذا الغرض:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lewtun/tiny-random-mt5', 'api_call': \"AutoModel.from_pretrained('lewtun/tiny-random-mt5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': \"nlp('Once upon a time...')\", 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random mt5 model for text generation' </s>"
"<s>[INST]إنشاء نظام تحليل الصوت لتقدير مشاعر المتحدثين.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"
"<s>[INST] كيفية توليد نص قصير بناءً على صورة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"
"<s>[INST] بناء نظام تعرف على الكلام وتحويل الصوت إلى نص.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"
"<s>[INST] استخدام تقنية تحليل اللغة الطبيعية لتصنيف النصوص الطبية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"
"<s>[INST] بناء نظام ترجمة تلقائية بين اللغات المختلفة باستخدام نماذج تعلم آلي.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"
"<s>[INST] تطوير نظام للتعرف على الكائنات في الصور باستخدام تقنيات الرؤية الحاسوبية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"
"<s>[INST] تصميم نظام لتحليل المشاهد في مقاطع الفيديو وتحديد الأنماط الحركية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"
"<s>[INST] تنفيذ تحليل شبكات التواصل الاجتماعي لفهم سلوك المستخدمين واكتشاف الاتجاهات.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"
"<s>[INST] تطوير نظام لتعبئة النماذج الطبية تلقائياً باستخدام تقنيات تعلم الآلة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"
"<s>[INST] استخدام تقنيات التعلم النقلي لتحسين أداء نماذج التعلم الآلي في مجال جديد.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'DeepPavlov/rubert-base-cased', 'api_call': \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'Russian part of Wikipedia and news data', 'accuracy': ''}, 'description': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].' </s>"
"<s>[INST]نحن نبني تطبيقًا لتسجيل المعلومات الصوتية ونريد تحويل هذه المعلومات إلى نص. يمكنك استخدام واجهة برمجة التطبيقات التالية لتحقيق ذلك.? [/INST]'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/wavlm-large', 'api_call': \"Wav2Vec2Model.from_pretrained('microsoft/wavlm-large')\", 'api_arguments': 'speech input', 'python_environment_requirements': 'transformers', 'example_code': 'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.', 'performance': {'dataset': 'SUPERB benchmark', 'accuracy': 'state-of-the-art performance'}, 'description': 'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.' </s>"
"<s>[INST] ترغب شركتنا في تحليل المكالمات الهاتفية للعملاء وتحويلها إلى نص قابل للتحليل. يمكنك استخدام API التالي لتحقيق هذا الهدف.? [/INST]'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/wavlm-large', 'api_call': \"Wav2Vec2Model.from_pretrained('microsoft/wavlm-large')\", 'api_arguments': 'speech input', 'python_environment_requirements': 'transformers', 'example_code': 'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.', 'performance': {'dataset': 'SUPERB benchmark', 'accuracy': 'state-of-the-art performance'}, 'description': 'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.' </s>"
"<s>[INST] تطلب عمليات البحث والإنقاذ أداة لتحويل النص الصوتي من استغاثات الضحايا إلى نص مفهوم. يمكنك استخدام الAPI التالي لتلبية هذه الاحتياجات.? [/INST]'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/wavlm-large', 'api_call': \"Wav2Vec2Model.from_pretrained('microsoft/wavlm-large')\", 'api_arguments': 'speech input', 'python_environment_requirements': 'transformers', 'example_code': 'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.', 'performance': {'dataset': 'SUPERB benchmark', 'accuracy': 'state-of-the-art performance'}, 'description': 'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.' </s>"
"<s>[INST] نود تحسين تجربة تعلم اللغات لطلابنا من خلال توفير أداة لتحويل النص إلى كلام بأداء عالٍ. يمكنك استخدام الAPI التالي لتحقيق هذا الهدف.? [/INST]'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/wavlm-large', 'api_call': \"Wav2Vec2Model.from_pretrained('microsoft/wavlm-large')\", 'api_arguments': 'speech input', 'python_environment_requirements': 'transformers', 'example_code': 'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.', 'performance': {'dataset': 'SUPERB benchmark', 'accuracy': 'state-of-the-art performance'}, 'description': 'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.' </s>"
"<s>[INST] تريد منظمتنا تحسين عمليات الإثراء التعليمي من خلال تطوير أداة لتحويل المقالات التعليمية إلى مواد صوتية. يمكنك الوصول إلى ذلك باستخدام الAPI التالي.? [/INST]'domain': 'Audio Automatic Speech Recognition', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/wavlm-large', 'api_call': \"Wav2Vec2Model.from_pretrained('microsoft/wavlm-large')\", 'api_arguments': 'speech input', 'python_environment_requirements': 'transformers', 'example_code': 'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.', 'performance': {'dataset': 'SUPERB benchmark', 'accuracy': 'state-of-the-art performance'}, 'description': 'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.' </s>"
"<s>[INST]تريدين إضافة تقنية التعرف على الوجوه إلى تطبيقك. يمكنك استخدام خدمة Hugging Face Transformers للقيام بذلك.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"
"<s>[INST] قم بتطوير تطبيق لتحليل مشاعر الوجوه في الصور. يمكنك استخدام خدمة Hugging Face Transformers لتحقيق هذا الهدف.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"
"<s>[INST] تهدف إلى تحسين تقنية تمييز الألوان في الصور. يمكنك استخدام خدمة Hugging Face Transformers لاستخراج المزيد من المعلومات حول الألوان في الصور.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"
"<s>[INST] تحتاج إلى تصنيف الإشارات المرورية في الصور. استخدم Hugging Face Transformers لتنفيذ هذه المهمة بكفاءة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"
"<s>[INST] قم بتطبيق تقنية تعرف أوتوماتيكي للنصوص المكتوبة في الصور. يوفر Hugging Face Transformers حلاً فعالاً لهذه الغرض.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"
"<s>[INST] ترغب في تطوير تطبيق يمكنه تحليل تاريخ الصور المقدمة. يمكنك استخدام Hugging Face Transformers لتحديد التواريخ والأحداث الموجودة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"
"<s>[INST] تحتاج إلى تحسين دقة التعرف على المناظر الطبيعية في الصور. يتيح Hugging Face Transformers إمكانية تحسين وتدريب نماذج التعرف على الصور.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"
"<s>[INST] تخطط لتطوير تطبيق لتحديد وتصنيف الكائنات الفردية في الصور. تعتبر Hugging Face Transformers خيارًا ممتازًا لتحقيق هذا الهدف.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"
"<s>[INST] تريد تطبيقًا يعمل على تحليل وتصنيف الأطعمة في الصور. يمكنك استخدام خدمة Hugging Face Transformers لاستخراج معلومات حول أنواع الطعام المعروضة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"
"<s>[INST] تريد تحسين تصنيف المناظر العامة في الصور التي يلتقطها تطبيقك. يمكنك تحقيق ذلك عن طريق استخدام خدمة Hugging Face Transformers لتصنيف المناظر العامة بدقة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'google/vit-base-patch16-224-in21k', 'api_call': \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224-in21k'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", 'performance': {'dataset': 'ImageNet-21k', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.' </s>"
"<s>[INST]قم بتوليد خطوط توجيه لشبكة الطرق في مدينة معينة باستخدام البيانات الجغرافية. هل يمكنك مساعدتي في استخدام UniXcoder لهذا الغرض؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"
"<s>[INST] اكتب مقطعاً شعرياً قصيراً يحتوي على كلمة \"حرية\" باستخدام خوارزمية UniXcoder لفهم اللغة العربية بشكل أفضل.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"
"<s>[INST] استخدم UniXcoder لتوليد نموذج تنبؤي لدرجة الحرارة في مدينة معينة استناداً إلى بيانات الطقس السابقة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"
"<s>[INST] أنشىء واجهة مستخدم تفاعلية تسمح بتحويل الصور إلى رسومات فنية باستخدام تقنيات UniXcoder لتحليل المحتوى البصري.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"
"<s>[INST] توليد توصيف لأغنية موسيقية معينة باستخدام UniXcoder لفهم الشعور والمضمون الناتج.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"
"<s>[INST] قم بتوليد تعليمات تشغيل مختصرة لجهاز إلكتروني معين باستخدام UniXcoder لتوليد النصوص التقنية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"
"<s>[INST] أنشىء نص تعليمي متميز حول موضوع معين باستخدام UniXcoder لضمان جودة التحليل اللغوي والتوجيه.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"
"<s>[INST] قم بتوليد تحليل شخصية لشخصية أدبية معينة باستخدام UniXcoder لفهم السلوك والعوامل النفسية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"
"<s>[INST] أنشىء ملخصاً تنفيذياً لأحداث تاريخية هامة باستخدام أسلوب الكتابة الأكاديمي ودقة UniXcoder.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"
"<s>[INST] قم بتوليد توصيف مشهد طبيعي باستخدام UniXcoder لتعزيز التفاصيل والإحساس بالفضاء والطبيعة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Engineering', 'api_name': 'microsoft/unixcoder-base', 'api_call': \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, 'python_environment_requirements': {'transformers': 'from transformers import AutoTokenizer, AutoModel'}, 'example_code': \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.' </s>"
"<s>[INST]قم بإنشاء تطبيق يقوم بتطبيق تقنية التعلم الآلي لتحليل مشاعر التغريدات على وسائل التواصل الاجتماعي.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"
"<s>[INST] كتب برنامج يستخدم تقنية تعلم عميق لتوليد نصوص شعرية جديدة.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"
"<s>[INST] قم بإنشاء واجهة تطبيق تستخدم API للترجمة الآلية بين اللغات المختلفة.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"
"<s>[INST] انشئ تطبيق يستخدم API لتحليل الصور واستخراج الكائنات والمشاهد الموجودة في الصورة.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"
"<s>[INST] قم بتطوير تطبيق يستخدم تقنية التعلم العميق لتصنيف نصوص البريد الإلكتروني إلى فئات مختلفة.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"
"<s>[INST] أنشئ تطبيق يستخدم تقنية البصمة الصوتية لتحقق الهوية الصوتية لمستخدميه.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"
"<s>[INST] كتب برنامج يستخدم API لتحليل نغمة الصوت والتعرف على المقطع الموسيقي.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"
"<s>[INST] قم ببناء تطبيق يستخدم API لتحويل النص إلى كلام باللغة العربية.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"
"<s>[INST] أنشئ تطبيق يستخدم API لتحليل الشبكات الاجتماعية واستخراج التقارير والإحصائيات.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"
"<s>[INST] كتب برنامج يستخدم تقنية التعلم الآلي لتحسين أداء نظام توصية المنتجات في تطبيق التسوق الإلكتروني.? [/INST]'domain': 'Natural Language Processing Question Answering', 'framework': 'Transformers', 'functionality': 'Feature Extraction', 'api_name': 'facebook/dpr-question_encoder-single-nq-base', 'api_call': \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", 'api_arguments': ['input_ids'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output', 'performance': {'dataset': [{'name': 'NQ', 'accuracy': {'top_20': 78.4, 'top_100': 85.4}}, {'name': 'TriviaQA', 'accuracy': {'top_20': 79.4, 'top_100': 85.0}}, {'name': 'WQ', 'accuracy': {'top_20': 73.2, 'top_100': 81.4}}, {'name': 'TREC', 'accuracy': {'top_20': 79.8, 'top_100': 89.1}}, {'name': 'SQuAD', 'accuracy': {'top_20': 63.2, 'top_100': 77.2}}]}, 'description': 'Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).' </s>"
"<s>[INST]تحتاج شركتك إلى تحليل محتوى صور لاكتشاف الأماكن العامة في صور الشوارع.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"
"<s>[INST] ترغب شركتك في دمج خدمة معرفة الكلام الذكي في تطبيقك الحالي.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"
"<s>[INST] يجب على فريقك تطوير نموذج للتعرف على الأشخاص في أشرطة الفيديو.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"
"<s>[INST] عليك تصميم واجهة مستخدم جديدة لتطبيق الهاتف المحمول الخاص بشركتك.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"
"<s>[INST] نحن بحاجة إلى تطوير أداة لتحليل نغمات الموسيقى وتوصيف الأغاني بناءً على بيانات الصوت.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"
"<s>[INST] قم بتطوير نموذج للتنبؤ بأداء الأسهم في البورصة باستخدام بيانات السوق التاريخية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"
"<s>[INST] يجب على فريق التطوير تكامل واجهات برمجة التطبيقات (APIs) لعدة خدمات خارجية في تطبيقك الجديد.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"
"<s>[INST] تهدف شركتك إلى تحسين تصنيف نماذج التعلم الآلي لنشاطك التجاري.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"
"<s>[INST] ابتكر تقنية لتتبع مراحل الإنتاج في خط الإنتاج باستخدام تكنولوجيا الواقع المعزز.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"
"<s>[INST] يجب على فريق التطوير تنفيذ نظام سجلات مركزية لتتبع عمليات الوصول إلى البيانات داخل التطبيق.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Audio Spectrogram', 'api_name': 'audio-spectrogram-transformer', 'api_call': \"ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'One custom ast model for testing of HF repos' </s>"
"<s>[INST]قم بتوليف موسيقى مميزة تعبر عن موقفك في الحياة الحالي.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"
"<s>[INST] صمم شخصية كرتونية تعبر عن شعورك بالإبداع والتحدي.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"
"<s>[INST] قم بإنشاء تحليل احصائي لنتائج استبيانك الأخير لتحديد الاتجاهات والتوجهات لديك.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"
"<s>[INST] ضع خطة لتحسين أداء موقع الويب الخاص بشركتك بالنسبة لمستخدمي الهواتف المحمولة.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"
"<s>[INST] أنشىء روبوت يقوم بتحديد مكان الأشياء في الغرفة وتصنيفها.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"
"<s>[INST] قم بتصميم نموذج تعلم آلي لتنبؤ بأداء الأسهم في السوق المالية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"
"<s>[INST] أنشىء تطبيقًا يقدم نصائح يومية لتعزيز الصحة النفسية والرفاهية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"
"<s>[INST] قم بتطوير برنامج سطح المكتب الذي يتيح للمستخدمين تتبع أوقات العمل وإدارة المهام.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"
"<s>[INST] أنشىء موقع ويب يوفر دليلًا للمسافرين لاستكشاف أفضل الوجهات السياحية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"
"<s>[INST] قم بإعداد برنامج تدريبي لتعليم البرمجة للأطفال باستخدام أساليب تفاعلية وألعاب تعليمية.? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'rasa/LaBSE', 'api_call': \"AutoModel.from_pretrained('rasa/LaBSE')\", 'api_arguments': 'input_text', 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.' </s>"
"<s>[INST]ترغب في تقديم توصية لمقهى تستخدم خدمة تجميع المعلومات لفهم تفضيلات العملاء. هل يمكن لواجهة برمجة التطبيقات (API) أن تقدم هذه الخدمة؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"
"<s>[INST] تريد بناء تطبيق لمساعدة المستخدمين في تعلم اللغة الإنجليزية من خلال توليد نصوص قصيرة مع شرح توضيحي. هل تتوفر واجهة برمجة التطبيقات (API) لمثل هذه الغرض؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"
"<s>[INST] تريد إنشاء نظام لتحديد تصنيف المفاهيم في الوقت الحقيقي. هل توفر واجهة برمجة التطبيقات (API) تلك الوظيفة؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"
"<s>[INST] تريد تحليل المحتوى النصي لرسائل البريد الإلكتروني لفهم الانطباعات والمواضيع الرئيسية. هل يمكن لواجهة برمجة التطبيقات (API) تلبية هذه الاحتياجات؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"
"<s>[INST] تود استخدام واجهة برمجة التطبيقات (API) لتوليد أسماء الشركات بناءً على النصوص الخاصة بالمجال الذي تعمل به. هل توجد واجهة برمجة التطبيقات (API) تقدم تلك الخدمة؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"
"<s>[INST] تسعى لتطوير نظام المحادثات الذكية وترغب في استخدام واجهة برمجة التطبيقات (API) لتحليل نغمة الصوت للتعرف على المشاعر. هل تتوفر واجهة برمجة التطبيقات (API) تقدم هذه الخدمة؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"
"<s>[INST] تخطط لإنشاء منصة لتحليل مقاطع نصية للأفلام والتوصل إلى استنتاجات حول مشاعر الشخصيات ومراحل القصة. هل يمكن تحقيق ذلك باستخدام واجهة برمجة التطبيقات (API)؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"
"<s>[INST] تبحث عن طريقة لتحديد المزيد من التفاصيل حول كتب معينة في التخصصات الأكاديمية من خلال استخدام واجهة برمجة التطبيقات (API). هل تتوفر واجهة برمجة التطبيقات (API) لدعم هذا النوع من العمل؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"
"<s>[INST] تحتاج إلى استخدام واجهة برمجة التطبيقات (API) لتحديد أكثر الكلمات شيوعاً في نصوص أدبية. هل يمكن أن توفر واجهة برمجة التطبيقات (API) هذه الخدمة؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"
"<s>[INST] تبحث عن واجهة برمجة التطبيقات (API) تعمل على تحليل محتوى النصوص الصوتية لفهم الأفكار والمضامين بشكل شامل. هل تقدم أي من الواجهات البرمجية هذه الخدمة؟? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sentence-transformers/distilbert-base-nli-mean-tokens', 'api_call': \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", 'api_arguments': ['sentences'], 'python_environment_requirements': 'pip install -U sentence-transformers', 'example_code': \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", 'performance': {'dataset': 'https://seb.sbert.net', 'accuracy': 'Not provided'}, 'description': 'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.' </s>"
"<s>[INST]استخدم الـ API لتحويل نص قصير إلى كلام مسموع لاستخدامه في تطبيقك الذي يساعد ذوي الإعاقة البصرية.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"
"<s>[INST] قم بتوليد قصة عن الطبيعة باستخدام الـ API المقدم، ثم قم بتطبيق عملية تحويل النص إلى كلام.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"
"<s>[INST] قم بتحليل عنصر معين في المشهد الذي تلتقطه كاميرات مركبات القيادة الذاتية باستخدام API المقدم.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"
"<s>[INST] استخدم الـ API الخاص بتوليد التضاريس لإنشاء مشهد طبيعي مفصل.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"
"<s>[INST] أنشئ تطبيقًا يقوم بتصنيف الصور الطبيعية باستخدام الـ API المقدم.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"
"<s>[INST] قم بتطبيق تقنية التعرف على الكلام لتحويل الأوامر الصوتية إلى نص مفهوم.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"
"<s>[INST] استخدم الـ API لتحويل مقالات علمية إلى نصوص محكمة تخصصية.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"
"<s>[INST] قم بتصميم تطبيق لتوليد رسوم بيانية تفاعلية باستخدام الـ API.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"
"<s>[INST] أنشئ تطبيقًا يستخدم الـ API للكشف عن اللهجات الصوتية في النصوص الصوتية.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"
"<s>[INST] استخدم الـ API لتحويل الصور الثنائية الملونة إلى أبيض وأسود.? [/INST]'domain': 'Natural Language Processing Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Document-level embeddings of research papers', 'api_name': 'malteos/scincl', 'api_call': \"AutoModel.from_pretrained('malteos/scincl')\", 'api_arguments': {'tokenizer': \"AutoTokenizer.from_pretrained('malteos/scincl')\", 'model': \"AutoModel.from_pretrained('malteos/scincl')\"}, 'python_environment_requirements': {'transformers': '4.13.0'}, 'example_code': \"from transformers import AutoTokenizer, AutoModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\n\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\n\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\n\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\n\\nresult = model(**inputs)\\n\\nembeddings = result.last_hidden_state[:, 0, :]\", 'performance': {'dataset': 'SciDocs', 'accuracy': {'mag-f1': 81.2, 'mesh-f1': 89.0, 'co-view-map': 85.3, 'co-view-ndcg': 92.2, 'co-read-map': 87.7, 'co-read-ndcg': 94.0, 'cite-map': 93.6, 'cite-ndcg': 97.4, 'cocite-map': 91.7, 'cocite-ndcg': 96.5, 'recomm-ndcg': 54.3, 'recomm-P@1': 19.6}}, 'description': 'SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.' </s>"
"<s>[INST]ترغب شركتنا في تطوير تطبيق لتحليل مشاهد الفيديو لرصد الحيوانات في البرية. يمكنك استخدام واجهة برمجة التطبيقات (API) التالية لاستخراج الميزات اللازمة:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"
"<s>[INST] لدينا تطبيق لتعلم اللغات ونحتاج إلى خاصية توليد النصوص التوضيحية للكلمات. يمكنك استخدام الAPI التالي لتوليد النصوص بناءً على الكلمات:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"
"<s>[INST] تخطط شركتنا لتطوير تطبيق للتعرف على الأشياء في الصور، ونريد استخدام مكتبة تعلم الآلة متعددة الاستخدامات لتحقيق هذه الخاصية. يمكنك استخدام الواجهة البرمجية التالية لتحقيق ذلك:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"
"<s>[INST] لدينا تطبيق للذكاء الاصطناعي ونحتاج لنموذج يمكنه توقع سلوك المستخدمين استنادًا إلى سجلات النشاط. يمكنك استخدام الواجهة البرمجية التالية لإجراء هذا التوقع:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"
"<s>[INST] تقوم شركتنا بتطوير تطبيق للترجمة التلقائية وترغب في استخدام نموذج قادر على فهم النصوص بلغة معينة لتحقيق أفضلية داخل المجال. يمكنك استخدام الواجهة البرمجية التالية للترجمة:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"
"<s>[INST] نحن في حاجة إلى تطوير نموذج لتصنيف الموسيقى في تطبيقنا ونفضل استخدام نموذج مسبق التدريب لذلك. يمكنك استخدام الواجهة البرمجية التالية لإنشاء نموذج تصنيف الموسيقى:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"
"<s>[INST] تطويرنا تطبيقًا لتعلم الآلة ونريد تنفيذ خوارزمية التصنيف الرؤوسي باستخدام إطار عمل مناسب. يمكنك استخدام الواجهة البرمجية التالية لتنفيذ الخوارزمية:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"
"<s>[INST] لدينا تطبيق لتصنيف الأفلام ونريد استخدام تقنيات تحليل النصوص لتقديم تصنيفات دقيقة للمحتوى النصي. يمكنك استخدام الواجهة البرمجية التالية لتنفيذ تحليل النصوص:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"
"<s>[INST] شركتنا تعمل على تطبيق لتحليل البيانات ونحب أن يتضمن هذا التطبيق تقنية استكشاف البيانات المكثفة. يمكنك استخدام الواجهة البرمجية التالية لاستخراج الميزات بشكل مكثف:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"
"<s>[INST] ترغب شركتنا في تطوير نظام لتصنيف النصوص الطبية ونحب استخدام نموذج متعدد المهام لذلك. يمكنك استخدام الواجهة البرمجية التالية لاستخدام نموذج التصنيف:? [/INST]'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'sberbank-ai/sbert_large_mt_nlu_ru', 'api_call': \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", 'api_arguments': ['sentences', 'padding', 'truncation', 'max_length', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n\\n# Sentences we want sentence embeddings for sentences = ['?']\\n\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", 'performance': {'dataset': 'Russian SuperGLUE', 'accuracy': 'Not provided'}, 'description': 'BERT large model multitask (cased) for Sentence Embeddings in Russian language.' </s>"
"<s>[INST]نبحث عن طريقة لتحسين توقعاتنا للمخاطر المالية المحتملة في قطاع البنوك.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"
"<s>[INST] تخطط شركتنا لإنشاء نظام لتحديد مكان الجوال لموظفينا بدقة عالية.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"
"<s>[INST] تحتاج شركتنا لواجهة برمجة استخراج بيانات لتقديم تقارير دورية عن أداء الموظفين.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"
"<s>[INST] ترغب شركتنا في تطوير نظام لتحليل معاني النصوص واكتشاف المواضيع الرئيسية في المقالات الصحفية.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"
"<s>[INST] تهدف شركتنا لتطوير نظام لتنقيب البيانات واستخراج الأنماط الاستهلاكية للمستهلكين.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"
"<s>[INST] ترغب شركتنا في تصميم نظام لاستخراج الموسيقى من مقاطع الفيديو الخاصة بالأفلام.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"
"<s>[INST] تحتاج شركتنا إلى تطوير أداة لتصنيف الصور الطبية بناءً على أنواع الأمراض.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"
"<s>[INST] نحن بصدد تطوير تطبيق لتحليل السوق ومتابعة أداء الأسهم لمستثمرينا.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"
"<s>[INST] نحتاج لإنشاء أداة لتحليل ردود الفعل على المنتجات وتصنيفها بمستويات الرضا.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"
"<s>[INST] ترغب شركتنا في تقديم أداة لقياس أثر التسويق الرقمي على زيادة المبيعات.? [/INST]'domain': 'Natural Language Processing Sentence Similarity', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'setu4993/LaBSE', 'api_call': \"BertModel.from_pretrained('setu4993/LaBSE')\", 'api_arguments': ['english_sentences', 'italian_sentences', 'japanese_sentences'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", 'performance': {'dataset': 'CommonCrawl and Wikipedia', 'accuracy': 'Not Specified'}, 'description': 'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.' </s>"
"<s>[INST]إنشاء نظام للتوصيات السياحية يستند إلى تفسيرات المستخدم.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"
"<s>[INST] كتابة برنامج يستخدم تقنية المعالجة اللغوية الطبيعية لفهم الانفعالات في النصوص.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"
"<s>[INST] بناء نظام استشاري يقدم نصائح قانونية عبر تحليل النصوص القانونية.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"
"<s>[INST] إنشاء نظام يقوم بتوليد أسماء تجارية جديدة بناءً على استخدام تحليل اللغة الطبيعية.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"
"<s>[INST] تنفيذ نظام يقوم بتحويل الخطوط اليدوية إلى نصوص مكتوبة بالحاسوب.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"
"<s>[INST] بناء نظام لتحليل الشخصيات في الروايات واستخلاص سماتها الرئيسية باستخدام تقنيات معالجة اللغة الطبيعية.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"
"<s>[INST] تطوير نظام لتوليد شعر جديد باستخدام تقنيات الذكاء الاصطناعي.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"
"<s>[INST] استخدام تحليل اللغة الطبيعية لتقديم تحليلات سريعة للنصوص الطبية المعقدة.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"
"<s>[INST] بناء نظام لتحليل المقالات الصحفية واستخراج أهم الموضوعات المتداولة.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"
"<s>[INST] تنفيذ نظام يقوم بتحويل الخطوط العربية اليدوية إلى نصوص مكتوبة بالحاسوب.? [/INST]'domain': 'Natural Language Processing Token Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'lanwuwei/BERTOverflow_stackoverflow_github', 'api_call': \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", 'api_arguments': {'pretrained_model_name_or_path': 'lanwuwei/BERTOverflow_stackoverflow_github'}, 'python_environment_requirements': {'transformers': '*', 'torch': '*'}, 'example_code': 'from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)', 'performance': {'dataset': \"StackOverflow's 10 year archive\", 'accuracy': 'Not provided'}, 'description': \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\" </s>"
"<s>[INST]تريد شركتنا تطوير تطبيق خاص بتحليل نغمات الصوت واستخراج معلومات دقيقة منها.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"
"<s>[INST] ترغب شركتنا في تطوير تطبيق للتعرف التلقائي على وجوه الأشخاص في الصور.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"
"<s>[INST] تخطط شركتنا لتطوير نظام لتحليل وتصنيف النصوص التلقائي.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"
"<s>[INST] تهتم شركتنا بتطوير نظام ذكاء اصطناعي لتحليل مشاهد الفيديو والتعرف على الأشياء والإجراءات المحددة في الفيديو.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"
"<s>[INST] ترغب شركتنا في تطوير نظام لتوليد نصوص تلقائياً بناءً على الصور.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"
"<s>[INST] تنوي شركتنا تطوير تطبيق للتعرف على النصوص في الصور واستخراج المعلومات منها.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"
"<s>[INST] تخطط شركتنا لتطوير نظام يمكنه تحويل النص إلى كلام بشكل تلقائي.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"
"<s>[INST] أنا بحاجة إلى تطوير تطبيق يمكنه تعريف شخصيات الأفلام والتسميات في الصور بشكل تلقائي.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"
"<s>[INST] تريد شركتنا تطوير تطبيق يمكنه تنبؤ بموازين الموسيقى استنادًا إلى النغمات الصوتية الحالية.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"
"<s>[INST] تحتاج شركتنا إلى تطوير نظام لتحليل المشاهد الفيديو وكشف الأنشطة الغير مرغوب فيها.? [/INST]'domain': 'Computer Vision Video Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Feature Extraction', 'api_name': 'microsoft/xclip-base-patch16-zero-shot', 'api_call': \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': [{'name': 'HMDB-51', 'accuracy': 44.6}, {'name': 'UCF-101', 'accuracy': 72.0}, {'name': 'Kinetics-600', 'accuracy': 65.2}]}, 'description': 'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.' </s>"
"<s>[INST]نود بناء نموذج تفاعلي يتمكن من تحليل مشاكل الرياضيات. هل يوجد API يمكن استخدامه لإنشاء ذلك؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"
"<s>[INST] نبحث عن API يمكن استخدامه لتحويل النصوص الطبية إلى صور توضيحية للأعضاء في الجسم. هل يمكنك توجيهنا إلى واحدة مثل هذه؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"
"<s>[INST] نريد إنشاء نظام يساعد المشترين عبر الإنترنت على تجربة المنتجات الرقمية قبل الشراء. هل هناك API يمكن استخدامه لهذا الغرض؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"
"<s>[INST] نحتاج إلى تطبيق يستطيع تحليل المشاعر في النصوص وتأكيد صحة المعلومات المنقولة. هل هناك API ملائم لهذه الوظيفة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"
"<s>[INST] نبحث عن وسيلة لضبط صور معتمة لتحسين جودتها وجعلها أكثر وضوحاً. هل يوجد API يتناسب مع ذلك؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"
"<s>[INST] ننوي تطوير تطبيق استكشافي يقوم بعرض المعالم السياحية الشهيرة على خريطة ثلاثية الأبعاد. هل هناك API يمكننا استخدامه لهذا الغرض؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"
"<s>[INST] نريد تصميم نظام لتحويل الصور الثنائية إلى الصور الألوان. هل يمكن أن تساعدنا في العثور على API مناسب؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"
"<s>[INST] نحتاج إلى API قادرة على تحويل النص إلى رموز باركود لتسهيل عمليات المتجر الإلكتروني. هل يمكنك توجيهنا إلى واحدة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"
"<s>[INST] نود تطوير تطبيق يتيح للمستخدمين تحويل النصوص إلى ملفات صوتية بصيغ مختلفة. هل يتوفر API يمكن استخدامه لهذا الغرض؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"
"<s>[INST] نريد إنشاء نظام يقدم توصيات مخصصة للمستخدمين استنادًا إلى تفاعلاتهم السابقة. هل هناك API يمكن أن نستخدمه لهذا الهدف؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'runwayml/stable-diffusion-v1-5', 'api_call': 'StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionPipeline', 'torch': 'import torch'}, 'example_code': {'model_id': 'model_id = runwayml/stable-diffusion-v1-5', 'pipe': 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'pipe_to_cuda': 'pipe = pipe.to(cuda)', 'prompt': 'prompt = a photo of an astronaut riding a horse on mars', 'image': 'image = pipe(prompt).images[0]', 'save_image': 'image.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.' </s>"
"<s>[INST]تحتاج منصتنا التعليمية إلى نظام يتيح تحويل النصوص إلى كلام بصوت طبيعي لطلابنا. هل يمكنك استخدام API من Hugging Face Transformers لتحقيق هذا؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"
"<s>[INST] نود تطوير نظام لتوليد وصف صوتي للصور المرفقة في تطبيقنا الجديد. هل يمكنك استعمال API من Hugging Face Transformers لتحقيق هذه المهمة الحساسة؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"
"<s>[INST] شركتنا تحتاج إلى نظام يحدد الأشياء في الصور ويترجمها إلى نصوص مفهومة. هل يمكنك استخدام API من Hugging Face Transformers لتحقيق هذا الهدف الهام؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"
"<s>[INST] نريد تطوير أداة تستخدم الذكاء الاصطناعي لتأليف نصوص أدبية فائقة الجودة. هل يمكنك استعمال API من Hugging Face Transformers لمساعدتنا في إنجاز هذه المهمة الإبداعية؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"
"<s>[INST] نحتاج إلى تطوير نظام يستخدم التعلم الآلي لتحليل الصور واقتراح وصف دقيق لها. هل يمكنك استخدام API من Hugging Face Transformers في هذا المجال التقني؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"
"<s>[INST] نريد تصميم تطبيق يستخدم الذكاء الاصطناعي لتحويل الخطوط اليدوية إلى نصوص طباعية قابلة للتحرير. هل يمكنك استعمال API من Hugging Face Transformers لتحقيق هذا الهدف المبتكر؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"
"<s>[INST] تطبيقنا يحتاج إلى نظام يمكنه ترجمة النصوص الإنجليزية إلى لغة الإشارة لفئة المعوقين سمعياً. هل يمكنك استخدام API من Hugging Face Transformers لتدريب هذا النظام الخاص؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"
"<s>[INST] نحن بحاجة إلى أداة تستخدم الذكاء الاصطناعي لفهم محتوى الصوتيات التي يقدمها موظفونا. هل يمكنك استعمال API من Hugging Face Transformers لبناء نظام كتابة نصوص اعتماداً على المحتوى الصوتي؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"
"<s>[INST] يتم استخدام تطبيقنا في مساعدة المحامين في البحث عن نقاط قانونية محددة. نريد تنفيذ نظام يحول النصوص القانونية إلى كلام يتم فهمه بسهولة. هل يمكنك استخدام API من Hugging Face Transformers لإنشاء هذا النظام؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"
"<s>[INST] نود تطوير تطبيق لتوليد وصف بصري دقيق للصور الطبية. هل يمكنك مساعدتنا في استخدام API من Hugging Face Transformers لتحقيق هذا الهدف الحيوي؟? [/INST]'domain': 'Multimodal Feature Extraction', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/dragon-plus-context-encoder', 'api_call': \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", 'api_arguments': ['pretrained'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", 'performance': {'dataset': 'MS MARCO', 'accuracy': 39.0}, 'description': 'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.' </s>"
"<s>[INST]نريد إنشاء تطبيق يستخدم تقنية التعلم الآلي لتحليل صور الأطعمة وتقديم معلومات تغذوية دقيقة حولها. هل يمكنك مساعدتنا في تنفيذ هذا؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"
"<s>[INST] يجب علينا تطوير تطبيق لتحديد مواقع الأمان العامة في مدينة معينة باستخدام تكنولوجيا خرائط الذكاء الاصطناعي. ما هي الخطوات اللازمة للبدء في هذا المشروع؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"
"<s>[INST] نرغب في إنشاء نظام لتحليل بيانات الصوت وتحديد نغمات موسيقية محددة. كيف يمكننا استخدام تكنولوجيا التعلم الآلي لتحقيق هذا الهدف؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"
"<s>[INST] لدينا مشروع لتطوير تطبيق يستخدم تقنيات التعلم الآلي للتعرف على وجوه الأشخاص في الفيديوهات. ما هي الأدوات والخطوات اللازمة لتحقيق ذلك؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"
"<s>[INST] نحن بحاجة إلى تطوير خدمة ويب تعتمد على تحليل اللغة الطبيعية لتحويل النص إلى لغة برمجة. كيف يمكن استخدام تقنيات التعلم الآلي لتنفيذ هذه الوظيفة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"
"<s>[INST] نود إنشاء تطبيق يستخدم تكنولوجيا التعلم الآلي لتوصيات الموسيقى. ما هي الخطوات الأساسية لبناء نموذج تنبؤي لتوصيات الموسيقى؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"
"<s>[INST] نريد تطوير تطبيق يستخدم تعلم الآلة لتوليد نصوص إبداعية وشعرية. كيف يمكن استخدام تكنولوجيا التوليف النصي لتحقيق هذا الهدف؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"
"<s>[INST] نحتاج إلى برنامج يستخدم تعلم الآلة لتصنيف أفلام السينما تلقائيا استنادا إلى محتواها. كيف يمكن تنفيذ هذا النوع من التحليل باستخدام تقنيات التعلم الآلي؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"
"<s>[INST] نريد تطوير تطبيق يمكنه تحليل الرسائل النصية وتحديد المشاعر المعبر عنها. ما هي الأدوات البرمجية الأساسية التي نحتاج إليها لبناء هذا النوع من التطبيقات؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"
"<s>[INST] يتطلب مشروعنا إنشاء نظام يستخدم تعلم الآلة لتحليل بيانات السلوك على الإنترنت والتنبؤ بالسلوك المستقبلي. كيف يمكن تطوير نموذج تنبؤي لهذه الغاية باستخدام تقنيات التعلم الآلي؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'CompVis/stable-diffusion-v1-4', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'transformers', 'scipy'], 'example_code': 'import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.' </s>"
"<s>[INST]تحتاج شركة تصميم أزياء إلى طريقة لتحويل رسومات قديمة إلى تصاميم حديثة بواسطة الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"
"<s>[INST] نريد تطوير نظام يتغلب على مشكلة التشويش في الصور الطبية ويزيل الضوضاء للحصول على صور واضحة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"
"<s>[INST] تطوير تطبيق لتحليل وتصنيف الفيديوهات الرياضية. ابحث عن API لاستخدام الذكاء الاصطناعي في تحديد نوع الرياضة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"
"<s>[INST] تحتاج شركة تأمين لبناء نظام للكشف التلقائي عن حوادث السيارات من الصور. ابحث عن API لتحليل الصور وتحديد الأضرار.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"
"<s>[INST] مطلوب تطوير نظام يقوم بتحويل النصوص العربية إلى كلام بصوت واضح وجودة عالية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"
"<s>[INST] نريد تطبيقًا يمكنه تحليل شهادات التقارير الطبية واستخراج المعلومات الرئيسية. ابحث عن API لـ OCR التخصصي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"
"<s>[INST] تحتاج وكالة العقارات إلى عرض ثلاثي الأبعاد للعقارات المتاحة للبيع. ابحث عن API يحول الصور إلى نماذج ثلاثية الأبعاد.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"
"<s>[INST] لدينا تطبيق يقوم بتحليل نغمات الموسيقى الكلاسيكية. نحتاج إلى API يساعدنا في تقدير عصر الموسيقى استنادًا إلى النغمات.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"
"<s>[INST] مطلوب تطوير نظام يقوم بفحص الأفلام وتحليل المشاهد المؤثرة. ابحث عن API تقوم بتحديد المشاهد الرئيسية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"
"<s>[INST] تطوير نظام توصية للمطاعم يستند إلى تحليل صور الطعام. ابحث عن API لتحليل الأطعمة في الصور.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': \"StableDiffusionPipeline.from_pretrained('prompthero/openjourney')\", 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.' </s>"
"<s>[INST]أضف تأثيرات بصرية مذهلة على فيديو لتحسين جودته وإضفاء رونق عليه.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"
"<s>[INST] تحتاج منصة التجارة الإلكترونية الخاصة بك إلى عرض صور مُحسنة تلقائيًا للمنتج عندما يتم تحميله. استخدم هذا الاستدعاء لتوليد صور ملائمة تلقائيًا.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"
"<s>[INST] أنشئ خوارزمية لتحويل الصور الرقمية إلى لوحات فنية تشبه أسلوب الرسامين الشهيرين باستخدام هذا الاستدعاء.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"
"<s>[INST] قم بتطوير نظام تعرف على الوجوه في الصور باستخدام التعلم العميق لمساعدة عميلك في مشروع الأمان والمراقبة الذي يعمل عليه. توثيق النظام الذي قمت بتطويره.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"
"<s>[INST] أنشئ نظامًا يقوم بتنبؤ درجة تحمل الإجهاد للمستخدمين باستخدام إشارات الوجوه في الصور. اعتمد على هذا الاستدعاء لتحقيق ذلك.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"
"<s>[INST] استخدم تقنية التعلم العميق لتوليد نص خلاق بالعربية استنادًا إلى مدخلات ثنائية، مثل التحليل اللغوي الضوئي. استنادًا إلى الاستدعاء المرفق، أنشئ نظام لإنتاج نصوص خلاقة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"
"<s>[INST] قم بتطوير نظام ترجمة آلية للنصوص بين العربية والإنجليزية يعتمد على شبكات الترجمة العصبية. استخدم هذا الاستدعاء لتحقيق ترجمة نصوص فعالة بين اللغتين.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"
"<s>[INST] أنشئ تطبيقًا يوفر تحليلًا شاملًا للنصوص باللغة العربية باستخدام تقنيات التعلم الآلي لفهم المحتوى واستخراج الأفكار الرئيسية. تعتمد على هذا الاستدعاء لبناء نظام تحليل نصوص.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"
"<s>[INST] تطوير نظام استدعاء لتوليد تلقائي لخط العربية يستند على التعلم العميق لمقدمي الخدمات اللغوية. استخدم هذا الاستدعاء لتنفيذ تحويل النص إلى خط كتابي بالعربية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"
"<s>[INST] أحتاج إلى تطوير نظام مدمج لترجمة النصوص إلى كلام بالعربية وتنفيذه. استخدم هذا الاستدعاء لتحقيق الهدف.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')\", 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.' </s>"
"<s>[INST]عُرضنا فكرة لبرنامج تدريبي للموظفين لتعلم كيفية التعامل مع العملاء الصعبين. يرجى استخدام الـ API التالي لتطوير محتوى تدريبي مصاحب بصور توضيحية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"
"<s>[INST] تطوير بوت للذكاء الاصطناعي لمساعدة الطلاب في فحص وتقديم التدريب لامتحانات اللغة الإنجليزية. استخدم الـ API أدناه لتوليد صور تعليمية تساعد الطلاب في فهم المفاهيم بشكل أفضل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"
"<s>[INST] نحن بصدد تطوير منصة تعليمية تفاعلية لتعليم الرياضيات للأطفال. يُطلب منك استخدام الـ API المُبين أدناه لإنشاء صور توضيحية تعليمية لمساعدة الأطفال في فهم المفاهيم الرياضية بشكل أفضل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"
"<s>[INST] لدينا مشروع لتصميم دليل سياحي رقمي لمدينة القاهرة. نرغب في إضافة صور تفاعلية توضح معالم الجذب السياحي في القاهرة. مستخدم المُعلومات المقدمة أدناه لتطوير الصور المناسبة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"
"<s>[INST] نقوم بتطوير تطبيق لمشاركة الوصفات الغذائية. يرجى استخدام الـ API التالي لتوليد صور توضيحية تكشف عن عملية تحضير وصفة معينة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"
"<s>[INST] عُزّز منصتك الرقمية للتعليم عن بُعد بصور تفاعلية تُظهر عملية تفكيك الجزيئات الكيميائية. يُطلب منك استخدام الـ API المُوجود أدناه لتصميم الصور المناسبة للمحتوى التعليمي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"
"<s>[INST] نريد توفير مرجع تدريبي رقمي لتعليم البرمجة للمبتدئين. يرجى استخدام الـ API المذكور أدناه لإنشاء صور توضيحية تساعد الطلاب في فهم مفاهيم البرمجة بشكل أفضل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"
"<s>[INST] نحن بصدد تطوير لعبة تعليمية للأطفال لتحسين مهارات القراءة والكتابة. يُرجى التعرف على الـ API المُوجود أدناه لتوليد صور توضيحية مميزة لمساعدة الأطفال في تحقيق أهداف التعلم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"
"<s>[INST] لدى شركتنا مشروع لتصميم برنامج تدريبي لتعليم اللغة الإسبانية. يُطلب منك استخدام الـ API المُعرف أدناه لإعداد صور تعليمية تُعزز فهم الطلاب للمفاهيم اللغوية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.' </s>"
"<s>[INST]كتابة قصيدة تعبر عن فصل الخريف وجمال الألوان في هذا الوقت من العام.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"
"<s>[INST] أكتب خطوات بسيطة لتحضير وجبة إفطار صحية وشهية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"
"<s>[INST] إنشاء صفحة ويب بسيطة تعرض معلومات عن فنان مشهور وأعماله الفنية البارزة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"
"<s>[INST] كتابة مقال تعليمي حول أساليب تحسين الذاكرة وزيادة التركيز.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"
"<s>[INST] إعداد جدول بيانات يحتوي على معلومات حول أنواع الزهور وألوانها المختلفة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"
"<s>[INST] إنشاء تطبيق يقدم نصائح يومية لتحسين الصحة العامة ونوعية الحياة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"
"<s>[INST] تصميم بوستر إعلاني لفعالية ثقافية تحتوي على الفن والموسيقى.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"
"<s>[INST] تحليل بيانات مبيعات شهرية وإنشاء تقرير يوضح الاتجاهات والإحصائيات الهامة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"
"<s>[INST] تطوير تطبيق لإدارة المهام الشخصية وإضافة مهام جديدة بالأولويات.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"
"<s>[INST] كتابة نص ترويجي جذاب لمنتج جديد يعتمد على التقنية الحديثة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'hakurei/waifu-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')\", 'api_arguments': {'prompt': 'text', 'guidance_scale': 'number'}, 'python_environment_requirements': {'torch': 'torch', 'autocast': 'from torch', 'StableDiffusionPipeline': 'from diffusers'}, 'example_code': \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", 'performance': {'dataset': 'high-quality anime images', 'accuracy': 'not available'}, 'description': 'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.' </s>"
"<s>[INST]نحن نقوم بتطوير منصة تعليمية للأطفال الذين يعانون من صعوبات في التعلم، نحتاج إلى واجهة برمجية تحويل النص إلى صوت لتعزيز تفاعل الأطفال مع المحتوى.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"
"<s>[INST] مطعمنا الجديد يقدم قائمة طعام متعددة اللغات، نحتاج إلى واجهة برمجية تقوم بترجمة النصوص إلى لغات متعددة لتلبية احتياجات زبائننا الدوليين.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"
"<s>[INST] نود تطوير تطبيق ترجمة فورية لمساعدة الأشخاص في التواصل عبر اللغات المختلفة، نأمل الحصول على واجهة برمجية تقوم بهذه المهمة بدقة وسرعة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"
"<s>[INST] شركتنا ترغب في تنفيذ نظام يقوم بتحليل محتوى وسائل التواصل الاجتماعي لفهم مشاعر العملاء بشكل أفضل، نرجو منكم تزويدنا بواجهة برمجية لتحليل النصوص واستخراج المشاعر.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"
"<s>[INST] لقد وفرنا تطبيقًا للسياح لاستكشاف معالم المدينة، نريد واجهة برمجية تحسّن جودة ودقّة الوصف المرئي لهذه المعالم لزيادة جاذبيتها.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"
"<s>[INST] نريد إضافة ميزة جديدة لتطبيق الروبوت الخاص بنا لتقديم نصائح تداول مالية، نحتاج إلى واجهة برمجية تحليل النصوص المالية وتقديم تقارير شاملة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"
"<s>[INST] تطبيقنا يُعنى بتعليم الطهي، نريد تكامل واجهة برمجية تتمكن من تحليل نصوص الوصف للوصفات واقتراح تعديلات وتحسينات لزيادة جاذبيتها للمستخدمين.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"
"<s>[INST] نحن نعمل على تطبيق لتعليم اللغات الأجنبية، نحتاج إلى واجهة برمجية تحليل النصوص وتقديم تمارين لتحسين مفردات المستخدمين بشكل فعّال.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"
"<s>[INST] تطبيقنا يُقدم خدمات القراءة الصوتية للكتب، نرغب في تنزيل واجهة برمجية تحليل النصوص لتقديم روايات بتأثيرات صوتية مثيرة لاكتساب اهتمام أكبر من القرّاء.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"
"<s>[INST] نحن نقدم منصة تعليم عبر الإنترنت، نود دمج واجهة برمجية ترجمة نصوص الدورات التعليمية لتوفير دورات متعددة اللغات للطلاب الدوليين.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-mse', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)'}, 'python_environment_requirements': ['diffusers'], 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': [{'name': 'COCO 2017 (256x256, val, 5000 images)', 'accuracy': {'rFID': '4.70', 'PSNR': '24.5 +/- 3.7', 'SSIM': '0.71 +/- 0.13', 'PSIM': '0.92 +/- 0.27'}}, {'name': 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)', 'accuracy': {'rFID': '1.88', 'PSNR': '27.3 +/- 4.7', 'SSIM': '0.83 +/- 0.11', 'PSIM': '0.65 +/- 0.34'}}]}, 'description': 'This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.' </s>"
"<s>[INST]يرغب مطورو التطبيقات في إنشاء صورة لعروس ترتدي فستانًا أبيض.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"
"<s>[INST] تحتاج شركة التسويق لخلق صورة تُظهر منظرًا لشلال في غابة استوائية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"
"<s>[INST] يطلب مهندس البرمجيات إنشاء صورة لشخص يقود دراجة نارية في الصحراء.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"
"<s>[INST] ترغب شر\\n\\n5. instruction: العملاء في إنشاء صورة توضح حفل زفاف في الهواء الطلق.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"
"<s>[INST] تحتاج شركة الأزياء إلى إنشاء صورة لشخصية مرتدية فستان أزرق على سجادة حمراء.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"
"<s>[INST] يريد مدير الإعلانات إنشاء صورة لعبوة منتج تتميز بالتصميم الجذاب.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"
"<s>[INST] تقوم شركة السياحة بالحاجة إلى إنشاء صورة لمشهد خلاب لغروب الشمس على الشاطئ.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"
"<s>[INST] تطلب شركة التصميم إنشاء صورة لمطبخ حديث بتصميم أنيق.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"
"<s>[INST] ترغب وكالة الفضاء في إنشاء صورة لمذنب يجتاح السماء الليلية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.' </s>"
"<s>[INST]تريد شركتنا إنشاء تطبيق يسمح لمستخدميه بتحويل النصوص إلى صور ملهمة وإبداعية. استخدم الخدمة التالية لتحقيق ذلك.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"
"<s>[INST] تريد شركتنا تطوير نظام لتوليد رسومات فنية احترافية استنادًا إلى النصوص المدخلة. استخدم الخدمة التالية لتحقيق ذلك.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"
"<s>[INST] نحتاج في شركتنا لتطبيق يمكّن المستخدمين من تحويل أفكارهم ومشاعرهم إلى صور فنية. ساعدنا في استخدام الخدمة التالية لتحقيق هذا.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"
"<s>[INST] تسعى شركتنا لتطوير منتج يمكّن المستخدمين من إنشاء صور فريدة بناءً على وصف نصي. استخدم الخدمة التالية لتحقيق هدفنا.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"
"<s>[INST] تحتاج شركتنا لحلول تقنية تساعد في تحويل النصوص إلى صور ملهمة وواقعية. تعاونا مع الخدمة التالية لتحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"
"<s>[INST] نود في شركتنا استخدام تقنية تحويل النصوص إلى صور لتطوير منتج فني مبتكر. حدد الخدمة التالية للمساعدة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"
"<s>[INST] تبحث شركتنا عن حلاً يُمكّن من توليد صور فنية عالية الجودة بناءً على وصف نصي. اعتمد على الخدمة التالية لتحقيق هدفنا.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"
"<s>[INST] تريد شركتنا تطوير تطبيق يدعم إنشاء صور فنية إبداعية بالاستناد إلى الوصف النصي. استخدم الخدمة التالية لتحقيق ذلك الهدف.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"
"<s>[INST] في إطار تطوير منتجات جديدة، تهدف شركتنا لإنشاء تطبيق يستخدم تقنية تحويل النصوص إلى صور فنية. استفد من الخدمة التالية لتحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"
"<s>[INST] تحتاج شركتنا لتكامل تقنية تحويل النصوص إلى صور لتحقيق عمليات التصميم الإبداعية. ساهم بالخدمة التالية لإكمال هذه العملية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Realistic_Vision_V1.4', 'api_call': \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", 'api_arguments': {'prompt': 'string', 'negative_prompt': 'string'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': \"from transformers import pipeline\\n\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\n\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\n\\nresult = model(prompt, negative_prompt=negative_prompt)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.' </s>"
"<s>[INST]تريد مدير المدرسة إنشاء نظام لإدارة المحتوى التعليمي الرقمي. اقترح استخدام API لتوليد وتعديل صور تعليمية بناءً على النصوص.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"
"<s>[INST] لكتابة تطبيق للتعرف الصوتي للكلام والتحويله إلى نص، نحتاج إلى API لتوليد توجيهات يمكن تفسيرها بصرياً.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"
"<s>[INST] تطوير تطبيق لتعليم الرسم للأطفال. اقترح استخدام API لتجسيد رسومات توضيحية استناداً إلى وصفات نصية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"
"<s>[INST] يحتاج فريق التصميم إلى أداة تسهل تحويل تصورات المستخدمين إلى صور قابلة للتحريك. اقترح استخدام API لتوليد وتعديل صور متحركة استناداً إلى الوصفات النصية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"
"<s>[INST] تهدف مؤسسة تعليمية إلى تنمية منصة تفاعلية تساعد الطلاب في فهم المفاهيم الصعبة بشكل أسهل. اقترح أداة تستخدم API لتوليد صور توضيحية استناداً إلى النصوص.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"
"<s>[INST] يُطلب منك تطوير أداة تقوم بتوليد صور فنية مستقبلية بناءً على الرؤى الإبداعية. اقترح استخدام API لهذا الغرض.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"
"<s>[INST] لتوسيع قاعدة مستخدمي تطبيقنا التعليمي، نريد إضافة صور توضيحية تعليمية. اقترح API لتحويل الوصفات التعليمية إلى صور.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"
"<s>[INST] تتطلب تجربة التعلم عن بُعد لمنصتك التعليمية وجود عناصر بصرية تعزز التحفيز للطلاب. اختر API يمكنه تحويل مفاهيم التحفيز إلى صور ملهمة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"
"<s>[INST] تطوير منصة تسويقية تسعى لانتقاء الصور الفنية المناسبة لحسابات عملائك. قم باختيار API يمكنه توليد وتعديل الصور الفنية بناءً على الوصفات.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"
"<s>[INST] لدعم التعليم الذاتي على منصة التعلم الإلكتروني لنادي البرمجة، نريد أداة لتحويل تفاصيل الأكواد إلى رسومات تفهمها الطلاب بسهولة. اقترح API لتوليد صور تعليمية للأكواد.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-inpainting', 'api_call': \"StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')\", 'api_arguments': ['prompt', 'image', 'mask_image'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\\n\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.' </s>"
"<s>[INST]لدى فريقنا مشروع لتطوير تطبيق للتعليم عن برامج الذكاء الاصطناعي، ونحتاج إلى توليد صور توضيحية للخوارزميات.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"
"<s>[INST] نريد تصميم تطبيق للهواتف الذكية للبساتين والحدائق الذكية، نحتاج إلى توليد صور للنباتات والزهور.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"
"<s>[INST] يجري فريقنا البحث عن تصميم تفاعلي لمعرض فني رقمي، ونحتاج إلى إنشاء صور لوحات فنية وتحف معروضة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"
"<s>[INST] نريد تطوير تطبيق يقدم جولات افتراضية للمتاحف الشهيرة، نحتاج إلى إنشاء صور لمحتويات المعارض.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"
"<s>[INST] لدينا مشروع لإنشاء تطبيق للمحافظة على البيئة، ونحتاج إلى توليد صور توعوية عن التدوير وحماية الحياة البرية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"
"<s>[INST] يجري فريق البحث في شركتنا تطوير منصة تعليمية للأطفال، نريد إنشاء صور توضيحية للمفاهيم التعليمية المعقدة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"
"<s>[INST] عندنا فكرة لمشروع تطويري لتطبيق نشر قصص قصيرة، نحتاج إلى توليد صور توضيحية للقصص المختارة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"
"<s>[INST] نريد إنشاء تطبيق للسفر يتيح للمستخدمين استكشاف الأماكن السياحية، نحتاج إلى صور توضيحية للوجهات السياحية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"
"<s>[INST] تعمل شركتنا على تطوير تطبيق لألعاب الفيديو، نحتاج إلى توليد رسومات للشخصيات والمناظر في اللعبة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"
"<s>[INST] لدينا فريق يبني تطبيقًا للسفر بالزمن، ونريد توليد صور توضيحية للتاريخ والحقبات الزمنية المختلفة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-photoreal-2.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')\", 'api_arguments': {'prompt': 'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens'}, 'python_environment_requirements': {'torch': 'torch.float16', 'diffusers': 'StableDiffusionPipeline'}, 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Stable Diffusion 1.5', 'accuracy': 'Not specified'}, 'description': 'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.' </s>"
"<s>[INST]تطلب شركتنا تصميم لوحة فنية تحتوي على صورة لمدينة futuristica.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"
"<s>[INST] تريد عميلنا تصميم لوحة فنية تجمع بين الطبيعة والرقص.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"
"<s>[INST] تحتاج شركتنا إلى تصميم تصميم جرافيكي يحتوي على صورة لسماء مليئة بالنجوم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"
"<s>[INST] أنشئ رسمًا لمشهد طبيعي يحتوي على شلال.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"
"<s>[INST] طلب عميلنا تصميم شعار لشركته يعكس التكنولوجيا والابتكار.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"
"<s>[INST] نحتاج إلى تصميم رسم توضيحي لمدينة ذكية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"
"<s>[INST] اختر عميلنا تصميم دعوة زفاف مميزة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"
"<s>[INST] تطلب شركة عميلة تصميم لوحة فنية تجسد الحياة البرية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"
"<s>[INST] أنشئ رسماً يصوّر منظراً بحرياً جميلًا.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"
"<s>[INST] ترغب شركة العميل في تصميم لوحة فنية تجسد الهدوء والاسترخاء.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.' </s>"
"<s>[INST]نريد تطوير تطبيق يتيح للمستخدمين إنشاء قصص تفاعلية مصورة. هل يمكنك استخدام واجهة برمجة التطبيقات لتحويل النصوص إلى صور؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] لدينا حدث ترويجي قادم ونريد إنتاج بوسترات إعلانية مبتكرة. هل يوجد واجهة برمجة تقوم بتحويل النصوص الترويجية إلى صور جذابة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] نحن بصدد تطوير تطبيق لإنشاء بطاقات معايدة مخصصة. هل هناك واجهة برمجة يمكن استخدامها لتحويل النصوص إلى صور بطاقات جذابة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] تحتاج مدرستنا إلى وسيلة لإنشاء صور توضيحية لدروس العلوم. هل يمكنك استخدام واجهة برمجة لتحويل النصوص التوضيحية إلى صور معبرة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] ترغب شركتنا في تصميم قائمة طعام بصرية جذابة ومتميزة. هل يمكن استخدام واجهة برمجة لتحويل قائمة الطعام إلى صور إبداعية؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] نود إنشاء صور فنية تشجيعية لفريق كرة القدم المدرسي. هل توجد واجهة برمجة تقوم بتحويل النصوص التحفيزية إلى صور ملونة وجذابة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] نرغب في تصميم شعار جديد لشركتنا. هل توجد واجهة برمجة لتحويل النصوص إلى صور تعبيرية تستخدم لإنشاء الشعارات؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] نود إنتاج صور رمزية لمنتجاتنا الجديدة لاستخدامها في التسويق عبر الإنترنت. هل يمكن استخدام واجهة برمجة لتحويل النصوص إلى صور رمزية فريدة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] نرغب في تصميم بوسترات تعريفية لورش عملنا القادمة. هل يمكن استخدام واجهة برمجة لتحويل النصوص الورشة إلى صور جذابة وإبداعية؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] يجّمع مشروعنا القادم بين الابتكار والتكنولوجيا، ونريد إنشاء صور توضيحية تعبر عن رؤيتنا. هل يمكن استخدام واجهة برمجة لتحويل النصوص التعبيرية إلى صور إبداعية؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'andite/anything-v4.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')\", 'api_arguments': {'model_id': 'andite/anything-v4.0', 'torch_dtype': 'torch.float16', 'device': 'cuda', 'prompt': 'hatsune_miku'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline', 'torch': 'torch'}, 'example_code': {'from diffusers import StableDiffusionPipeline': '', 'import torch': '', 'model_id = andite/anything-v4.0': '', 'pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)': '', 'pipe = pipe.to(cuda)': '', 'prompt = hatsune_miku': '', 'image = pipe(prompt).images[0]': '', 'image.save(./hatsune_miku.png)': ''}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST]قم بإنشاء صورة تصويرية تمثل مشهدًا ليليًا لمدينة حديثة متلألئة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"
"<s>[INST] ساعدنا في إنشاء صورة تصويرية تعبر عن فصل الشتاء بألوانه الباردة والثلوج الناعمة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"
"<s>[INST] قم بتوليد صورة توضح مشهدًا طبيعيًا ساحرًا بجبال شاهقة ونهر جارٍ بينها.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"
"<s>[INST] أكتب نصًا يصف أحد أكثر الأزقة التاريخية شهرةً وألقًا في العالم العربي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"
"<s>[INST] باستخدام النص المعطى، أنشئ صورة رقمية تعبيرية عن الشجاعة والقوة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"
"<s>[INST] ابتكر صورة توضح فن تحضير الأطعمة العربية التقليدية بألوانها ونكهاتها الفريدة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"
"<s>[INST] كتبت نصًا يصف جلسة صلاة هادئة في مسجد تاريخي عتيق. اصنع صورة تعبيرية تجسد هذا الموقف بأقصى قدر من الواقعية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"
"<s>[INST] أنشئ صورة توضح جمال شاطئ البحر بالمناظر الذهنية الرائعة للبحر الزرقاء الخلابة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"
"<s>[INST] استخدم النص المُعطى لتوليد صورة تعبيرية تمثل حفل زفاف ساحر في قصر تاريخي مهيب.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"
"<s>[INST] قم بكتابة وصف خيالي لشخصية بطل خيالية قوية وقم بتوليد صورتها التوضيحية باستخدام النص المعطى.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney-v4', 'api_call': \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", 'api_arguments': {'text': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': \"generate_image('your text here')\", 'performance': {'dataset': 'Midjourney v4 images', 'accuracy': 'Not provided'}, 'description': 'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.' </s>"
"<s>[INST]تحتاج شركتنا إلى إنشاء نظام للكشف عن احتمالات الإصابة بالأمراض النفسية من النصوص الطبية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"
"<s>[INST] نريد تطوير تطبيق للتعرف على أشكال الوجوه والتعبيرات الوجهية. نحتاج إلى واجهة برمجة تطبيقات لتحقيق ذلك.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"
"<s>[INST] يريد فريق التسويق إنشاء نموذج للتنبؤ بالسلوك على وسائل التواصل الاجتماعي من خلال تحليل النصوص.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"
"<s>[INST] نحن بحاجة إلى تقديم أداة لتوليد نصوص توجيهية لتدريب العملاء على استخدام تطبيقنا الجديد.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"
"<s>[INST] تبحث شركتنا عن طريقة لتحليل بيانات الصوت والتمييز بين الأصوات المختلفة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"
"<s>[INST] تحتاج شركتنا إلى تصميم نظام لتحسين جودة الصور الطبية وتوحيد التنسيق.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"
"<s>[INST] يرغب قسم البحث والتطوير في تطوير نظام لتحليل النصوص الطبية لاكتشاف الاختلالات اللغوية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"
"<s>[INST] نحن بحاجة إلى تطوير أداة لاستخراج المعلومات الهامة من النصوص الطبية لمساعدة الأطباء في صنع القرارات.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"
"<s>[INST] تريد شركتنا تطوير أداة لتحويل النصوص إلى بيانات جدولية لتسهيل التحليل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"
"<s>[INST] تبحث شركتنا عن حل لتحسين جودة البيانات الصوتية وتنقيتها من الضوضاء.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'stabilityai/sd-vae-ft-ema', 'api_call': \"StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))\", 'api_arguments': {'model': 'CompVis/stable-diffusion-v1-4', 'vae': 'AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)'}, 'python_environment_requirements': {'diffusers': 'diffusers library'}, 'example_code': 'from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)', 'performance': {'dataset': {'COCO 2017 (256x256, val, 5000 images)': {'accuracy': {'rFID': 4.42, 'PSNR': '23.8 +/- 3.9', 'SSIM': '0.69 +/- 0.13', 'PSIM': '0.96 +/- 0.27'}}, 'LAION-Aesthetics 5+ (256x256, subset, 10000 images)': {'accuracy': {'rFID': 1.77, 'PSNR': '26.7 +/- 4.8', 'SSIM': '0.82 +/- 0.12', 'PSIM': '0.67 +/- 0.34'}}}}, 'description': 'This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.' </s>"
"<s>[INST]كتب طالب أولاد صالح مقالة عن أهمية التعليم في المجتمع. استخدم الآي بي ذو القوة العالية \"stabilityai/stable-diffusion-2-depth\" لتحويل النص إلى صورة تمثل فكرة المقالة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"
"<s>[INST] قررت شركة تحليل البيانات تحليل بيانات العملاء لتحديد احتياجاتهم. أستخدم آي بي \"stabilityai/stable-diffusion-2-depth\" لتوليد صورة تعبر عن نتائج التحليل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"
"<s>[INST] أعدت السلطات المحلية حملة للتوعية بأهمية فصل النفايات. استخدم الآي بي لتوليد صورة توضح أهمية الحملة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"
"<s>[INST] تقررت الشركة تحسين تجربة المستخدم لتطبيق الهواتف الذكية. استخدم الآي بي لإنشاء صورة تعبر عن تصميم جديد للتطبيق.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"
"<s>[INST] تواجه شركة تسويق تحدي في تحديد الشعار المناسب لعميل جديد. قم بإستخدام الآي بي لإنتاج صورة تعبر عن الشعور المطلوب للعلامة التجارية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"
"<s>[INST] يعمل الباحثون على تطوير أداة لتحويل الكتب الورقية إلى نصوص رقمية. إستخدم الآي بي لتوليد صورة توضح عملية التحويل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"
"<s>[INST] أنشىء مطوّر تطبيق لعبة تعليمية للأطفال. إستخدم الآي بي لإنشاء صورة تعبر عن فكرة اللعبة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"
"<s>[INST] تسعى منظمة غير ربحية لزيادة الوعي بأهمية الطاقة المتجددة. إستخدم الآي بي لإنتاج صورة توضح فوائد الاعتماد على الطاقة المتجددة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"
"<s>[INST] يعتزم المطورون تطوير تطبيق للأشخاص الذين يعانون من الاكتئاب. استخدم الآي بي لإنشاء صورة تعبر عن أهمية الدعم النفسي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"
"<s>[INST] يرغب الناشر في تصميم غلاف كتاب جديد بشكل ملهم. أستخدم الآي بي لإنتاج صورة تلخص موضوع الكتاب.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Generate and modify images based on text prompts', 'api_name': 'stabilityai/stable-diffusion-2-depth', 'api_call': \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')\", 'api_arguments': {'prompt': 'Text prompt to generate image', 'image': 'Initial image (optional)', 'negative_prompt': 'Negative text prompt to avoid certain features', 'strength': 'Strength of the prompt effect on the generated image'}, 'python_environment_requirements': ['pip install -U git+https://github.com/huggingface/transformers.git', 'pip install diffusers transformers accelerate scipy safetensors'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\n\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.' </s>"
"<s>[INST]يتم تطوير تطبيق لمساعدة الطلاب في تعلم الرياضيات. يُطلب منك تنفيذ واجهة رسومية تحفز الطلاب على حل المسائل الرياضية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"
"<s>[INST] تريد شركة الطاقة الشمسية عرض معلومات استهلاك الطاقة الشمسية على شكل رسوم بيانية ديناميكية على موقع الويب الخاص بها.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"
"<s>[INST] مطور تطبيق يبحث عن طريقة لإدراج خريطة تظهر مواقع المستخدمين في التطبيق.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"
"<s>[INST] مالك متجر على الإنترنت يرغب في تحسين تجربة المستخدم في البحث عن المنتجات. يجب عليك توفير خاصية البحث الذكي باستخدام تقنيات الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"
"<s>[INST] تطبيق إدارة المهام يحتاج إلى قدرة على إنشاء تذكيرات فورية للمهام المهمة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"
"<s>[INST] مطور تطبيق يحتاج إلى إضافة خاصية مكالمات صوتية وفيديو بجودة عالية إلى تطبيق التواصل الحالي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"
"<s>[INST] فريق تطوير يرغب في إضافة خاصية تعريف الوجه إلى تطبيق الهاتف الذكي الخاص بهم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"
"<s>[INST] شركة تقدم خدمات التوصيل ترغب في تحسين نظام التوصيل الخاص بها من خلال رصد حركة المركبات وتقديم تقارير عن الأداء.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"
"<s>[INST] مالك موقع ويب يريد تحسين تجربة المستخدم من خلال تقديم محتوى ديناميكي معروض بناءً على سلوك المستخدمين.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"
"<s>[INST] مبرمج يبحث عن طريقة لتطوير تطبيق للعب ألعاب الفيديو على محاكي متقدم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'EimisAnimeDiffusion_1.0v', 'api_call': \"DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')\", 'api_arguments': \"['prompt']\", 'python_environment_requirements': 'huggingface_hub', 'example_code': \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.' </s>"
"<s>[INST]لدينا مشروع لتطوير تطبيق للتعرف على الكلاب من الصور. نحتاج إلى إنشاء صور لكلب وتحديد سلالته من الصورة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"
"<s>[INST] نحن بصدد إنشاء تطبيق للتعرف على النباتات من الصور. نود إنشاء صور لنبات محدد وتحديد اسمه العلمي من الصورة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"
"<s>[INST] ترغب شركتنا في بناء نظام لتحليل انحناءات الطرق من الصور الجوية. نريد إنشاء صور توضح درجة انحناء الطريق بدقة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"
"<s>[INST] نحتاج إلى تصميم نظام يستطيع تحديد أنواع السحب من الصور الجوية. يجب إنشاء صور لسحب مختلفة وتصنيفها بناءً على النوع.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"
"<s>[INST] لدينا حاجة للكشف عن العناصر الغريبة في صور المدينة الفلكية. نأمل في إنشاء صور تظهر العناصر الغريبة بوضوح.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"
"<s>[INST] نتطلع إلى تصميم نظام لتحليل بيئات البحر الأحمر من الصور الجوية. نريد إنتاج صور توضح مواطن الاستقرار البيئي في المنطقة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"
"<s>[INST] نحتاج إلى توليد صور لتحديد نقاط الاصطدام في البيئات المختلفة. يتوجب إنشاء صور لنقاط الاصطدام بدقة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"
"<s>[INST] أنا بحاجة إلى توليد صور توضح تأثير تغير المناخ على الغابات. يجب إنشاء صور لتغيرات في البيئة الغابية بدقة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"
"<s>[INST] لدينا مشروع لتطوير نظام لتحليل مواقع البناء من الصور الجوية. نريد إنشاء صور توضح هياكل البناء بدقة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"
"<s>[INST] نود تصميم نظام لتحليل تأثير الزلازل على المباني من الصور الجوية. يطلب منك إنشاء صور تظهر تأثير الزلازل بوضوح.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': \"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))\", 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.' </s>"
"<s>[INST]تحتاج إلى إنشاء تصميم شخصية كرتونية ذات ملامح أنيقة وجريئة باستخدام تقنية Text-to-Image. طلب من الآلة رسم شخصية تبدو كسيدة من عصر النهضة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"
"<s>[INST] قم بإنشاء صورة فنية تمثل منظرًا طبيعيًا هادئًا، مع استخدام النموذج Text-to-Image. استخدم وصف \"غروب الشمس على بحيرة هادئة\" كبداية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"
"<s>[INST] تابع خطوات إعداد نموذج Text-to-Image لرسم صورة تجسد مفهوم الخيال العلمي بواسطة نموذج Nitro Diffusion. استخدم جملة \"روبوتات فضائية متقدمة تحتل المجرة\" كمدخل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"
"<s>[INST] ابدأ بتوليف نموذج Text-to-Image لرسم صورة تعبر عن فكرة حلم المسافرين بالفضاء الخارجي. ضع الجملة \"رحلة إلى المريخ وإطلالة من صنوف الكوكب\" لتكوين الصورة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"
"<s>[INST] قم بإعداد طلب لإنشاء صورة فنية تمثل منطقة ساحرة معمارية قديمة باستخدام خوارزمية Text-to-Image. استخدم العبارة \"مدينة قديمة بأبراج شاهقة وأزقة ضيقة\" للتوجيه.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"
"<s>[INST] استخدم خوارزمية Text-to-Image لخلق صورة تؤكد على روعة الطبيعة مع وجود نباتات غريبة. ابدأ بالعبارة \"غابة ساحرة مليئة بالنباتات الخلابة\" كدليل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"
"<s>[INST] تحتاج إلى توليف النموذج Text-to-Image لرسم صورة تعكس جمال وأناقة الزهور الاستوائية. استعمل مقطع \"ورود استوائية ملونة وساحرة على شاطئ البحر\" كمدخل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"
"<s>[INST] أنشئ نموذج Text-to-Image لرسم صورة تجسد منظر الغسق في الصحراء بأشعة الشمس الأخيرة. استخدم النص \"تضاءلت النجوم وانبعثت ألوان الغروب\" للإشارة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"
"<s>[INST] قم بدمج النموذج Text-to-Image لخلق صورة تصويرية لجسر عتيق فوق نهر هادئ. استخدم جملة \"جسر قديم يعبر نهرًا هادئًا في غسق اليوم\" لتوجيه الرسم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"
"<s>[INST] اطلب من خوارزمية Text-to-Image رسم صورة تمثل هدوء المناظر الطبيعية القطبية. استخدم عبارة \"صمت قطبي وسحر الشمس القطبية\" لتوجيه التصميم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': \"StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.' </s>"
"<s>[INST]أنشئ قصة خيالية قصيرة تبدأ بعبارة \"في عالم بعيد جداً\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"
"<s>[INST] عميلك يبحث عن تطبيق يقوم بتحويل نص إلى صورة افتراضية لمعلومات المالية. توضح له كيف نستخدم الـ API لتحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"
"<s>[INST] أنشئ قصيدة شعرية من 10 أبيات تستخدم فيها كلمة \"حب\". استخدم الـ API لتحويل النص إلى صورة تعبر عن المشهد الذي توصفه القصيدة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"
"<s>[INST] اكتب سكريبت لفيلم قصير يحكي عن مغامرة في الفضاء. استخدم النص الناتج لتوليد صور توضح مشاهد الفيلم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"
"<s>[INST] تطلب عميلك إنتاج صورة توضح مفهوم السلام لديه. قم بكتابة نص يصف هذا المفهوم واستخدم الـ API لتحويله إلى صورة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"
"<s>[INST] أنت بحاجة إلى إضافة صورة رمزية جديدة لملفك الشخصي. اكتب نبذة قصيرة تصف تلك الصورة ثم استخدم الـ API لتوليدها من النص.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"
"<s>[INST] كتابة قصة خيالية تتحدث عن عالم ساحر. اشرح للكاتب كيفية استخدام الـ API لإنشاء صورة تجسد هذا العالم الخيالي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"
"<s>[INST] طلب عميلك إضافة صورة لمقال يتحدث عن الطبيعة وجمالها. استخدم الـ API لتحويل فقرة من المقال إلى صورة تعبر عن هذا المحتوى.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"
"<s>[INST] كتابة رواية قصيرة تبدأ بـ \"في قرية ساحرة عاشت أسرار عظيمة\". قم باستخدام الـ API لتحويل هذه الجملة إلى صورة تعكس بداية القصة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"
"<s>[INST] عميلك يريد تصميم شعار جديد لشركته. اكتب جملة تعبر عن رؤية الشركة ثم استخدم الـ API لتوليد صورة تعكس هذه الرؤية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Linaqruf/anything-v3.0', 'api_call': \"Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A text-to-image model that generates images from text descriptions.' </s>"
"<s>[INST]قم بإنشاء تصميم لوجو جديد لشركتك يحتوي على عنصر الرقمية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"
"<s>[INST] اكتب خطة عمل لإطلاق منتج جديد يستهدف فئة الشباب باللغة العربية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"
"<s>[INST] تحتاج إلى كتابة مقالة حول تأثير التكنولوجيا على التعليم الجامعي. استخدم الأمثلة والأبحاث الحديثة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"
"<s>[INST] أنشئ مجموعة من الإعلانات المبوبة عبر الإنترنت لترويج لخدماتك الجديدة في مدينتك.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"
"<s>[INST] قم بإعداد خطة تسويقية لمنتجك الحالي لزيادة مبيعاته خلال الربع القادم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"
"<s>[INST] عندك مشروع بحثي تحتاج إلى تحليل بيانات تفاعلات الأعمدة. استخدم أداة تحليل البيانات لمساعدتك في ذلك.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"
"<s>[INST] أدخل برنامج تدريبي لتطوير مهاراتك القيادية بالتعاون مع فريقك.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"
"<s>[INST] أكتب نص إعلاني جذاب لجذب عملاء جدد لخدماتك الاستشارية في مجال الأعمال.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"
"<s>[INST] قم بتصميم مجموعة من الرسوم التوضيحية لتوضيح عملية التسوق عبر الإنترنت لمتجرك الإلكتروني.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"
"<s>[INST] اكتب مقالًا تقنيًا يشرح نظرة عامة عن تقنية الواقع المعزز وتطبيقاتها المستقبلية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'wavymulder/Analog-Diffusion', 'api_call': \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['transformers'], 'example_code': \"text_to_image('analog style landscape')\", 'performance': {'dataset': 'analog photographs', 'accuracy': 'Not specified'}, 'description': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\" </s>"
"<s>[INST]قم بتصميم شخصية خيالية بشعر متوهج وعيون لامعة، مرتدية ملابس مبتكرة وتحمل سيفًا سحريًا.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"
"<s>[INST] أنشئ لوحة فنية تصور مشهدًا طبيعيًا خريفيًا مليئًا بأشجار القيقب والألوان الدافئة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"
"<s>[INST] قم بإنشاء لوحة فنية تصور مدينة مستقبلية تتوهج بالأضواء والتكنولوجيا الحديثة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"
"<s>[INST] ابتكر شخصية خيالية لعالم الخيال العلمي، تجمع بين عناصر الآليين والمخلوقات الخرافية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"
"<s>[INST] ابتكر تصميمًا لشخصية خيالية تعبر عن القوة والحكمة في عالم الخيال السحري.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"
"<s>[INST] أنشئ صورة فنية تمثل مشهدًا هادئًا في الريف، مع وجود مروج خضراء وسماء زرقاء صافية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"
"<s>[INST] اصنع لوحة فنية تصوّر شخصية خيالية تتحول إلى حيوان خرافي بألوانه البرّاقة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"
"<s>[INST] قم بإنشاء تصميم يتضمن منظرًا فضائيًا لكوكب غريب مع أقمار صناعية تحوم حوله.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"
"<s>[INST] ابتكر تصميمًا لمخلوق بحري خيالي يجمع بين خصائص الأسماك والمخلوقات البحرية الأخرى.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"
"<s>[INST] أنشئ صورة فنية تجسد مشهدًا من عصور ملحمية مع وجود معارك ملحمية ومحاربين قويين.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-diffusion-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')\", 'api_arguments': ['prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'high quality art', 'accuracy': 'not provided'}, 'description': 'Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.' </s>"
"<s>[INST]? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'dreamlike-art/dreamlike-anime-1.0', 'api_call': \"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-anime-1.0')\", 'api_arguments': ['prompt', 'negative_prompt'], 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': \"from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\nnegative_prompt = 'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry'\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\nimage.save(./result.jpg)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\" </s>"
"<s>[INST]يرغب الباحثون في تطوير نظام يحول الكتب الإلكترونية إلى نص مسموع للأشخاص ذوي الإعاقة البصرية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"
"<s>[INST] قم بتحويل سلسلة نصية إلى تنسيق صوتي باستخدام خدمة المحولات.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"
"<s>[INST] قم بإنشاء واجهة مستخدم تفاعلية تعرض تصنيفات فنية لأعمال الفنانين باستخدام واجهة برمجة تطبيقات تحليل الصور.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"
"<s>[INST] يرغب الباحثون في تطوير نظام يقوم بتحليل المحتوى الصوتي لمقاطع الفيديو وتحويله إلى نص.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"
"<s>[INST] قم بتطوير نظام يقوم بتحليل الصوت وتحديد العوامل التي قد تؤثر على جودة التسجيل.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"
"<s>[INST] أنشئ تطبيقًا يستخدم تقنية التعلم العميق لتحديد سلالة الكلب من الصور.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"
"<s>[INST] قم بتطوير نظام يقوم بتحليل النصوص القانونية وتحديد المفاهيم الرئيسية والكلمات المفتاحية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"
"<s>[INST] أنشئ تطبيقًا يستخدم خدمة تفريق الألوان لتحويل الصور إلى رسوم فنية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"
"<s>[INST] يرغب العلماء في تحليل الموجات الصوتية وتحديد الترددات العالية والمنخفضة الموجودة بها.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"
"<s>[INST] قم بتحليل المشاهد الفيديو وتحديد الأشياء المتحركة فيها باستخدام نموذج التعرف على الأشياء.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'Lykon/DreamShaper', 'api_call': \"pipeline('text-to-image', model=Lykon/DreamShaper)\", 'api_arguments': '', 'python_environment_requirements': 'transformers, torch', 'example_code': 'https://huggingface.co/spaces/Lykon/DreamShaper-webui', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper' </s>"
"<s>[INST]أريد إنشاء صورة لمجموعة من الأزهار متنوعة وجميلة. هل يمكنك توجيهي لذلك؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"
"<s>[INST] كتبت قصيدة عن الشتاء، وأرغب في رؤية صورة تعكس أجواء الشعر. ما هو الخطوات التي يجب اتباعها باستخدام ال API؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"
"<s>[INST] أرغب في تصميم غلاف كتاب فني يتناسب مع قصتي. هل يمكنك توجيهي لاستخدام ال API؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"
"<s>[INST] لدي قصة قصيرة عن مغامرةٍ سحرية، وأود رؤية صورة تجسد أحداثها. كيف يمكنني الاستفادة من هذا ال API؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"
"<s>[INST] أريد إنشاء صورة تجسد الهدوء والاسترخاء. هل يمكن ال API مساعدتي في ذلك؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"
"<s>[INST] كتبت قصة خيالية عن عالم ساحر، وأرغب في إنشاء صورة تعكس خيال القصة. هل يمكنني استخدام هذا ال API للحصول على ذلك؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"
"<s>[INST] أريد رسمًا لمشهد صيفي زرعًا وخضرة. هل يمكن ال API مساعدتي في تحقيق ذلك؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"
"<s>[INST] أود رؤية صورة لمدينة خيالية متقدمة تعكس التطور التكنولوجي والفضاء. هل يمكن ال API توليد هذا النوع من الصور؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"
"<s>[INST] أحب الطيور وأود رؤية صورة لمجموعة متنوعة من الطيور الجميلة. هل يمكن استخدام ال API لهذا الغرض؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"
"<s>[INST] لدي قصة خيالية تستند على عالم الفانتازيا وأريد رؤية لوحة فنية تجسد خيال القصة. هل ال API مناسب لهذا الغرض؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_v2.2_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v2.2_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'StableDiffusionPipeline, DPMSolverMultistepScheduler', 'torch': 'torch'}, 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'Various datasets', 'accuracy': 'Not specified'}, 'description': \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\" </s>"
"<s>[INST]كتابة قصيدة شعرية تبدأ بسطر \"في ليلة ساحرة من زمان\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"
"<s>[INST] توليد صورة تصويرية تصف مناظر طبيعية على شاطئ البحر بإستخدام جملة \"في هدوء مطلق يمتد البحر بلا نهاية\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"
"<s>[INST] إنشاء لقطة فنية تعبر عن الحب والحنان باستخدام جملة \"عندما يلتقي القلبان، تتفتح زهرة جديدة بجمال لا يصدق\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"
"<s>[INST] ابتكار صورة توضح مفهوم الحرية والطيران بجملة \"عندما تتحرر روحك، تعانق السماء بأجنحة من ضوء\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"
"<s>[INST] إخراج صورة فنية تصور السلام والهدوء بعبارة \"عندما تعود الطبيعة لسكونها، ينعم العالم بالسلام\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"
"<s>[INST] عمل صورة تعبيرية عن الخيال والإبداع مع جملة \"عندما تطير الأفكار إلى عوالم غير مألوفة\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"
"<s>[INST] تحويل حكاية قديمة إلى صورة فنية مع بداية \"في زمن بعيد وبلاد بعيدة، حيث\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"
"<s>[INST] إبداع صورة فنية تستعرض تضاريس الحياة بجملة \"في متاهات الوقت تنسجم ذكريات الأمس مع أحلام الغد\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"
"<s>[INST] عرض حكاية خيالية بصورة فنية بابتداء جملة \"في سراديب الزمن وأفق الخيال، استيقظت كائنات الظلام\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"
"<s>[INST] توليد صورة فنية تمثل التلاقي بين الطبيعة والتكنولوجيا بعبارة \"عندما تتحد الأشجار بأسلاك التكنولوجيا\".? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'gsdf/Counterfeit-V2.5', 'api_call': \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", 'api_arguments': 'text', 'python_environment_requirements': 'transformers', 'example_code': '((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field', 'performance': {'dataset': 'EasyNegative', 'accuracy': 'Not provided'}, 'description': 'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.' </s>"
"<s>[INST]يمكننا تخصيص نظام تعلم آلي لتوقع أداء الأسهم على المدى القصير باستخدام البيانات التاريخية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"
"<s>[INST] نود تطوير نظام توليد نصوص إبداعية باستخدام خوارزمية توليد اللغة الطبيعية الحديثة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"
"<s>[INST] نحتاج إلى بناء نظام لتصنيف الصور الطبية بدقة عالية لتسهيل عملية تشخيص الأمراض.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"
"<s>[INST] نرغب في إنشاء نظام ترجمة فورية للنصوص بين لغات متعددة باستخدام تقنيات الترجمة الآلية الحديثة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"
"<s>[INST] يتطلب لدينا تطبيق إنترنت سريع وموثوق به لمشروعنا الجديد.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"
"<s>[INST] تحتاج شركتنا إلى نظام تحليل بيانات فعال لفهم اتجاهات العملاء وتحسين تجربتهم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"
"<s>[INST] تبحث منظمتنا عن تطوير أداة لتحليل المخاطر المتعلقة بالتداول عبر الإنترنت.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"
"<s>[INST] تهدف شركتنا إلى تطوير تطبيق ذكي يسهل عملية التسوق عبر الإنترنت للمستخدمين.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"
"<s>[INST] نحتاج إلى نظام لتحليل بيانات الأداء الرياضي وتوقع نتائج المباريات باستخدام الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"
"<s>[INST] نود إنشاء أداة لتحويل النصوص إلى كلام واقعي بواسطة تقنيات توليد الصوت الحديثة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'vintedois-diffusion-v0-1', 'api_call': \"pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", 'api_arguments': ['prompt', 'CFG Scale', 'Scheduler', 'Steps', 'Seed'], 'python_environment_requirements': ['transformers'], 'example_code': \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", 'performance': {'dataset': 'large amount of high quality images', 'accuracy': 'not specified'}, 'description': 'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.' </s>"
"<s>[INST]نحن بحاجة إلى تطوير نظام لتحليل سلوك المستهلكين استنادًا إلى بياناتهم الشخصية وتوقع احتياجاتهم المستقبلية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"
"<s>[INST] تخطط شركتنا لتطوير تطبيق للتعرف على الأشخاص في الصور وتوفير تفاصيل حولهم. نريد استخدام تقنيات الذكاء الاصطناعي لهذا الغرض.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"
"<s>[INST] نريد إنشاء نموذج للكشف عن التجميعات الجيولوجية في الصور الجوية، وتحديد أماكنها بدقة عالية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"
"<s>[INST] نحن بهدف تطوير نظام توصيات لتحسين تجربة التداول في السوق المالية. نرغب في استخدام التحليل التقني لتوليد هذه التوصيات.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"
"<s>[INST] نريد إنشاء أداة لتحليل وتصنيف السيارات في الصور الفوتوغرافية استنادًا إلى موديلاتها وسنوات الصنع.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"
"<s>[INST] ترغب شركتنا في تطوير نظام لتقدير أعداد السكان في مناطق محددة باستخدام تقنيات تحليل الصور الجوية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"
"<s>[INST] نود تحسين موديل توقع الإقبال على شراء المنتجات الغذائية باستخدام إشارات الدماغ. هل يمكنكم مساعدتنا في تصميم ذلك؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"
"<s>[INST] نريد تطوير نظام للتعرف على أنماط التصميم في الملابس والأزياء باستخدام الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"
"<s>[INST] نحتاج إلى تصميم أداة لقياس وتحليل نوعية الهواء باستخدام الصور الفضائية. هل يمكنكم توجيهنا لاستخدام هذه التقنية؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"
"<s>[INST] ترغب شركتنا في تطبيق تقنيات الذكاء الاصطناعي لتمييز وتصنيف النباتات في الصور الطبيعية. كيف يمكننا البدء في هذا المشروع؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image generation and modification based on text prompts', 'api_name': 'stabilityai/stable-diffusion-x4-upscaler', 'api_call': \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')\", 'api_arguments': {'model_id': 'stabilityai/stable-diffusion-x4-upscaler', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors', 'xformers (optional, for memory efficient attention)'], 'example_code': 'pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\n\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\n\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.' </s>"
"<s>[INST]لدينا مشروع لإنشاء منصة تعليمية عبر الإنترنت، نحتاج لدمج خاصية تحويل النص إلى صوت لتسهيل تجربة التعلم للطلاب.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] نريد تطوير تطبيق تسوق عبر الإنترنت يعرض منتجات معينة، ونحتاج لخاصية توجيه المستخدمين إلى المنتجات عن طريق وصف صوتي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] نحتاج إلى نظام يمكنه تحويل النصوص العلمية complex إلى تصوير ثلاثي الأبعاد.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] نود تطوير تطبيق لتعليم اللغات الأجنبية يساعد الطلاب على تحسين مهارات النطق، نحتاج إلى API لتحويل النصوص إلى تسجيلات صوتية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] نحتاج إلى تقنية تسمح لنا بتحويل تعليمات الصوت إلى دلائل تفاعلية للألعاب التعليمية التفاعلية.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] نريد إضافة وظيفة تحويل الروايات النصية إلى صور رقمية ملونة عالية الدقة في تطبيقنا الحالي.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] تطوير منصة تفاعلية لتعليم الرياضيات للأطفال الصغار، نحتاج إلى API لتحويل المعادلات الرياضية إلى رسوم بيانية ملونة وسهلة الفهم.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] نحتاج إلى تقنية تمكننا من تحويل وصف الفيديوهات إلى توضيح صوتي مع وضع تسجيلات صوتية للإضافة على الفيديو.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] تطوير نظام روبوتي لتقديم محاضرات تفاعلية في المؤتمرات الافتراضية، نريد API لتحويل النصوص التقنية إلى رسوم توضيحية ثلاثية الأبعاد.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST] تصميم تطبيق لتعليم القراءة للأطفال الصغار، نحتاج API لتحويل النصوص القصصية إلى رسوم توضيحية متحركة.? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': \"StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')\", 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.' </s>"
"<s>[INST]قم بتطوير تطبيق لتحويل النص إلى كلام لمستخدمين متحدين بصرياً.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"
"<s>[INST] بناء تطبيق للتعرف على الأشياء من الصور لفئة ذوي الاحتياجات الخاصة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"
"<s>[INST] قم ببناء تطبيق لتحليل المشاهد من ملفات الفيديو لمساعدة أشخاص ذوي إعاقة بصرية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"
"<s>[INST] إنشاء تطبيق لترجمة النصوص من لغة إلى أخرى لتحسين تجربة المستخدمين الذين لا يجيدون لغة معينة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"
"<s>[INST] تطوير تطبيق لتحليل المشاهد الطبيعية من الصور لمساعدة الأشخاص الذين يعانون من ضعف البصر.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"
"<s>[INST] برمجة تطبيق لتحديد المواقع الجغرافية من الصور لأفراد ذوي الاحتياجات الخاصة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"
"<s>[INST] تنفيذ تطبيق لتحليل التعابير الوجهية من الصور لتوفير تجربة أفضل لفئة الصم والبكم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"
"<s>[INST] إنشاء تطبيق لتوصيف الفيديوهات باللغة العربية من أجل تعزيز الوصولية للمحتوى.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"
"<s>[INST] تطوير تطبيق لتعريف العناصر في الصور بالأصوات لدعم فهم البيئة لذوي الاحتياجات الخاصة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"
"<s>[INST] بناء تطبيق لتحليل الصورة وتقديم وصف صوتي لمساعدة الأشخاص ذوي الإعاقة البصرية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'nlpconnect/vit-gpt2-image-captioning', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')\", 'api_arguments': {'model': 'nlpconnect/vit-gpt2-image-captioning'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL'], 'example_code': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", 'performance': {'dataset': 'Not provided', 'accuracy': 'Not provided'}, 'description': 'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.' </s>"
"<s>[INST]نود إنشاء نماذج ثلاثية الأبعاد لمسيرات في شوارع المدينة. ما هو الخطوات اللازمة باستخدام API المشار إليه؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"
"<s>[INST] نحن بصدد تطوير تطبيق للألعاب المحمولة تحتاج إلى إنشاء صور جذابة للشخصيات الخيالية. كيف يمكننا استخدام API المشار إليه؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"
"<s>[INST] لدينا مشروع لتصميم منشآت فضائية متقدمة، ونحتاج إلى توليد صور مفصلة للمحطات. كيف يمكننا استخدام API المعني في هذا السياق؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"
"<s>[INST] أود إنتاج لوحة فنية تجسد شروق الشمس على البحر. هل بإمكانك مساعدتي في استخدام الAPI المشار إليه؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"
"<s>[INST] تطوير شركتنا تطبيق للتعليم الطبي و نريد إنشاء رسوم متحركة توضح عمليات جراحية معقدة. كيف يمكن استخدام الـ API في هذا السياق؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"
"<s>[INST] بنيتُ روبوتاً للإسعافات الأولية وأرغب في توليد صور توضيحية لوظائفه. كيف يمكنني استخدام الAPI المشار إليه؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"
"<s>[INST] أريد تصميم منزل ذكي متكامل، هل يمكن استخدام الـ API لتوليد صور توضيحية للمنزل؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"
"<s>[INST] نريد تصميم لعبة تربيوهية تعليمية تتضمن معالم التاريخ العالمي. كيف يمكننا استخدام الـ API لإنشاء الصور اللازمة؟? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"
"<s>[INST] لدينا مشروع لإنتاج سلسلة كتب للأطفال باللغة العربية، ونحتاج لصور م? [/INST]'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Upscaling', 'api_name': 'stabilityai/sd-x2-latent-upscaler', 'api_call': 'StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)', 'api_arguments': {'prompt': 'text prompt', 'image': 'low resolution latents', 'num_inference_steps': 20, 'guidance_scale': 0, 'generator': 'torch generator'}, 'python_environment_requirements': ['git+https://github.com/huggingface/diffusers.git', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)', 'performance': {'dataset': 'LAION-2B', 'accuracy': 'Not specified'}, 'description': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\" </s>"
"<s>[INST]قم بتحليل تطبيق للجهاز اللوحي يحتوي على عدة تقنيات تعليمية متعددة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"
"<s>[INST] قم بتصميم نظام حجز لمطعم يعتمد على تقنيات الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"
"<s>[INST] أنشئ تطبيق للمواعدة يعتمد على تحليل النصوص والمحادثات لتوفير توصيات دقيقة للمستخدمين.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"
"<s>[INST] قم بتنفيذ نظام مراقبة للمرافق العامة يستخدم تقنيات التعرف على الوجوه.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"
"<s>[INST] قم بتطوير تطبيق توصيل يعتمد على تقنيات تحليل البيانات الجغرافية لتحديد أفضل الطرق والوقت المثالي للتوصيل.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"
"<s>[INST] أنشئ منصة للتدريب عن بعد توفر محتوى تعليمي شامل وتفاعلي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"
"<s>[INST] قم بتطوير تطبيق للتحليل الوبائي الذي يستخدم تقنيات تعلم الآلة للتنبؤ بانتشار الأمراض.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"
"<s>[INST] أنشئ نظام إدارة المخازن الذكي الذي يعتمد على تقنيات الإنترنت الأشياء.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"
"<s>[INST] قم بتصميم تطبيق لتتبع اللياقة البدنية يستخدم تقنيات تحليل البيانات الحيوية لمستخدميه.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"
"<s>[INST] ابتكر منصة للتعليم الإلكتروني تعتمد على تقنيات التفاعل الصوتي والردود التلقائية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'kha-white/manga-ocr-base', 'api_call': \"pipeline('ocr', model='kha-white/manga-ocr-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'manga109s', 'accuracy': ''}, 'description': 'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.' </s>"
"<s>[INST]كتابة قصيدة تحمل موضوع الشجاعة والتحدي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"
"<s>[INST] تحليل محتوى نصي لفهم المشاعر التي يحملها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"
"<s>[INST] إنشاء مقالة تقنية حول آخر التطورات في مجال الذكاء الصناعي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"
"<s>[INST] تحويل صورة إلى رسم كاريكاتوري مضحك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"
"<s>[INST] تصميم مخطط بياني يعرض توزيع الإنفاق الشهري على النفقات المختلفة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"
"<s>[INST] إنشاء تطبيق ويب بسيط يعرض معلومات عن الطقس باستخدام API الجوية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"
"<s>[INST] نظّم ورشة عمل عبر الإنترنت حول أسس تطوير تطبيقات الهاتف.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"
"<s>[INST] تحويل نص إلى لغة برمجية باستخدام تقنية توليد النصوص.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"
"<s>[INST] تصميم قالب موقع ويب احترافي لشركة تكنولوجيا.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"
"<s>[INST] تطوير تطبيق محمول يتيح للمستخدمين تصوير الطعام والحصول على وصف لذلك الطعام تلقائياً.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-base', 'api_call': \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", 'api_arguments': ['raw_image', 'text', 'return_tensors'], 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'COCO', 'accuracy': {'CIDEr': '+2.8%'}}, 'description': 'BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).' </s>"
"<s>[INST]يجب على فريقنا تطوير تطبيق لتعليم اللغة الإنجليزية للأطفال، ونحتاج إلى واجهة برمجية تقوم بتحليل صور الألعاب التعليمية لنقدم تعليقات نصية على النشاطات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"
"<s>[INST] نحتاج إلى تطوير منصة لتسويق الملابس عبر الإنترنت، ونريد استخدام واجهة برمجية تقوم بتحليل الصور لتوفير توصيات نصية بناء على أسلوب اللباس.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"
"<s>[INST] تتطلب متجرنا الإلكتروني إمكانية تحليل الصور لتسهيل عملية تصنيف المنتجات، يرجى توفير إرشادات حول الاستخدام الصحيح لواجهة برمجية تحليل الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"
"<s>[INST] نريد إضافة ميزة لتطبيقنا الطبي لتحويل الوصف النصي للأعراض إلى تشخيص طبي مبني على الصور، هل يمكنك مساعدتنا في الحصول على واجهة برمجية مناسبة؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"
"<s>[INST] يرغب مشروعنا البحثي في استخدام تقنية التعلم العميق لتحليل صور الأفلام القديمة واستخراج وصف قصير لكل صورة، هل هناك واجهة برمجية توفر هذه الإمكانية؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"
"<s>[INST] نحتاج إلى تطبيق يستطيع تحليل الصور المأخوذة من الأقمار الصناعية لتقديم تقارير عن التغيرات البيئية، ما الواجهة البرمجية المناسبة لذلك؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"
"<s>[INST] تحتاج مشروعنا الجديد إلى إضافة خاصية تعريف الأشخاص من الصور التي يتم تحميلها، هل يمكنكم توجيهنا لاستخدام واجهة برمجية تقوم بتعريف ووصف الأشخاص بدقة؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"
"<s>[INST] نريد تطوير تطبيق مساعدة للمعاقين من ذوي الاحتياجات الخاصة، يجب أن يكون للتطبيق خاصية تحليل الصور لتمكين المستخدمين من تعرف على الأشياء من حولهم، هل هناك واجهة برمجية توفر هذه الخاصية؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"
"<s>[INST] يتطلب مشروعنا الإبداعي إمكانية تحويل الصور إلى رسوم بيانية، نحتاج استخدام واجهة برمجية تساعدنا في تحويل العناصر البصرية إلى تمثيل بياني يمكن استخدامه في تصاميمنا.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"
"<s>[INST] أرغب في تطوير تطبيق يقوم بتحويل النصوص الشعرية إلى صور فنية تعبر عن المضمون، ما هي الواجهة البرمجية التي يمكن استخدامها لهذا الغرض؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Transformers', 'functionality': 'Image Captioning', 'api_name': 'blip-image-captioning-large', 'api_call': 'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'api_arguments': {'raw_image': 'Image', 'text': 'Optional Text'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForConditionalGeneration', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, BlipForConditionalGeneration', 'load_processor': 'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_model': 'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)', 'load_image': \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'conditional_captioning': 'text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))', 'unconditional_captioning': 'inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'COCO', 'accuracy': {'image-text retrieval': '+2.7% recall@1', 'image captioning': '+2.8% CIDEr', 'VQA': '+1.6% VQA score'}}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.' </s>"
"<s>[INST]لدينا مشروع لتطوير تطبيق يتعلق بالصور الطبية، ونريد استخدام تقنيات التحويل لتحليل الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] نتاج شركتنا يتضمن مجموعة كبيرة من النصوص الإبداعية، ونود تصنيف هذه النصوص بشكل تلقائي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] نظرًا لتنوع المنتجات التي نقوم بتصميمها، نحتاج إلى API يمكنه تحويل الصور إلى نصوص لإضافة تسميات توضيحية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تعمل شركتنا على تطوير تطبيق لمقارنة الأسعار من متاجر مختلفة، نحتاج إلى API لتحليل نصوص التقييمات لتقديم ملخصات للمستخدمين.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] لدينا مشروع لتطوير منصة للتعليم عن بعد، نحتاج إلى تقنية تساعدنا في تحويل النصوص المكتوبة يدويًا إلى نصوص صوتية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] نريد تحسين تجربة مستخدمي تطبيقنا الذكي باستخدام تقنيات تعلم الآلة، لذا نحتاج إلى API يقوم بتوصيف نمط استخدام التطبيق.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] نحتاج إلى آلية تعمل على فحص الوثائق القانونية والتحقق من صحتها بشكل آلي، يمكنكم توجيهنا إلى الAPI المناسب؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] لدينا منصة لمشاركة الصور بين المستخدمين، ونريد دمج تقنية التعرف على الأشياء في الصور لتوفير وظيفة البحث بالصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تعمل شركتنا على منصة لتنظيم الأحداث والمؤتمرات، نرغب في دمج خاصية تحويل التواريخ المكتوبة بالنصوص إلى تواريخ قابلة للتحليل.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] نحن نعمل على تطبيق يجمع بين اللياقة البدنية والتغذية الصحية، ونريد إضافة ميزة تقديم وصفات طعام استنادًا إلى الصور التي يقوم المستخدم بالتقاطها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST]تريد شركتك إنتاج صور فنية من مجموعة نصوص شعرية. هل يمكنك توجيهنا إلى الواجهة البرمجية المناسبة لذلك؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] لديكم تطبيق لمشاركة الصور وترغبون في إضافة خاصية تحليل المحتوى النصي داخل الصور. هل تتوفر لديكم واجهة برمجية لهذا الغرض؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] أرغب في إنشاء تطبيق يمكنه تحليل اللوحات الفنية واستخراج المعلومات منها. هل يمكنك إيجاد واجهة برمجية تدعم ذلك؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] ترغب في تطوير تطبيق لتحليل الإعلانات التجارية واستخراج المعلومات الخاصة بها من خلال الصور. هل يمكنك الإرشاد إلى الواجهة البرمجية المناسبة؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] تحتاج إلى إضافة خاصية لتحليل الأزياء في الصور لتطبيق الأزياء الخاص بشركتك. هل توفر خدمة تطبيق برمجي لهذا الغرض؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] تريد تطوير نظام لتصنيف الطعام من الصور، هل يمكنك توجيهنا إلى الواجهة البرمجية المناسبة لهذا الغرض؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] تبحث عن واجهة برمجية لتحليل الإعلانات السيارات من الصور، هل تستطيع مساعدتي في العثور على الحل؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] تحتاج إلى إنشاء تطبيق يمكنه تحليل مشاهد الطبيعة واستخراج المعلومات المتعلقة بها. هل تتيح واجهة برمجية لهذه المهمة؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] تريد تطوير تطبيق لتحليل المناظر الطبيعية واستخراج تفاصيلها من الصور، هل يمكنك إرشادي إلى الواجهة البرمجية المناسبة؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] تبحث عن واجهة برمجية لتحليل الفنون التشكيلية في الصور، هل يمكنك توجيهنا إلى الأداة المناسبة لذلك؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-opt-2.7b', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'api_arguments': {'img_url': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', 'question': 'how many dogs are in the picture?'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': {'import_requests': 'import requests', 'import_PIL': 'from PIL import Image', 'import_transformers': 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'load_processor': \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_model': \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", 'load_image': \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'process_inputs': \"inputs = processor(raw_image, question, return_tensors='pt')\", 'generate_output': 'out = model.generate(**inputs)', 'decode_output': 'print(processor.decode(out[0], skip_special_tokens=True))'}, 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST]تحتاج إلى تحديد نوع الزهور الموجودة في الصورة والتعرف عليها، ابحث عن نموذج يستخدم شبكات تحويلية للقيام بذلك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"
"<s>[INST] تريد تحليل محتوى الفيديو الخاص بك للكشف عن الأشياء المعروضة فيه، استخدم نموذج يعتمد على الذكاء الاصطناعي لتحقيق ذلك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"
"<s>[INST] تحتاج إلى تحليل الصور الطبية لتحديد الأورام الموجودة فيها، استخدم نموذج يستخدم تقنيات Deep Learning لهذا الغرض.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"
"<s>[INST] ترغب في تحسين دقة تحديد النص في الصور، استخدم نموذج يعتمد على شبكات تحويلية محسنة لهذه المهمة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"
"<s>[INST] تخطط لتطوير تطبيق يتيح للمستخدمين تعديل الصور باستخدام تقنيات الذكاء الاصطناعي، استخدم أحد النماذج الجاهزة لتحقيق ذلك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"
"<s>[INST] تحتاج إلى تحويل الصور الى نص، ابحث عن نموذج يمكن استخدامه لهذا الغرض بشكل فعال.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"
"<s>[INST] ترغب في معرفة التفاصيل عن مكونات الصورة وتحليلها، استخدم نموذج يستخدم تقنيات Deep Learning لاستخراج المعلومات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"
"<s>[INST] تريد تقديم خدمة لتصنيف الصور بناءً على المحتوى الظاهر فيها، استخدم نموذج يعتمد على الشبكات العصبية لتحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"
"<s>[INST] تبحث عن طريقة لتحويل النصوص اليدوية في الصور إلى نصوص رقمية، استخدم نموذج يتميز بالدقة في التعرف على النصوص باللغة العربية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"
"<s>[INST] تود تحويل النصوص الموجودة في الصور إلى كود برمجي، ابحث عن نموذج يستخدم تقنيات Deep Learning لتحقيق ذلك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.' </s>"
"<s>[INST]كتابة نص إعلاني جذاب يروج لمنتجنا الجديد على وسائل التواصل الاجتماعي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] تطوير تطبيق جوال يتيح للمستخدمين تقديم طلبات الجلوس في المطاعم المحلية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] إعداد تقرير شهري يحتوي على نتائج أداء مبيعات منتجاتنا على الموقع الإلكتروني.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] تحليل أداء حملة البريد الإلكتروني الأخيرة لزيادة معدل الانتقال إلى صفحة المنتج.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] إنشاء نموذج توقعي للتنبؤ بالطلب على منتجاتنا باستخدام الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] تصميم واجهة مستخدم جذابة لتطبيق الهاتف الذكي الخاص بشركتنا.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] تنفيذ اختبار وحدات لضمان جودة تطبيقنا الجديد قبل إطلاقه للجمهور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] تحسين أداء موقعنا على الويب من خلال تحسين سرعة التحميل وتجربة المستخدم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] إنشاء تطبيق ويب يوفر خدمات الحجز لعملائنا في مجال الفنادق والسفر.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] تنفيذ تحليل استراتيجي للسوق لفهم احتياجات العملاء وتحديد فرص جديدة للنمو.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'naver-clova-ix/donut-base', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'result = donut(image_path)', 'performance': {'dataset': 'arxiv:2111.15664', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST]عميلك يرغب في إنشاء تطبيق لتعليم اللغة الإنجليزية للأطفال. يحتاج الى واجهة تفاعلية تقدم أسئلة عن الصور لتحفيز تعلمهم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"
"<s>[INST] مطورك يرغب في إنشاء تطبيق للتعرف التلقائي على النباتات والحيوانات عن طريق استخدام الصور. احتاج باستخدام نموذج لتحديد أنواع الكائنات في الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"
"<s>[INST] تريد شركتك إنشاء تطبيق لتحليل الشخصيات في الأفلام عن طريق الصور. يتطلب تحديد الشخصيات الرئيسية وتقديم تحليلات حول أدوارهم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"
"<s>[INST] صديقك يريد تطوير تطبيق لمساعدة الطلاب في دراستهم عن بسهولة بتقديم اسئلة مرئية تجذب انتباههم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"
"<s>[INST] تخطط لإنشاء منصة لفهم نوع العملاء المستهدفين عبر تحليل صورهم. تريد ان تجمع بين تقنيات تحليل الصور وتحليل النص لتكوين فهم شامل.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"
"<s>[INST] عميلك يخطط لإنشاء تطبيق لتحفيز مستخدميه على ممارسة التمارين الرياضية بواسطة عرض صور تحفيزية. يحتاج الى نموذج يحدد نوع التمرينات المعروضة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"
"<s>[INST] تود إنشاء تطبيق يساعد السياح في فهم تاريخ المواقع السياحية عن طريق استخدام الصور. يدعم التطبيق تسلسل الأحداث التاريخية بالنص والصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"
"<s>[INST] تحتاج الى تطبيق يمكنه تحليل أعماق البحار من خلال الصور التي تلتقطها غواصاتك. يحتاج الى نموذج يحدد الكائنات البحرية في الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"
"<s>[INST] مطورك يريد إنشاء تطبيق لتسهيل فهم الرسوم البيانية والبيانات المعقدة عبر عرضها بشكل بصري مفهوم. يحتاج الى نموذج يحدد المعلومات الرئيسية في الرسوم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"
"<s>[INST] تحتاج الى تطبيق يمكنه تقديم توصيات لمطعمك عن طريق تحليل الصور التي التقطها العملاء للأطعمة. يحتاج الى نموذج يحدد الأطعمة ويقدم التوصيات المناسبة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'promptcap-coco-vqa', 'api_call': \"PromptCap('vqascore/promptcap-coco-vqa')\", 'api_arguments': {'prompt': 'string', 'image': 'string'}, 'python_environment_requirements': 'pip install promptcap', 'example_code': ['import torch', 'from promptcap import PromptCap', 'model = PromptCap(vqascore/promptcap-coco-vqa)', 'if torch.cuda.is_available():', '  model.cuda()', 'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?', 'image = glove_boy.jpeg', 'print(model.caption(prompt, image))'], 'performance': {'dataset': {'coco': {'accuracy': '150 CIDEr'}, 'OK-VQA': {'accuracy': '60.4%'}, 'A-OKVQA': {'accuracy': '59.6%'}}}, 'description': 'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).' </s>"
"<s>[INST]نبحث عن طريقة لتوليد نصوص تفسيرية لصور تاريخية. هل يمكنك تقديم اقتراح حول كيفية استخدام Hugging Face Transformers لهذا الغرض؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"
"<s>[INST] نحتاج إلى توليد أسماء شخصيات خيالية لقصصنا القصيرة. هل يمكنك إرشادنا حول كيفية استخدام Hugging Face Transformers لإنشاء هذه الأسماء؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"
"<s>[INST] نرغب في تحويل تواريخ التاريخ إلى نصوص قصيرة وسهلة الفهم. هل بإمكانك استخدام Hugging Face Transformers من أجل ذلك؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"
"<s>[INST] نود توليد تسميات معبرة للمناظر الطبيعية التي نصورها. هل يوجد واجهة برمجية مناسبة تعمل بتكنولوجيا Hugging Face Transformers؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"
"<s>[INST] نبحث عن طريقة لتوليد تعليقات ساخرة للصور الكوميدية. هل يمكن Hugging Face Transformers مساعدتنا في هذا الأمر؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"
"<s>[INST] يتعذر علينا إيجاد أفكار لعناوين مدوناتنا. هل يمكن استخدام Hugging Face Transformers لتوليد عناوين مثيرة وملهمة؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"
"<s>[INST] نحتاج إلى إنشاء وصف قصير للأطعمة التي نبيعها في المتجر. هل يوجد API يستند إلى Hugging Face Transformers لمساعدتنا في هذا؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"
"<s>[INST] نود توليد عبارات تعبيرية تناسب منتجاتنا الجديدة. هل يوجد API يستند إلى Hugging Face Transformers يمكن استخدامه لهذا الغرض؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"
"<s>[INST] نبحث عن طريقة لإنشاء نصوص تعليمية تشرح استخدام منتجاتنا. هل يمكن Hugging Face Transformers مساعدتنا في ذلك؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"
"<s>[INST] نود توليد تعليقات إيجابية للصور التي نشاركها على وسائل التواصل الاجتماعي. هل بإمكانك اقتراح API يعتمد على Hugging Face Transformers من أجل ذلك؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-coco', 'api_call': \"pipeline('text-generation', model='microsoft/git-base-coco')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'See the model hub for fine-tuned versions on a task that interests you.', 'performance': {'dataset': 'COCO', 'accuracy': 'Refer to the paper for evaluation results.'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).' </s>"
"<s>[INST]يرغب فريقنا في تطوير نظام توصيات للمستخدمين بناءً على تاريخ تصفحهم على الموقع. اقترح استخدام تحليل البيانات لهذا الغرض.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"
"<s>[INST] نريد تطوير نظام تحليل تعليقات المستخدمين على منتجاتنا لاستخراج الرأي العام. اقترح استخدام تحليل المشاعر لتحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"
"<s>[INST] نحتاج إلى تحديث نظام البحث في موقعنا ليشمل البحث عن الكتب الصوتية. يرجى توضيح كيفية تضمين هذه الخاصية باستخدام Elasticsearch API.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"
"<s>[INST] يجب علينا تطوير نظام تصنيف للأفلام الجديدة وتصنيفها في فئات محددة. يمكنك الاقتراح على استخدام تقنيات تعلم الآلة لهذا الغرض.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"
"<s>[INST] نريد إنشاء نظام يحسب معدل إنجاز الموظفين في الشركة بناءً على بيانات الحضور والمغادرة الخاصة بهم. كيف يمكن تحقيق ذلك باستخدام Python API؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"
"<s>[INST] نريد تنفيذ نظام تتبع للمستندات الواردة إلى الشركة وتصنيفها تلقائيًا حسب نوعها. هل يمكن استخدام خوارزمية تقاطع الانتقاء لتحقيق ذلك؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"
"<s>[INST] يرغب فريق الإحصائيات في تحليل بيانات العملاء لتوقع سلوكهم المستقبلي. استخدم خوارزمية \"Random Forest\" لتحليل البيانات واقترح كيفية تنفيذها باستخدام Python.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"
"<s>[INST] يرغب فريق التسويق في تحليل أداء إعلاناتنا على وسائل التواصل الاجتماعي. استخدم تقنية تحليل الشبكات العصبية لفهم الاستجابة للإعلانات وقم بتقديم توجيهات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"
"<s>[INST] يرغب فريق الأبحاث في تطوير نظام لتحليل المقالات العلمية واستخراج المعلومات الهامة منها. استخدم خوارزمية \"NLP\" القائمة على الشبكات العصبية لتحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"
"<s>[INST] نريد تطوير نظام لتحليل تقارير المبيعات الشهرية وتوقع الإيرادات المستقبلية. اقترح استخدام طرق التحليل الزمني لذلك وتوضيح كيفية تنفيذها باستخدام Python.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'AICVTG_What_if_a_machine_could_create_captions_automatically', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')\", 'api_arguments': {'image_paths': 'List of image file paths', 'max_length': 20, 'num_beams': 8}, 'python_environment_requirements': {'transformers': 'from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer', 'torch': 'import torch', 'Image': 'from PIL import Image'}, 'example_code': \"predict_step(['Image URL.jpg'])\", 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'This is an image captioning model training by Zayn' </s>"
"<s>[INST]قم بإنشاء شخصية خيالية تحمل إجابة لغز مثل \"ما هو الشيء الذي يمشي بأربعة أرجل في الصباح وبثلاثة في المساء؟\".? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] قم بتطوير نموذج يستطيع تحليل مشهد يحتوي على تشويه بصري وتقديم توصيات لإصلاحه.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] أنشئ قصيدة شعرية قصيرة تعبر عن جمال الطبيعة في فصل الربيع.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] اكتب مقالًا علميًا يشرح كيفية تأثير التكنولوجيا على المجتمع الحديث.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] أكتب نص تعليمي يشرح كيفية تنظيم المعلومات باستخدام الرسوم البيانية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] أنشئ كتابة سينمائية تصف مواجهة بين بطل وشرير في قصة خيالية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] كتابة مقال ساخر يعبر عن تجارب مضحكة في الحياة اليومية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] أنشئ تقريرًا علميًا يشرح أهمية الحفاظ على التنوع البيولوجي في العالم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] كتابة رسالة شكر تعبر عن امتنانك لزملاء العمل في فريقك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] أنشئ نص قصير يعبر عن رؤيتك للعالم بعد عقود من الآن.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')\", 'api_arguments': ['raw_image', 'question'], 'python_environment_requirements': ['transformers', 'requests', 'PIL'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import BlipProcessor, Blip2ForConditionalGeneration', 'processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)', 'model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)', \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", 'question = how many dogs are in the picture?', 'inputs = processor(raw_image, question, return_tensors=pt)', 'out = model.generate(**inputs)', 'print(processor.decode(out[0], skip_special_tokens=True))'], 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST]كتب مقالة تحدث عن أهمية الذكاء الاصطناعي في تحسين الخدمات الصحية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] ابحث عن أفضل طريقة لتحسين أداء نموذج التعلم العميق الذي تستخدمه في تصنيف الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] قم بتطوير نظام توصيل طلبات الطعام عبر الإنترنت باستخدام خرائط العالم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] أكتب برنامج باستخدام نموذج معالجة اللغة الطبيعية لتحليل رأي العملاء حول منتجات شركتك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] ابنِ نظام توصيل مستندات آلي يقوم بتسليم الملفات بين الموظفين في الشركة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] عرّف نظام مصادقة ذكي يستخدم تقنيات التعرف على الوجوه للدخول إلى مبنى الشركة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] أنشئ برنامج يستخدم خوارزمية تعلّم آلي لتحديد الكائنات في الصور الطبية لتسهيل تشخيص الأمراض.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] انشئ تطبيق يستخدم خرائط جوجل لعرض مواقع الشركات في المنطقة باستخدام واجهة برمجة التطبيقات لخرائط جوجل.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] قم ببناء نظام إدارة المشاريع الخاص بشركتك باستخدام خوارزميات التعلم الآلي لتحسين التخطيط وتوزيع الموارد.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] أنشئ برمجية تستخدم تقنيات الذكاء الاصطناعي لتحليل البيانات المالية وتقديم توصيات لأداء الاستثمارات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'blip2-flan-t5-xxl', 'api_call': \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\", 'api_arguments': {'raw_image': 'Image', 'question': 'Text'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'LAION', 'accuracy': 'Not provided'}, 'description': 'BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST]تحتاج شركتك إلى تحديد محتوى النص في الصور بشكل تلقائي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] عندك حاجة لمعالجة النصوص اليدوية بشكل تلقائي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] لديك حاجة لتحويل الخطوط المكتوبة بخط اليد إلى نصوص مكتوبة على الحاسوب.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تريد تصميم نظام يتعرف على الأشخاص ويعبر عنهم بشكل نصي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تود القدرة على تحليل المشاهد السينمائية واستخراج المشاعر والأحاسيس منها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تحتاج إلى نظام يمكنه تحويل الصور الطبية إلى تقارير نصية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تحتاج لتطوير نظام لتحليل التعبيرات الوجهية واستخدامها في فهم ردود الأفعال.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] ترغب في تصميم نظام يمكنه تحليل الصور الفضائية واستخراج المعلومات النصية منها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تريد إنشاء نظام يلخص المحتوى التلقائي من الصور الفوتوغرافية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تحتاج إلى تطوير نظام قادر على فهم الرسومات التوضيحية واستخلاص النصوص المتعلقة بها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-large-handwritten'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST]قم بتطبيق نموذج لتوليد تعليقات على صور لفيديوهات درجة الوضوح المرتفعة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"
"<s>[INST] استخدم النموذج الخاص بـ \"ydshieh/vit-gpt2-coco-en\" لتحليل صورة واستخراج محتواها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"
"<s>[INST] قم باستخدام النموذج \"ydshieh/vit-gpt2-coco-en\" لتوليد نص توضيحي لصورة من إخراج الكاميرا.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"
"<s>[INST] أنشئ تعليقًا لصورة معينة باستخدام نموذج التشفير والفك \"ydshieh/vit-gpt2-coco-en\".? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"
"<s>[INST] قم بتحليل صورة وإنشاء وصف لمحتواها باستخدام واجهة \"ydshieh/vit-gpt2-coco-en\".? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"
"<s>[INST] اكتب كودًا يستخدم النموذج \"ydshieh/vit-gpt2-coco-en\" لتحويل صورة معينة إلى نص وصفي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"
"<s>[INST] اكتب دالة تستخدم النموذج \"ydshieh/vit-gpt2-coco-en\" لتحليل صورة وإنشاء سياق تصويري لها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"
"<s>[INST] استخدم API \"ydshieh/vit-gpt2-coco-en\" لتحويل تسلسل صور إلى نصوص توضيحية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"
"<s>[INST] أنشئ تعليقاً على صورة باستخدام وظيفة \"ydshieh/vit-gpt2-coco-en\" لتحليل الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"
"<s>[INST] ابتكر تعليقاً توضيحياً لصورة معينة باستخدام نموذج \"ydshieh/vit-gpt2-coco-en\" لتوليد الوصف.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image-to-Text', 'api_name': 'ydshieh/vit-gpt2-coco-en', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", 'api_arguments': {'loc': 'ydshieh/vit-gpt2-coco-en'}, 'python_environment_requirements': ['torch', 'requests', 'PIL', 'transformers'], 'example_code': 'import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.' </s>"
"<s>[INST]نريد تطوير أداة تساعد المتعاملين باللغة العربية على ترجمة النصوص إلى لغات أخرى بشكل فعال.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] تحتاج منظمة خيرية إلى أداة لتحليل مشاريعها وقياس تأثيرها على المجتمع.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] نبحث عن برنامج يقوم بتصنيف الأفلام استنادًا إلى تفاصيل المؤامرة الخاصة بها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] لدينا تطبيق للتعلم عن بسلك، ونحتاج إلى أداة لتوليد وظائف اللغة الطبيعية بجودة عالية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] نشرك يرغب في بناء نظام إدارة مخزون يستند إلى تحليلات التوقع الذكي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] يريد مطعمنا تحسين خدمة الزبائن من خلال تحليل تقييماتهم عبر وسائل التواصل الاجتماعي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] تريد شركتنا تطوير نظام يقوم بالتعرف على النصوص غير المرغوب فيها عبر البريد الإلكتروني.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] تحلم منظمتنا بإنشاء مساعد صوتي يستجيب لأسئلة المستخدمين بصورة ذكية وسلسة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] نود تطوير أداة تحليل الصور الطبية وتقديم تقارير دقيقة للأطباء عن التشخيص.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST] تريد منظمتنا تطوير نظام يحسن ترتيب نتائج البحث الداخلي للموقع الإلكتروني.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'text2text-generation', 'api_name': 'blip2-opt-6.7b', 'api_call': \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", 'api_arguments': 'image, optional text', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation', 'performance': {'dataset': 'LAION', 'accuracy': 'Not specified'}, 'description': 'BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.' </s>"
"<s>[INST]تخطط لتطوير تطبيق للتعليم عن بعد يستخدم التعلم الآلي لتوفير مراجع تعليمية للطلاب. يمكنك استخدام API التالي لدمج خوارزميات التعلم الآلي في التطبيق.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تريد إنشاء نظام توصية لتحسين تجربة التسوق عبر الإنترنت لزبائنك. يمكنك استخدام ال API التالي لتطوير نموذج توصية قائم على الاهتمامات السابقة للمستخدم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تريد تحسين تجربة المستخدم على موقع الويب الخاص بشركتك من خلال توفير إجابات أكثر دقة في قسم الدعم الفني. يمكنك استخدام ال API التالي لتدريب نموذج لتحليل لغة الطلبات الواردة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تريد تنفيذ نظام للكشف عن الغش في الامتحانات الإلكترونية. يمكنك استخدام ال API التالي لتطوير نموذج لتحليل الصور الرقمية لإكتشاف الغش.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تهدف لبناء نظام توصية مخصص لتحسين تجربة المستخدم على منصة التداول الإلكترونية لشركتك. يمكنك استخدام API التالي لتطوير نموذج توصية معتمد على الفهم العميق لسلوكيات التداول.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تخطط لتطوير تطبيق لمساعدة الطلاب على تحسين مهارات اللغة الإنجليزية الخاصة بهم. يمكنك استخدام API التالي لدمج نموذج تعلم الآلة لتصحيح نطق الكلمات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تريد أن تظهر نتائج تصنيف الصور بطريقة تفاعلية لمستخدمي موقع الويب الخاص بك. يمكنك استخدام ال API التالي لتطوير واجهة رسومية تعرض تفاصيل نتائج تصنيف الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تهدف إلى تعزيز تواجد شركتك على وسائل التواصل الاجتماعي من خلال إنشاء نصوص واضحة وجذابة. يمكنك استخدام ال API التالي لتطوير نموذج توليد النصوص.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تريد تحسين استجابة النظام الخاص بشركتك لطلبات العملاء من خلال فهم نغمة الكلام والشعور من خلال النصوص. يمكنك استخدام API التالي لتطوير نموذج لتحليل المشاعر.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تريد تكامل نظام الدفع الخاص بك مع خدمة تقديم التوصيات للعملاء استنادًا إلى عادات الشراء السابقة. يمكنك استخدام API التالي لتطوير جزء من النظام الذي يعرض التوصيات للمستخدمين.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-base-handwritten', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/trocr-base-handwritten'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST]مطلوب تطبيق يمكنه تحديد نوع الأشجار من الصور التي يلتقطها المستخدم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"
"<s>[INST] يتطلب تطوير تطبيق للمراقبة الصحية تقنيات تصنيف الصور الطبية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"
"<s>[INST] لدينا مشروع لتطوير تطبيق يمكنه تحليل الصور الفضائية لتساعد في توقع الكوارث الطبيعية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"
"<s>[INST] يطلب عميلنا إنشاء تطبيق يمكنه تحليل الصور لتقديم توصيات غذائية مبنية على فحص الوجبات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"
"<s>[INST] يحتاج مشروعنا إلى تكامل نظام تعليم آلي يمكنه فهم المواقف الاجتماعية من خلال تحليل الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"
"<s>[INST] نحتاج إلى تطبيق يستخدم تقنيات التعلم الآلي لتحليل الصور الطبية وتقديم تشخيص دقيق.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"
"<s>[INST] تطوير تطبيق يمكنه تحديد الأعطال في معدات الشركة من خلال تحليل صورها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"
"<s>[INST] يطلب عميلنا إنشاء أداة لتحليل الصور الأمنية والكشف عن أنماط غير معتادة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"
"<s>[INST] تطوير تطبيق لتقديم توصيات فنية للمصممين بناءً على تحليل صور الأزياء.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"
"<s>[INST] تنفيذ تطبيق لتحليل الصور الفنية واقتراح أعمال فنية متشابهة للفنانين.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'donut-base-finetuned-cord-v2', 'api_call': \"AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')\", 'api_arguments': {'image': 'path_to_image'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", 'performance': {'dataset': 'CORD', 'accuracy': 'Not provided'}, 'description': 'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.' </s>"
"<s>[INST]تنظم مدرستك مسابقة لأفضل رسمة تصف فصل الشتاء. استخدم API لتوليد نص قصير يصف الرسمة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"
"<s>[INST] كتبت قصيدة عن الطبيعة وترغب في تحويلها إلى صورة فنية. استخدم API لتحويل النص إلى صورة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"
"<s>[INST] قم بإعداد عرض تقديمي حول تطوير تطبيقات الهاتف الذكي. استخدم API لإنشاء شرائح تقديمية توضح العناصر الرئيسية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"
"<s>[INST] تخطيط لرحلة برية إلى الصحراء وترغب في كتابة قصيدة توضح جمال الطبيعة هناك. استخدم API لتحول الفكرة إلى نص شعري.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"
"<s>[INST] تخيلت عرضًا تفاعليًا حول تاريخ العلوم وتريد توليد نص توضيحي للأفكار الرئيسية. استخدم API لإنشاء النص.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"
"<s>[INST] أردت كتابة مقالة عن التنمية المستدامة وتحتاج إلى تحسين الجودة. استخدم ال API المرتبط لتوليد نص محسن للمقالة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"
"<s>[INST] تود كتابة كتاب خيال علمي وتحتاج إلى مساعدة في بناء العالم المخيل للقصة. استخدم API لإنشاء وصف مفصل للعوالم خيالية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"
"<s>[INST] تخيلت فلمًا قصيرًا يحكي قصة حب رومانسية وتحتاج إلى كتابة نص السيناريو. استخدم ال API لتوليد نص السيناريو.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"
"<s>[INST] تخيلت تطبيقًا جديدًا لمنصة التواصل الاجتماعي وتريد توليد وصف مفصل لطريقة عمل التطبيق. استخدم API لإنشاء وصف دقيق.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"
"<s>[INST] تريد تحسين خطابك العام في المؤتمر القادم وتبحث عن طريقة لجعل الخطاب أكثر جاذبية. استخدم API لتحسين نص الخطاب.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-coco', 'api_call': \"GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'COCO', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"
"<s>[INST]احتفظ بعينات ماء الأمطار في حديقتك واستخدمها لري النباتات.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] أنشئ قائمة بالمهام اليومية التي تريد إنجازها وحدد أهمية كل مهمة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] ابحث عن مطعم يقدم وجبات صحية وشهية في منطقتك واستكشف قائمتهم.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] قم بتصوير منظر طبيعي جميل وشاركه مع أصدقائك على وسائل التواصل الاجتماعي.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] اختر مسلسل تلفزيوني جديد لمشاهدته واستعرض تقييماته عبر منصة مراجعات الأفلام.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] ابحث عن أحدث أخبار التكنولوجيا وتقنيات الذكاء الاصطناعي عن طريق استخدام موقع الأخبار المتخصصة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] قم بتطبيق تمارين الرياضة الأساسية في منزلك لتعزيز قوة العضلات والمرونة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] اطلب وجبة مأكولات بحرية لذيذة من أحد مطاعم السوشي المحلية واستمتع بتجربة فريدة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] ابدأ يومك بتناول وجبة فطور صحية تحتوي على فواكه وحبوب كاملة لزيادة طاقتك وتحفيز نشاطك.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] استخدم تطبيق مراقبة النوم على هاتفك لتتبع جودة نومك وتحسين عادات نومك.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-chartqa-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')\", 'api_arguments': ['t5x_checkpoint_path', 'pytorch_dump_path', 'use-large'], 'python_environment_requirements': 'transformers', 'example_code': 'python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE', 'performance': {'dataset': 'ChartQA', 'accuracy': 'Not provided'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST]أنشئ تطبيقًا يقوم بتحليل مشاهد الفيديو وتتبّع جسم الإنسان.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] قم بتدريب نموذج لتقدير عمر الأشخاص في الصور باستخدام بيانات تدريب موجودة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] ابن شاتبوت المتكلم لمساعدة المستخدمين في المراقبة الصوتية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] تحقق من استخدام أبل الشهير للذكاء الاصطناعي في برنامج Siri الصوتي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم الروبوتات الذكية لتقديم الطلبات وخدمة العملاء.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] قم بتحليل المشهد الكبير من خلال تصوير جوي للزراعة داخل المزارع.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] أنشئ تطبيقًا يستخدم تقنية التعلم العميق لتحليل المستندات اليدوية بوضوح.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] تطوّر نظام دعم القرارات الذكي للمساعدة في تحليل البيانات واتخاذ القرارات الاستراتيجية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] أنشئ تطبيق يُشفر الرسائل النصية لضمان الخصوصية والأمان.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST] إنشاء تطبيق يحلل نثر الكتب والنصوص لاستخراج المعلومات الأساسية والإحصائيات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')\", 'api_arguments': {'t5x_checkpoint_path': 'PATH_TO_T5X_CHECKPOINTS', 'pytorch_dump_path': 'PATH_TO_SAVE'}, 'python_environment_requirements': {'transformers': '4.15.0', 'torch': '1.10.1'}, 'example_code': 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)', 'performance': {'dataset': [{'name': 'Documents', 'accuracy': 'N/A'}, {'name': 'Illustrations', 'accuracy': 'N/A'}, {'name': 'User Interfaces', 'accuracy': 'N/A'}, {'name': 'Natural Images', 'accuracy': 'N/A'}]}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.' </s>"
"<s>[INST]قم بتحليل محتوى الصورة واستخراج نصوص متعلقة بها باستخدام النموذج الجديد.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"
"<s>[INST] إنشاء برنامج يقوم بتحويل نص إنجليزي إلى لغة أخرى باستخدام النموذج المعتمد.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"
"<s>[INST] قم بتصميم نظام لمعالجة اللغة الطبيعية لتحديد المشاعر في النصوص المدخلة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"
"<s>[INST] تنفيذ نظام لتوليد الرموز الموسيقية من النصوص الشعرية بواسطة النموذج المحدد.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"
"<s>[INST] استخدام النموذج الجديد لتحليل الصور واستخراج الكائنات والمواضيع المعروضة فيها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"
"<s>[INST] تطوير تطبيق للتعرف التلقائي على مضمون الصور باستخدام النموذج المستخدم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"
"<s>[INST] إنشاء نظام لتوليد نصوص شعرية مستوحاة من الرسومات الفنية باستخدام النموذج الجديد.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"
"<s>[INST] بناء تطبيق يستخدم النموذج الخاص لتحليل مفهوم الصور ثم إنشاء نص يصفه.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"
"<s>[INST] تصميم نظام لتحويل النصوص الطبيعية إلى تعبيرات فنية بصرية باستخدام النموذج المحدد.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"
"<s>[INST] تنفيذ برنامج لتحليل اللغات القديمة وترجمة النصوص إلى لغات حديثة باستخدام النموذج المطلوب.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/pix2struct-textcaps-base', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", 'api_arguments': {'images': 'image', 'text': 'text', 'return_tensors': 'pt', 'max_patches': 512}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor', 'url = https://www.ilankelman.org/stopsigns/australia.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)', 'processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)', 'inputs = processor(images=image, return_tensors=pt)', 'predictions = model.generate(**inputs)', 'print(processor.decode(predictions[0], skip_special_tokens=True))'], 'performance': {'dataset': 'TextCaps', 'accuracy': 'state-of-the-art'}, 'description': 'Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.' </s>"
"<s>[INST]كتابة قصيدة شعرية تبدأ بسطر \"في ليلة من ليالي الشتاء الباردة\".? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] أنشئ درسًا تفاعليًا يشرح كيفية بناء تطبيق ويب باستخدام React.js وNode.js.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] أنا بحاجة إلى إعداد مقالة بحثية عن تأثير الذكاء الاصطناعي في صناعة التصميم الجرافيكي. يمكنك مساعدتي في استخدام بيانات ومعادلات حول هذا الموضوع؟? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] قم بتوليد قصة خيالية تدور حول رحلة في الزمن لعالم موازٍ.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] إنشاء تطبيق ديناميكي يوفر قائمة تشغيل موسيقى استنادًا إلى تفضيلات المستخدم باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] أنشئ تطبيقًا يقوم بتحليل وتصنيف الصور لتقديم تفاصيل دقيقة عن المحتوى البصري.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] تطوير نظام استشعار ذكي يتيح للمستخدمين مراقبة مستويات درجة حرارة المنزل عن بُعد وتلقي تنبيهات في حالة وجود تغييرات كبيرة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] إعداد تطبيق يقوم بتحليل نمط الكتابة اليدوية لتحديد الهوية الشخصية باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] إنشاء موقع ويب للتعليم عن بُعد يوفر محتوى تعليمي تفاعلي لطلاب مختلف المراحل الدراسية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] إنشاء نظام توصية للأفلام يستند إلى تحليل تفضيلات المستخدم وتقييمات الأفلام باستخدام خوارزميات تعلم الآلة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Captioning', 'api_name': 'microsoft/git-base', 'api_call': \"pipeline('image-to-text', model='microsoft/git-base')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': 'git_base(image)', 'performance': {'dataset': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Captions (CC12M)', 'ALT200M'], 'accuracy': 'Refer to the paper for evaluation results'}, 'description': \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST]تحتاج منظمتك إلى تحقيق بيانات نصية من صور جودة عالية. يمكنك استخدام واجهة برمجة التطبيقات (API) التالية لذلك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\", 'api_arguments': {'TrOCRProcessor': \"from_pretrained('microsoft/trocr-large-printed')\", 'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'pip install transformers', 'PIL': 'pip install pillow', 'requests': 'pip install requests'}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] لتحسين خدمة عملاءك، يمكنك تطبيق نموذج تحليل صور لفهم عاطفة العملاء من خلال صورهم. قم بتكامل الواجهة برمجة التطبيقات (API) التالية للقيام بذلك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\", 'api_arguments': {'TrOCRProcessor': \"from_pretrained('microsoft/trocr-large-printed')\", 'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'pip install transformers', 'PIL': 'pip install pillow', 'requests': 'pip install requests'}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] لديك مراقبة للتنبؤ بتغييرات الطقس باستخدام الصور. يمكنك استخدام API التالية للتعرف على النص في الصور بسرعة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\", 'api_arguments': {'TrOCRProcessor': \"from_pretrained('microsoft/trocr-large-printed')\", 'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'pip install transformers', 'PIL': 'pip install pillow', 'requests': 'pip install requests'}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] ستقوم بتطوير تطبيق للتعلم عن بُعد. يمكنك استخدام API التالية لتحويل المحتوى النصي في الصور إلى نص قابل للتحليل.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\", 'api_arguments': {'TrOCRProcessor': \"from_pretrained('microsoft/trocr-large-printed')\", 'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'pip install transformers', 'PIL': 'pip install pillow', 'requests': 'pip install requests'}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] ترغب في تطوير نظام لمساعدة الأشخاص الذين يعانون من صعوبات في القراءة. قم بتضمين API التالية لتحويل النص المكتوب على الصور إلى نص قابل للقراءة صوتيًا.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\", 'api_arguments': {'TrOCRProcessor': \"from_pretrained('microsoft/trocr-large-printed')\", 'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'pip install transformers', 'PIL': 'pip install pillow', 'requests': 'pip install requests'}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تخطط لتطوير تطبيق لتسجيل الدراسة الذاتية. يمكنك إضافة تقنية تعرف على الصور لتحويل الكتابة اليدوية إلى نص قابل للتحليل. استخدم الAPI التالية لذلك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\", 'api_arguments': {'TrOCRProcessor': \"from_pretrained('microsoft/trocr-large-printed')\", 'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'pip install transformers', 'PIL': 'pip install pillow', 'requests': 'pip install requests'}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تعمل في مجال التعليم الإلكتروني وتريد توفير أداة تساعد الطلاب على تحويل ملفات PDF إلى نص. قم بدمج الAPI التالية لتحقيق ذلك بسرعة وفعالية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\", 'api_arguments': {'TrOCRProcessor': \"from_pretrained('microsoft/trocr-large-printed')\", 'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'pip install transformers', 'PIL': 'pip install pillow', 'requests': 'pip install requests'}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تدير منصة لإنشاء الكتب الإلكترونية وترغب في توفير أداة لتحويل النص الموجود في الصور إلى نص بصيغة يمكن تحريرها. استخدم الواجهة برمجة التطبيقات (API) التالية لتحقيق ذلك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\", 'api_arguments': {'TrOCRProcessor': \"from_pretrained('microsoft/trocr-large-printed')\", 'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'pip install transformers', 'PIL': 'pip install pillow', 'requests': 'pip install requests'}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST] تبحث عن حلاً لتحويل النص الموجود في صور المستندات إلى نص قابل للتحرير. قم بدمج الواجهة برمجة التطبيقات (API) التالية لتوفير هذه الخدمة بشكل فعال لمؤسستك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-large-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\", 'api_arguments': {'TrOCRProcessor': \"from_pretrained('microsoft/trocr-large-printed')\", 'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'pip install transformers', 'PIL': 'pip install pillow', 'requests': 'pip install requests'}, 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not provided'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.' </s>"
"<s>[INST]قم بتطوير تطبيق يساعد الطلاب على تعلم الرياضيات من خلال تحدي الألغاز الرياضية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/deplot', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\", 'api_arguments': {'images': 'image', 'text': 'question', 'return_tensors': 'pt', 'max_new_tokens': 512}, 'python_environment_requirements': {'transformers': 'Pix2StructForConditionalGeneration, Pix2StructProcessor', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nimport requests\\nfrom PIL import Image\\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\\nprocessor = Pix2StructProcessor.from_pretrained('google/deplot')\\nurl = https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, text=Generate underlying data table of the figure below:, return_tensors=pt)\\npredictions = model.generate(**inputs, max_new_tokens=512)\\nprint(processor.decode(predictions[0], skip_special_tokens=True))\", 'performance': {'dataset': 'ChartQA', 'accuracy': '24.0% improvement over finetuned SOTA'}, 'description': 'DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.' </s>"
"<s>[INST] لدى شركة تقنية تهدف لتبسيط عمليات الإدارة اللوجستية، انشئ واجهة برمجة تطبيقات تمكن العملاء من تتبع شحناتهم على الموقع.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/deplot', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\", 'api_arguments': {'images': 'image', 'text': 'question', 'return_tensors': 'pt', 'max_new_tokens': 512}, 'python_environment_requirements': {'transformers': 'Pix2StructForConditionalGeneration, Pix2StructProcessor', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nimport requests\\nfrom PIL import Image\\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\\nprocessor = Pix2StructProcessor.from_pretrained('google/deplot')\\nurl = https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, text=Generate underlying data table of the figure below:, return_tensors=pt)\\npredictions = model.generate(**inputs, max_new_tokens=512)\\nprint(processor.decode(predictions[0], skip_special_tokens=True))\", 'performance': {'dataset': 'ChartQA', 'accuracy': '24.0% improvement over finetuned SOTA'}, 'description': 'DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.' </s>"
"<s>[INST] أنشئ تطبيقًا يعرض معلومات جوية حالية لموقع محدد باستخدام بيانات الطقس المفتوحة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/deplot', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\", 'api_arguments': {'images': 'image', 'text': 'question', 'return_tensors': 'pt', 'max_new_tokens': 512}, 'python_environment_requirements': {'transformers': 'Pix2StructForConditionalGeneration, Pix2StructProcessor', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nimport requests\\nfrom PIL import Image\\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\\nprocessor = Pix2StructProcessor.from_pretrained('google/deplot')\\nurl = https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, text=Generate underlying data table of the figure below:, return_tensors=pt)\\npredictions = model.generate(**inputs, max_new_tokens=512)\\nprint(processor.decode(predictions[0], skip_special_tokens=True))\", 'performance': {'dataset': 'ChartQA', 'accuracy': '24.0% improvement over finetuned SOTA'}, 'description': 'DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.' </s>"
"<s>[INST] إنشئ تطبيق لمساعدة المستخدمين في تحسين نوعية نومهم باستخدام تقنيات النوم الذكي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/deplot', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\", 'api_arguments': {'images': 'image', 'text': 'question', 'return_tensors': 'pt', 'max_new_tokens': 512}, 'python_environment_requirements': {'transformers': 'Pix2StructForConditionalGeneration, Pix2StructProcessor', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nimport requests\\nfrom PIL import Image\\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\\nprocessor = Pix2StructProcessor.from_pretrained('google/deplot')\\nurl = https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, text=Generate underlying data table of the figure below:, return_tensors=pt)\\npredictions = model.generate(**inputs, max_new_tokens=512)\\nprint(processor.decode(predictions[0], skip_special_tokens=True))\", 'performance': {'dataset': 'ChartQA', 'accuracy': '24.0% improvement over finetuned SOTA'}, 'description': 'DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.' </s>"
"<s>[INST] قم بتطوير تطبيق يسهل على المستخدمين العثور على المطاعم القريبة وعرض تفاصيل حول القائمة والموقع.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/deplot', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\", 'api_arguments': {'images': 'image', 'text': 'question', 'return_tensors': 'pt', 'max_new_tokens': 512}, 'python_environment_requirements': {'transformers': 'Pix2StructForConditionalGeneration, Pix2StructProcessor', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nimport requests\\nfrom PIL import Image\\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\\nprocessor = Pix2StructProcessor.from_pretrained('google/deplot')\\nurl = https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, text=Generate underlying data table of the figure below:, return_tensors=pt)\\npredictions = model.generate(**inputs, max_new_tokens=512)\\nprint(processor.decode(predictions[0], skip_special_tokens=True))\", 'performance': {'dataset': 'ChartQA', 'accuracy': '24.0% improvement over finetuned SOTA'}, 'description': 'DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.' </s>"
"<s>[INST] قم بتطوير تطبيق لمساعدة المستخدمين في تنظيم مواعيدهم ومهامهم اليومية بفعالية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/deplot', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\", 'api_arguments': {'images': 'image', 'text': 'question', 'return_tensors': 'pt', 'max_new_tokens': 512}, 'python_environment_requirements': {'transformers': 'Pix2StructForConditionalGeneration, Pix2StructProcessor', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nimport requests\\nfrom PIL import Image\\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\\nprocessor = Pix2StructProcessor.from_pretrained('google/deplot')\\nurl = https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, text=Generate underlying data table of the figure below:, return_tensors=pt)\\npredictions = model.generate(**inputs, max_new_tokens=512)\\nprint(processor.decode(predictions[0], skip_special_tokens=True))\", 'performance': {'dataset': 'ChartQA', 'accuracy': '24.0% improvement over finetuned SOTA'}, 'description': 'DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.' </s>"
"<s>[INST] أنشئ واجهة برمجة تطبيقات لموقع إخباري يعرض أحدث الأخبار والمقالات عن مجال التكنولوجيا.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/deplot', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\", 'api_arguments': {'images': 'image', 'text': 'question', 'return_tensors': 'pt', 'max_new_tokens': 512}, 'python_environment_requirements': {'transformers': 'Pix2StructForConditionalGeneration, Pix2StructProcessor', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nimport requests\\nfrom PIL import Image\\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\\nprocessor = Pix2StructProcessor.from_pretrained('google/deplot')\\nurl = https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, text=Generate underlying data table of the figure below:, return_tensors=pt)\\npredictions = model.generate(**inputs, max_new_tokens=512)\\nprint(processor.decode(predictions[0], skip_special_tokens=True))\", 'performance': {'dataset': 'ChartQA', 'accuracy': '24.0% improvement over finetuned SOTA'}, 'description': 'DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.' </s>"
"<s>[INST] قم بتطوير تطبيق يوفر نصائح وخطط تمارين رياضية شخصية وفقًا لأهداف كل مستخدم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/deplot', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\", 'api_arguments': {'images': 'image', 'text': 'question', 'return_tensors': 'pt', 'max_new_tokens': 512}, 'python_environment_requirements': {'transformers': 'Pix2StructForConditionalGeneration, Pix2StructProcessor', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nimport requests\\nfrom PIL import Image\\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\\nprocessor = Pix2StructProcessor.from_pretrained('google/deplot')\\nurl = https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, text=Generate underlying data table of the figure below:, return_tensors=pt)\\npredictions = model.generate(**inputs, max_new_tokens=512)\\nprint(processor.decode(predictions[0], skip_special_tokens=True))\", 'performance': {'dataset': 'ChartQA', 'accuracy': '24.0% improvement over finetuned SOTA'}, 'description': 'DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.' </s>"
"<s>[INST] أنشئ تطبيقًا يسمح للمستخدمين بتشغيل موسيقى بناءً على المزاج الحالي باستخدام خدمة موسيقية شهيرة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/deplot', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\", 'api_arguments': {'images': 'image', 'text': 'question', 'return_tensors': 'pt', 'max_new_tokens': 512}, 'python_environment_requirements': {'transformers': 'Pix2StructForConditionalGeneration, Pix2StructProcessor', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nimport requests\\nfrom PIL import Image\\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\\nprocessor = Pix2StructProcessor.from_pretrained('google/deplot')\\nurl = https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, text=Generate underlying data table of the figure below:, return_tensors=pt)\\npredictions = model.generate(**inputs, max_new_tokens=512)\\nprint(processor.decode(predictions[0], skip_special_tokens=True))\", 'performance': {'dataset': 'ChartQA', 'accuracy': '24.0% improvement over finetuned SOTA'}, 'description': 'DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.' </s>"
"<s>[INST] قم بتطوير تطبيق يتيح لمستخدميه القيام بحجز تذاكر الطيران ومشاهدة مواعيد الرحلات بسهولة وسرعة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'google/deplot', 'api_call': \"Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\", 'api_arguments': {'images': 'image', 'text': 'question', 'return_tensors': 'pt', 'max_new_tokens': 512}, 'python_environment_requirements': {'transformers': 'Pix2StructForConditionalGeneration, Pix2StructProcessor', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nimport requests\\nfrom PIL import Image\\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\\nprocessor = Pix2StructProcessor.from_pretrained('google/deplot')\\nurl = https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, text=Generate underlying data table of the figure below:, return_tensors=pt)\\npredictions = model.generate(**inputs, max_new_tokens=512)\\nprint(processor.decode(predictions[0], skip_special_tokens=True))\", 'performance': {'dataset': 'ChartQA', 'accuracy': '24.0% improvement over finetuned SOTA'}, 'description': 'DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.' </s>"
"<s>[INST]يتطلّب مشروعنا إنشاء نموذج يتنبأ بسعر المنازل بناءً على معلومات مثل عدد الغرف والموقع.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textcaps', 'api_call': \"AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'TextCaps', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] نحتاج إلى تصميم نظام يمكنه تحليل نغمات الموسيقى والتعرف على الأغاني بناءً على هذه النغمات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textcaps', 'api_call': \"AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'TextCaps', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] نريد بناء نظام يستطيع تحويل النصوص الإنجليزية إلى نصوص باللغة العربية باستخدام تقنيات توليد اللغة الطبيعية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textcaps', 'api_call': \"AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'TextCaps', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] تحتاج شركتنا إلى نظام يمكنه التعرف على الجودة الصوتية لملفات الصوت بناءً على معايير محددة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textcaps', 'api_call': \"AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'TextCaps', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] نرغب في تنفيذ نظام يحتسب تشابه الصور من خلال مقارنة البيانات البصرية بينها باستخدام خوارزميات فحص الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textcaps', 'api_call': \"AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'TextCaps', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] نحتاج إلى تطبيق يمكنه تحليل نغمات الغناء والتعرف على الأصوات الغنائية بناءً على مميزات مثل الطول والتردد.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textcaps', 'api_call': \"AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'TextCaps', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] ترغب منظمتنا في تطوير نظام يستطيع التعرف على الكائنات في الصور وتصنيفها باستخدام موديل شبكات عصبونية مدرب.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textcaps', 'api_call': \"AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'TextCaps', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] نريد تطبيقًا يستطيع تمييز بين الشخص والحيوان في الصور باستخدام تقنيات التعرف على الجسم.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textcaps', 'api_call': \"AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'TextCaps', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] ترغب منظمتنا في بشراء نظام قادر على تحليل مشاهد الفيديو وتعقب الأشخاص المتحركين فيها.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textcaps', 'api_call': \"AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'TextCaps', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] نريد تصميم تطبيق قادر على تحليل الملامح الوجهية في الصور والتعرف على الأشخاص باستخدام تقنيات التعرف على الوجوه.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textcaps', 'api_call': \"AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')\", 'api_arguments': 'image, text', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'TextCaps', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST]يمكنك استخدام API لتوليد عبارة تصف صورة بحسوبة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-r-textcaps', 'api_call': \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'TextCaps', 'accuracy': ''}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] قم بتشغيل النموذج العقلي الاصطناعي لتوليد توصيف للفيلم الذي تختاره.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-r-textcaps', 'api_call': \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'TextCaps', 'accuracy': ''}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] اسأل النموذج الاصطناعي عن درجة حرارة اليوم واستخدام بيانات GPS لمدينة معينة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-r-textcaps', 'api_call': \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'TextCaps', 'accuracy': ''}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] قم بتوليد شرح لصورة تعرض معمارية تاريخية باستخدام واجهة التحويل إلى النصوص.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-r-textcaps', 'api_call': \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'TextCaps', 'accuracy': ''}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] استخدم API لتوليد تصورات منظمة لاجتماع طلابي عبر الإنترنت.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-r-textcaps', 'api_call': \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'TextCaps', 'accuracy': ''}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] اسأل النموذج حول أفضل وجهة سياحية في العالم وقم بتوليد جواب نموذجي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-r-textcaps', 'api_call': \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'TextCaps', 'accuracy': ''}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] طلب من النموذج توليد قصيدة باللغة العربية يمكن أن تستخدم في نوادي الأدب.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-r-textcaps', 'api_call': \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'TextCaps', 'accuracy': ''}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] تعلم النموذج عن تاريخ المصريات النسائية واطلبه توليد ملخص تاريخي لتلك الحركة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-r-textcaps', 'api_call': \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'TextCaps', 'accuracy': ''}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] استخدم API لتوليد العبارات الشاعرية المستوحاة من صورة طبيعية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-r-textcaps', 'api_call': \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'TextCaps', 'accuracy': ''}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] قم بإنشاء قصة خيالية مشوقة بمساعدة النموذج اللغوي? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-r-textcaps', 'api_call': \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'TextCaps', 'accuracy': ''}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST]تريد صديقك تحويل صورة فنية إلى نص. يمكنك استخدام API التالي لتحقيق ذلك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-stage1', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\", 'api_arguments': {'url': 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'torch'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nimport torch\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\n\\npixel_values = processor(image, return_tensors='pt').pixel_values\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.' </s>"
"<s>[INST] قم بإنشاء ملصق إعلاني بتقنية OCR. يمكنك استخدام الـ API التالي لتحويل النص إلى صورة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-stage1', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\", 'api_arguments': {'url': 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'torch'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nimport torch\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\n\\npixel_values = processor(image, return_tensors='pt').pixel_values\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.' </s>"
"<s>[INST] تريد تحويل مقطع فيديو إلى نص كتابي. يمكنك استخدام التكنولوجيا الحديثة والـ API التالي لتحقيق ذلك.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-stage1', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\", 'api_arguments': {'url': 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'torch'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nimport torch\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\n\\npixel_values = processor(image, return_tensors='pt').pixel_values\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.' </s>"
"<s>[INST] أنشئ ملصقًا إعلانيًا ملونًا باستخدام تطبيق OCR. يمكنك الاعتماد على هذا الـ API لتحويل النص إلى صورة بدقة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-stage1', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\", 'api_arguments': {'url': 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'torch'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nimport torch\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\n\\npixel_values = processor(image, return_tensors='pt').pixel_values\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.' </s>"
"<s>[INST] تريد تحويل وصف فيلم إلى رسالة نصية. يمكنك استخدام هذا الـ API لتعريف الأحداث بصورة دقيقة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-stage1', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\", 'api_arguments': {'url': 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'torch'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nimport torch\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\n\\npixel_values = processor(image, return_tensors='pt').pixel_values\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.' </s>"
"<s>[INST] قم بتحويل تسجيل صوتي إلى كتابة مكتوبة بواسطة هذا الـ API لتحويل الصوت إلى نص.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-stage1', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\", 'api_arguments': {'url': 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'torch'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nimport torch\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\n\\npixel_values = processor(image, return_tensors='pt').pixel_values\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.' </s>"
"<s>[INST] أحضر نصًا قصيرًا وحوله إلى صورة جذابة باستخدام هذا الـ API لتحويل النص إلى صورة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-stage1', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\", 'api_arguments': {'url': 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'torch'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nimport torch\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\n\\npixel_values = processor(image, return_tensors='pt').pixel_values\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.' </s>"
"<s>[INST] قم بتوليد شعار مميز لعلامتك التجارية باستخدام API التالي لتحويل النص إلى صورة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-stage1', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\", 'api_arguments': {'url': 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'torch'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nimport torch\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\n\\npixel_values = processor(image, return_tensors='pt').pixel_values\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.' </s>"
"<s>[INST] تريد تحويل لوحة فنية إلى نص. يمكنك استخدام هذا الـ API لتحويل الصورة إلى كلمات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-stage1', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\", 'api_arguments': {'url': 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'torch'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nimport torch\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\n\\npixel_values = processor(image, return_tensors='pt').pixel_values\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.' </s>"
"<s>[INST] قم بإعداد تقرير مصور لعرض تقديمي باستخدام هذا الـ API لتحويل النص إلى صورة فورًا.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-stage1', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\", 'api_arguments': {'url': 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'torch'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nimport torch\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\n\\npixel_values = processor(image, return_tensors='pt').pixel_values\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\", 'performance': {'dataset': 'IAM', 'accuracy': 'Not provided'}, 'description': 'TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.' </s>"
"<s>[INST]تطوير تطبيق لتحويل النص المكتوب يدويًا إلى نص رقمي.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\", 'api_arguments': {'image': \"Image.open(requests.get(url, stream=True).raw).convert('RGB')\", 'processor': \"TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\"}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.' </s>"
"<s>[INST] إنشاء تطبيق لتحويل الصور اليدوية إلى نصوص رقمية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\", 'api_arguments': {'image': \"Image.open(requests.get(url, stream=True).raw).convert('RGB')\", 'processor': \"TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\"}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.' </s>"
"<s>[INST] تصميم نظام لتحليل صور الأوقات القديمة وتحويلها إلى نصوص.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\", 'api_arguments': {'image': \"Image.open(requests.get(url, stream=True).raw).convert('RGB')\", 'processor': \"TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\"}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.' </s>"
"<s>[INST] إنشاء تطبيق لتحويل الكتب المطبوعة إلى نصوص رقمية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\", 'api_arguments': {'image': \"Image.open(requests.get(url, stream=True).raw).convert('RGB')\", 'processor': \"TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\"}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.' </s>"
"<s>[INST] بناء نظام لتحويل النصوص اليدوية في الصور إلى نصوص رقمية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\", 'api_arguments': {'image': \"Image.open(requests.get(url, stream=True).raw).convert('RGB')\", 'processor': \"TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\"}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.' </s>"
"<s>[INST] تصميم تطبيق لتحليل الأدوية من النصوص في الصور الطبية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\", 'api_arguments': {'image': \"Image.open(requests.get(url, stream=True).raw).convert('RGB')\", 'processor': \"TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\"}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.' </s>"
"<s>[INST] إعداد تطبيق لتحويل الرسائل اليدوية إلى نصوص قابلة للتحرير.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\", 'api_arguments': {'image': \"Image.open(requests.get(url, stream=True).raw).convert('RGB')\", 'processor': \"TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\"}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.' </s>"
"<s>[INST] تصميم بوت يتم تعلمه ذاتيًا لقراءة النصوص من الصور.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\", 'api_arguments': {'image': \"Image.open(requests.get(url, stream=True).raw).convert('RGB')\", 'processor': \"TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\"}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.' </s>"
"<s>[INST] بناء نظام يعتمد على الذكاء الاصطناعي لتحويل الشهادات الورقية إلى نصوص.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\", 'api_arguments': {'image': \"Image.open(requests.get(url, stream=True).raw).convert('RGB')\", 'processor': \"TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\"}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.' </s>"
"<s>[INST] تصميم تطبيق لتحويل نصوص اليدوية في الصور الكتبية إلى نصوص رقمية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/trocr-small-printed', 'api_call': \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\", 'api_arguments': {'image': \"Image.open(requests.get(url, stream=True).raw).convert('RGB')\", 'processor': \"TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\"}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", 'performance': {'dataset': 'SROIE', 'accuracy': 'Not specified'}, 'description': 'TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.' </s>"
"<s>[INST]تصميم تطبيق لمراقبة الحالة الصحية الشخصية باستخدام تحليل الصور الطبية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'mgp-str', 'api_call': \"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\", 'api_arguments': {'model_name': 'alibaba-damo/mgp-str-base'}, 'python_environment_requirements': {'packages': ['transformers']}, 'example_code': \"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\nimport requests\\nfrom PIL import Image\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\noutputs = model(pixel_values)\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\", 'performance': {'dataset': 'MJSynth and SynthText', 'accuracy': None}, 'description': 'MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.' </s>"
"<s>[INST] تطوير نظام تقديم توصيات لمستخدمي تطبيق الأزياء الذكية باستخدام تحليل الصور. ابحث عن API لتحليل الملابس والتصميمات.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'mgp-str', 'api_call': \"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\", 'api_arguments': {'model_name': 'alibaba-damo/mgp-str-base'}, 'python_environment_requirements': {'packages': ['transformers']}, 'example_code': \"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\nimport requests\\nfrom PIL import Image\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\noutputs = model(pixel_values)\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\", 'performance': {'dataset': 'MJSynth and SynthText', 'accuracy': None}, 'description': 'MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.' </s>"
"<s>[INST] إنشاء تطبيق لتحليل وتصنيف الأطعمة من صور الطعام باستخدام تقنيات التعلم العميق.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'mgp-str', 'api_call': \"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\", 'api_arguments': {'model_name': 'alibaba-damo/mgp-str-base'}, 'python_environment_requirements': {'packages': ['transformers']}, 'example_code': \"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\nimport requests\\nfrom PIL import Image\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\noutputs = model(pixel_values)\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\", 'performance': {'dataset': 'MJSynth and SynthText', 'accuracy': None}, 'description': 'MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.' </s>"
"<s>[INST] تقديم تطبيق لتقييم المشاهير والشخصيات العامة من الصور باستخدام تحليل الصور. ابحث عن API للتعرف على الوجوه والمميزات الشخصية.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'mgp-str', 'api_call': \"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\", 'api_arguments': {'model_name': 'alibaba-damo/mgp-str-base'}, 'python_environment_requirements': {'packages': ['transformers']}, 'example_code': \"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\nimport requests\\nfrom PIL import Image\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\noutputs = model(pixel_values)\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\", 'performance': {'dataset': 'MJSynth and SynthText', 'accuracy': None}, 'description': 'MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.' </s>"
"<s>[INST] تصميم تطبيق للتعرف على الحيوانات والأنواع من الصور باستخدام تقنيات تعلم الآلة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'mgp-str', 'api_call': \"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\", 'api_arguments': {'model_name': 'alibaba-damo/mgp-str-base'}, 'python_environment_requirements': {'packages': ['transformers']}, 'example_code': \"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\nimport requests\\nfrom PIL import Image\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\noutputs = model(pixel_values)\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\", 'performance': {'dataset': 'MJSynth and SynthText', 'accuracy': None}, 'description': 'MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.' </s>"
"<s>[INST] تطوير تطبيق لتقديم توصيات صحية وتغذية للمستخدمين استنادًا إلى صور الوجبات. ابحث عن API تقديم تحليل تغذية الأطعمة.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'mgp-str', 'api_call': \"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\", 'api_arguments': {'model_name': 'alibaba-damo/mgp-str-base'}, 'python_environment_requirements': {'packages': ['transformers']}, 'example_code': \"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\nimport requests\\nfrom PIL import Image\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\noutputs = model(pixel_values)\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\", 'performance': {'dataset': 'MJSynth and SynthText', 'accuracy': None}, 'description': 'MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.' </s>"
"<s>[INST] إنشاء تطبيق للكشف عن الإصابات أو الأمراض من الصور الطبية باستخدام تقنيات التعلم العميق.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'mgp-str', 'api_call': \"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\", 'api_arguments': {'model_name': 'alibaba-damo/mgp-str-base'}, 'python_environment_requirements': {'packages': ['transformers']}, 'example_code': \"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\nimport requests\\nfrom PIL import Image\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\noutputs = model(pixel_values)\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\", 'performance': {'dataset': 'MJSynth and SynthText', 'accuracy': None}, 'description': 'MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.' </s>"
"<s>[INST] تطوير تطبيق لتحليل وتصنيف الأجسام السيارات من الصور باستخدام تقنيات التعلم العميق.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'mgp-str', 'api_call': \"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\", 'api_arguments': {'model_name': 'alibaba-damo/mgp-str-base'}, 'python_environment_requirements': {'packages': ['transformers']}, 'example_code': \"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\nimport requests\\nfrom PIL import Image\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\noutputs = model(pixel_values)\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\", 'performance': {'dataset': 'MJSynth and SynthText', 'accuracy': None}, 'description': 'MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.' </s>"
"<s>[INST] إنشاء تطبيق للتعرف على الأشخاص من صورهم باستخدام تحليل الوجوه وميزات الوجه.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'mgp-str', 'api_call': \"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\", 'api_arguments': {'model_name': 'alibaba-damo/mgp-str-base'}, 'python_environment_requirements': {'packages': ['transformers']}, 'example_code': \"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\nimport requests\\nfrom PIL import Image\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\noutputs = model(pixel_values)\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\", 'performance': {'dataset': 'MJSynth and SynthText', 'accuracy': None}, 'description': 'MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.' </s>"
"<s>[INST] تطوير تطبيق لتحليل وتصنيف النباتات من الصور باستخدام تقنيات التعلم العميق.? [/INST]'domain': 'Multimodal Image-to-Text', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'mgp-str', 'api_call': \"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\", 'api_arguments': {'model_name': 'alibaba-damo/mgp-str-base'}, 'python_environment_requirements': {'packages': ['transformers']}, 'example_code': \"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\nimport requests\\nfrom PIL import Image\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\noutputs = model(pixel_values)\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\", 'performance': {'dataset': 'MJSynth and SynthText', 'accuracy': None}, 'description': 'MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.' </s>"
"<s>[INST]ترغب في إضافة ميزة لتحويل النصوص إلى عروض تقديمية متحركة.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': {'torch_dtype': 'torch.float16', 'variant': 'fp16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': 'import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\n\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\n\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid, ImageNet, LAION5B', 'accuracy': 'N/A'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.' </s>"
"<s>[INST] تطلب منك شركتك تحليل نصوص لإنشاء قصص تفاعلية.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': {'torch_dtype': 'torch.float16', 'variant': 'fp16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': 'import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\n\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\n\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid, ImageNet, LAION5B', 'accuracy': 'N/A'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.' </s>"
"<s>[INST] بحاجة إلى إنتاج مقاطع فيديو توضيحية للوصفات الغذائية من النصوص.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': {'torch_dtype': 'torch.float16', 'variant': 'fp16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': 'import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\n\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\n\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid, ImageNet, LAION5B', 'accuracy': 'N/A'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.' </s>"
"<s>[INST] تريد تحويل محتوى العروض التقديمية إلى مقاطع فيديو توضيحية.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': {'torch_dtype': 'torch.float16', 'variant': 'fp16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': 'import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\n\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\n\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid, ImageNet, LAION5B', 'accuracy': 'N/A'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.' </s>"
"<s>[INST] تخطط لإضافة تكنولوجيا توليد محتوى فيديو مبني على النصوص إلى منصتك.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': {'torch_dtype': 'torch.float16', 'variant': 'fp16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': 'import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\n\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\n\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid, ImageNet, LAION5B', 'accuracy': 'N/A'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.' </s>"
"<s>[INST] تحتاج إلى إنشاء فيديوهات ترويجية للمنتجات باستخدام تقنية تحويل النصوص إلى فيديو.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': {'torch_dtype': 'torch.float16', 'variant': 'fp16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': 'import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\n\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\n\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid, ImageNet, LAION5B', 'accuracy': 'N/A'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.' </s>"
"<s>[INST] ترغب في توليد مقاطع فيديو قصيرة توضح الأفكار الأساسية في النصوص التعليمية.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': {'torch_dtype': 'torch.float16', 'variant': 'fp16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': 'import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\n\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\n\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid, ImageNet, LAION5B', 'accuracy': 'N/A'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.' </s>"
"<s>[INST] تريد إنشاء مقاطع فيديو متحركة لتوضيح العروض التقديمية المهمة.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': {'torch_dtype': 'torch.float16', 'variant': 'fp16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': 'import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\n\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\n\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid, ImageNet, LAION5B', 'accuracy': 'N/A'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.' </s>"
"<s>[INST] تبحث عن تقنية لإنشاء فيديوهات قصيرة توضح إجراءات العمليات الصناعية.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': {'torch_dtype': 'torch.float16', 'variant': 'fp16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': 'import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\n\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\n\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid, ImageNet, LAION5B', 'accuracy': 'N/A'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.' </s>"
"<s>[INST] تريد تحويل العروض التقديمية إلى فيديوهات توضيحية متحركة بأسلوب إبداعي.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': {'torch_dtype': 'torch.float16', 'variant': 'fp16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': 'import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\n\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\n\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid, ImageNet, LAION5B', 'accuracy': 'N/A'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.' </s>"
"<s>[INST]\"تطوير نظام أساسي يمكنه تحليل تعليقات العملاء من مصادر متعددة وإنشاء تقارير تحليل المشاعر في الوقت الفعلي.\"، \"تنفيذ واجهة برمجة التطبيقات لتحويل الكلام إلى نص والتي تقوم بنسخ الملفات الصوتية في الوقت الفعلي وتدعم لغات متعددة.\"، \"تطوير واجهة برمجة تطبيقات لتصنيف الصور تحدد بدقة الأنواع المختلفة من الزهور استنادًا إلى ميزاتها المرئية.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'chavinlo/TempoFunk', 'api_call': \"pipeline('text-to-video', model='chavinlo/TempoFunk')\", 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.' </s>"
"<s>[INST] إنشاء واجهة برمجة تطبيقات للتنبؤ بالطقس توفر تحديثات كل ساعة حول مستويات درجة الحرارة والرطوبة وهطول الأمطار لموقع محدد.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'chavinlo/TempoFunk', 'api_call': \"pipeline('text-to-video', model='chavinlo/TempoFunk')\", 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.' </s>"
"<s>[INST] إنشاء واجهة برمجة تطبيقات لمحرك التوصيات يقترح أفلامًا مخصصة بناءً على تفضيلات المستخدم وسجل العرض.، صمم واجهة برمجة تطبيقات chatbot يمكنها الإجابة على استفسارات العملاء وتوفير معلومات المنتج وبدء عمليات المبيعات.، قم بتطوير واجهة برمجة تطبيقات تلخيص النص التي تكثف المقالات أو المستندات الطويلة في صورة موجزة ملخصات.، إنشاء واجهة برمجة تطبيقات لتحليل البيانات المالية تعالج بيانات سوق الأوراق المالية، وتولد رؤى، وتتنبأ بالاتجاهات المستقبلية.، تنفيذ واجهة برمجة تطبيقات لترجمة اللغة يمكنها ترجمة النص بين لغات متعددة بدقة وكفاءة.، إنشاء واجهة برمجة تطبيقات للوجه واجهة برمجة تطبيقات التعرف التي يمكنها التعرف على الأفراد من الصور أو مقاطع الفيديو المباشرة بدقة عالية.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'chavinlo/TempoFunk', 'api_call': \"pipeline('text-to-video', model='chavinlo/TempoFunk')\", 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.' </s>"
"<s>[INST]إنشاء نظام يقوم بتحليل الصور واستخراج المعلومات النصية منها، يمكنك استخدام خدمة تحويل النص إلى فيديو من Hugging Face API لتحقيق هذا الغرض.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'ImRma/Brucelee', 'api_call': \"pipeline('text-to-video', model='ImRma/Brucelee')\", 'api_arguments': ['your_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Hugging Face model for converting Persian and English text into video.' </s>"
"<s>[INST] قم بتصميم نظام يقوم بتحليل النصوص الطبية وتحويلها إلى فيديوهات توضيحية، باستخدام خدمة تحويل النص إلى فيديو من Hugging Face API.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'ImRma/Brucelee', 'api_call': \"pipeline('text-to-video', model='ImRma/Brucelee')\", 'api_arguments': ['your_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Hugging Face model for converting Persian and English text into video.' </s>"
"<s>[INST] قم بتطوير نظام يستخدم تحليل النصوص لتوليد فيديوهات تعريفية عن منتجك، يمكنك استخدام خدمة تحويل النص إلى فيديو من Hugging Face API.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'ImRma/Brucelee', 'api_call': \"pipeline('text-to-video', model='ImRma/Brucelee')\", 'api_arguments': ['your_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Hugging Face model for converting Persian and English text into video.' </s>"
"<s>[INST] إنشاء نظام يمكنه تحليل التقارير المالية وتحويلها إلى فيديوهات تبسيطية، باستخدام خدمة تحويل النص إلى فيديو من Hugging Face API.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'ImRma/Brucelee', 'api_call': \"pipeline('text-to-video', model='ImRma/Brucelee')\", 'api_arguments': ['your_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Hugging Face model for converting Persian and English text into video.' </s>"
"<s>[INST] قم بتصميم نظام يستخدم تحليل النصوص لتوليد فيديوهات تثقيفية عن الثقافة العامة، باستخدام خدمة تحويل النص إلى فيديو من Hugging Face API.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'ImRma/Brucelee', 'api_call': \"pipeline('text-to-video', model='ImRma/Brucelee')\", 'api_arguments': ['your_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Hugging Face model for converting Persian and English text into video.' </s>"
"<s>[INST] قم بتطوير نظام يقوم بتحليل النصوص السياسية وتحويلها إلى عروض تقديمية بصرية، باستخدام خدمة تحويل النص إلى فيديو من Hugging Face API.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'ImRma/Brucelee', 'api_call': \"pipeline('text-to-video', model='ImRma/Brucelee')\", 'api_arguments': ['your_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Hugging Face model for converting Persian and English text into video.' </s>"
"<s>[INST] تصميم نظام يحلل التغريدات على وسائل التواصل الاجتماعي ويحولها إلى مقاطع فيديو مشوقة، باستخدام خدمة تحويل النص إلى فيديو من Hugging Face API.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'ImRma/Brucelee', 'api_call': \"pipeline('text-to-video', model='ImRma/Brucelee')\", 'api_arguments': ['your_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Hugging Face model for converting Persian and English text into video.' </s>"
"<s>[INST] إنشاء نظام يحلل المقالات العلمية ويحولها إلى فيديوهات توضيحية، باستخدام خدمة تحويل النص إلى فيديو من Hugging Face API.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'ImRma/Brucelee', 'api_call': \"pipeline('text-to-video', model='ImRma/Brucelee')\", 'api_arguments': ['your_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Hugging Face model for converting Persian and English text into video.' </s>"
"<s>[INST] قم بتصميم نظام يحلل التقارير البيئية ويحولها إلى فيديوهات توعوية، باستخدام خدمة تحويل النص إلى فيديو من Hugging Face API.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'ImRma/Brucelee', 'api_call': \"pipeline('text-to-video', model='ImRma/Brucelee')\", 'api_arguments': ['your_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Hugging Face model for converting Persian and English text into video.' </s>"
"<s>[INST] تحتاج شركتنا إلى نظام يستخدم تحليل الخطابات العلنية لتوليد فيديوهات إعلانية، يمكنك استخدام خدمة تحويل النص إلى فيديو من Hugging Face API كحل لهذا الغرض.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'ImRma/Brucelee', 'api_call': \"pipeline('text-to-video', model='ImRma/Brucelee')\", 'api_arguments': ['your_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Hugging Face model for converting Persian and English text into video.' </s>"
"<s>[INST]يتطلب مشروعنا تطوير نظام قادر على تحليل مشاعر المستخدمين من خلال مقاطع الفيديو. هل يمكنك استخدام API من Hugging Face لبناء نموذج يستطيع ذلك؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'camenduru/text2-video-zero', 'api_call': \"pipeline('text-to-video', model='camenduru/text2-video-zero')\", 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.' </s>"
"<s>[INST] نود تصميم واجهة برمجية قادرة على تحويل النصوص المكتوبة باللغة العربية إلى كتابات بخط اليد. أي API من Hugging Face تُنصح باستخدامه لهذا الغرض؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'camenduru/text2-video-zero', 'api_call': \"pipeline('text-to-video', model='camenduru/text2-video-zero')\", 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.' </s>"
"<s>[INST] يجب علينا تطوير نظام يستطيع تحليل النصوص الطبية الإنجليزية وتحويلها إلى نصوص بلغة الإشارة الأمريكية. هل يدعم Hugging Face مثل هذا النوع من التحويلات؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'camenduru/text2-video-zero', 'api_call': \"pipeline('text-to-video', model='camenduru/text2-video-zero')\", 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.' </s>"
"<s>[INST] تحتاج مشروعنا إلى نظام يعتمد على الذكاء الاصطناعي لتحليل الصور وإنشاء نصوص وصفية توضح محتوى الصور. هل يمكن استخدام API الخاصة بهذا؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'camenduru/text2-video-zero', 'api_call': \"pipeline('text-to-video', model='camenduru/text2-video-zero')\", 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.' </s>"
"<s>[INST] نرغب في إنشاء نظام يستطيع استخراج المعلومات الهامة من نصوص تحددها المستخدم وتقديمها بشكل مختصر. هل يوفر Hugging Face API لهذا الغرض؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'camenduru/text2-video-zero', 'api_call': \"pipeline('text-to-video', model='camenduru/text2-video-zero')\", 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.' </s>"
"<s>[INST] لدينا حاجة لتطوير نظام يُمكن من إنشاء تقارير مالية مفصلة استنادًا إلى البيانات المالية الخام. هل يعرض Hugging Face خدمة API لهذا الغرض؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'camenduru/text2-video-zero', 'api_call': \"pipeline('text-to-video', model='camenduru/text2-video-zero')\", 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.' </s>"
"<s>[INST] نريد بناء نظام يقوم بتحويل الأرقام إلى كتابات بالكلام الطبيعي. هل يوفر Hugging Face API مجموعات نماذج مناسبة لهذه العملية؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'camenduru/text2-video-zero', 'api_call': \"pipeline('text-to-video', model='camenduru/text2-video-zero')\", 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.' </s>"
"<s>[INST] يهدف مشروعنا إلى تطوير نظام قادر على تحليل السلوك اللغوي في النصوص وتقديم تقارير تحليلية. هل يقدم Hugging Face خدمات API لمثل هذه المهام؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'camenduru/text2-video-zero', 'api_call': \"pipeline('text-to-video', model='camenduru/text2-video-zero')\", 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.' </s>"
"<s>[INST] تحتاج منصتنا إلى تطوير نظام يتمكن من توليد نصوص إعلانية جذابة استنادًا إلى قواعد محددة. هل يمكننا استخدام بعض النماذج المتوفرة في Hugging Face API لتحقيق ذلك؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'camenduru/text2-video-zero', 'api_call': \"pipeline('text-to-video', model='camenduru/text2-video-zero')\", 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.' </s>"
"<s>[INST] تحتاج شركتنا إلى نظام يستطيع تحليل النصوص الطبية وتوليد شرح صوتي للتشخيص. هل توفر Hugging Face نماذج API لهذا التوجيه؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'camenduru/text2-video-zero', 'api_call': \"pipeline('text-to-video', model='camenduru/text2-video-zero')\", 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.' </s>"
"<s>[INST]تطوير نظام لإنشاء ملخصات مرئية للأوراق العلمية لتعزيز الفهم والاحتفاظ بالمعرفة.\\n -، إنشاء تطبيق ويب للترجمة اللغوية في الوقت الحقيقي لتسهيل التواصل متعدد اللغات في المؤتمرات العالمية.\\n -، تصميم نظام لتحليل المشاعر في منشورات وسائل التواصل الاجتماعي لتحديد الاتجاهات والآراء العامة حول المواضيع الحالية.\\n -? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b-legacy', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b-legacy')\", 'api_arguments': ['prompt', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': 'import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b-legacy, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': ['LAION5B', 'ImageNet', 'Webvid'], 'accuracy': 'Not provided'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.' </s>"
"<s>[INST] تطوير روبوت محادثة يعمل بالذكاء الاصطناعي لمساعدة العملاء في حل الاستفسارات الشائعة وتقديم توصيات مخصصة.\\n -? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b-legacy', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b-legacy')\", 'api_arguments': ['prompt', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': 'import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b-legacy, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': ['LAION5B', 'ImageNet', 'Webvid'], 'accuracy': 'Not provided'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.' </s>"
"<s>[INST] تنفيذ صورة نظام التعرف لتصنيف أنواع الحياة البرية من خلال صور مصيدة الكاميرا في مشاريع الحفاظ على البيئة.\\n -? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b-legacy', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b-legacy')\", 'api_arguments': ['prompt', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': 'import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b-legacy, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': ['LAION5B', 'ImageNet', 'Webvid'], 'accuracy': 'Not provided'}, 'description': 'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.' </s>"
"<s>[INST]قم بإنشاء تطبيق لتعليم اللغة الإنجليزية للأطفال الصغار باستخدام تقنيات الذكاء الاصطناعي. استخدم واجهة مستخدم تفاعلية وجذابة تسهل عملية التعلم.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video-synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': ['prompt', 'num_inference_steps', 'num_frames'], 'python_environment_requirements': ['pip install git+https://github.com/huggingface/diffusers transformers accelerate'], 'example_code': 'pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid', 'accuracy': 'Not specified'}, 'description': 'A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.' </s>"
"<s>[INST] كتب كتابًا جديدًا واستخدم تقنيات التحليل النصي لفهم توجهات الأدب الحديث. قم بتحليل النصوص وتلخيص المعلومات الرئيسية.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video-synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': ['prompt', 'num_inference_steps', 'num_frames'], 'python_environment_requirements': ['pip install git+https://github.com/huggingface/diffusers transformers accelerate'], 'example_code': 'pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid', 'accuracy': 'Not specified'}, 'description': 'A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.' </s>"
"<s>[INST] قم بإنشاء تطبيق لتتبع عادات النوم الخاصة بك وتحليلها لتحسين نوعية نومك باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video-synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': ['prompt', 'num_inference_steps', 'num_frames'], 'python_environment_requirements': ['pip install git+https://github.com/huggingface/diffusers transformers accelerate'], 'example_code': 'pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid', 'accuracy': 'Not specified'}, 'description': 'A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.' </s>"
"<s>[INST] قم بإنشاء نظام توصيات لتوصية المستخدمين بالكتب الجديدة باستخدام إطار عمل التعلم العميق.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video-synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': ['prompt', 'num_inference_steps', 'num_frames'], 'python_environment_requirements': ['pip install git+https://github.com/huggingface/diffusers transformers accelerate'], 'example_code': 'pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid', 'accuracy': 'Not specified'}, 'description': 'A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.' </s>"
"<s>[INST] قم بإنشاء تطبيق لمساعدة المهندسين في تصميم الهياكل الهندسية باستخدام تقنيات الهندسة العكسية.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video-synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': ['prompt', 'num_inference_steps', 'num_frames'], 'python_environment_requirements': ['pip install git+https://github.com/huggingface/diffusers transformers accelerate'], 'example_code': 'pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid', 'accuracy': 'Not specified'}, 'description': 'A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.' </s>"
"<s>[INST] قم بإنشاء نظام لتتبع وتحليل بيانات الطقس الحالي والتوقعات المستقبلية باستخدام خدمة تطبيقات تحليل البيانات الجوية.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video-synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': ['prompt', 'num_inference_steps', 'num_frames'], 'python_environment_requirements': ['pip install git+https://github.com/huggingface/diffusers transformers accelerate'], 'example_code': 'pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid', 'accuracy': 'Not specified'}, 'description': 'A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.' </s>"
"<s>[INST] قم ببناء نظام لتحليل بيانات العملاء وتوصية المنتجات المناسبة لهم باستخدام تقنيات تحليل البيانات الضخمة.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video-synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': ['prompt', 'num_inference_steps', 'num_frames'], 'python_environment_requirements': ['pip install git+https://github.com/huggingface/diffusers transformers accelerate'], 'example_code': 'pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid', 'accuracy': 'Not specified'}, 'description': 'A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.' </s>"
"<s>[INST] قم بإنشاء تطبيق للكشف عن التزوير في الصور وتقييم صحة الصور الرقمية باستخدام تقنيات الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video-synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': ['prompt', 'num_inference_steps', 'num_frames'], 'python_environment_requirements': ['pip install git+https://github.com/huggingface/diffusers transformers accelerate'], 'example_code': 'pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid', 'accuracy': 'Not specified'}, 'description': 'A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.' </s>"
"<s>[INST] قم ببناء نظام لتتبع وتحليل بيانات الأسهم وتوقع اتجاهات الأسواق المالية باستخدام خدمة تطبيقات تحليل سوق الأسهم.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video-synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': ['prompt', 'num_inference_steps', 'num_frames'], 'python_environment_requirements': ['pip install git+https://github.com/huggingface/diffusers transformers accelerate'], 'example_code': 'pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid', 'accuracy': 'Not specified'}, 'description': 'A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.' </s>"
"<s>[INST] قم بإنشاء تطبيق لمساعدة المستخدمين في تعلم مبادئ البرمجة وتطوير مهاراتهم البرمجية بطريقة تفاعلية وتشجيعية.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video-synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': \"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')\", 'api_arguments': ['prompt', 'num_inference_steps', 'num_frames'], 'python_environment_requirements': ['pip install git+https://github.com/huggingface/diffusers transformers accelerate'], 'example_code': 'pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid', 'accuracy': 'Not specified'}, 'description': 'A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.' </s>"
"<s>[INST]لديك مهمة بناء تطبيق للتحويل النصي إلى صورة متحركة. يمكنك استخدام API التالي لتحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'duncan93/video', 'api_call': \"BaseModel.from_pretrained('duncan93/video')\", 'api_arguments': '', 'python_environment_requirements': 'Asteroid', 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.' </s>"
"<s>[INST] عليك تطوير تطبيق يستخدم API لتحليل النغمة الصوتية والتعرف على المقاطع الموسيقية. إليك API الذي يمكن أن يساعدك في ذلك.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'duncan93/video', 'api_call': \"BaseModel.from_pretrained('duncan93/video')\", 'api_arguments': '', 'python_environment_requirements': 'Asteroid', 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.' </s>"
"<s>[INST] كيفية استخدام API لكتابة تطبيق يقوم بتحويل الكلام إلى نص مكتوب وفهمه بصريًا.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'duncan93/video', 'api_call': \"BaseModel.from_pretrained('duncan93/video')\", 'api_arguments': '', 'python_environment_requirements': 'Asteroid', 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.' </s>"
"<s>[INST] يتعين عليك تطوير تطبيق يستخدم API لتحويل النصوص إلى صور متحركة تلخص المعلومات المالية.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'duncan93/video', 'api_call': \"BaseModel.from_pretrained('duncan93/video')\", 'api_arguments': '', 'python_environment_requirements': 'Asteroid', 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.' </s>"
"<s>[INST] قم بتجربة استخدام API لكتابة تطبيق يحلل النغمات الصوتية ويعرف المقاطع الموسيقية باستخدام تقنية الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'duncan93/video', 'api_call': \"BaseModel.from_pretrained('duncan93/video')\", 'api_arguments': '', 'python_environment_requirements': 'Asteroid', 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.' </s>"
"<s>[INST] كيفية استخدام API لبناء تطبيق يقوم بتحويل النص إلى فيديو مع تفسير بصري للمعلومات النصية.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'duncan93/video', 'api_call': \"BaseModel.from_pretrained('duncan93/video')\", 'api_arguments': '', 'python_environment_requirements': 'Asteroid', 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.' </s>"
"<s>[INST] قم بكتابة برنامج يستفيد من API لتحليل الصوت والتعرف على الموسيقى بناءً على النغمات.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'duncan93/video', 'api_call': \"BaseModel.from_pretrained('duncan93/video')\", 'api_arguments': '', 'python_environment_requirements': 'Asteroid', 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.' </s>"
"<s>[INST] عميلك يبحث عن تطبيق يقوم بتحليل التسجيلات الصوتية والتحويلها إلى نص مفهوم بصريًا، استخدم API لتحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'duncan93/video', 'api_call': \"BaseModel.from_pretrained('duncan93/video')\", 'api_arguments': '', 'python_environment_requirements': 'Asteroid', 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.' </s>"
"<s>[INST] كتب تطبيقًا يعتمد على API لتحويل التسجيلات الصوتية إلى نصوص مكتوبة بصورة مفهومة بصريًا.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'duncan93/video', 'api_call': \"BaseModel.from_pretrained('duncan93/video')\", 'api_arguments': '', 'python_environment_requirements': 'Asteroid', 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.' </s>"
"<s>[INST] عندك مهمة كتابة تطبيق يقوم بتحويل التسجيلات الصوتية إلى نصوص مُفسّرة بصورة بصرية، يمكنك استخدام API التالي لتحقيق الهدف المنشود.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'duncan93/video', 'api_call': \"BaseModel.from_pretrained('duncan93/video')\", 'api_arguments': '', 'python_environment_requirements': 'Asteroid', 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.' </s>"
"<s>[INST]إعداد ملف تعريف شخصي لشخصية خيالية تحمل اسم \"زهرة البحر\" وصفها بكل تفاصيلها بما في ذلك مظهرها الخارجي وشخصيتها الداخلية.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'mo-di-bear-guitar', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet'), torch_dtype=torch.float16)\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/mo-di-diffusion\\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = a magical princess is playing guitar, modern disney style\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'Not mentioned', 'accuracy': 'Not mentioned'}, 'description': 'Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.' </s>"
"<s>[INST] كتابة خطوات تصنيع قطعة أثاث من الخشب بشكل يدوي بدايةً من اختيار الخامات إلى التشطيب النهائي.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'mo-di-bear-guitar', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet'), torch_dtype=torch.float16)\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/mo-di-diffusion\\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = a magical princess is playing guitar, modern disney style\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'Not mentioned', 'accuracy': 'Not mentioned'}, 'description': 'Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.' </s>"
"<s>[INST] تصميم برنامج تدريبي يستهدف تعليم مبادئ البرمجة للأطفال ويحتوي على أنشطة تفاعلية وممتعة.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'mo-di-bear-guitar', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet'), torch_dtype=torch.float16)\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/mo-di-diffusion\\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = a magical princess is playing guitar, modern disney style\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'Not mentioned', 'accuracy': 'Not mentioned'}, 'description': 'Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.' </s>"
"<s>[INST] انشاء نظام حجز غرف الفندق عبر الإنترنت يتيح للعملاء اختيار الغرف وتحديد مواعيد الوصول والمغادرة.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'mo-di-bear-guitar', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet'), torch_dtype=torch.float16)\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/mo-di-diffusion\\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = a magical princess is playing guitar, modern disney style\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'Not mentioned', 'accuracy': 'Not mentioned'}, 'description': 'Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.' </s>"
"<s>[INST] كتابة تقرير تحليلي عن تأثيرات التقنية الحديثة على سوق العمل وسبل تطوير مهارات العمل لمواكبة التطورات.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'mo-di-bear-guitar', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet'), torch_dtype=torch.float16)\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/mo-di-diffusion\\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = a magical princess is playing guitar, modern disney style\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'Not mentioned', 'accuracy': 'Not mentioned'}, 'description': 'Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.' </s>"
"<s>[INST] تطوير تطبيق لتنظيم مواعيد الاجتماعات وتعيين مهام للفريق بشكل فعال ومتناسق.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'mo-di-bear-guitar', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet'), torch_dtype=torch.float16)\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/mo-di-diffusion\\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = a magical princess is playing guitar, modern disney style\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'Not mentioned', 'accuracy': 'Not mentioned'}, 'description': 'Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.' </s>"
"<s>[INST] بناء نظام تتبع المهام اليومية لموظفي الشركة لمساعدتهم في تنظيم وتنفيذ مهامهم بكفاءة.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'mo-di-bear-guitar', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet'), torch_dtype=torch.float16)\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/mo-di-diffusion\\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = a magical princess is playing guitar, modern disney style\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'Not mentioned', 'accuracy': 'Not mentioned'}, 'description': 'Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.' </s>"
"<s>[INST] إنشاء تطبيق لقياس معدل ضربات القلب عند ممارسة التمارين الرياضية باستخدام تقنيات الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'mo-di-bear-guitar', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet'), torch_dtype=torch.float16)\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/mo-di-diffusion\\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = a magical princess is playing guitar, modern disney style\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'Not mentioned', 'accuracy': 'Not mentioned'}, 'description': 'Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.' </s>"
"<s>[INST] تصميم نظام ذكي لإدارة المستودعات يستخدم تقنيات تتبع المخزون وتحسين عمليات اللوجستيات.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'mo-di-bear-guitar', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet'), torch_dtype=torch.float16)\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/mo-di-diffusion\\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = a magical princess is playing guitar, modern disney style\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'Not mentioned', 'accuracy': 'Not mentioned'}, 'description': 'Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.' </s>"
"<s>[INST] كتابة إعلان تسويقي جذاب لمنتج جديد يستخدم أسلوب القصص القصيرة والرسائل الملهمة.? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'mo-di-bear-guitar', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet'), torch_dtype=torch.float16)\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/mo-di-diffusion\\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = a magical princess is playing guitar, modern disney style\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'Not mentioned', 'accuracy': 'Not mentioned'}, 'description': 'Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.' </s>"
"<s>[INST]لتحليل مشاكل الصحة النفسية وتوفير الدعم للمرضى، نريد تطوير أداة تفاعلية تستخدم تقنية الذكاء الاصطناعي. هل يمكن استخدام API لتطوير هذه الأداة؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'redshift-man-skiing', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/redshift-diffusion\\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = (redshift style) spider man is skiing\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': \"Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\" </s>"
"<s>[INST] ترغب الشركة في تطوير نظام لتوصيات الموسيقى الشخصية بناءً على تفضيلات المستخدمين. كيف يمكن تحقيق ذلك باستخدام واجهة برمجية معينة؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'redshift-man-skiing', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/redshift-diffusion\\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = (redshift style) spider man is skiing\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': \"Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\" </s>"
"<s>[INST] نود تطوير منصة لتحليل مخاطر الأمان لتحسين أمان نظامنا الحالي. هل هناك API تقدم هذه الإمكانية؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'redshift-man-skiing', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/redshift-diffusion\\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = (redshift style) spider man is skiing\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': \"Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\" </s>"
"<s>[INST] يهدف مشروعنا إلى تطوير نظام للتعلم الآلي في مجال التعليم لتحسين تجربة التعلم للطلاب. هل يمكن استخدام API لتحقيق ذلك؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'redshift-man-skiing', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/redshift-diffusion\\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = (redshift style) spider man is skiing\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': \"Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\" </s>"
"<s>[INST] نحتاج إلى تطوير منصة لتحليل بيانات المرضى وتوفير تقارير دقيقة عن حالتهم الصحية. هل توجد واجهة برمجية تدعم هذه المتطلبات؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'redshift-man-skiing', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/redshift-diffusion\\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = (redshift style) spider man is skiing\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': \"Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\" </s>"
"<s>[INST] لتطوير بوابة عقارات عبر الإنترنت توفر معلومات شاملة ودقيقة للعملاء، نريد استخدام تقنيات تعلم الآلة. ما هي الواجهة البرمجية المثلى لتحقيق ذلك؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'redshift-man-skiing', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/redshift-diffusion\\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = (redshift style) spider man is skiing\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': \"Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\" </s>"
"<s>[INST] كيف يمكننا استخدام API لتطوير نظام لتحليل تغريدات المستخدمين وفهم مشاعرهم واهتماماتهم؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'redshift-man-skiing', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/redshift-diffusion\\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = (redshift style) spider man is skiing\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': \"Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\" </s>"
"<s>[INST] نود تطوير نظام لتوصيات الأفلام للمستخدمين بناءً على تاريخ مشاهداتهم السابقة. ما هي الطريقة المناسبة لتحقيق ذلك باستخدام API؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'redshift-man-skiing', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/redshift-diffusion\\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = (redshift style) spider man is skiing\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': \"Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\" </s>"
"<s>[INST] نحتاج إلى تطوير نظام للذكاء الاصطناعي يستطيع تقديم توصيات ذكية للمستخدمين في مجال السفر والسياحة. هل يمكن استخدام API لهذا الغرض؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'redshift-man-skiing', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/redshift-diffusion\\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = (redshift style) spider man is skiing\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': \"Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\" </s>"
"<s>[INST] لتحليل سلوك المستخدمين عبر الإنترنت وتحسين تجربتهم، نريد تطوير نظام يستخدم تقنيات تعلم الآلة. هل يمكن اقتراح واجهة برمجية تتناسب مع هذا الهدف؟? [/INST]'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video Generation', 'api_name': 'redshift-man-skiing', 'api_call': \"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))\", 'api_arguments': {'prompt': 'string', 'video_length': 'int', 'height': 'int', 'width': 'int', 'num_inference_steps': 'int', 'guidance_scale': 'float'}, 'python_environment_requirements': ['torch', 'tuneavideo'], 'example_code': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/redshift-diffusion\\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = (redshift style) spider man is skiing\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': \"Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\" </s>"
"<s>[INST]قم بتطوير تطبيق يقدم خدمة تحويل النصوص من العربية إلى لغة أخرى باستخدام خدمة API التالية كمصدر مرجعي:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-textvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/git-base-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\", 'performance': {'dataset': 'TextVQA', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\" </s>"
"<s>[INST] انشئ تطبيق يعتمد على خدمة API للتعرف على الأشياء والمشاهد في الصور لتوفير وصف تلقائي لمحتوى الصورة. استخدم الـ API التالي كمرجع:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-textvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/git-base-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\", 'performance': {'dataset': 'TextVQA', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\" </s>"
"<s>[INST] قم ببناء تطبيق يستخدم خدمة API لتقديم تحليل توجيهي لاحتياجات العملاء باستخدام تقنيات تعلم الآلة. يمكنك استخدام الـ API التالي كمرجع:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-textvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/git-base-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\", 'performance': {'dataset': 'TextVQA', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\" </s>"
"<s>[INST] أضف وظيفة تحليل النصوص العربية إلى تطبيقك عن طريق استخدام API متخصصة في تحليل اللغة الطبيعية. الرجاء الاطلاع على البيانات التالية للمساعدة في تنفيذ هذه الوظيفة:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-textvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/git-base-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\", 'performance': {'dataset': 'TextVQA', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\" </s>"
"<s>[INST] إنشاء تطبيق يوفر خدمة استعلامات دقيقة باستخدام خوارزمية تجهيز اللغة الطبيعية والتعلم الآلي. يُشار إلى الـ API التالي لاستخدامه في تطبيقك:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-textvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/git-base-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\", 'performance': {'dataset': 'TextVQA', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\" </s>"
"<s>[INST] بناء تطبيق قادر على تحليل الموسيقى باستخدام خدمة API للتعرف على الأغاني والإيقاعات. استخدم الـ API التالي لإغناء تطبيقك بميزات التعرف على الموسيقى:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-textvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/git-base-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\", 'performance': {'dataset': 'TextVQA', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\" </s>"
"<s>[INST] أنشئ تطبيقًا يوفر خدمة تحليل وتصنيف النصوص العربية باستخدام خدمة متقدمة في تعلم الآلة. يتعين استخدام الـ API التالي لتحقيق ذلك:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-textvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/git-base-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\", 'performance': {'dataset': 'TextVQA', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\" </s>"
"<s>[INST] قم بتطوير تطبيق يستفيد من خدمة API لتحليل نمط الكتابة وتقديم توصيات لتحسين الكتابة. يمكنك استخدام الـ API التالي لتضمين هذه الوظيفة:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-textvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/git-base-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\", 'performance': {'dataset': 'TextVQA', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\" </s>"
"<s>[INST] أضف إمكانية تحويل النصوص العربية إلى كتابات يدوية باستخدام خدمة API معتمدة على شبكات عصبية متعمقة. يتوجب استخدام الـ API التالي لإدراج هذه الميزة في تطبيقك:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-textvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/git-base-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\", 'performance': {'dataset': 'TextVQA', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\" </s>"
"<s>[INST] ابتكر تطبيقًا يعتمد على خدمة API لتوفير تصحيح وقواعد للغة العربية لتحسين اللغة المكتوبة. استعن بالـ API التالي لإضافة تلك الوظيفة في تطبيقك:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-textvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/git-base-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\", 'performance': {'dataset': 'TextVQA', 'accuracy': 'Refer to the paper'}, 'description': \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\" </s>"
"<s>[INST]قم بتطوير نظام يستخدم التعلم الآلي للتنبؤ بسلوك العملاء بناءً على البيانات التاريخية. هل يمكنك الاستفادة من واجهة برمجة التطبيقات التي تقدمها Scikit-Learn لبناء نظام نمذجة تنبؤي؟، \"إنشاء أداة لتحليل المشاعر على تعليقات وسائل التواصل الاجتماعي لقياس الرأي العام.\" هل يمكنك استخدام واجهة برمجة التطبيقات من TextBlob لأغراض تحليل المشاعر؟، \"قم بتطوير نظام لتلخيص الأوراق البحثية المطولة تلقائيًا.\" هل من الممكن استخدام واجهة برمجة التطبيقات التي توفرها Sumy لمهام تلخيص النص؟، إنشاء نظام توصية يقترح أفلامًا للمستخدمين استنادًا إلى سجل المشاهدة الخاص بهم. هل يمكنك استخدام واجهة برمجة التطبيقات التي تقدمها LightFM لتوصيات التصفية التعاونية؟، \"صمم روبوت دردشة لدعم العملاء يمكنه تقديم استجابات آلية للاستفسارات الشائعة.\" هل تنصح باستخدام واجهة برمجة التطبيقات التي توفرها Rasa لإنشاء روبوت دردشة يعمل بالذكاء الاصطناعي للمحادثة؟\"، \"قم بإنشاء نظام لتحديد رسائل البريد الإلكتروني العشوائية وتصفيتها من صناديق البريد الوارد للمستخدمين. هل هناك واجهة برمجة تطبيقات متاحة من SpamAssassin يمكن استخدامها للكشف عن رسائل البريد الإلكتروني العشوائية ؟\"، \"قم بتطوير أداة يمكنها تصنيف الصور إلى فئات مختلفة بناءً على محتواها. هل يمكنك الاستفادة من واجهة برمجة التطبيقات التي يوفرها TensorFlow لمهام تصنيف الصور؟، \"إنشاء نظام لاستخراج الكيانات المسماة من المستندات النصية لتحسين استرجاع المعلومات.\" هل هناك واجهة برمجة تطبيقات متاحة من SpaCy لمهام التعرف على الكيانات المسماة؟، \"قم بإنشاء تطبيق يمكنه إنشاء تسميات توضيحية للصور تلقائيًا.\" هل يمكنك استخدام واجهة برمجة التطبيقات المقدمة من Show and Tell لمهام التسميات التوضيحية للصور؟، \"قم بتطوير نظام لتحليل المشاعر بشأن مراجعات المنتجات لتقييم آراء العملاء.\" هل من الممكن استخدام واجهة برمجة التطبيقات التي تقدمها VADER لمهام تحليل المشاعر؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-base-vqav2', 'api_call': \"pipeline('visual-question-answering', model='microsoft/git-base-vqav2')\", 'api_arguments': 'image, question', 'python_environment_requirements': ['transformers'], 'example_code': \"vqa(image='path/to/image.jpg', question='What is in the image?')\", 'performance': {'dataset': 'VQAv2', 'accuracy': 'Refer to the paper for evaluation results'}, 'description': 'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.' </s>"
"<s>[INST]\"قم بإنشاء نظام يحلل المشاعر في منشورات وسائل التواصل الاجتماعي باستخدام نماذج التعلم الآلي.\" \\n -? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'ivelin/donut-refexp-combined-v1', 'api_call': \"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa(image='path/to/image.jpg', question='What is the color of the object?')\", 'performance': {'dataset': 'ivelin/donut-refexp-combined-v1', 'accuracy': 'N/A'}, 'description': 'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.' </s>"
"<s>[INST] تطوير نظام للتعرف على الكيانات المسماة في النصوص القانونية لتحسين استخلاص المعلومات.\\n -? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'ivelin/donut-refexp-combined-v1', 'api_call': \"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa(image='path/to/image.jpg', question='What is the color of the object?')\", 'performance': {'dataset': 'ivelin/donut-refexp-combined-v1', 'accuracy': 'N/A'}, 'description': 'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.' </s>"
"<s>[INST] إنشاء نظام لتلخيص النص لإنشاء ملخصات موجزة للمستندات القانونية المطولة تلقائيًا.\\n -? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'ivelin/donut-refexp-combined-v1', 'api_call': \"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa(image='path/to/image.jpg', question='What is the color of the object?')\", 'performance': {'dataset': 'ivelin/donut-refexp-combined-v1', 'accuracy': 'N/A'}, 'description': 'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.' </s>"
"<s>[INST] إنشاء نظام لتحليل مشاعر مراجعات العملاء لفهم آراء العملاء وملاحظاتهم.\\n -? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'ivelin/donut-refexp-combined-v1', 'api_call': \"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa(image='path/to/image.jpg', question='What is the color of the object?')\", 'performance': {'dataset': 'ivelin/donut-refexp-combined-v1', 'accuracy': 'N/A'}, 'description': 'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.' </s>"
"<s>[INST] تطوير نظام لتصنيف النصوص لتصنيف المستندات القانونية إلى موضوعات أو سمات ذات صلة.\\n -? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'ivelin/donut-refexp-combined-v1', 'api_call': \"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa(image='path/to/image.jpg', question='What is the color of the object?')\", 'performance': {'dataset': 'ivelin/donut-refexp-combined-v1', 'accuracy': 'N/A'}, 'description': 'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.' </s>"
"<s>[INST] إنشاء نظام لتحليل تشابه المستندات لتحديد أوجه التشابه بين النصوص القانونية لأغراض المقارنة.\\n -، إنشاء نظام لاستخراج العبارات الرئيسية في المستندات القانونية لتحديد المصطلحات والمفاهيم المهمة.\\n -، تطوير نظام لنمذجة اللغة لإنشاء نص قانوني يعتمد على مطالبات أو سياقات معينة.\\n -? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'ivelin/donut-refexp-combined-v1', 'api_call': \"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa(image='path/to/image.jpg', question='What is the color of the object?')\", 'performance': {'dataset': 'ivelin/donut-refexp-combined-v1', 'accuracy': 'N/A'}, 'description': 'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.' </s>"
"<s>[INST] إنشاء نظام لتلخيص المستندات لتكثيف الملخصات القانونية المطولة في ملخصات موجزة للمراجعة السريعة.\\n -? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'ivelin/donut-refexp-combined-v1', 'api_call': \"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa(image='path/to/image.jpg', question='What is the color of the object?')\", 'performance': {'dataset': 'ivelin/donut-refexp-combined-v1', 'accuracy': 'N/A'}, 'description': 'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.' </s>"
"<s>[INST] إنشاء نظام لإنشاء النص لأتمتة صياغة الاتفاقيات أو العقود القانونية استناداً إلى معايير محددة.\\n -? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'ivelin/donut-refexp-combined-v1', 'api_call': \"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': \"vqa(image='path/to/image.jpg', question='What is the color of the object?')\", 'performance': {'dataset': 'ivelin/donut-refexp-combined-v1', 'accuracy': 'N/A'}, 'description': 'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.' </s>"
"<s>[INST]قم بإنشاء تطبيق يستخدم الـ API لتوليد نصوص فنية توافق مع الصور الرقمية.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-large-vqav2', 'api_call': \"AutoModel.from_pretrained('microsoft/git-large-vqav2')\", 'api_arguments': {'model': 'microsoft/git-large-vqav2', 'task': 'visual-question-answering', 'device': 0}, 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import pipeline; vqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0); results = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})\", 'performance': {'dataset': 'VQAv2', 'accuracy': 'Refer to the paper'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.' </s>"
"<s>[INST] استخدم الـ API لتطوير تطبيق يمكنه تحليل وتصنيف النصوص الأدبية بناءً على أنماطها اللغوية.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-large-vqav2', 'api_call': \"AutoModel.from_pretrained('microsoft/git-large-vqav2')\", 'api_arguments': {'model': 'microsoft/git-large-vqav2', 'task': 'visual-question-answering', 'device': 0}, 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import pipeline; vqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0); results = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})\", 'performance': {'dataset': 'VQAv2', 'accuracy': 'Refer to the paper'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.' </s>"
"<s>[INST] عين الـ API لتصميم تطبيق يستخدم الذكاء الاصطناعي لتوصيف الصور الشخصية والعثور على التفاصيل الفريدة بها.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-large-vqav2', 'api_call': \"AutoModel.from_pretrained('microsoft/git-large-vqav2')\", 'api_arguments': {'model': 'microsoft/git-large-vqav2', 'task': 'visual-question-answering', 'device': 0}, 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import pipeline; vqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0); results = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})\", 'performance': {'dataset': 'VQAv2', 'accuracy': 'Refer to the paper'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.' </s>"
"<s>[INST] تطوير تطبيق يخدم استخدام الـ API لإنشاء نماذج تعليم آلي لرسم التصوير الفني.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-large-vqav2', 'api_call': \"AutoModel.from_pretrained('microsoft/git-large-vqav2')\", 'api_arguments': {'model': 'microsoft/git-large-vqav2', 'task': 'visual-question-answering', 'device': 0}, 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import pipeline; vqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0); results = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})\", 'performance': {'dataset': 'VQAv2', 'accuracy': 'Refer to the paper'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.' </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم الـ API لتحليل العواطف المعبرة في الصور الشخصية.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-large-vqav2', 'api_call': \"AutoModel.from_pretrained('microsoft/git-large-vqav2')\", 'api_arguments': {'model': 'microsoft/git-large-vqav2', 'task': 'visual-question-answering', 'device': 0}, 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import pipeline; vqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0); results = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})\", 'performance': {'dataset': 'VQAv2', 'accuracy': 'Refer to the paper'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.' </s>"
"<s>[INST] إعداد تطبيق يعتمد على الـ API لتوليد صور فنية مستلهمة من روايات أدبية شهيرة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-large-vqav2', 'api_call': \"AutoModel.from_pretrained('microsoft/git-large-vqav2')\", 'api_arguments': {'model': 'microsoft/git-large-vqav2', 'task': 'visual-question-answering', 'device': 0}, 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import pipeline; vqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0); results = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})\", 'performance': {'dataset': 'VQAv2', 'accuracy': 'Refer to the paper'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.' </s>"
"<s>[INST] استخدم الـ API لإنشاء تطبيق يقوم بتحليل الصور الفوتوغرافية الفنية للتعرف على العلامات المائية والتوقيعات الرقمية.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-large-vqav2', 'api_call': \"AutoModel.from_pretrained('microsoft/git-large-vqav2')\", 'api_arguments': {'model': 'microsoft/git-large-vqav2', 'task': 'visual-question-answering', 'device': 0}, 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import pipeline; vqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0); results = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})\", 'performance': {'dataset': 'VQAv2', 'accuracy': 'Refer to the paper'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.' </s>"
"<s>[INST] أنشئ تطبيقًا يستفيد من الـ API لتوليد نماذج لوحات فنية شهيرة باستخدام تقنيات الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-large-vqav2', 'api_call': \"AutoModel.from_pretrained('microsoft/git-large-vqav2')\", 'api_arguments': {'model': 'microsoft/git-large-vqav2', 'task': 'visual-question-answering', 'device': 0}, 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import pipeline; vqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0); results = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})\", 'performance': {'dataset': 'VQAv2', 'accuracy': 'Refer to the paper'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.' </s>"
"<s>[INST] تصميم تطبيق يُعتمد على الـ API لتحليل وتصنيف الصور الفوتوغرافية الفنية وتوفير توصيات للأعمال المماثلة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-large-vqav2', 'api_call': \"AutoModel.from_pretrained('microsoft/git-large-vqav2')\", 'api_arguments': {'model': 'microsoft/git-large-vqav2', 'task': 'visual-question-answering', 'device': 0}, 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import pipeline; vqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0); results = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})\", 'performance': {'dataset': 'VQAv2', 'accuracy': 'Refer to the paper'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.' </s>"
"<s>[INST] إعداد تطبيق يستعين بالـ API لاكتشاف الألوان المستخدمة في الصور الفنية وتوفير بيانات متعلقة بالتنسيق اللوني.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/git-large-vqav2', 'api_call': \"AutoModel.from_pretrained('microsoft/git-large-vqav2')\", 'api_arguments': {'model': 'microsoft/git-large-vqav2', 'task': 'visual-question-answering', 'device': 0}, 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import pipeline; vqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0); results = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})\", 'performance': {'dataset': 'VQAv2', 'accuracy': 'Refer to the paper'}, 'description': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.' </s>"
"<s>[INST]عند بناء نظام تعلم آلي لتحليل صور طبية، يمكن استخدام ال API التالي:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dandelin/vilt-b32-finetuned-vqa', 'api_call': \"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'text': 'How many cats are there?'}, 'python_environment_requirements': {'transformers': 'ViltProcessor, ViltForQuestionAnswering', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': 'from transformers import ViltProcessor, ViltForQuestionAnswering\\nimport requests\\nfrom PIL import Image\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntext = How many cats are there?\\nprocessor = ViltProcessor.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\nmodel = ViltForQuestionAnswering.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\n\\nencoding = processor(image, text, return_tensors=pt)\\noutputs = model(**encoding)\\nlogits = outputs.logits\\nidx = logits.argmax(-1).item()\\nprint(Predicted answer:, model.config.id2label[idx])', 'performance': {'dataset': 'VQAv2', 'accuracy': 'to do'}, 'description': 'Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository.' </s>"
"<s>[INST] لتحليل نصوص الويب وفهم معناها، يمكن استخدام ال API التالي:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dandelin/vilt-b32-finetuned-vqa', 'api_call': \"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'text': 'How many cats are there?'}, 'python_environment_requirements': {'transformers': 'ViltProcessor, ViltForQuestionAnswering', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': 'from transformers import ViltProcessor, ViltForQuestionAnswering\\nimport requests\\nfrom PIL import Image\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntext = How many cats are there?\\nprocessor = ViltProcessor.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\nmodel = ViltForQuestionAnswering.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\n\\nencoding = processor(image, text, return_tensors=pt)\\noutputs = model(**encoding)\\nlogits = outputs.logits\\nidx = logits.argmax(-1).item()\\nprint(Predicted answer:, model.config.id2label[idx])', 'performance': {'dataset': 'VQAv2', 'accuracy': 'to do'}, 'description': 'Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository.' </s>"
"<s>[INST] يمكنك استخدام ال API التالي لتطوير نظام لتحليل مشاهد الفيديو:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dandelin/vilt-b32-finetuned-vqa', 'api_call': \"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'text': 'How many cats are there?'}, 'python_environment_requirements': {'transformers': 'ViltProcessor, ViltForQuestionAnswering', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': 'from transformers import ViltProcessor, ViltForQuestionAnswering\\nimport requests\\nfrom PIL import Image\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntext = How many cats are there?\\nprocessor = ViltProcessor.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\nmodel = ViltForQuestionAnswering.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\n\\nencoding = processor(image, text, return_tensors=pt)\\noutputs = model(**encoding)\\nlogits = outputs.logits\\nidx = logits.argmax(-1).item()\\nprint(Predicted answer:, model.config.id2label[idx])', 'performance': {'dataset': 'VQAv2', 'accuracy': 'to do'}, 'description': 'Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository.' </s>"
"<s>[INST] لتصميم نظام لتحليل معلومات البورصة، يمكنك استخدام ال API التالي:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dandelin/vilt-b32-finetuned-vqa', 'api_call': \"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'text': 'How many cats are there?'}, 'python_environment_requirements': {'transformers': 'ViltProcessor, ViltForQuestionAnswering', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': 'from transformers import ViltProcessor, ViltForQuestionAnswering\\nimport requests\\nfrom PIL import Image\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntext = How many cats are there?\\nprocessor = ViltProcessor.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\nmodel = ViltForQuestionAnswering.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\n\\nencoding = processor(image, text, return_tensors=pt)\\noutputs = model(**encoding)\\nlogits = outputs.logits\\nidx = logits.argmax(-1).item()\\nprint(Predicted answer:, model.config.id2label[idx])', 'performance': {'dataset': 'VQAv2', 'accuracy': 'to do'}, 'description': 'Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository.' </s>"
"<s>[INST] عند بطلب تطوير نظام لتنبؤ بحركة الأسعار، يمكنك الاعتماد على ال API التالي:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dandelin/vilt-b32-finetuned-vqa', 'api_call': \"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'text': 'How many cats are there?'}, 'python_environment_requirements': {'transformers': 'ViltProcessor, ViltForQuestionAnswering', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': 'from transformers import ViltProcessor, ViltForQuestionAnswering\\nimport requests\\nfrom PIL import Image\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntext = How many cats are there?\\nprocessor = ViltProcessor.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\nmodel = ViltForQuestionAnswering.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\n\\nencoding = processor(image, text, return_tensors=pt)\\noutputs = model(**encoding)\\nlogits = outputs.logits\\nidx = logits.argmax(-1).item()\\nprint(Predicted answer:, model.config.id2label[idx])', 'performance': {'dataset': 'VQAv2', 'accuracy': 'to do'}, 'description': 'Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository.' </s>"
"<s>[INST] لإنشاء نظام لتصنيف الصور الطبية، يمكنك استخدام ال API التالي:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dandelin/vilt-b32-finetuned-vqa', 'api_call': \"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'text': 'How many cats are there?'}, 'python_environment_requirements': {'transformers': 'ViltProcessor, ViltForQuestionAnswering', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': 'from transformers import ViltProcessor, ViltForQuestionAnswering\\nimport requests\\nfrom PIL import Image\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntext = How many cats are there?\\nprocessor = ViltProcessor.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\nmodel = ViltForQuestionAnswering.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\n\\nencoding = processor(image, text, return_tensors=pt)\\noutputs = model(**encoding)\\nlogits = outputs.logits\\nidx = logits.argmax(-1).item()\\nprint(Predicted answer:, model.config.id2label[idx])', 'performance': {'dataset': 'VQAv2', 'accuracy': 'to do'}, 'description': 'Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository.' </s>"
"<s>[INST] لتحسين جودة الصور الطبية باستخدام تقنيات التعلم الآلي، يمكنك الاعتماد على ال API التالي:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dandelin/vilt-b32-finetuned-vqa', 'api_call': \"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'text': 'How many cats are there?'}, 'python_environment_requirements': {'transformers': 'ViltProcessor, ViltForQuestionAnswering', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': 'from transformers import ViltProcessor, ViltForQuestionAnswering\\nimport requests\\nfrom PIL import Image\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntext = How many cats are there?\\nprocessor = ViltProcessor.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\nmodel = ViltForQuestionAnswering.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\n\\nencoding = processor(image, text, return_tensors=pt)\\noutputs = model(**encoding)\\nlogits = outputs.logits\\nidx = logits.argmax(-1).item()\\nprint(Predicted answer:, model.config.id2label[idx])', 'performance': {'dataset': 'VQAv2', 'accuracy': 'to do'}, 'description': 'Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository.' </s>"
"<s>[INST] عند تطوير نظام لتحليل الشفرات المصدرية، يمكن استخدام ال API التالي:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dandelin/vilt-b32-finetuned-vqa', 'api_call': \"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'text': 'How many cats are there?'}, 'python_environment_requirements': {'transformers': 'ViltProcessor, ViltForQuestionAnswering', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': 'from transformers import ViltProcessor, ViltForQuestionAnswering\\nimport requests\\nfrom PIL import Image\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntext = How many cats are there?\\nprocessor = ViltProcessor.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\nmodel = ViltForQuestionAnswering.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\n\\nencoding = processor(image, text, return_tensors=pt)\\noutputs = model(**encoding)\\nlogits = outputs.logits\\nidx = logits.argmax(-1).item()\\nprint(Predicted answer:, model.config.id2label[idx])', 'performance': {'dataset': 'VQAv2', 'accuracy': 'to do'}, 'description': 'Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository.' </s>"
"<s>[INST] لإنشاء نظام لتحليل بيانات العملاء والتنبؤ بسلوكهم، يمكنك الاستفادة من ال API التالي:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dandelin/vilt-b32-finetuned-vqa', 'api_call': \"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'text': 'How many cats are there?'}, 'python_environment_requirements': {'transformers': 'ViltProcessor, ViltForQuestionAnswering', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': 'from transformers import ViltProcessor, ViltForQuestionAnswering\\nimport requests\\nfrom PIL import Image\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntext = How many cats are there?\\nprocessor = ViltProcessor.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\nmodel = ViltForQuestionAnswering.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\n\\nencoding = processor(image, text, return_tensors=pt)\\noutputs = model(**encoding)\\nlogits = outputs.logits\\nidx = logits.argmax(-1).item()\\nprint(Predicted answer:, model.config.id2label[idx])', 'performance': {'dataset': 'VQAv2', 'accuracy': 'to do'}, 'description': 'Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository.' </s>"
"<s>[INST] يمكنك استخدام ال API التالي لتنفيذ نظام للكشف عن الاحتيال في معاملات الدفع:? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dandelin/vilt-b32-finetuned-vqa', 'api_call': \"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'text': 'How many cats are there?'}, 'python_environment_requirements': {'transformers': 'ViltProcessor, ViltForQuestionAnswering', 'requests': 'requests', 'PIL': 'Image'}, 'example_code': 'from transformers import ViltProcessor, ViltForQuestionAnswering\\nimport requests\\nfrom PIL import Image\\n\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntext = How many cats are there?\\nprocessor = ViltProcessor.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\nmodel = ViltForQuestionAnswering.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\n\\nencoding = processor(image, text, return_tensors=pt)\\noutputs = model(**encoding)\\nlogits = outputs.logits\\nidx = logits.argmax(-1).item()\\nprint(Predicted answer:, model.config.id2label[idx])', 'performance': {'dataset': 'VQAv2', 'accuracy': 'to do'}, 'description': 'Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository.' </s>"
"<s>[INST]قم بتطوير نظام لتحليل المشاعر على تعليقات وسائل التواصل الاجتماعي لتوفير رؤى لاستراتيجيات التسويق. ما هي واجهة برمجة التطبيقات (API) التي توصي بها لهذه المهمة؟، تنفيذ نظام توصية لمنتجات التجارة الإلكترونية استنادًا إلى سجل تصفح المستخدم. ما هي واجهة برمجة التطبيقات المناسبة لهذه المهمة؟، قم بتطوير أداة تلخيص المقالات الإخبارية لمساعدة الصحفيين في فهم النقاط الرئيسية بسرعة. ما هي واجهة برمجة التطبيقات التي يمكنك استخدامها لهذا الغرض؟، قم بإنشاء روبوت محادثة لدعم العملاء للتعامل مع الاستفسارات الشائعة وتقديم استجابات فورية. ما هي واجهة برمجة التطبيقات التي توصي بها لبناء برنامج الدردشة الآلي؟، \"تطوير نظام تحويل الكلام إلى نص لتحويل التسجيلات الصوتية إلى مستندات نصية لأغراض التحليل.\" ما هي واجهة برمجة التطبيقات التي ينبغي استخدامها لهذه المهمة؟، قم ببناء نظام لاكتشاف الكائنات في الصور باستخدام التعلم العميق لمساعدة عميلك في مشروع الأمان والمراقبة الخاص به. ما هي واجهة برمجة التطبيقات (API) التي ستكون مناسبة لتنفيذ هذا النظام؟، \"قم بتطوير أداة تحليل المشاعر لتحليل مراجعات العملاء وتعليقاتهم لاستخراج الأفكار لتحسين المنتج.\" ما هي واجهة برمجة التطبيقات التي توصي بها لتحليل المشاعر؟، \"تنفيذ نظام توصية لمنصات بث الأفلام استنادًا إلى تفضيلات المستخدم وسجل العرض.\" ما هي واجهة برمجة التطبيقات المثالية لبناء نظام التوصيات هذا؟، \"تصميم نظام لترجمة اللغة الآلية للمساعدة في ترجمة المستندات والتواصل بين الفرق متعددة اللغات.\" ما هي واجهة برمجة التطبيقات التي ينبغي استخدامها لهذه المهمة؟، قم بتطوير نظام لتصنيف النص لتصنيف المقالات الإخبارية إلى موضوعات مختلفة لتسهيل التنقل. ما هي واجهة برمجة التطبيقات الموصى بها لبناء نظام تصنيف النص هذا؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'blip-vqa-base', 'api_call': \"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-base')\", 'api_arguments': {'raw_image': 'Image', 'question': 'String'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForQuestionAnswering', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-base)\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'VQA', 'accuracy': '+1.6% in VQA score'}, 'description': 'BLIP is a Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is trained on visual question answering with a base architecture (using ViT base backbone).' </s>"
"<s>[INST]لدينا حاجة لتطبيق قادر على تحليل المشاعر في النصوص والتأكد من صحة المعلومات. هل يتوفر API مناسب لهذه المهمة؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'Salesforce/blip-vqa-capfilt-large', 'api_call': \"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\", 'api_arguments': {'raw_image': 'RGB image', 'question': 'string'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForQuestionAnswering'}, 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'VQA', 'accuracy': '+1.6% in VQA score'}, 'description': 'BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.' </s>"
"<s>[INST] هل يمكننا استخدام تقنية التعلم العميق لتحليل صور الأفلام القديمة واستخراج وصف موجز لكل صورة؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'Salesforce/blip-vqa-capfilt-large', 'api_call': \"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\", 'api_arguments': {'raw_image': 'RGB image', 'question': 'string'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForQuestionAnswering'}, 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'VQA', 'accuracy': '+1.6% in VQA score'}, 'description': 'BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.' </s>"
"<s>[INST] هل بإمكاننا تطوير نظام يستخدم التعلم الآلي لتحليل الصور واقتراح وصف دقيق لكل صورة باستخدام API من Hugging Face Transformers؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'Salesforce/blip-vqa-capfilt-large', 'api_call': \"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\", 'api_arguments': {'raw_image': 'RGB image', 'question': 'string'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForQuestionAnswering'}, 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'VQA', 'accuracy': '+1.6% in VQA score'}, 'description': 'BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.' </s>"
"<s>[INST] نود تحليل المشاعر في النصوص وتحقق صحة المعلومات، هل يوجد واجهة برمجية متوفرة لهذا الغرض؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'Salesforce/blip-vqa-capfilt-large', 'api_call': \"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\", 'api_arguments': {'raw_image': 'RGB image', 'question': 'string'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForQuestionAnswering'}, 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'VQA', 'accuracy': '+1.6% in VQA score'}, 'description': 'BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.' </s>"
"<s>[INST] نرغب في تطوير نظام ينفذ التعلم الآلي لتحليل الصور وتقديم اقتراح لوصفها بدقة، هل يمكن استخدام API من Hugging Face Transformers لهذا الغرض؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'Salesforce/blip-vqa-capfilt-large', 'api_call': \"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\", 'api_arguments': {'raw_image': 'RGB image', 'question': 'string'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForQuestionAnswering'}, 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'VQA', 'accuracy': '+1.6% in VQA score'}, 'description': 'BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.' </s>"
"<s>[INST] عندنا حاجة لنظام قادر على تحليل المشاعر في النصوص والتحقق من صحة المعلومات. هل تتوفر API مناسبة لتلبية هذه الاحتياجات؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'Salesforce/blip-vqa-capfilt-large', 'api_call': \"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\", 'api_arguments': {'raw_image': 'RGB image', 'question': 'string'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForQuestionAnswering'}, 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'VQA', 'accuracy': '+1.6% in VQA score'}, 'description': 'BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.' </s>"
"<s>[INST] هل يمكننا تطبيق تقنية التعلم العميق لتحليل صور الأفلام القديمة واستخراج وصف قصير لكل صورة؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'Salesforce/blip-vqa-capfilt-large', 'api_call': \"BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')\", 'api_arguments': {'raw_image': 'RGB image', 'question': 'string'}, 'python_environment_requirements': {'transformers': 'BlipProcessor, BlipForQuestionAnswering'}, 'example_code': \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", 'performance': {'dataset': 'VQA', 'accuracy': '+1.6% in VQA score'}, 'description': 'BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.' </s>"
"<s>[INST]تكامل خاصية استشعار الحركة والتصوير البصري في تطبيقك للتعرف على الأشياء والحركات من حوله.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textvqa', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'TextVQA', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] عقد وثائق API لترجمة نصوص بين عدة لغات في تطبيقك.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textvqa', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'TextVQA', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] تفعيل تعلم آلي لتحليل مشاكل الصحة النفسية في تطبيقك.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textvqa', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'TextVQA', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] استخدام API لتحليل بيانات المرور وتوقع الازدحامات في الشوارع.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textvqa', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'TextVQA', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] تكامل واجهة برمجة التطبيقات للتعرف على نصوص الصور في تطبيقك.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textvqa', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'TextVQA', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] استخدام API لتوليد نصوص تلقائيًا استنادًا إلى مدخلات لغوية محددة في تطبيقك.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textvqa', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'TextVQA', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] تكامل API لتحليل نغمات الصوت والتعرف على الأصوات الموسيقية في تطبيق لتحرير الصوتيات.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textvqa', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'TextVQA', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] تدريب نظام ذكاء اصطناعي على تصنيف الصور الطبية وكشف الأمراض فيها.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textvqa', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'TextVQA', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] تضمين واجهة برمجة تطبيقات للتفاعل مع قاعدة بيانات غير متصلة في تطبيق الويب الخاص بك.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textvqa', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'TextVQA', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST] استخدام API لتحليل الموارد الطبيعية والتنبؤ بالكوارث البيئية في منطقتك.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'git-large-textvqa', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\", 'api_arguments': 'image, question', 'python_environment_requirements': 'transformers', 'example_code': 'For code examples, we refer to the documentation.', 'performance': {'dataset': 'TextVQA', 'accuracy': 'See table 11 in the paper for more details.'}, 'description': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\" </s>"
"<s>[INST]قم بتطوير تطبيق يمكنه تحليل اللغة العربية في الصور واستخراج المعلومات الرئيسية باستخدام واجهة برمجة التطبيقات \"hf-transformers/arabic-text-analysis\".? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering', 'api_call': \"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file', 'question': 'your_question'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random model for Visual Question Answering using the VILT framework.' </s>"
"<s>[INST] انشئ واجهة برمجة تطبيقات تستخدم خوارزمية لتحليل محتوى الصوت وكتابته.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering', 'api_call': \"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file', 'question': 'your_question'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random model for Visual Question Answering using the VILT framework.' </s>"
"<s>[INST] انشئ نظام تخطيط لتوجيه المركبات داخل ميناء بحري باستخدام تقنيات الذكاء الاصطناعي.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering', 'api_call': \"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file', 'question': 'your_question'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random model for Visual Question Answering using the VILT framework.' </s>"
"<s>[INST] ضع خريطة تفاعلية على موقع الويب تعرض معلومات جغرافية باستخدام واجهة برمجة تطبيقات \"mapbox/maps-gl\".? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering', 'api_call': \"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file', 'question': 'your_question'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random model for Visual Question Answering using the VILT framework.' </s>"
"<s>[INST] اكتب تطبيق يستخدم تقنيات تعلم الآلة لتوقع احتياجات المستخدمين وتوفير تجربة تسوق مخصصة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering', 'api_call': \"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file', 'question': 'your_question'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random model for Visual Question Answering using the VILT framework.' </s>"
"<s>[INST] تطوير نظام ذكاء اصطناعي للتعرف على الأمراض الجلدية من الصور الطبية.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering', 'api_call': \"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file', 'question': 'your_question'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random model for Visual Question Answering using the VILT framework.' </s>"
"<s>[INST] عمل نظام لتحليل سلوك المستخدمين على المواقع الإلكترونية باستخدام خوارزميات التعلم الآلي.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering', 'api_call': \"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file', 'question': 'your_question'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random model for Visual Question Answering using the VILT framework.' </s>"
"<s>[INST] طور تطبيقا يستخدم تقنيات الواقع الافتراضي لتدريب طلاب الطب على إجراء الجراحات.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering', 'api_call': \"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file', 'question': 'your_question'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random model for Visual Question Answering using the VILT framework.' </s>"
"<s>[INST] إنشاء أداة تحليل بيانات تمكن المستخدمين من تحويل نصوص الإنترنت إلى بيانات كمية.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering', 'api_call': \"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file', 'question': 'your_question'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random model for Visual Question Answering using the VILT framework.' </s>"
"<s>[INST] تطوير تطبيق يستخدم تكنولوجيا البلوكشين لتتبع السلسلة اللوجستية في سلسلة التوريد.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-ViltForQuestionAnswering', 'api_call': \"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file', 'question': 'your_question'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random model for Visual Question Answering using the VILT framework.' </s>"
"<s>[INST]مؤسسة تعليمية تود تطوير نظام تعليمي يستخدم تقنيات التعلم الآلي لتحليل أداء الطلاب وتقديم توجيهات. ننصح باستخدام واجهة برمجية Turing Text توفر وظائف تحليل النصوص والتفاعل مع المستخدمين بشكل ذكي.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'azwierzc/vilt-b32-finetuned-vqa-pl', 'api_call': \"pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')\", 'api_arguments': {'image': 'path_to_image', 'question': 'question_text'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the Polish language.' </s>"
"<s>[INST] شركة تقنية ترغب في تطوير نظام يستخدم التعلم العميق لتحليل البيانات الضخمة في الوقت الفعلي. يمكنك استخدام واجهة برمجية DeepAI Insights لإجراء تحليل متطور للبيانات وتوليد تقارير دقيقة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'azwierzc/vilt-b32-finetuned-vqa-pl', 'api_call': \"pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')\", 'api_arguments': {'image': 'path_to_image', 'question': 'question_text'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the Polish language.' </s>"
"<s>[INST] تطبيق التواصل الاجتماعي يسعى لتحسين تجربة المستخدم في اقتراح المحتوى المناسب. يمكنك استخدام واجهة برمجية Recommendation Engine لتطوير نظام ذكي لاقتراح المحتوى بناءً على تفضيلات المستخدمين.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'azwierzc/vilt-b32-finetuned-vqa-pl', 'api_call': \"pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')\", 'api_arguments': {'image': 'path_to_image', 'question': 'question_text'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the Polish language.' </s>"
"<s>[INST] شركة عقارية تود تنفيذ نظام إلكتروني لإظهار العقارات بشكل ثلاثي الأبعاد للمتعاملين. يمكنك استخدام واجهة برمجية 3D Rendering API لإنشاء صور واقعية للعقارات وتعزيز تجربة المستخدم.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'azwierzc/vilt-b32-finetuned-vqa-pl', 'api_call': \"pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')\", 'api_arguments': {'image': 'path_to_image', 'question': 'question_text'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the Polish language.' </s>"
"<s>[INST] منظمة صحية ترغب في تطوير نظام لاستخدام الذكاء الاصطناعي في تشخيص الأمراض. يمكنك الاعتماد على واجهة برمجية Medical AI الخاصة بالتشخيص الطبي الذكي لتحسين دقة التشخيص وسرعته.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'azwierzc/vilt-b32-finetuned-vqa-pl', 'api_call': \"pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')\", 'api_arguments': {'image': 'path_to_image', 'question': 'question_text'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the Polish language.' </s>"
"<s>[INST] شركة إعلانات ترغب في تحليل البيانات للتفاعل مع العملاء بشكل أفضل. يمكنك استخدام واجهة برمجية Customer Interaction Analysis لتنفيذ تحليلات عميقة وتحسين استراتيجيات التواصل مع العملاء.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'azwierzc/vilt-b32-finetuned-vqa-pl', 'api_call': \"pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')\", 'api_arguments': {'image': 'path_to_image', 'question': 'question_text'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the Polish language.' </s>"
"<s>[INST] شركة تقنية تبحث عن واجهة برمجية لتحديد الملابس والاكسسوارات في الصور بتقنيات تطبق الذكاء الاصطناعي. يمكنك استخدام واجهة برمجية Clothing Detection API لتحويل الصور إلى بيانات قابلة للتحليل والمعالجة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'azwierzc/vilt-b32-finetuned-vqa-pl', 'api_call': \"pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')\", 'api_arguments': {'image': 'path_to_image', 'question': 'question_text'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the Polish language.' </s>"
"<s>[INST] مستشفى يرغب في تحسين منظومة إدارة المواعيد للمرضى. ننصح باستخدام واجهة برمجية Appointment Scheduling API لتسهيل حجز وإدارة المواعيد الطبية بشكل أوتوماتيكي وفعال.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'azwierzc/vilt-b32-finetuned-vqa-pl', 'api_call': \"pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')\", 'api_arguments': {'image': 'path_to_image', 'question': 'question_text'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the Polish language.' </s>"
"<s>[INST] شركة تجارة إلكترونية ترغب في تحسين تجربة التسوق عبر تقنيات التعلم الآلي. يمكنك استخدام واجهة برمجية E-commerce Recommendation لإنشاء نظام توصيات شخصية للعملاء لزيادة مبيعاتك.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'azwierzc/vilt-b32-finetuned-vqa-pl', 'api_call': \"pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')\", 'api_arguments': {'image': 'path_to_image', 'question': 'question_text'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the Polish language.' </s>"
"<s>[INST] منظمة غير ربحية تقدم خدمات تعليمية عبر الإنترنت تبحث عن واجهة برمجية لتوفير دورات تعليمية متقدمة. نقترح استخدام واجهة برمجية Educational Content API لتقديم محتوى تعليمي ذكي ومنظم للطلاب.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'azwierzc/vilt-b32-finetuned-vqa-pl', 'api_call': \"pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')\", 'api_arguments': {'image': 'path_to_image', 'question': 'question_text'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the Polish language.' </s>"
"<s>[INST]تنفيذ نظام لتحليل الصور الطبية وتقديم تشخيص تلقائي للأمراض باستخدام تقنيات التعلم العميق.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'sheldonxxxx/OFA_model_weights', 'api_call': \"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.' </s>"
"<s>[INST] تصميم نظام للتعرف على النصوص العربية المكتوبة يمكنه التفاعل مع تطبيقات الهاتف النقال.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'sheldonxxxx/OFA_model_weights', 'api_call': \"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.' </s>"
"<s>[INST] بناء نظام للتعرف على التعبيرات الوجهية وتحليل المشاعر من الصور الوجهية باستخدام شبكات عصبية عميقة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'sheldonxxxx/OFA_model_weights', 'api_call': \"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.' </s>"
"<s>[INST] تنفيذ نظام للتعرف على الأشياء والمخلوقات البحرية من الصور البحرية باستخدام تقنيات تعلم الآلة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'sheldonxxxx/OFA_model_weights', 'api_call': \"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.' </s>"
"<s>[INST] بناء نظام للاقتراح التلقائي للمستخدمين عناصر غذائية يمكنهم استهلاكها بناءً على أذواقهم واحتياجاتهم الصحية.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'sheldonxxxx/OFA_model_weights', 'api_call': \"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.' </s>"
"<s>[INST] تطوير نظام لتصنيف الأفلام باستخدام التعلم الآلي وتقديم توصيات مخصصة للمستخدمين بناءً على تفضيلاتهم السينمائية.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'sheldonxxxx/OFA_model_weights', 'api_call': \"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.' </s>"
"<s>[INST] بناء نظام لتحليل سلوك المستهلكين عبر التسوق الإلكتروني والتنبؤ بتفضيلاتهم وقرارات الشراء المستقبلية.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'sheldonxxxx/OFA_model_weights', 'api_call': \"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.' </s>"
"<s>[INST] تنفيذ نظام إلكتروني للتعرف على النصوص القانونية واستخراج المعلومات الرئيسية منها بما في ذلك القوانين والبنود والحكم.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'sheldonxxxx/OFA_model_weights', 'api_call': \"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.' </s>"
"<s>[INST] بناء نظام لتحليل أداء الطلاب وتقديم توصيات تعليمية تعتمد على أدائهم السابق واستجابتهم للتعلم.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'sheldonxxxx/OFA_model_weights', 'api_call': \"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.' </s>"
"<s>[INST] تنفيذ نظام للتعرف على وجوه المشاهير في الصور وتحديد هوياتهم باستخدام شبكات تعلم الآلة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'sheldonxxxx/OFA_model_weights', 'api_call': \"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.' </s>"
"<s>[INST]\"قم بإنشاء سير عمل متعدد الخطوات يعالج بيانات الإدخال، وينفذ استخراج الميزات، ويغذي الميزات المستخرجة في نموذج محول متعدد الوسائط للتصنيف.\"، \"تعاون مع فريق رؤية الكمبيوتر لدمج واجهة برمجة تطبيقات الكشف عن الكائنات في الوقت الفعلي في تطبيقك لتحديد وتوطين كائنات متعددة في صورة معينة.، قم بتطوير مسار يقوم بمعالجة البيانات النصية مسبقًا، وإنشاء التضمينات باستخدام BERT، وإدخالها في نموذج تحليل المشاعر الدقيق لتصنيف المشاعر.، تنفيذ روبوت الدردشة نظام يستخدم نموذج GPT-3 مُدرب مسبقًا لإنشاء استجابات شبيهة بالاستجابات البشرية لاستفسارات المستخدم وتفاعلاته.، إنشاء نظام توصيات يستفيد من تقنيات التصفية التعاونية لاقتراح توصيات أفلام مخصصة بناءً على بيانات تفاعل المستخدم.، التكامل. واجهة برمجة التطبيقات للتعرف على الصوت في تطبيقك لتحويل إدخال الصوت المنطوق إلى نص لمزيد من المعالجة والتحليل.، قم بتطوير نظام ترجمة لغة يستخدم نموذج محول لتحويل النص من لغة إلى أخرى بدقة وطلاقة عالية.،  تنفيذ وحدة التعرف على الوجوه التي تستخدم نموذج CNN المُدرب مسبقًا للكشف عن الوجوه والتعرف عليها في الصور أو تدفقات الفيديو.، إنشاء نظام للكشف عن الحالات الشاذة يستخدم نموذج التشفير التلقائي لتحديد الأنماط غير العادية أو القيم المتطرفة في بيانات السلاسل الزمنية للمراقبة وأغراض التنبيه.، \"إنشاء نظام لتوليد الرسم البياني المعرفي الذي يستخدم نموذج الشبكة العصبية للرسم البياني لاستخراج المعلومات المنظمة من البيانات النصية غير المنظمة وبناء العلاقات بين الكيانات.\"? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'vilt-finetuned-vqasi', 'api_call': \"ViltModel.from_pretrained('tufa15nik/vilt-finetuned-vqasi')\", 'api_arguments': {'model': 'tufa15nik/vilt-finetuned-vqasi', 'tokenizer': 'tufa15nik/vilt-finetuned-vqasi'}, 'python_environment_requirements': {'transformers': '>=4.11.3'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Visual Question Answering model fine-tuned on the VQASI dataset by tufa15nik using the ViLT architecture. The model is designed to answer questions based on the content of an input image.' </s>"
"<s>[INST]تحتاج منظمة غير ربحية لتطوير منصة لتعليم اللغات بواسطة النصوص السمعية، استخدم ال API التالي لتحقيق ذلك.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQA', 'api_call': \"pipeline('visual-question-answering', model='GuanacoVQA').\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers, torch', 'example_code': 'N/A', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'N/A'}, 'description': 'A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4.' </s>"
"<s>[INST] شركة تكنولوجيا ترغب في إنشاء نظام ذكاء اصطناعي يستطيع التعرف على أنماط الإستخدام في تطبيقات الهاتف الذكي، يمكنك استخدام ال API التالي لتطبيق ذلك.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQA', 'api_call': \"pipeline('visual-question-answering', model='GuanacoVQA').\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers, torch', 'example_code': 'N/A', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'N/A'}, 'description': 'A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4.' </s>"
"<s>[INST] كيان حكومي يحتاج لتطوير نظام استشاري يستخدم الذكاء الاصطناعي للتحليل اللغوي للوثائق الرسمية، يمكنك استخدام ال API التالي لتنفيذ هذا المشروع.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQA', 'api_call': \"pipeline('visual-question-answering', model='GuanacoVQA').\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers, torch', 'example_code': 'N/A', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'N/A'}, 'description': 'A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4.' </s>"
"<s>[INST] مطورون يرغبون في إنشاء تطبيق لتحليل مستويات السعادة في النصوص الاجتماعية، استخدم ال API التالي للقيام بذلك.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQA', 'api_call': \"pipeline('visual-question-answering', model='GuanacoVQA').\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers, torch', 'example_code': 'N/A', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'N/A'}, 'description': 'A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4.' </s>"
"<s>[INST] شركة تسويق تبحث عن طريقة لتحليل تغريدات العملاء لاستخراج احتياجاتهم، يمكنك استخدام ال API التالي لتصميم الحل المناسب.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQA', 'api_call': \"pipeline('visual-question-answering', model='GuanacoVQA').\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers, torch', 'example_code': 'N/A', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'N/A'}, 'description': 'A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4.' </s>"
"<s>[INST] فريق بحث جامعي يرغب في تطوير نظام لفهم وتحليل أدبيات اللغة العربية القديمة، يستطيع ال API التالي مساعدتك في تحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQA', 'api_call': \"pipeline('visual-question-answering', model='GuanacoVQA').\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers, torch', 'example_code': 'N/A', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'N/A'}, 'description': 'A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4.' </s>"
"<s>[INST] شركة تأمين تستعرض إضافة خاصية التعرف على التوقعات العملية باستخدام الذكاء الصناعي، يمكن استخدام ال API التالي لتحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQA', 'api_call': \"pipeline('visual-question-answering', model='GuanacoVQA').\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers, torch', 'example_code': 'N/A', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'N/A'}, 'description': 'A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4.' </s>"
"<s>[INST] جهة حكومية تعمل على تحسين أداء المدارس ترغب في تطوير أداة لتحليل تقارير التقييم الطلابي، يمكنك استخدام ال API التالي لتصميم التطبيق المناسب.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQA', 'api_call': \"pipeline('visual-question-answering', model='GuanacoVQA').\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers, torch', 'example_code': 'N/A', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'N/A'}, 'description': 'A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4.' </s>"
"<s>[INST] حيث إن شركة عقارات ترغب في تنفيذ نظام تعريف الصور لحفظ العقارات، يمكنك استخدام ال API التالي لتطبيق هذا النظام بكفاءة.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQA', 'api_call': \"pipeline('visual-question-answering', model='GuanacoVQA').\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers, torch', 'example_code': 'N/A', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'N/A'}, 'description': 'A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4.' </s>"
"<s>[INST] قطاع السياحة يبحث عن تطبيق يستخدم التعلم الآلي لتحليل الصور السياحية، استخدم ال API التالي لتصميم التطبيق المطلوب.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQA', 'api_call': \"pipeline('visual-question-answering', model='GuanacoVQA').\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers, torch', 'example_code': 'N/A', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'N/A'}, 'description': 'A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4.' </s>"
"<s>[INST]محتاج لتقديم تشخيص دقيق لإصابة سرطان الثدي باستخدام الصور الطبية، كيف يمكنني استخدام API لبناء نموذج تشخيصي؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'temp_vilt_vqa', 'api_call': \"pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')\", 'api_arguments': {'model': 'Bingsu/temp_vilt_vqa', 'tokenizer': 'Bingsu/temp_vilt_vqa'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A visual question answering model for answering questions related to images using the Hugging Face Transformers library.' </s>"
"<s>[INST] كيف يمكنني إنشاء نموذج للكشف عن وجود أمراض القلب من خلال تحليل التحاليل الطبية باستخدام الـ API؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'temp_vilt_vqa', 'api_call': \"pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')\", 'api_arguments': {'model': 'Bingsu/temp_vilt_vqa', 'tokenizer': 'Bingsu/temp_vilt_vqa'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A visual question answering model for answering questions related to images using the Hugging Face Transformers library.' </s>"
"<s>[INST] أريد تحليل نغمة صوتية للكشف عن التوتر، هل يمكنني استخدام API لتطوير نموذج تنبؤي بالحالة النفسية؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'temp_vilt_vqa', 'api_call': \"pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')\", 'api_arguments': {'model': 'Bingsu/temp_vilt_vqa', 'tokenizer': 'Bingsu/temp_vilt_vqa'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A visual question answering model for answering questions related to images using the Hugging Face Transformers library.' </s>"
"<s>[INST] كيف يمكنني تصميم نظام توصيات لتوصيف الأفلام للمستخدمين بناءً على تفضيلاتهم باستخدام تقنيات الذكاء الاصطناعي؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'temp_vilt_vqa', 'api_call': \"pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')\", 'api_arguments': {'model': 'Bingsu/temp_vilt_vqa', 'tokenizer': 'Bingsu/temp_vilt_vqa'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A visual question answering model for answering questions related to images using the Hugging Face Transformers library.' </s>"
"<s>[INST] أرغب في تدريب نموذج لتحديد مناطق الاهتمام في صور فضائية، هل يمكنني استخدام API لتحقيق ذلك بكفاءة عالية؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'temp_vilt_vqa', 'api_call': \"pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')\", 'api_arguments': {'model': 'Bingsu/temp_vilt_vqa', 'tokenizer': 'Bingsu/temp_vilt_vqa'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A visual question answering model for answering questions related to images using the Hugging Face Transformers library.' </s>"
"<s>[INST] كيف يمكنني تطوير نموذج للتعرف على النصوص الطبية الكتابية باستخدام تقنيات الـ NLP؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'temp_vilt_vqa', 'api_call': \"pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')\", 'api_arguments': {'model': 'Bingsu/temp_vilt_vqa', 'tokenizer': 'Bingsu/temp_vilt_vqa'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A visual question answering model for answering questions related to images using the Hugging Face Transformers library.' </s>"
"<s>[INST] كيف يمكنني استخدام تقنيات تعلم الآلة لتصنيف أنواع النباتات من الصور النباتية؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'temp_vilt_vqa', 'api_call': \"pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')\", 'api_arguments': {'model': 'Bingsu/temp_vilt_vqa', 'tokenizer': 'Bingsu/temp_vilt_vqa'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A visual question answering model for answering questions related to images using the Hugging Face Transformers library.' </s>"
"<s>[INST] ما هي الخطوات اللازمة لاستخدام API لتوليد أفضلية ترجمة النصوص بين اللغات المختلفة؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'temp_vilt_vqa', 'api_call': \"pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')\", 'api_arguments': {'model': 'Bingsu/temp_vilt_vqa', 'tokenizer': 'Bingsu/temp_vilt_vqa'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A visual question answering model for answering questions related to images using the Hugging Face Transformers library.' </s>"
"<s>[INST] كيف يمكنني تحسين نظام البحث في المواقع الإلكترونية باستخدام تقنيات تجميع المعلومات؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'temp_vilt_vqa', 'api_call': \"pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')\", 'api_arguments': {'model': 'Bingsu/temp_vilt_vqa', 'tokenizer': 'Bingsu/temp_vilt_vqa'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A visual question answering model for answering questions related to images using the Hugging Face Transformers library.' </s>"
"<s>[INST] أريد تصميم نظام لتعريف وتصنيف الأشكال الهندسية في الصور، ما هي الطريقة المثلى لتحقيق ذلك باستخدام تقنيات التعلم العميق؟? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Visual Question Answering', 'api_name': 'temp_vilt_vqa', 'api_call': \"pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')\", 'api_arguments': {'model': 'Bingsu/temp_vilt_vqa', 'tokenizer': 'Bingsu/temp_vilt_vqa'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A visual question answering model for answering questions related to images using the Hugging Face Transformers library.' </s>"
"<s>[INST]قم بتطوير نظام لتحسين تجربة التسوق عبر الإنترنت باستخدام API للتعرف على الصور.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'api_call': \"pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')\", 'api_arguments': {'model': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'tokenizer': 'JosephusCheung/GuanacoVQAOnConsumerHardware'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': 'vqa(image_path, question)', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'unknown'}, 'description': 'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.' </s>"
"<s>[INST] إنشاء تطبيق للمساعدة في تحديد فئات الأخبار والمقالات باستخدام API لمحرك تصنيف النصوص.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'api_call': \"pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')\", 'api_arguments': {'model': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'tokenizer': 'JosephusCheung/GuanacoVQAOnConsumerHardware'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': 'vqa(image_path, question)', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'unknown'}, 'description': 'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.' </s>"
"<s>[INST] بناء نظام للتعرف على الكائنات في الفيديوهات باستخدام API للرؤية الحاسوبية.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'api_call': \"pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')\", 'api_arguments': {'model': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'tokenizer': 'JosephusCheung/GuanacoVQAOnConsumerHardware'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': 'vqa(image_path, question)', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'unknown'}, 'description': 'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم API لتحسين جودة الصور الطبية وزيادة وضوحها.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'api_call': \"pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')\", 'api_arguments': {'model': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'tokenizer': 'JosephusCheung/GuanacoVQAOnConsumerHardware'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': 'vqa(image_path, question)', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'unknown'}, 'description': 'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.' </s>"
"<s>[INST] إنشاء نظام لتحسين كفاءة عمليات الإنتاج باستخدام API للتنبؤ بالأعطال في المعدات.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'api_call': \"pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')\", 'api_arguments': {'model': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'tokenizer': 'JosephusCheung/GuanacoVQAOnConsumerHardware'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': 'vqa(image_path, question)', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'unknown'}, 'description': 'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.' </s>"
"<s>[INST] بناء نظام لتحسين التفاعل مع المستخدمين عبر الدردشة الآلية باستخدام API لمحرك اللغة الطبيعية.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'api_call': \"pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')\", 'api_arguments': {'model': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'tokenizer': 'JosephusCheung/GuanacoVQAOnConsumerHardware'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': 'vqa(image_path, question)', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'unknown'}, 'description': 'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.' </s>"
"<s>[INST] بناء تطبيق يستخدم API للكشف عن علامات التشوه الوراثي في الصور الطبية.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'api_call': \"pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')\", 'api_arguments': {'model': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'tokenizer': 'JosephusCheung/GuanacoVQAOnConsumerHardware'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': 'vqa(image_path, question)', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'unknown'}, 'description': 'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.' </s>"
"<s>[INST] إنشاء نظام للكشف عن الاحتيال في الدفعات الرقمية باستخدام API لتحليل النصوص وتعريف نمط الغش.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'api_call': \"pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')\", 'api_arguments': {'model': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'tokenizer': 'JosephusCheung/GuanacoVQAOnConsumerHardware'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': 'vqa(image_path, question)', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'unknown'}, 'description': 'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.' </s>"
"<s>[INST] بناء تطبيق يستخدم API لتعزيز تجربة المستخدم في التصفح عبر الويب من خلال تصنيف المحتوى وتوصيات شخصية.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'api_call': \"pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')\", 'api_arguments': {'model': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'tokenizer': 'JosephusCheung/GuanacoVQAOnConsumerHardware'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': 'vqa(image_path, question)', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'unknown'}, 'description': 'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.' </s>"
"<s>[INST] بناء نظام لتحليل السلوك العملائي وتوقع احتياجاتهم باستخدام API للتعلم الآلي.? [/INST]'domain': 'Multimodal Visual Question Answering', 'framework': 'Hugging Face', 'functionality': 'Visual Question Answering', 'api_name': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'api_call': \"pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')\", 'api_arguments': {'model': 'JosephusCheung/GuanacoVQAOnConsumerHardware', 'tokenizer': 'JosephusCheung/GuanacoVQAOnConsumerHardware'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': 'vqa(image_path, question)', 'performance': {'dataset': 'JosephusCheung/GuanacoVQADataset', 'accuracy': 'unknown'}, 'description': 'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.' </s>"
"<s>[INST]احتاج عميلنا إلى واجهة برمجية تمكّن تطبيقه من تحويل النصوص العربية إلى كلام مسموع لمستخدميه.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': ['SQuAD2.0', 'DocVQA'], 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] لدى شركتنا حاجة مستعجلة لتطبيق يقوم بالتعرف على النصوص في الصور وتحويلها إلى نص قابل للتحرير.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': ['SQuAD2.0', 'DocVQA'], 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] تملك شركتنا موقع إلكتروني وترغب في تحسين تجربة المستخدم بإدراج نظام إجابة تلقائية للأسئلة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': ['SQuAD2.0', 'DocVQA'], 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] يخطط عميلنا لتطوير تطبيق مطعم ذكي، يحتاج إلى توليد قوائم الطعام الخاصة بالمطعم بشكل ديناميكي.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': ['SQuAD2.0', 'DocVQA'], 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] نحن بحاجة في شركتنا إلى API يدعم تقديم تحليل نصوص سريع وفعال لمساعدتنا في مراجعة المحتوى الكبير.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': ['SQuAD2.0', 'DocVQA'], 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] نود تطوير تطبيق للتعلم الآلي يحول النصوص الإنجليزية إلى لغة آخرى بشكل سلس ودقيق.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': ['SQuAD2.0', 'DocVQA'], 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] عميلنا يعمل على تطوير منصة تواصل اجتماعي، يبحث عن API يمكّن من تحليل مشاركات المستخدمين لاكتشاف اهتماماتهم.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': ['SQuAD2.0', 'DocVQA'], 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] لدينا مشروع لتطوير تطبيق لإنشاء ملصقات فنية، نرغب في استخدام واجهة برمجية لتوليد نص على هذه الملصقات.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': ['SQuAD2.0', 'DocVQA'], 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] نود إدراج ميزة تعريب النصوص في تطبيقنا، نبحث عن واجهة برمجية توفر خدمة تحويل النصوص الإنجليزية إلى نصوص عربية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': ['SQuAD2.0', 'DocVQA'], 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST]اكتب قصة قصيرة تبدأ بـ \"في قرية سحرية تعيش فيها أسرار عظيمة\". استخدم واجهة برمجة التطبيقات المتوفرة لتحويل هذه الجملة إلى صورة تعكس بداية القصة.، قم بتطوير برنامج يستخدم تقنية معالجة اللغات الطبيعية لفهم المشاعر في النصوص.، تحتاج شركة السيارات إلى تطوير نظام صيانة تنبؤي من أجل تقليل الأعطال غير المتوقعة للمركبة. كيف يمكن تطبيق تقنيات التعلم الآلي لهذا الغرض؟، إنشاء حوار خيالي بين شخصيتين يناقشان تأثير الذكاء الاصطناعي على المجتمع. استخدم واجهة برمجة التطبيقات المتوفرة لتصور هذه المحادثة.، اكتب قصيدة عن جمال الطبيعة باستخدام الاستعارات والرمزية. استخدم واجهة برمجة التطبيقات (API) لإنشاء تمثيل فني للقصيدة.، صمم قصة قصيرة تدور أحداثها في عالم مستقبلي حيث تجاوزت التكنولوجيا المشاعر الإنسانية. استخدم واجهة برمجة التطبيقات (API) لتصور مشهد رئيسي من القصة.، اكتب مقترحًا حول استخدام روبوتات الدردشة التي تعمل بالذكاء الاصطناعي لتعزيز خدمة العملاء في صناعة الضيافة. استخدم واجهة برمجة التطبيقات لتصور التنفيذ المحتمل لروبوتات الدردشة.، قم بتطوير سرد يصف رحلة عبر محاكاة الواقع الافتراضي. استخدم واجهة برمجة التطبيقات (API) لإنشاء رسم توضيحي يلتقط لحظة محورية في الرحلة.، اكتب مقالًا علميًا يشرح فوائد وتحديات تطبيق تقنية blockchain في إدارة سلسلة التوريد. استخدم واجهة برمجة التطبيقات لإنشاء أداة مساعدة مرئية للمقالة.، \"أنشئ قصة حول مجموعة من المغامرين الذين يستكشفون موقع آثار قديمة. استخدم واجهة برمجة التطبيقات لإنشاء صورة للاكتشاف الأول للمغامرين داخل الآثار.\"? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased-finetuned-docvqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.12.2', 'torch==1.8.0+cu101', 'datasets==1.14.0', 'tokenizers==0.10.3'], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 1.194}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST]تطبيق API لتحليل وترجمة النصوص التقنية إلى لغة مفهومة للجمهور العام.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'xhyi/layoutlmv3_docvqa_t11c5000', 'api_call': \"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'LayoutLMv3 model trained for document question answering task.' </s>"
"<s>[INST] تستخدم API لتطوير نظام تحليل الصور الطبية واستخراج المعلومات الطبية الهامة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'xhyi/layoutlmv3_docvqa_t11c5000', 'api_call': \"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'LayoutLMv3 model trained for document question answering task.' </s>"
"<s>[INST] قم بتجربة API لتحليل بيانات المرور وتوليد تقارير أمان الطرق.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'xhyi/layoutlmv3_docvqa_t11c5000', 'api_call': \"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'LayoutLMv3 model trained for document question answering task.' </s>"
"<s>[INST] استخدم API لتصميم نظام لتحليل المحتوى الاجتماعي ومراقبة التفاعلات الإلكترونية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'xhyi/layoutlmv3_docvqa_t11c5000', 'api_call': \"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'LayoutLMv3 model trained for document question answering task.' </s>"
"<s>[INST] حدد البيئة لتطبيق API لتحليل سلوك المستخدمين على الويب.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'xhyi/layoutlmv3_docvqa_t11c5000', 'api_call': \"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'LayoutLMv3 model trained for document question answering task.' </s>"
"<s>[INST] قم بالاستعلام عن API لتوليد مخططات بيانات توضيحية لتحليل الأداء التجاري.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'xhyi/layoutlmv3_docvqa_t11c5000', 'api_call': \"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'LayoutLMv3 model trained for document question answering task.' </s>"
"<s>[INST] استخدم API لتصميم نظام لتحليل وتصنيف النصوص القانونية تلقائيًا.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'xhyi/layoutlmv3_docvqa_t11c5000', 'api_call': \"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'LayoutLMv3 model trained for document question answering task.' </s>"
"<s>[INST] يمكنك الاعتماد على API لتطوير نظام لتحليل البيانات الطبية واكتشاف الأنماط الطبية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'xhyi/layoutlmv3_docvqa_t11c5000', 'api_call': \"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'LayoutLMv3 model trained for document question answering task.' </s>"
"<s>[INST] تطبيق API لتوجيه نظام لتحليل النصوص الأدبية وتوليد تفسيرات نصية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'xhyi/layoutlmv3_docvqa_t11c5000', 'api_call': \"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'LayoutLMv3 model trained for document question answering task.' </s>"
"<s>[INST] استفد من API لتحليل البلاعات الرقمية والكشف عن أنماط الاحتيال الإلكتروني.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'xhyi/layoutlmv3_docvqa_t11c5000', 'api_call': \"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'LayoutLMv3 model trained for document question answering task.' </s>"
"<s>[INST]قم بتطوير تطبيق يقوم بتحويل الخطوط اليدوية إلى نصوص قابلة للتحرير.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-invoices', 'api_call': \"pipeline('question-answering', model='impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"qa_pipeline(question='your question', context='your document context')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'not provided'}, 'description': 'This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] لتنفيذ محرك بحث للبحث في قواعد المعرفة، يجب استخدام API للبحث النصي في الوثائق.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-invoices', 'api_call': \"pipeline('question-answering', model='impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"qa_pipeline(question='your question', context='your document context')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'not provided'}, 'description': 'This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] ابنِ تطبيقًا يستخدم تقنية التعلم الآلي لتحليل مشاعر النصوص وتصنيفها، يجب استخدام API لتحليل المشاعر.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-invoices', 'api_call': \"pipeline('question-answering', model='impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"qa_pipeline(question='your question', context='your document context')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'not provided'}, 'description': 'This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] تنفيذ نظام لتحليل ومقارنة بيانات السوق لشركات محددة، تأتي API لاسترجاع وتحليل البيانات الحيوية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-invoices', 'api_call': \"pipeline('question-answering', model='impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"qa_pipeline(question='your question', context='your document context')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'not provided'}, 'description': 'This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] لتنفيذ تطبيق يقدم توصيات شخصية للأفلام، يحتاج النظام إلى API لاسترجاع معلومات الأفلام.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-invoices', 'api_call': \"pipeline('question-answering', model='impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"qa_pipeline(question='your question', context='your document context')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'not provided'}, 'description': 'This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] إنشاء تطبيق يعتمد على تحليل لغة الإشارة، يجب استخدام API لتحويل الصور إلى نص مفهوم.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-invoices', 'api_call': \"pipeline('question-answering', model='impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"qa_pipeline(question='your question', context='your document context')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'not provided'}, 'description': 'This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] لتنفيذ نظام لتحليل وتصنيف العملاء بناءً على سجلاتهم، يتعين استخدام API لتحويل البيانات النصية إلى معلومات ذاتية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-invoices', 'api_call': \"pipeline('question-answering', model='impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"qa_pipeline(question='your question', context='your document context')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'not provided'}, 'description': 'This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] لتنفيذ تطبيق يقوم بتحليل مراكز التسوق وتقديم توجيهات داخلها، يحتاج النظام إلى API للتعرف على المواقع الداخلية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-invoices', 'api_call': \"pipeline('question-answering', model='impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"qa_pipeline(question='your question', context='your document context')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'not provided'}, 'description': 'This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] لتنفيذ نظام لتحليل النصوص الطبية، يحتاج النظام إلى API لتوجيه التحليلات النصية الطبية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-invoices', 'api_call': \"pipeline('question-answering', model='impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"qa_pipeline(question='your question', context='your document context')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'not provided'}, 'description': 'This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] ابنِ تطبيقًا يقوم بمقارنة ترجمات النصوص بين لغات مختلفة، يحتاج النظام إلى API لترجمة النصوص.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-invoices', 'api_call': \"pipeline('question-answering', model='impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"qa_pipeline(question='your question', context='your document context')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'not provided'}, 'description': 'This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST]? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'dperales/layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"LayoutLMv2ForQuestionAnswering.from_pretrained('dperales/layoutlmv2-base-uncased_finetuned_docvqa')\", 'api_arguments': {'model': 'dperales/layoutlmv2-base-uncased_finetuned_docvqa'}, 'python_environment_requirements': {'transformers': 'latest'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for Document Question Answering based on the LayoutLMv2 architecture, fine-tuned on the DocVQA dataset.' </s>"
"<s>[INST]أرغب في تطوير تطبيق يمكنه تحليل وتصنيف الرسومات التخطيطية للهندسة المعمارية. هل يمكنك توجيهي نحو API مناسب لهذا الغرض؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'vision-encoder-decoder', 'api_name': 'naver-clova-ix/donut-base-finetuned-docvqa', 'api_call': \"pipeline('document-question-answering', model='donut-base-finetuned-docvqa')\", 'api_arguments': {'image': 'path_to_image', 'question': 'your_question'}, 'python_environment_requirements': 'Transformers', 'example_code': \"from transformers import pipeline\\n\\n# Initialize the pipeline\\ndoc_qa = pipeline('document-question-answering', model='naver-clova-ix/donut-base-finetuned-docvqa')\\n\\n# Load an image and ask a question\\nimage_path = 'path_to_image'\\nquestion = 'your_question'\\n\\n# Get the answer\\nanswer = doc_qa({'image': image_path, 'question': question})\\nprint(answer)\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository. Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] نود تطوير تطبيق يستطيع تمييز أنماط الكتابة اليدوية والتعرف على الحروف. هل تستطيع توجيهنا إلى API مناسب لهذا الغرض؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'vision-encoder-decoder', 'api_name': 'naver-clova-ix/donut-base-finetuned-docvqa', 'api_call': \"pipeline('document-question-answering', model='donut-base-finetuned-docvqa')\", 'api_arguments': {'image': 'path_to_image', 'question': 'your_question'}, 'python_environment_requirements': 'Transformers', 'example_code': \"from transformers import pipeline\\n\\n# Initialize the pipeline\\ndoc_qa = pipeline('document-question-answering', model='naver-clova-ix/donut-base-finetuned-docvqa')\\n\\n# Load an image and ask a question\\nimage_path = 'path_to_image'\\nquestion = 'your_question'\\n\\n# Get the answer\\nanswer = doc_qa({'image': image_path, 'question': question})\\nprint(answer)\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository. Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] أحتاج إلى واجهة برمجة تطبيقات تسمح بالتحقق من هوية الأشخاص باستخدام تقنيات التعرف على الوجوه. هل يمكنك مساعدتي في العثور على API مناسب لهذا الغرض؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'vision-encoder-decoder', 'api_name': 'naver-clova-ix/donut-base-finetuned-docvqa', 'api_call': \"pipeline('document-question-answering', model='donut-base-finetuned-docvqa')\", 'api_arguments': {'image': 'path_to_image', 'question': 'your_question'}, 'python_environment_requirements': 'Transformers', 'example_code': \"from transformers import pipeline\\n\\n# Initialize the pipeline\\ndoc_qa = pipeline('document-question-answering', model='naver-clova-ix/donut-base-finetuned-docvqa')\\n\\n# Load an image and ask a question\\nimage_path = 'path_to_image'\\nquestion = 'your_question'\\n\\n# Get the answer\\nanswer = doc_qa({'image': image_path, 'question': question})\\nprint(answer)\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository. Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST]تريد تطوير تطبيق يتيح للمستخدم تحليل وتصنيف النصوص العربية. يمكنك استخدام الواجهة البرمجية التالية لتنفيذ الخوارزمية:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'CZ_DVQA_layoutxlm-base', 'api_call': \"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Document Question Answering model based on LayoutXLM.' </s>"
"<s>[INST] تريد تطوير تطبيق لتنفيذ عمليات النقل الآلي بين اللغات المختلفة. يمكنك استخدام الواجهة البرمجية التالية لتنفيذ هذه الوظيفة:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'CZ_DVQA_layoutxlm-base', 'api_call': \"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Document Question Answering model based on LayoutXLM.' </s>"
"<s>[INST] أنشئ تطبيقًا يستخدم خوارزمية الاستدلال البصري لتحليل الأشكال والهياكل المرئية. اعتمد في ذلك على الواجهة البرمجية التالية:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'CZ_DVQA_layoutxlm-base', 'api_call': \"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Document Question Answering model based on LayoutXLM.' </s>"
"<s>[INST] ترغب في بشرح تصنيف النصوص وتصنيفها إلى تصنيفات معينة. يمكنك استخدام الواجهة البرمجية التالية لتنفيذ عملية التصنيف:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'CZ_DVQA_layoutxlm-base', 'api_call': \"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Document Question Answering model based on LayoutXLM.' </s>"
"<s>[INST] تنوي تطوير تطبيق لتحويل النصوص المكتوبة إلى كلام مسموع. يمكنك استخدام الواجهة البرمجية التالية لتحقيق هذه المهمة:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'CZ_DVQA_layoutxlm-base', 'api_call': \"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Document Question Answering model based on LayoutXLM.' </s>"
"<s>[INST] أنشئ تطبيقًا يستخدم الذكاء الاصطناعي لتحليل سلوك المستخدمين عبر الإنترنت. يمكنك استخدام الواجهة البرمجية التالية لتنفيذ هذا التحليل:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'CZ_DVQA_layoutxlm-base', 'api_call': \"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Document Question Answering model based on LayoutXLM.' </s>"
"<s>[INST] أريد تطوير تطبيق لتحليل الصور الطبية والكشف عن الأمراض. يمكنك استخدام الواجهة البرمجية التالية لتحقيق هذا الهدف:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'CZ_DVQA_layoutxlm-base', 'api_call': \"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Document Question Answering model based on LayoutXLM.' </s>"
"<s>[INST] ترغب في تطوير تطبيق يستخدم الـ API لتحليل الأفلام وتصنيفها حسب المحتوى. يمكنك استخدام الواجهة البرمجية التالية لتحقيق ذلك:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'CZ_DVQA_layoutxlm-base', 'api_call': \"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Document Question Answering model based on LayoutXLM.' </s>"
"<s>[INST] أنشئ تطبيقًا لتقديم توصيات للمستخدمين استنادًا إلى تاريخ تصفحهم على الإنترنت. يمكنك استخدام الواجهة البرمجية التالية لتحقيق هذه الوظيفة:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'CZ_DVQA_layoutxlm-base', 'api_call': \"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Document Question Answering model based on LayoutXLM.' </s>"
"<s>[INST] تريد تطوير تطبيق يقوم بتحليل المقالات الصحفية واستخراج الأحداث الرئيسية منها. يمكنك استخدام الواجهة البرمجية التالية لتنفيذ عملية الاستخراج:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'CZ_DVQA_layoutxlm-base', 'api_call': \"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Document Question Answering model based on LayoutXLM.' </s>"
"<s>[INST]كتابة سيناريو درامي قصير يتضمن حوارات بين شخصيتين. كيف يمكن للكتاب استخدام API لتحليل الحوارات وتحسين تنسيقها؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-vqa', 'api_call': \"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering using the LayoutLM architecture.' </s>"
"<s>[INST] إنشاء تقرير احصائي يوضح نتائج استطلاع رأي عام عبر وسائل التواصل الاجتماعي. كيف يمكن للمحللين استخدام API لتحليل وتصنيف آراء المستخدمين بشكل فعال؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-vqa', 'api_call': \"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering using the LayoutLM architecture.' </s>"
"<s>[INST] كتابة مقالة علمية تتناول أحدث الاكتشافات في مجال الذكاء الاصطناعي. ما هي الخطوات اللازمة لاستخدام API لتوليد نماذج تحليلية تساهم في البحث العلمي؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-vqa', 'api_call': \"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering using the LayoutLM architecture.' </s>"
"<s>[INST] تطوير تطبيق يقوم بتحليل بيانات المبيعات للشركات الصغيرة. كيف يمكن للمطورين استخدام API لتحليل المعلومات وتقديم توصيات لتحسين الأداء التجاري؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-vqa', 'api_call': \"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering using the LayoutLM architecture.' </s>"
"<s>[INST] إعداد دليل عملي للمستخدمين حول كيفية تنقيح نص قانوني باستخدام تكنولوجيا المعالجة اللغوية الطبيعية. كيف يمكن للمستخدمين الاستفادة من API لتحليل وتصحيح النصوص القانونية بدقة؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-vqa', 'api_call': \"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering using the LayoutLM architecture.' </s>"
"<s>[INST] تطوير تطبيق يتيح للمستخدمين تحويل صورة إلى نص قابل للتحرير. كيف يمكن للمطورين استخدام API لتحويل الصور إلى نصوص قابلة للتحرير؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-vqa', 'api_call': \"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering using the LayoutLM architecture.' </s>"
"<s>[INST] كتابة دليل عملي للمستخدمين حول كيفية تحسين جودة الكتابة الإبداعية باستخدام تحليل النصوص. كيف يمكن للكتاب استخدام API لتحليل وتحسين الخط السردي واستخدام المفردات؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-vqa', 'api_call': \"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering using the LayoutLM architecture.' </s>"
"<s>[INST] كتابة دليل دورة تدريبية تشرح كيفية إنشاء نماذج توقعات زمنية باستخدام تقنيات التعلم الآلي. كيف يمكن للمدربين استخدام API لتطوير نماذج توقعات زمنية دقيقة وفعالة؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-vqa', 'api_call': \"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering using the LayoutLM architecture.' </s>"
"<s>[INST] تطوير تطبيق يقدم ترجمة فورية للنصوص بين العديد من اللغات المختلفة. كيف يمكن للمطورين استخدام API لتحليل وترجمة النصوص بدقة وسلاسة؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-vqa', 'api_call': \"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering using the LayoutLM architecture.' </s>"
"<s>[INST] كتابة دليل تدريبي حول كيفية تحليل وفهم مشاعر العملاء من خلال مراجعة الملاحظات النصية. كيف يمكن للمسوقين استخدام API لاستخراج وتصنيف المشاعر بناءً على النصوص؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-vqa', 'api_call': \"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering using the LayoutLM architecture.' </s>"
"<s>[INST]قم بإنشاء نظام توصية للمستهلكين استنادًا إلى تفاصيل مشترياتهم السابقة وسلوكياتهم عبر تطبيق الجوّال. يمكنك استخدام ال API التالي لتدريب نموذج التوصية باستخدام البيانات الخاصة بالمستخدمين.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-invoices', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"nlp(question='What is the total amount?', context='your_invoice_text')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] عرض طريقة تكامل الدفع الإلكتروني في موقع الويب الخاص بك لضمان تجربة تسوق سلسة للعملاء. يمكنك استخدام ال API التالي للوصول إلى خدمات معالجة الدفع الآمنة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-invoices', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"nlp(question='What is the total amount?', context='your_invoice_text')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] حدد وحدات التطبيق الأساسية اللازمة لتطوير تطبيق جوّال يعرض معلومات طقس اليوم والتنبؤات المستقبلية. استخدم ال API التالي للحصول على بيانات الطقس المباشرة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-invoices', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"nlp(question='What is the total amount?', context='your_invoice_text')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] قم ببناء نظام تحليل النصوص الآلي لفهم مشاعر المستخدمين من مدخلاتهم اللغوية. يمكنك استخدام ال API التالي لإعداد نموذج معالجة اللغة الطبيعية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-invoices', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"nlp(question='What is the total amount?', context='your_invoice_text')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] طور تطبيقًا لنشر المقاطع الموسيقية الجديدة والمفضلة للمستخدمين مع إمكانية إنشاء قوائم تشغيل شخصية. يمكنك استخدام ال API التالي للوصول إلى مكتبة الموسيقى الرقمية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-invoices', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"nlp(question='What is the total amount?', context='your_invoice_text')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] قم بتصميم وتطوير منصة إلكترونية تجمع بين تقنيات الواقع الافتراضي والتعلم الآلي لتوفير تجربة تفاعلية فريدة. يمكنك استخدام ال API التالي للوصول إلى مكتبات تطوير الواقع الافتراضي.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-invoices', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"nlp(question='What is the total amount?', context='your_invoice_text')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] أنشئ تطبيقًا يستخدم تقنيات الذكاء الاصطناعي للتعرف الآلي على الأشخاص من خلال صورهم. يمكنك استخدام ال API التالي لتنفيذ تقنيات التعرف على الوجوه.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-invoices', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"nlp(question='What is the total amount?', context='your_invoice_text')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] عرّف نظام لتحليل بيانات العملاء وتوقع احتياجاتهم المستقبلية باستخدام تقنيات التعلم الآلي. يمكنك استخدام ال API التالي لتدريب نماذج التنبؤ بالسلوك العميل.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-invoices', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"nlp(question='What is the total amount?', context='your_invoice_text')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] قم بتكامل بوابة دفع إلكترونية آمنة في تطبيق الهاتف الجوّال لتمكين المستهلكين من عمليات الدفع الآمنة عبر التطبيق. يمكنك استخدام ال API التالي لتنفيذ خدمات دفع مريحة وموثوقة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-invoices', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"nlp(question='What is the total amount?', context='your_invoice_text')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST] قم بإعداد نموذج توقعي يستند إلى بيانات العملاء لتقديم توصيات مخصصة بشأن المنتجات أو الخدمات. يمكنك استخدام ال API التالي لتنفيذ خوارزميات التنبؤ بالتجميع.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlm-invoices', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers', 'example_code': \"nlp(question='What is the total amount?', context='your_invoice_text')\", 'performance': {'dataset': 'proprietary dataset of invoices, SQuAD2.0, and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.' </s>"
"<s>[INST]اكتشف معلومات حول أحدث أحداث الرياضة باستخدام API Sport News.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlmv3-base-mpdocvqa', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')\", 'api_arguments': ['image', 'question', 'context', 'boxes'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': 'import torch\\nfrom transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\\nprocessor = LayoutLMv3Processor.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa, apply_ocr=False)\\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)\\nimage = Image.open(example.jpg).convert(RGB)\\nquestion = Is this a question?\\ncontext = [Example]\\nboxes = [0, 0, 1000, 1000]\\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=pt)\\noutputs = model(**document_encoding)\\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\\nend_idx = torch.argmax(outputs.end_logits, axis=1)\\nanswers = self.processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()', 'performance': {'dataset': 'rubentito/mp-docvqa', 'accuracy': {'ANLS': 0.4538, 'APPA': 51.9426}}, 'description': 'This is pretrained LayoutLMv3 from Microsoft hub and fine-tuned on Multipage DocVQA (MP-DocVQA) dataset. This model was used as a baseline in Hierarchical multimodal transformers for Multi-Page DocVQA.' </s>"
"<s>[INST] عرض قائمة بأفضل الأفلام الجديدة باستخدام API تحليل أفلام IMDb.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlmv3-base-mpdocvqa', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')\", 'api_arguments': ['image', 'question', 'context', 'boxes'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': 'import torch\\nfrom transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\\nprocessor = LayoutLMv3Processor.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa, apply_ocr=False)\\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)\\nimage = Image.open(example.jpg).convert(RGB)\\nquestion = Is this a question?\\ncontext = [Example]\\nboxes = [0, 0, 1000, 1000]\\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=pt)\\noutputs = model(**document_encoding)\\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\\nend_idx = torch.argmax(outputs.end_logits, axis=1)\\nanswers = self.processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()', 'performance': {'dataset': 'rubentito/mp-docvqa', 'accuracy': {'ANLS': 0.4538, 'APPA': 51.9426}}, 'description': 'This is pretrained LayoutLMv3 from Microsoft hub and fine-tuned on Multipage DocVQA (MP-DocVQA) dataset. This model was used as a baseline in Hierarchical multimodal transformers for Multi-Page DocVQA.' </s>"
"<s>[INST] استخدام خدمة الطقس للحصول على توقعات الطقس عندك.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlmv3-base-mpdocvqa', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')\", 'api_arguments': ['image', 'question', 'context', 'boxes'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': 'import torch\\nfrom transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\\nprocessor = LayoutLMv3Processor.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa, apply_ocr=False)\\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)\\nimage = Image.open(example.jpg).convert(RGB)\\nquestion = Is this a question?\\ncontext = [Example]\\nboxes = [0, 0, 1000, 1000]\\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=pt)\\noutputs = model(**document_encoding)\\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\\nend_idx = torch.argmax(outputs.end_logits, axis=1)\\nanswers = self.processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()', 'performance': {'dataset': 'rubentito/mp-docvqa', 'accuracy': {'ANLS': 0.4538, 'APPA': 51.9426}}, 'description': 'This is pretrained LayoutLMv3 from Microsoft hub and fine-tuned on Multipage DocVQA (MP-DocVQA) dataset. This model was used as a baseline in Hierarchical multimodal transformers for Multi-Page DocVQA.' </s>"
"<s>[INST] ابحث عن أحدث الأخبار التقنية باستخدام API News Tech.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlmv3-base-mpdocvqa', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')\", 'api_arguments': ['image', 'question', 'context', 'boxes'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': 'import torch\\nfrom transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\\nprocessor = LayoutLMv3Processor.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa, apply_ocr=False)\\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)\\nimage = Image.open(example.jpg).convert(RGB)\\nquestion = Is this a question?\\ncontext = [Example]\\nboxes = [0, 0, 1000, 1000]\\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=pt)\\noutputs = model(**document_encoding)\\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\\nend_idx = torch.argmax(outputs.end_logits, axis=1)\\nanswers = self.processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()', 'performance': {'dataset': 'rubentito/mp-docvqa', 'accuracy': {'ANLS': 0.4538, 'APPA': 51.9426}}, 'description': 'This is pretrained LayoutLMv3 from Microsoft hub and fine-tuned on Multipage DocVQA (MP-DocVQA) dataset. This model was used as a baseline in Hierarchical multimodal transformers for Multi-Page DocVQA.' </s>"
"<s>[INST] تحقق من ترتيب فريقك المفضل في الدوري الأوروبي باستخدام API European League Standings.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlmv3-base-mpdocvqa', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')\", 'api_arguments': ['image', 'question', 'context', 'boxes'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': 'import torch\\nfrom transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\\nprocessor = LayoutLMv3Processor.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa, apply_ocr=False)\\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)\\nimage = Image.open(example.jpg).convert(RGB)\\nquestion = Is this a question?\\ncontext = [Example]\\nboxes = [0, 0, 1000, 1000]\\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=pt)\\noutputs = model(**document_encoding)\\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\\nend_idx = torch.argmax(outputs.end_logits, axis=1)\\nanswers = self.processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()', 'performance': {'dataset': 'rubentito/mp-docvqa', 'accuracy': {'ANLS': 0.4538, 'APPA': 51.9426}}, 'description': 'This is pretrained LayoutLMv3 from Microsoft hub and fine-tuned on Multipage DocVQA (MP-DocVQA) dataset. This model was used as a baseline in Hierarchical multimodal transformers for Multi-Page DocVQA.' </s>"
"<s>[INST] احصل على قائمة بأفضل المطاعم في منطقتك باستخدام API Restaurant Finder.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlmv3-base-mpdocvqa', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')\", 'api_arguments': ['image', 'question', 'context', 'boxes'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': 'import torch\\nfrom transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\\nprocessor = LayoutLMv3Processor.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa, apply_ocr=False)\\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)\\nimage = Image.open(example.jpg).convert(RGB)\\nquestion = Is this a question?\\ncontext = [Example]\\nboxes = [0, 0, 1000, 1000]\\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=pt)\\noutputs = model(**document_encoding)\\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\\nend_idx = torch.argmax(outputs.end_logits, axis=1)\\nanswers = self.processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()', 'performance': {'dataset': 'rubentito/mp-docvqa', 'accuracy': {'ANLS': 0.4538, 'APPA': 51.9426}}, 'description': 'This is pretrained LayoutLMv3 from Microsoft hub and fine-tuned on Multipage DocVQA (MP-DocVQA) dataset. This model was used as a baseline in Hierarchical multimodal transformers for Multi-Page DocVQA.' </s>"
"<s>[INST] استخدم API الذاكرة العاطفية لتحليل مشاعر نص معين.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlmv3-base-mpdocvqa', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')\", 'api_arguments': ['image', 'question', 'context', 'boxes'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': 'import torch\\nfrom transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\\nprocessor = LayoutLMv3Processor.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa, apply_ocr=False)\\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)\\nimage = Image.open(example.jpg).convert(RGB)\\nquestion = Is this a question?\\ncontext = [Example]\\nboxes = [0, 0, 1000, 1000]\\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=pt)\\noutputs = model(**document_encoding)\\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\\nend_idx = torch.argmax(outputs.end_logits, axis=1)\\nanswers = self.processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()', 'performance': {'dataset': 'rubentito/mp-docvqa', 'accuracy': {'ANLS': 0.4538, 'APPA': 51.9426}}, 'description': 'This is pretrained LayoutLMv3 from Microsoft hub and fine-tuned on Multipage DocVQA (MP-DocVQA) dataset. This model was used as a baseline in Hierarchical multimodal transformers for Multi-Page DocVQA.' </s>"
"<s>[INST] اطلب من النموذج الاصطناعي إنشاء تصميم جديد بناءً على الأفكار التي تقدمها.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlmv3-base-mpdocvqa', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')\", 'api_arguments': ['image', 'question', 'context', 'boxes'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': 'import torch\\nfrom transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\\nprocessor = LayoutLMv3Processor.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa, apply_ocr=False)\\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)\\nimage = Image.open(example.jpg).convert(RGB)\\nquestion = Is this a question?\\ncontext = [Example]\\nboxes = [0, 0, 1000, 1000]\\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=pt)\\noutputs = model(**document_encoding)\\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\\nend_idx = torch.argmax(outputs.end_logits, axis=1)\\nanswers = self.processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()', 'performance': {'dataset': 'rubentito/mp-docvqa', 'accuracy': {'ANLS': 0.4538, 'APPA': 51.9426}}, 'description': 'This is pretrained LayoutLMv3 from Microsoft hub and fine-tuned on Multipage DocVQA (MP-DocVQA) dataset. This model was used as a baseline in Hierarchical multimodal transformers for Multi-Page DocVQA.' </s>"
"<s>[INST] تحليل الصور باستخدام API الرؤية الحاسوبية للكشف عن الأشياء في الصورة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlmv3-base-mpdocvqa', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')\", 'api_arguments': ['image', 'question', 'context', 'boxes'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': 'import torch\\nfrom transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\\nprocessor = LayoutLMv3Processor.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa, apply_ocr=False)\\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)\\nimage = Image.open(example.jpg).convert(RGB)\\nquestion = Is this a question?\\ncontext = [Example]\\nboxes = [0, 0, 1000, 1000]\\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=pt)\\noutputs = model(**document_encoding)\\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\\nend_idx = torch.argmax(outputs.end_logits, axis=1)\\nanswers = self.processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()', 'performance': {'dataset': 'rubentito/mp-docvqa', 'accuracy': {'ANLS': 0.4538, 'APPA': 51.9426}}, 'description': 'This is pretrained LayoutLMv3 from Microsoft hub and fine-tuned on Multipage DocVQA (MP-DocVQA) dataset. This model was used as a baseline in Hierarchical multimodal transformers for Multi-Page DocVQA.' </s>"
"<s>[INST] اعثر على أفضل العروض في تجارة الإلكترونيات بواسطة API Deals Finder.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'layoutlmv3-base-mpdocvqa', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')\", 'api_arguments': ['image', 'question', 'context', 'boxes'], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': 'import torch\\nfrom transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\\nprocessor = LayoutLMv3Processor.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa, apply_ocr=False)\\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)\\nimage = Image.open(example.jpg).convert(RGB)\\nquestion = Is this a question?\\ncontext = [Example]\\nboxes = [0, 0, 1000, 1000]\\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=pt)\\noutputs = model(**document_encoding)\\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\\nend_idx = torch.argmax(outputs.end_logits, axis=1)\\nanswers = self.processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()', 'performance': {'dataset': 'rubentito/mp-docvqa', 'accuracy': {'ANLS': 0.4538, 'APPA': 51.9426}}, 'description': 'This is pretrained LayoutLMv3 from Microsoft hub and fine-tuned on Multipage DocVQA (MP-DocVQA) dataset. This model was used as a baseline in Hierarchical multimodal transformers for Multi-Page DocVQA.' </s>"
"<s>[INST]قم بتطوير تطبيق لتعليم اللغات يستخدم تقنيات الذكاء الاصطناعي لتحسين تجربة التعلم.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'vision-encoder-decoder', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': \"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\", 'api_arguments': 'image_path, question', 'python_environment_requirements': 'transformers', 'example_code': \"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] إنشاء نظام تحليل الصور لتصنيف الحيوانات البرية باستخدام تقنيات تعلم الآلة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'vision-encoder-decoder', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': \"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\", 'api_arguments': 'image_path, question', 'python_environment_requirements': 'transformers', 'example_code': \"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] تطوير نظام تتبع المواقع لمركبات الشحن لمساعدة الشركات في تحسين تسليم الطرود في الوقت المحدد.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'vision-encoder-decoder', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': \"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\", 'api_arguments': 'image_path, question', 'python_environment_requirements': 'transformers', 'example_code': \"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] يرغب العميل في تطوير تطبيق للتعرف على أنواع الأزهار من الصور التي يلتقطها عبر الهاتف. استخدم خدمة تعرف الصور لتحقيق هذا الهدف.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'vision-encoder-decoder', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': \"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\", 'api_arguments': 'image_path, question', 'python_environment_requirements': 'transformers', 'example_code': \"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] ابنِ نظام إدارة للمهام يستخدم تقنيات الذكاء الاصطناعي لتوزيع المهام بناءً على أولويات الفريق ومهارات أعضائه.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'vision-encoder-decoder', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': \"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\", 'api_arguments': 'image_path, question', 'python_environment_requirements': 'transformers', 'example_code': \"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] أنشئ نظام توصيات لتشجيع العملاء على شراء المنتجات المماثلة استناداً إلى سجل المشتريات السابق لهم.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'vision-encoder-decoder', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': \"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\", 'api_arguments': 'image_path, question', 'python_environment_requirements': 'transformers', 'example_code': \"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] تطوير نظام تصنيف النصوص لفلترة التعليقات السلبية عبر منصة وسائل التواصل الاجتماعي. استخدم تقنيات معالجة اللغة الطبيعية لهذا الغرض.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'vision-encoder-decoder', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': \"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\", 'api_arguments': 'image_path, question', 'python_environment_requirements': 'transformers', 'example_code': \"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] قم بتطوير تطبيق للتعرف على الأشخاص باستخدام تقنيات التعرف على الوجوه. يمكن للتطبيق استخدام ذلك في أغراض الأمان والمراقبة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'vision-encoder-decoder', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': \"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\", 'api_arguments': 'image_path, question', 'python_environment_requirements': 'transformers', 'example_code': \"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] أنشئ نظام لتحليل سلوك المستخدمين على الموقع الإلكتروني لتحسين تجربة تصفحهم. استخدم خوارزميات تعلم الآلة لهذا الغرض.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'vision-encoder-decoder', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': \"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\", 'api_arguments': 'image_path, question', 'python_environment_requirements': 'transformers', 'example_code': \"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST] تطوير نظام للكشف عن الاحتيال في المعاملات المالية باستخدام تقنيات الذكاء الاصطناعي. استخدم تحليل البيانات والتصنيف لتحديد المعاملات الاحتيالية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'vision-encoder-decoder', 'api_name': 'jinhybr/OCR-DocVQA-Donut', 'api_call': \"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')\", 'api_arguments': 'image_path, question', 'python_environment_requirements': 'transformers', 'example_code': \"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\", 'performance': {'dataset': 'DocVQA', 'accuracy': 'Not provided'}, 'description': 'Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.' </s>"
"<s>[INST]إنشاء نظام توصيف للأفلام يستند إلى الملصقات الخاصة بها.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMv3ForQuestionAnswering', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file'}, 'python_environment_requirements': ['transformers', 'torch', 'tensorflow'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.' </s>"
"<s>[INST] تطبيق نموذج للكشف عن الجودة في صور المنتجات لتحسين تجربة التسوق عبر الإنترنت.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMv3ForQuestionAnswering', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file'}, 'python_environment_requirements': ['transformers', 'torch', 'tensorflow'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.' </s>"
"<s>[INST] أنشئ بوت يقوم بتوليد نصوص إبداعية استناداً إلى الصور المرفقة به.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMv3ForQuestionAnswering', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file'}, 'python_environment_requirements': ['transformers', 'torch', 'tensorflow'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.' </s>"
"<s>[INST] قم بتطوير نظام للتعرف على الشعارات التجارية في الصور لتحليل الإعلانات.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMv3ForQuestionAnswering', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file'}, 'python_environment_requirements': ['transformers', 'torch', 'tensorflow'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.' </s>"
"<s>[INST] إعداد نظام يقوم بتصنيف الأشخاص في الصور حسب ملامحهم الوجهية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMv3ForQuestionAnswering', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file'}, 'python_environment_requirements': ['transformers', 'torch', 'tensorflow'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.' </s>"
"<s>[INST] تطبيق نموذج للكشف عن الأشياء المتشابهة في مجموعة من الصور.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMv3ForQuestionAnswering', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file'}, 'python_environment_requirements': ['transformers', 'torch', 'tensorflow'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.' </s>"
"<s>[INST] تحديد شخصيات الرسوم المتحركة في الصور باستخدام تقنيات التعرف على النص.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMv3ForQuestionAnswering', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file'}, 'python_environment_requirements': ['transformers', 'torch', 'tensorflow'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.' </s>"
"<s>[INST] بناء نظام لتحديد المواقع الجغرافية في الصور الفضائية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMv3ForQuestionAnswering', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file'}, 'python_environment_requirements': ['transformers', 'torch', 'tensorflow'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.' </s>"
"<s>[INST] تطوير برنامج لاكتشاف العناصر الموجودة في الصور الطبية وتصنيفها تلقائيًا.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMv3ForQuestionAnswering', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file'}, 'python_environment_requirements': ['transformers', 'torch', 'tensorflow'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.' </s>"
"<s>[INST] إعداد نظام يحسن جودة الصور الرقمية عن طريق إزالة الضوضاء وتحسين الوضوح.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMv3ForQuestionAnswering', 'api_call': \"LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')\", 'api_arguments': {'image': 'path/to/image/file'}, 'python_environment_requirements': ['transformers', 'torch', 'tensorflow'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.' </s>"
"<s>[INST]قم بإنشاء نظام يمكنه إنشاء أوصاف نصية من الصور. ابحث عن واجهة برمجة التطبيقات (API) التي توفر إمكانات تحويل الصورة إلى نص.، تحتاج إلى تحليل المشاعر في عدد كبير من منشورات الوسائط الاجتماعية. ابحث عن واجهة برمجة التطبيقات (API) التي توفر إمكانات تحليل المشاعر للبيانات النصية.، قم بتطوير نظام لتصنيف الصور بناءً على محتواها. ابحث عن واجهة برمجة التطبيقات (API) التي تدعم تصنيف الصور باستخدام نماذج التعلم العميق.، تريد تنفيذ برنامج chatbot لدعم العملاء. ابحث عن واجهة برمجة التطبيقات التي توفر إمكانات الذكاء الاصطناعي للمحادثة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'DataIntelligenceTeam/eurocorpV4', 'api_call': \"AutoModelForTokenClassification.from_pretrained('DataIntelligenceTeam/eurocorpV4')\", 'api_arguments': '', 'python_environment_requirements': 'transformers>=4.26.0.dev0, torch>=1.12.1+cu113, datasets>=2.2.2, tokenizers>=0.13.2', 'example_code': '', 'performance': {'dataset': 'sroie', 'accuracy': 0.982}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv3-large on the sroie dataset. It achieves the following results on the evaluation set: Loss: 0.1239, Precision: 0.9548, Recall: 0.9602, F1: 0.9575, Accuracy: 0.9819' </s>"
"<s>[INST] قم ببناء نظام لاكتشاف الكائنات في الصور. ابحث عن واجهة برمجة التطبيقات (API) التي توفر إمكانات الكشف عن الكائنات باستخدام نماذج مدربة مسبقًا.، قم بتطوير نظام لتحليل المستندات النصية واستخراج المعلومات الأساسية. ابحث عن واجهة برمجة التطبيقات (API) التي توفر إمكانات التعرف على الكيانات المسماة (NER).، أنشئ نظامًا للتوصية بالمحتوى المخصص بناءً على تفضيلات المستخدم. ابحث عن واجهة برمجة التطبيقات (API) التي توفر خدمات محرك التوصيات.، تنفيذ نظام لاكتشاف وترجمة لغات متعددة في الوقت الفعلي. ابحث عن واجهة برمجة التطبيقات (API) التي تدعم اكتشاف اللغة وترجمتها.، قم ببناء نظام لتحليل بيانات سوق الأوراق المالية التاريخية والتنبؤ بالاتجاهات المستقبلية. ابحث عن واجهة برمجة التطبيقات (API) التي توفر نماذج التعلم الآلي للتنبؤ المالي.، قم بتطوير نظام لتحديد رسائل البريد الإلكتروني العشوائية وتصنيفها. ابحث عن واجهة برمجة التطبيقات (API) التي توفر تصنيف البريد الإلكتروني استنادًا إلى تحليل المحتوى.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'DataIntelligenceTeam/eurocorpV4', 'api_call': \"AutoModelForTokenClassification.from_pretrained('DataIntelligenceTeam/eurocorpV4')\", 'api_arguments': '', 'python_environment_requirements': 'transformers>=4.26.0.dev0, torch>=1.12.1+cu113, datasets>=2.2.2, tokenizers>=0.13.2', 'example_code': '', 'performance': {'dataset': 'sroie', 'accuracy': 0.982}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv3-large on the sroie dataset. It achieves the following results on the evaluation set: Loss: 0.1239, Precision: 0.9548, Recall: 0.9602, F1: 0.9575, Accuracy: 0.9819' </s>"
"<s>[INST]قم بتطوير نظام لتحليل المشاعر في مراجعات العملاء لمساعدة عميلك على فهم ملاحظات العملاء بشكل أفضل وتحسين خدماتهم. قم بتوفير واجهة برمجة التطبيقات لهذه المهمة.? [/INST]'domain': 'Multimodal Document Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'frizwankhan/entity-linking-model-final', 'api_call': \"pipeline('question-answering', model='frizwankhan/entity-linking-model-final')\", 'api_arguments': {'image': 'path/to/image', 'question': 'your question'}, 'python_environment_requirements': {'huggingface': '4.12.0', 'torch': '1.9.0'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Document Question Answering model based on layoutlmv2' </s>"
"<s>[INST] تنفيذ نظام توصية لتوصيات الأفلام المخصصة بناءً على تفضيلات المستخدم وسجل العرض. قم بتوفير واجهة برمجة التطبيقات لجلب التوصيات لمعرف مستخدم معين.? [/INST]'domain': 'Multimodal Document Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'frizwankhan/entity-linking-model-final', 'api_call': \"pipeline('question-answering', model='frizwankhan/entity-linking-model-final')\", 'api_arguments': {'image': 'path/to/image', 'question': 'your question'}, 'python_environment_requirements': {'huggingface': '4.12.0', 'torch': '1.9.0'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Document Question Answering model based on layoutlmv2' </s>"
"<s>[INST] إنشاء نظام للتعرف على الصور يمكنه التعرف على أنواع مختلفة من النباتات من الصور الفوتوغرافية. قم بتطوير واجهة برمجة التطبيقات (API) لتلقي مدخلات الصور وإرجاع أنواع النباتات.، قم ببناء نظام تحويل الكلام إلى نص لتحويل التسجيلات الصوتية إلى نص. قم بتوفير نقطة نهاية API التي تقبل الملفات الصوتية وترجع النص المكتوب.? [/INST]'domain': 'Multimodal Document Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'frizwankhan/entity-linking-model-final', 'api_call': \"pipeline('question-answering', model='frizwankhan/entity-linking-model-final')\", 'api_arguments': {'image': 'path/to/image', 'question': 'your question'}, 'python_environment_requirements': {'huggingface': '4.12.0', 'torch': '1.9.0'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Document Question Answering model based on layoutlmv2' </s>"
"<s>[INST] قم بتطوير نظام chatbot يمكنه مساعدة العملاء في الاستفسارات الشائعة وتقديم الدعم. قم بإنشاء واجهة برمجة تطبيقات (API) يمكنها التعامل مع الرسائل الواردة والرد بالمعلومات ذات الصلة.، صمم نظامًا لاكتشاف النشاط الاحتيالي في المعاملات المالية للمساعدة في منع الرسوم غير المصرح بها. قم بتطوير واجهة برمجة تطبيقات (API) يمكنها تحليل بيانات المعاملات والإبلاغ عن الأنشطة المشبوهة.، قم ببناء نظام للترجمة التلقائية للغة لتسهيل التواصل بين المستخدمين الذين يتحدثون لغات مختلفة. إنشاء واجهة برمجة التطبيقات (API) لترجمة النص من لغة إلى أخرى.، تطوير نظام لتحديد المعالم والمواقع الشهيرة في الصور لتزويد المستخدمين بمعلومات حول الأماكن التي يلتقطونها في الصور. قم بإنشاء واجهة برمجة تطبيقات للتعرف على المعالم.، تصميم نظام للتنبؤ باتجاهات سوق الأوراق المالية بناءً على البيانات التاريخية ومؤشرات السوق. قم بتطوير واجهة برمجة التطبيقات (API) لتحليل بيانات المخزون وتقديم التوقعات.، إنشاء نظام لتحديد الكائنات وسماتها في الصور لتمكين البحث المرئي والتعرف على الكائنات. قم بتطوير واجهة برمجة التطبيقات (API) للكشف عن الكائنات وتصنيفها.? [/INST]'domain': 'Multimodal Document Question Answering', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'frizwankhan/entity-linking-model-final', 'api_call': \"pipeline('question-answering', model='frizwankhan/entity-linking-model-final')\", 'api_arguments': {'image': 'path/to/image', 'question': 'your question'}, 'python_environment_requirements': {'huggingface': '4.12.0', 'torch': '1.9.0'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Document Question Answering model based on layoutlmv2' </s>"
"<s>[INST]\"يهدف مشروعنا إلى تطوير تطبيق يمكنه تحليل صور الأقمار الصناعية للمساعدة في التنبؤ بالكوارث الطبيعية.\" \"يهدف مشروعنا البحثي إلى استخدام تقنية التعلم العميق لتحليل الصور من الأفلام القديمة واستخراج وصف مختصر لكل صورة.\" هل هناك واجهة برمجة تطبيقات توفر هذه الوظيفة؟، \"تتطلع مؤسستنا إلى تطوير نظام يمكنه التعرف على الكائنات في الصور وتصنيفها باستخدام نموذج شبكة عصبية مدرب.\"? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DocVQA', 'accuracy': ''}, 'description': 'A document question answering model finetuned on the DocVQA dataset using LayoutLMv2-base-uncased.' </s>"
"<s>[INST]تحتاج شركة الطاقة إلى نظام يمكنه تحديد وتحليل نقاط التلوث في محطات الطاقة باستخدام معلومات الصور الفضائية. يُرجى استخدام الـ API التالي لتحقيق ذلك.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': {'transformers': '>=4.0.0'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLMv2 model for document question answering.' </s>"
"<s>[INST] مطلوب تطوير منصة تعليمية تستخدم تقنية الصور الفضائية لتسهيل عملية تعليم الجغرافيا وعلوم البيئة. الرجاء استخدام الـ API التالي لبناء الحل.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': {'transformers': '>=4.0.0'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLMv2 model for document question answering.' </s>"
"<s>[INST] تنوي وكالة الأرصاد الجوية إنشاء نظام يعتمد على الصور الفضائية لتحسين تنبؤاتها بالظروف الجوية. يرجى تصميم واجهة تسهّل هذه العملية باستخدام الـ API المبين أدناه.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': {'transformers': '>=4.0.0'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLMv2 model for document question answering.' </s>"
"<s>[INST] يطلب معهد البحث العلمي تطوير نظام يتمكن من تحليل تأثير التغيرات المناخية باستخدام الصور الفضائية. يُرجى الاستعانة بالـ API التالي لبناء النظام.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': {'transformers': '>=4.0.0'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLMv2 model for document question answering.' </s>"
"<s>[INST] على مؤسسة الحفاظ على البيئة أن تبدأ في تقديم تقارير شهرية تعتمد على تحليل الصور الفضائية للمساهمة في حماية البيئة. الرجاء استخدام الـ API المذكورة لتنفيذ هذه الخطوة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': {'transformers': '>=4.0.0'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLMv2 model for document question answering.' </s>"
"<s>[INST] لضمان سلامة البنية التحتية، ترغب شركة البناء في استخدام تحليل الصور الفضائية في تفتيش ومراقبة مشاريعها. نفذ الحل بإستخدام الـ API الاتي.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': {'transformers': '>=4.0.0'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLMv2 model for document question answering.' </s>"
"<s>[INST] تستعين شركة الزراعة بتحليل الصور الفضائية لتحسين استراتيجيتها في رصد الزراعات والمحاصيل. استخدم الـ API التالية لتطبيق هذا النظام.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': {'transformers': '>=4.0.0'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLMv2 model for document question answering.' </s>"
"<s>[INST] يطلب مزود خدمات النقل تطوير نظام يستند إلى تحليل الصور الفضائية لتحسين متابعة الوقود وتقليل التلوث. يرجى استخدام الـ API المُذكورة أعلاه لإنشاء الحل.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': {'transformers': '>=4.0.0'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLMv2 model for document question answering.' </s>"
"<s>[INST] تحتاج شركة الأمن إلى نظام يقدم خدمة مراقبة وتحليل محليات الفعاليات باستخدام تصنيف الصور الفضائية. استخدم الـ API المعطاة لتطوير هذا النظام.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': {'transformers': '>=4.0.0'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLMv2 model for document question answering.' </s>"
"<s>[INST] تطبيق توصيل الطعام بحاجة إلى نظام يقيّم الطلبات وجودتها عبر تحليل الصور الفضائية للوجبات. قم بإستخدام الـ API الواردة لبناء هذا النظام.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': {'transformers': '>=4.0.0'}, 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLMv2 model for document question answering.' </s>"
"<s>[INST]لدينا تطبيق يهدف إلى تحديد مواقع الحوادث المرورية على الطرق السريعة بناءً على الصور. هل هناك API يمكن استخدامه لتحقيق ذلك؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"pipeline('question-answering', model='Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa')\", 'api_arguments': \"{'question': 'your_question', 'context': 'your_context'}\", 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased.' </s>"
"<s>[INST] نحتاج إلى تطوير نظام يمكنه تحديد اسماء الاشخاص في الصور الشخصية. هل يمكنك اقتراح API مناسب؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"pipeline('question-answering', model='Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa')\", 'api_arguments': \"{'question': 'your_question', 'context': 'your_context'}\", 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased.' </s>"
"<s>[INST] يتعين علينا تطوير نظام لتصنيف الأخبار آلياً بناءً على محتواها. هل يمكن استخدام أي API موجود لهذا الغرض؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"pipeline('question-answering', model='Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa')\", 'api_arguments': \"{'question': 'your_question', 'context': 'your_context'}\", 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased.' </s>"
"<s>[INST] نريد تطوير تطبيق يتيح للمستخدمين تحويل الصور الى رسومات بأسلوب الكاريكاتير. هل هناك API مناسب يمكن استخدامه لتحقيق هذا الهدف؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"pipeline('question-answering', model='Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa')\", 'api_arguments': \"{'question': 'your_question', 'context': 'your_context'}\", 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased.' </s>"
"<s>[INST] لدينا حاجة لنظام يستطيع تحديد الكلاب بناءً على صورها. هل هناك API متاح يمكن استخدامه لهذا الغرض؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"pipeline('question-answering', model='Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa')\", 'api_arguments': \"{'question': 'your_question', 'context': 'your_context'}\", 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased.' </s>"
"<s>[INST] نرغب في تطوير تطبيق يقوم بتحليل نغمات الموسيقى لتوصيف المزاج العام للأغاني. هل يمكن استخدام API معين لهذا الغرض؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"pipeline('question-answering', model='Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa')\", 'api_arguments': \"{'question': 'your_question', 'context': 'your_context'}\", 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased.' </s>"
"<s>[INST] يتعين علينا تطوير نظام للتعرف على الألوان في الصور. هل يمكنك توجيهنا نحو API مناسب لهذا الغرض؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"pipeline('question-answering', model='Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa')\", 'api_arguments': \"{'question': 'your_question', 'context': 'your_context'}\", 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased.' </s>"
"<s>[INST] ننوي تطوير تطبيق يحلل الانفعالات في النصوص الكتابية. هل هناك API يمكن استخدامه لتحقيق هذه الوظيفة؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"pipeline('question-answering', model='Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa')\", 'api_arguments': \"{'question': 'your_question', 'context': 'your_context'}\", 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased.' </s>"
"<s>[INST] يلزمنا نظام لتحديد الأشكال الهندسية في الصور. هل يمكن استخدام API معين لهذا الغرض؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"pipeline('question-answering', model='Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa')\", 'api_arguments': \"{'question': 'your_question', 'context': 'your_context'}\", 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased.' </s>"
"<s>[INST] نبحث عن طريقة لخلق تجربة تفاعلية لتصحيح النصوص الإنجليزية. هل هناك API مناسب يمكن استخدامه لهذا الغرض؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"pipeline('question-answering', model='Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa')\", 'api_arguments': \"{'question': 'your_question', 'context': 'your_context'}\", 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased.' </s>"
"<s>[INST]تريد شركتك البحث عن أفضل نظام لتحليل النصوص بشكل آلي لفهم مضمونها وتصنيفها.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '4.12.2', 'pytorch': '1.8.0+cu101', 'datasets': '1.14.0', 'tokenizers': '0.10.3'}, 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.087}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] تحتاج منظمتك إلى نظام يمكنه استخراج المعلومات من الصور بشكل تلقائي.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '4.12.2', 'pytorch': '1.8.0+cu101', 'datasets': '1.14.0', 'tokenizers': '0.10.3'}, 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.087}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] تخطط لإنشاء نظام توصية يستند إلى تحليل سلوك المستخدمين.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '4.12.2', 'pytorch': '1.8.0+cu101', 'datasets': '1.14.0', 'tokenizers': '0.10.3'}, 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.087}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] تحتاج منظمتك إلى نظام لتحويل الصور الى نص.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '4.12.2', 'pytorch': '1.8.0+cu101', 'datasets': '1.14.0', 'tokenizers': '0.10.3'}, 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.087}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] تريد منظمتك تطوير نظام يمكنه تقديم توصيات دقيقة للمستخدمين استنادًا إلى تاريخ تصفحهم.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '4.12.2', 'pytorch': '1.8.0+cu101', 'datasets': '1.14.0', 'tokenizers': '0.10.3'}, 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.087}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] تبحث منظمتك عن API لتعلم الآلة يمكنه تحليل النصوص وتوليفها في لحظة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '4.12.2', 'pytorch': '1.8.0+cu101', 'datasets': '1.14.0', 'tokenizers': '0.10.3'}, 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.087}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] تخطط شركتك لتطوير نظام يمكنه من تحليل الموسيقى وتصنيفها بشكل آلي.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '4.12.2', 'pytorch': '1.8.0+cu101', 'datasets': '1.14.0', 'tokenizers': '0.10.3'}, 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.087}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] تريد شركتك العمل على تطوير نظام قادر على الاستدلال المنطقي بشكل ذكي.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '4.12.2', 'pytorch': '1.8.0+cu101', 'datasets': '1.14.0', 'tokenizers': '0.10.3'}, 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.087}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] تحتاج منظمتك إلى نظام لفحص النصوص وتحديد الأخطاء الإملائية بطريقة تلقائية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '4.12.2', 'pytorch': '1.8.0+cu101', 'datasets': '1.14.0', 'tokenizers': '0.10.3'}, 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.087}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] ترغب منظمتك في تطوير نظام لتوليف النصوص بطريقة تجعلها أكثر جاذبية للقراء.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '4.12.2', 'pytorch': '1.8.0+cu101', 'datasets': '1.14.0', 'tokenizers': '0.10.3'}, 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.087}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST]تصميم نظام لتحليل المحتوى الصوتي في دروس اللغة وتقديم تقييمات دقيقة لأداء الطلاب.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/layoutlmv2-base-uncased')\", 'api_arguments': {'model': 'hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa'}, 'python_environment_requirements': {'transformers': '4.27.4', 'pytorch': '2.0.0+cu117', 'datasets': '2.11.0', 'tokenizers': '0.13.2'}, 'example_code': \"from transformers import pipeline\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\nquestion = 'What is the total amount?'\\ndocument = 'path/to/your/image/file.png'\\nresult = qa_pipeline(question=question, document=document)\\nprint(result)\", 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.843}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] بناء نظام لتحويل النصوص التاريخية إلى تسلسلات صوتية لتحسين تجربة القراءة للأشخاص ذوي الصعوبات البصرية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/layoutlmv2-base-uncased')\", 'api_arguments': {'model': 'hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa'}, 'python_environment_requirements': {'transformers': '4.27.4', 'pytorch': '2.0.0+cu117', 'datasets': '2.11.0', 'tokenizers': '0.13.2'}, 'example_code': \"from transformers import pipeline\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\nquestion = 'What is the total amount?'\\ndocument = 'path/to/your/image/file.png'\\nresult = qa_pipeline(question=question, document=document)\\nprint(result)\", 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.843}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] تنفيذ نظام للتعرف على اللغة الطبيعية واستخراج المعلومات الرئيسية من النصوص الصحية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/layoutlmv2-base-uncased')\", 'api_arguments': {'model': 'hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa'}, 'python_environment_requirements': {'transformers': '4.27.4', 'pytorch': '2.0.0+cu117', 'datasets': '2.11.0', 'tokenizers': '0.13.2'}, 'example_code': \"from transformers import pipeline\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\nquestion = 'What is the total amount?'\\ndocument = 'path/to/your/image/file.png'\\nresult = qa_pipeline(question=question, document=document)\\nprint(result)\", 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.843}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] تطوير نظام توصية بالموسيقى بناءً على اهتمامات المستخدم وتحليل سلوكه.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/layoutlmv2-base-uncased')\", 'api_arguments': {'model': 'hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa'}, 'python_environment_requirements': {'transformers': '4.27.4', 'pytorch': '2.0.0+cu117', 'datasets': '2.11.0', 'tokenizers': '0.13.2'}, 'example_code': \"from transformers import pipeline\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\nquestion = 'What is the total amount?'\\ndocument = 'path/to/your/image/file.png'\\nresult = qa_pipeline(question=question, document=document)\\nprint(result)\", 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.843}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] بناء نظام لتحليل مشاعر المستخدمين عبر تحليل النصوص الاجتماعية على وسائل التواصل الاجتماعي.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/layoutlmv2-base-uncased')\", 'api_arguments': {'model': 'hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa'}, 'python_environment_requirements': {'transformers': '4.27.4', 'pytorch': '2.0.0+cu117', 'datasets': '2.11.0', 'tokenizers': '0.13.2'}, 'example_code': \"from transformers import pipeline\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\nquestion = 'What is the total amount?'\\ndocument = 'path/to/your/image/file.png'\\nresult = qa_pipeline(question=question, document=document)\\nprint(result)\", 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.843}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] تنفيذ نظام لتحويل النصوص إلى خريطة مفاهيمية توضح العلاقات بين الكلمات.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/layoutlmv2-base-uncased')\", 'api_arguments': {'model': 'hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa'}, 'python_environment_requirements': {'transformers': '4.27.4', 'pytorch': '2.0.0+cu117', 'datasets': '2.11.0', 'tokenizers': '0.13.2'}, 'example_code': \"from transformers import pipeline\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\nquestion = 'What is the total amount?'\\ndocument = 'path/to/your/image/file.png'\\nresult = qa_pipeline(question=question, document=document)\\nprint(result)\", 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.843}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] تطوير نظام لتوليد أسماء الشخصيات لروايات الخيال باستخدام تقنيات توليد اللغة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/layoutlmv2-base-uncased')\", 'api_arguments': {'model': 'hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa'}, 'python_environment_requirements': {'transformers': '4.27.4', 'pytorch': '2.0.0+cu117', 'datasets': '2.11.0', 'tokenizers': '0.13.2'}, 'example_code': \"from transformers import pipeline\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\nquestion = 'What is the total amount?'\\ndocument = 'path/to/your/image/file.png'\\nresult = qa_pipeline(question=question, document=document)\\nprint(result)\", 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.843}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] تصميم نظام لتحليل وتصنيف الصور الطبية للكشف عن الأمراض.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/layoutlmv2-base-uncased')\", 'api_arguments': {'model': 'hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa'}, 'python_environment_requirements': {'transformers': '4.27.4', 'pytorch': '2.0.0+cu117', 'datasets': '2.11.0', 'tokenizers': '0.13.2'}, 'example_code': \"from transformers import pipeline\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\nquestion = 'What is the total amount?'\\ndocument = 'path/to/your/image/file.png'\\nresult = qa_pipeline(question=question, document=document)\\nprint(result)\", 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.843}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] تنفيذ نظام لتحليل وتصنيف الإنتاج الإبداعي للكتّاب من خلال تقييم محتوى النصوص.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/layoutlmv2-base-uncased')\", 'api_arguments': {'model': 'hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa'}, 'python_environment_requirements': {'transformers': '4.27.4', 'pytorch': '2.0.0+cu117', 'datasets': '2.11.0', 'tokenizers': '0.13.2'}, 'example_code': \"from transformers import pipeline\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\nquestion = 'What is the total amount?'\\ndocument = 'path/to/your/image/file.png'\\nresult = qa_pipeline(question=question, document=document)\\nprint(result)\", 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.843}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] بناء نظام لتوقع احتمالات ربح العملاء في لعبة الرهان الرياضي باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModel.from_pretrained('microsoft/layoutlmv2-base-uncased')\", 'api_arguments': {'model': 'hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa'}, 'python_environment_requirements': {'transformers': '4.27.4', 'pytorch': '2.0.0+cu117', 'datasets': '2.11.0', 'tokenizers': '0.13.2'}, 'example_code': \"from transformers import pipeline\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\nquestion = 'What is the total amount?'\\ndocument = 'path/to/your/image/file.png'\\nresult = qa_pipeline(question=question, document=document)\\nprint(result)\", 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.843}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST]لدي فكرة لتطبيق يقدم قصص مصورة للأطفال، هل يمكن استخدام API لتحويل النصوص إلى صور لكل صفحة من القصص؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers==4.12.3, Pytorch==1.8.0+cu101, Datasets==1.15.1, Tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.2207}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] أعمل على تطوير تطبيق لتعليم اللغات الأجنبية، وأرغب في دمج واجهة برمجية لتحويل النصوص إلى أصوات بلهجات مختلفة. هل يمكن استخدام API للقيام بذلك؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers==4.12.3, Pytorch==1.8.0+cu101, Datasets==1.15.1, Tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.2207}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] قمت بإنشاء تطبيق لعرض معارض فنية عبر الإنترنت، وأحتاج إلى استخدام واجهة برمجية تمكنني من تحويل صور اللوحات الفنية إلى وصف نصي تفصيلي. هل يوجد API مناسب لهذه الغرض؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers==4.12.3, Pytorch==1.8.0+cu101, Datasets==1.15.1, Tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.2207}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] أريد إنشاء تطبيق لتوليد تلميحات للأفلام استنادًا إلى مشاهد معينة، هل هناك API يمكنني استخدامه لتحويل مقاطع الفيديو إلى نصوص وصفية لهذه المشاهد؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers==4.12.3, Pytorch==1.8.0+cu101, Datasets==1.15.1, Tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.2207}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] أريد استخدام تقنية الذكاء الاصطناعي في تطبيقي لتحويل القصص القصيرة إلى رسوم متحركة، هل يمكنني الاستعانة بواجهة برمجية لتحويل النصوص إلى تسجيلات صوتية ومرئية؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers==4.12.3, Pytorch==1.8.0+cu101, Datasets==1.15.1, Tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.2207}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] أرغب في تنفيذ مشروع يستخدم التعرف على النصوص لتحويل الخط اليدوي إلى نصوص رقمية، هل تنصح باستخدام واجهة برمجية لتحقيق ذلك؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers==4.12.3, Pytorch==1.8.0+cu101, Datasets==1.15.1, Tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.2207}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] لدى تطبيق يهتم بتحليل النصوص الأدبية، أحتاج إلى واجهة برمجية لتحويل النصوص الشعرية إلى توليفات موسيقية. هل يمكنني الاعتماد على API لتحقيق ذلك؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers==4.12.3, Pytorch==1.8.0+cu101, Datasets==1.15.1, Tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.2207}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] أنا باحث في مجال التراث الثقافي وأرغب في تصميم تطبيق لتحويل النصوص التاريخية إلى فيديوهات توثيقية. هل يمكن استخدام واجهة برمجية لتحقيق هذا الهدف؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers==4.12.3, Pytorch==1.8.0+cu101, Datasets==1.15.1, Tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.2207}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] أعمل على تطبيق يهدف إلى تحسين التجربة السينمائية للمستخدمين، هل يمكنني استخدام واجهة برمجية لتحويل تعليقات المشاهدين إلى نصوص سيناريو قصصية؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers==4.12.3, Pytorch==1.8.0+cu101, Datasets==1.15.1, Tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.2207}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] أنا معلمة تربوية أبحث عن طريقة لتحويل نصوص تعليمية إلى موارد تعليمية متعددة الوسائط، هل يمكنني الاستفادة من API لتحقيق هذا الهدف؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': 'transformers==4.12.3, Pytorch==1.8.0+cu101, Datasets==1.15.1, Tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 2.2207}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST]تخطط مدرسة لتطوير تطبيق تعليمي للرياضيات من خلال استخدام مقاطع فيديو توضيحية. نريد استخدام التقنية الحديثة لتوليد صور توضيحية تعزز فهم المفاهيم الرياضية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '>=4.11.0'}, 'example_code': {}, 'performance': {'dataset': {}, 'accuracy': {}}, 'description': 'A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.' </s>"
"<s>[INST] مجموعة الأبحاث العلمية ترغب في بناء نموذج لتحليل الصور الفلكية واستخراج البيانات الهامة منها. نحتاج إلى استخدام الـ API المُقدم أدناه لتحليل الصور بشكل فعال.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '>=4.11.0'}, 'example_code': {}, 'performance': {'dataset': {}, 'accuracy': {}}, 'description': 'A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.' </s>"
"<s>[INST] شركة التسويق تريد إنشاء تطبيق يقدم معلومات شاملة حول العلامات التجارية المعروفة. نحتاج إلى API لتنفيذ نظام استشهادات وإدراج الصور ذات العلاقة في التطبيق.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '>=4.11.0'}, 'example_code': {}, 'performance': {'dataset': {}, 'accuracy': {}}, 'description': 'A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.' </s>"
"<s>[INST] شركة السيارات تبحث عن حل يمكّن العملاء من تصفح مجموعة السيارات المتاحة بسرعة. نحتاج إلى استخدام API لتوليد صور ثلاثية الأبعاد تعرض تفاصيل السيارات بشكل واقعي.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '>=4.11.0'}, 'example_code': {}, 'performance': {'dataset': {}, 'accuracy': {}}, 'description': 'A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.' </s>"
"<s>[INST] تريد مكتبة تطوير تطبيق تفاعلي للأطفال يعزز مفهوم القراءة السريعة. نحتاج إلى API لتوليد صور مصنفة تظهر النصوص بشكل تفاعلي وجذاب.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '>=4.11.0'}, 'example_code': {}, 'performance': {'dataset': {}, 'accuracy': {}}, 'description': 'A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.' </s>"
"<s>[INST] شركة العقارات ترغب في تطوير منصة تعرض العملاء العقارات المتاحة بوضوح. نحتاج إلى API لتوليد صور توضح تفاصيل المنازل والمباني بدقة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '>=4.11.0'}, 'example_code': {}, 'performance': {'dataset': {}, 'accuracy': {}}, 'description': 'A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.' </s>"
"<s>[INST] مدرسة لغات تريد إنشاء تطبيق تعليمي لتحسين مهارات اللغة الإنجليزية. نحتاج إلى API لتوليد صور توضيحية تساعد الطلاب على فهم القواعد اللغوية بشكل مبسط.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '>=4.11.0'}, 'example_code': {}, 'performance': {'dataset': {}, 'accuracy': {}}, 'description': 'A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.' </s>"
"<s>[INST] تنظم شركة الحفلات حدثًا لفترة العطلات وتحتاج إلى تصميم دعوات مبتكرة للمشاركين. نحتاج إلى API لتوليد صور ذات تصاميم إبداعية تناسب فكرة الحدث.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '>=4.11.0'}, 'example_code': {}, 'performance': {'dataset': {}, 'accuracy': {}}, 'description': 'A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.' </s>"
"<s>[INST] متجر إلكتروني يرغب في تحسين تجربة التسوق للعملاء عن طريق توفير صور شخصية للمنتجات. نحتاج إلى API لتوليد صور ثنائية الأبعاد تعرض المنتجات بشكل جذاب.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\", 'api_arguments': {}, 'python_environment_requirements': {'transformers': '>=4.11.0'}, 'example_code': {}, 'performance': {'dataset': {}, 'accuracy': {}}, 'description': 'A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.' </s>"
"<s>[INST]بناء نظام لتحليل الرسوم البيانية واستخراج البيانات الرئيسية منها، يمكنك استخدام API مثل Plotly لهذا الغرض.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLM model for document question answering.' </s>"
"<s>[INST] قم بتطوير نظام لتحليل النصوص القانونية واستخراج الأحكام والقوانين ذات الصلة، ويمكنك استخدام تقنيات NLP مثل مكتبة spaCy.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLM model for document question answering.' </s>"
"<s>[INST] إنشاء نظام لتحليل بيانات الطقس وتوقع الظروف الجوية المستقبلية، يمكنك استخدام API مثل OpenWeatherMap للوصول إلى البيانات الحالية والتوقعات.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLM model for document question answering.' </s>"
"<s>[INST] إنشاء نظام للتعرف على الوجوه في الصور وتصنيفها، يمكنك استخدام مكتبة OpenCV لهذا الغرض.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLM model for document question answering.' </s>"
"<s>[INST] أنشئ نظامًا لتصنيف الأخبار وتحليل محتواها لتحديد المواضيع الرئيسية، استخدم API مثل IBM Watson Natural Language Understanding لهذا الغرض.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLM model for document question answering.' </s>"
"<s>[INST] يرغب العميل في تطوير نظام لتحليل أصوات المشتقات الصوتية وتحديد المعلومات الهامة منها، يمكنك استخدام API مثل Google Cloud Speech-to-Text لهذا الغرض.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLM model for document question answering.' </s>"
"<s>[INST] بناء نظام لتتبع الأشخاص عن بعد من خلال تحليل تدفقات الفيديو، يمكنك استخدام مكتبة YOLO (You Only Look Once) للكشف عن الأشخاص.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLM model for document question answering.' </s>"
"<s>[INST] إنشاء نظام لفحص مستندات PDF واستخراج النصوص والبيانات المهمة منها، يمكنك استخدام مكتبة PyMuPDF (PyMuPDF) لهذا الغرض.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLM model for document question answering.' </s>"
"<s>[INST] بناء نظام لتحليل استخدام الإنترنت وتصنيف مواقع الويب على أساس المحتوى والنشاط، يمكنك استخدام API مثل Webhose.io لجمع البيانات.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLM model for document question answering.' </s>"
"<s>[INST] تحتاج إلى تطوير نظام لتحليل البيانات اللغوية واستخراج المعلومات الهامة من النصوص، يمكنك استخدام API مثل TextRazor لهذا الغرض.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023')\", 'api_arguments': {'question': 'string', 'context': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A LayoutLM model for document question answering.' </s>"
"<s>[INST]اكتشف كيف يمكن للنموذج العقلي الاصطناعي تحليل النصوص والموسيقى لتوليد توصيات مخصصة لمستخدمينك.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\", 'api_arguments': 'question, image', 'python_environment_requirements': 'transformers, torch, datasets, tokenizers', 'example_code': '', 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.3167}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] قم بإنشاء تطبيق يتيح للمستخدمين رفع صور للتعرف على الأعشاب الضارة في الحقول.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\", 'api_arguments': 'question, image', 'python_environment_requirements': 'transformers, torch, datasets, tokenizers', 'example_code': '', 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.3167}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] قم بإنشاء تطبيق يمكنه تحليل مشاهد الفيديو وتقديم توصيات لتحسين جودة الإنتاج.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\", 'api_arguments': 'question, image', 'python_environment_requirements': 'transformers, torch, datasets, tokenizers', 'example_code': '', 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.3167}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] اكتب تطبيقًا يعتمد على الذكاء الاصطناعي لاقتراح الوجبات الغذائية الصحية للمستخدمين بناءً على تفضيلاتهم واحتياجاتهم الغذائية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\", 'api_arguments': 'question, image', 'python_environment_requirements': 'transformers, torch, datasets, tokenizers', 'example_code': '', 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.3167}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] ابتكر تطبيق يستخدم تقنية معالجة اللغة الطبيعية لتحليل المقالات واستخراج المواضيع الرئيسية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\", 'api_arguments': 'question, image', 'python_environment_requirements': 'transformers, torch, datasets, tokenizers', 'example_code': '', 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.3167}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] قم بتطوير تطبيق يستخدم الذكاء الاصطناعي لتحليل أداء اللاعبين في رياضة معينة وتقديم توصيات لتحسينه.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\", 'api_arguments': 'question, image', 'python_environment_requirements': 'transformers, torch, datasets, tokenizers', 'example_code': '', 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.3167}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] اكتب تطبيقًا يستخدم تقنيات تعلم الآلة للتنبؤ بأداء السوق المالي وتقديم استراتيجيات استثمارية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\", 'api_arguments': 'question, image', 'python_environment_requirements': 'transformers, torch, datasets, tokenizers', 'example_code': '', 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.3167}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم تقنيات الذكاء الاصطناعي لتحليل الصوتيات واكتشاف الأصوات غير المألوفة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\", 'api_arguments': 'question, image', 'python_environment_requirements': 'transformers, torch, datasets, tokenizers', 'example_code': '', 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.3167}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] ابتكر تطبيقًا يستخدم تقنيات التعلم الآلي لتطوير نظام للتعرف على الوجوه وإدارة قواعد البيانات الخاصة بالمستخدمين.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\", 'api_arguments': 'question, image', 'python_environment_requirements': 'transformers, torch, datasets, tokenizers', 'example_code': '', 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.3167}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST] قم بإنشاء تطبيقًا يستخدم تقنيات الذكاء الاصطناعي لتحليل سلوك المستخدمين عبر الإنترنت وتوصيات تخصيصية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'layoutlmv2-base-uncased_finetuned_docvqa', 'api_call': \"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')\", 'api_arguments': 'question, image', 'python_environment_requirements': 'transformers, torch, datasets, tokenizers', 'example_code': '', 'performance': {'dataset': 'None', 'accuracy': {'Loss': 4.3167}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.' </s>"
"<s>[INST]قم بتطوير تطبيق يتيح عرض جولات افتراضية للمواقع التاريخية الشهيرة، واستخدم خدمة Hugging Face Transformers لإنشاء نموذج للإجابة على الأسئلة المتعلقة بتلك المواقع.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMForQuestionAnswering', 'api_call': \"AutoModelForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLM model for question answering. This model is not pretrained and serves as an example for the LayoutLM architecture.' </s>"
"<s>[INST] ابتكر تطبيقًا يستخدم تقنية التعلم العميق لتحليل وتصنيف الصور الطبية، يمكنك الانتفاع بخدمة Hugging Face Transformers لتطوير نموذج للكشف عن التفاصيل الطبية في الصور.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMForQuestionAnswering', 'api_call': \"AutoModelForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLM model for question answering. This model is not pretrained and serves as an example for the LayoutLM architecture.' </s>"
"<s>[INST] اخترع تطبيقًا يقوم بتحديد ومطابقة الوجوه في الصور، واستعمل خدمة Hugging Face Transformers لبناء نموذج سؤال وجواب يعمل على التعرف على تفاصيل الوجوه.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMForQuestionAnswering', 'api_call': \"AutoModelForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLM model for question answering. This model is not pretrained and serves as an example for the LayoutLM architecture.' </s>"
"<s>[INST] يهدف تطبيقك لمساعدة المستخدمين على تذوق أنواع القهوة المختلفة، قم باستخدام مكتبة Hugging Face Transformers لبناء نموذج سؤال وجواب يعرض معلومات عن أصناف القهوة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMForQuestionAnswering', 'api_call': \"AutoModelForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLM model for question answering. This model is not pretrained and serves as an example for the LayoutLM architecture.' </s>"
"<s>[INST] قم بتطوير تطبيق لتحليل وتصنيف الحيوانات في الصور، يمكنك استخدام خدمة Hugging Face Transformers لإنشاء نموذج للإجابة على أسئلة تتعلق بأنواع الحيوانات المعروضة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMForQuestionAnswering', 'api_call': \"AutoModelForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLM model for question answering. This model is not pretrained and serves as an example for the LayoutLM architecture.' </s>"
"<s>[INST] تطلب تطوير تطبيق يساعد في تحليل واستخراج المعلومات من الشلالات الطبيعية. قم بالاستفادة من خدمة Hugging Face Transformers لبناء نموذج سؤال وجواب يوضح المزيد حول الشلالات.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMForQuestionAnswering', 'api_call': \"AutoModelForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLM model for question answering. This model is not pretrained and serves as an example for the LayoutLM architecture.' </s>"
"<s>[INST] ابتكر تطبيقًا يعمل على تحليل وتصنيف السيارات في الصور، تستطيع الاستعانة بخدمة Hugging Face Transformers لبناء نموذج يجيب عن أي أسئلة متعلقة بأنواع السيارات.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMForQuestionAnswering', 'api_call': \"AutoModelForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLM model for question answering. This model is not pretrained and serves as an example for the LayoutLM architecture.' </s>"
"<s>[INST] تخطط لتطوير تطبيق يهدف لفهم وتحليل أنماط الأزياء في الصور. يمكنك الاستفادة من خدمة Hugging Face Transformers لبناء نموذج سؤال وجواب حول أحدث الصيحات في عالم الموضة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMForQuestionAnswering', 'api_call': \"AutoModelForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLM model for question answering. This model is not pretrained and serves as an example for the LayoutLM architecture.' </s>"
"<s>[INST] قم باختراع تطبيق يقوم بتحديد وتصنيف النباتات في الصور، واستخدم خدمة Hugging Face Transformers لإنشاء نموذج للإجابة على أسئلة تتعلق بأنواع النباتات الظاهرة في الصور.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMForQuestionAnswering', 'api_call': \"AutoModelForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLM model for question answering. This model is not pretrained and serves as an example for the LayoutLM architecture.' </s>"
"<s>[INST] تنوي تطوير تطبيق يقدم جولات افتراضية للمواقع الطبيعية الشهيرة، يمكنك استخدام خدمة Hugging Face Transformers لبناء نموذج سؤال وجواب يتناول معلومات عن تلك الأماكن.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiny-random-LayoutLMForQuestionAnswering', 'api_call': \"AutoModelForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random LayoutLM model for question answering. This model is not pretrained and serves as an example for the LayoutLM architecture.' </s>"
"<s>[INST]يجب عليك تطوير نظام يمكنه فهم التعبيرات اللغوية المعقدة وإعادة صياغتها بشكل مبسط.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': 'SQuAD2.0 and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] يتوجب عليك تحسين نظام قراءة الخطوط اليدوية الإلكترونية لتحسين دقته.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': 'SQuAD2.0 and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] نود تطوير نظام قادر على تقديم تحليل دقيق وشامل للصور الطبية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': 'SQuAD2.0 and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] ينبغي عليك تطوير نظام لتصنيف النصوص الطبية إلى فئات مختلفة بناءً على محتواها.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': 'SQuAD2.0 and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] نحتاج إلى نظام يستطيع إجراء تحليل للأفعال في اللغة العربية واستخراج المعلومات الضرورية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': 'SQuAD2.0 and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] تحتاج إلى تصميم نظام قادر على تحسين جودة الصور الطبية لتسهيل عمليات التشخيص.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': 'SQuAD2.0 and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] نريد نظاماً يستطيع تحليل البيانات السمعية وتحويلها إلى نصوص مفهومة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': 'SQuAD2.0 and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] يتوجب عليك تطوير نظام يستطيع تحليل الإيماءات وفهم دلالاتها في سياقات محددة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': 'SQuAD2.0 and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] نريد تصميم نظام قادر على الكشف عن الوجوه في الصور وتحديد هويتها.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': 'SQuAD2.0 and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST] ينبغي عليك تطوير نظام يستطيع تنسيق النصوص بطريقة جذابة ومتناسقة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face', 'functionality': 'Question Answering', 'api_name': 'impira/layoutlm-document-qa', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))\", 'api_arguments': ['image_url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': 'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)', 'performance': {'dataset': 'SQuAD2.0 and DocVQA', 'accuracy': 'Not provided'}, 'description': 'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.' </s>"
"<s>[INST]كيف يمكنني استخدام API لتحويل الصور إلى نص؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers==4.15.0', 'torch==1.8.0+cu101', 'datasets==1.17.0', 'tokenizers==0.10.3'], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 4.3332}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحويل النصوص إلى كتالوجات في تصميم المنتجات؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers==4.15.0', 'torch==1.8.0+cu101', 'datasets==1.17.0', 'tokenizers==0.10.3'], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 4.3332}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحديد تصنيف الأشياء في صور معينة؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers==4.15.0', 'torch==1.8.0+cu101', 'datasets==1.17.0', 'tokenizers==0.10.3'], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 4.3332}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحويل الصور الثلاثية الأبعاد إلى صور ثنائية الأبعاد؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers==4.15.0', 'torch==1.8.0+cu101', 'datasets==1.17.0', 'tokenizers==0.10.3'], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 4.3332}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحليل توجيهات الكتابة اليدوية في صور؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers==4.15.0', 'torch==1.8.0+cu101', 'datasets==1.17.0', 'tokenizers==0.10.3'], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 4.3332}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتوليد وجوه بشرية خيالية استنادًا إلى وصف نصي؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers==4.15.0', 'torch==1.8.0+cu101', 'datasets==1.17.0', 'tokenizers==0.10.3'], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 4.3332}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] كيف أقوم بتحليل التفاصيل النصية في مجموعة كبيرة من النصوص باستخدام API؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers==4.15.0', 'torch==1.8.0+cu101', 'datasets==1.17.0', 'tokenizers==0.10.3'], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 4.3332}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتوليد نماذج ثلاثية الأبعاد لآثار تاريخية معروفة؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers==4.15.0', 'torch==1.8.0+cu101', 'datasets==1.17.0', 'tokenizers==0.10.3'], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 4.3332}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحديد الأماكن السياحية في صور المعالم الشهيرة؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers==4.15.0', 'torch==1.8.0+cu101', 'datasets==1.17.0', 'tokenizers==0.10.3'], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 4.3332}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحليل المشاهد الصوتية واستخراج النصوص منها؟? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\", 'api_arguments': 'question, context', 'python_environment_requirements': ['transformers==4.15.0', 'torch==1.8.0+cu101', 'datasets==1.17.0', 'tokenizers==0.10.3'], 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 4.3332}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.' </s>"
"<s>[INST]نريد تطوير أداة تساعد في تحويل الصور إلى نص تلقائياً. استخدم الـ API التالي لتحقيق ذلك:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.15.0, torch==1.8.0+cu101, datasets==1.17.0, tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 8.5806}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] تطلب شركتنا القدرة على تحويل النص إلى لغة إشارة. يمكنك استخدام الـ API التالي لتحقيق ذلك:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.15.0, torch==1.8.0+cu101, datasets==1.17.0, tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 8.5806}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] عميلنا يحتاج إلى نظام يمكنه تحليل الترجمات الآلية للنصوص. يمكنك استخدام الـ API التالي لتحقيق هذا الغرض:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.15.0, torch==1.8.0+cu101, datasets==1.17.0, tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 8.5806}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] تحتاج شركتنا إلى نظام لتوليد بيانات نصية تحاكي الردود الآلية على الدردشة. يمكنك استخدام الـ API التالي لهذا الغرض:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.15.0, torch==1.8.0+cu101, datasets==1.17.0, tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 8.5806}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] نود تطوير نظام يستخدم الذكاء الاصطناعي لتقديم توصيات للأسئلة المتداولة بشكل متكرر. قم باستخدام الـ API التالي لتنفيذ ذلك:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.15.0, torch==1.8.0+cu101, datasets==1.17.0, tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 8.5806}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] نريد تطوير نظام قادر على التعرف على أوجه الإبهام في الإدارة. استخدم الـ API التالي لهذا الغرض:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.15.0, torch==1.8.0+cu101, datasets==1.17.0, tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 8.5806}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] تحتاج منظمتنا إلى نظام لتحليل الملامح الوجهية وتقديم تقييمات للتعبيرات الوجهية. يمكنك استخدام الـ API التالي لهذا الهدف:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.15.0, torch==1.8.0+cu101, datasets==1.17.0, tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 8.5806}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] عملاؤنا يتوقون لنظام يستطيع تقديم تحليلات تفصيلية للنصوص الصوتية. استخدم الـ API التالي لذلك:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.15.0, torch==1.8.0+cu101, datasets==1.17.0, tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 8.5806}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] تطلب شركتنا إنشاء نظام يمكنه تحديد الأشياء غير المرغوب فيها في الفيديوهات. يمكنك استخدام الـ API التالي لتطبيق هذا النظام:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.15.0, torch==1.8.0+cu101, datasets==1.17.0, tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 8.5806}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST] نريد تطوير نظام للتعرف التلقائي على الأشخاص من خلال صورهم. استخدم الـ API التالي لهذا الهدف:? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa', 'api_call': \"pipeline('question-answering', model='tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.15.0, torch==1.8.0+cu101, datasets==1.17.0, tokenizers==0.10.3', 'example_code': '', 'performance': {'dataset': 'unknown', 'accuracy': {'Loss': 8.5806}}, 'description': 'This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.' </s>"
"<s>[INST]كتابة تقرير علمي يستعرض أهم الاكتشافات العلمية في مجال الذكاء الاصطناعي خلال العقد الأخير.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Graph Classification', 'api_name': 'graphormer-base-pcqm4mv2', 'api_call': \"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers', 'example_code': 'See the Graph Classification with Transformers tutorial.', 'performance': {'dataset': 'PCQM4M-LSCv2', 'accuracy': 'Not provided'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling.' </s>"
"<s>[INST] تطوير تطبيق يقوم بتحديد نوع الطعام من الوجبة الغذائية المدخلة عبر صورة.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Graph Classification', 'api_name': 'graphormer-base-pcqm4mv2', 'api_call': \"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers', 'example_code': 'See the Graph Classification with Transformers tutorial.', 'performance': {'dataset': 'PCQM4M-LSCv2', 'accuracy': 'Not provided'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling.' </s>"
"<s>[INST] إنشاء روبوت تواصل اجتماعي قادر على توليد أفكار لمقاطع الفيديو الترويجية.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Graph Classification', 'api_name': 'graphormer-base-pcqm4mv2', 'api_call': \"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers', 'example_code': 'See the Graph Classification with Transformers tutorial.', 'performance': {'dataset': 'PCQM4M-LSCv2', 'accuracy': 'Not provided'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling.' </s>"
"<s>[INST] تصميم نموذج توقعي لتوقيت انهيار سوق الأسهم باستخدام البيانات التاريخية.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Graph Classification', 'api_name': 'graphormer-base-pcqm4mv2', 'api_call': \"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers', 'example_code': 'See the Graph Classification with Transformers tutorial.', 'performance': {'dataset': 'PCQM4M-LSCv2', 'accuracy': 'Not provided'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling.' </s>"
"<s>[INST] تطوير تطبيق يقوم بتحليل مشاعر المستخدمين من خلال تقييماتهم النصية على منصة التواصل الاجتماعي.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Graph Classification', 'api_name': 'graphormer-base-pcqm4mv2', 'api_call': \"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers', 'example_code': 'See the Graph Classification with Transformers tutorial.', 'performance': {'dataset': 'PCQM4M-LSCv2', 'accuracy': 'Not provided'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling.' </s>"
"<s>[INST] بناء نظام توصيات لتطبيق التعلم الآلي الخاص بتحليل النصوص لتوجيه المستخدمين نحو المحتوى الأكثر تناسبًا لاهتماماتهم.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Graph Classification', 'api_name': 'graphormer-base-pcqm4mv2', 'api_call': \"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers', 'example_code': 'See the Graph Classification with Transformers tutorial.', 'performance': {'dataset': 'PCQM4M-LSCv2', 'accuracy': 'Not provided'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling.' </s>"
"<s>[INST] تصميم نظام تحليل لغة الإشارة من الفيديوهات لتحويلها إلى نصوص.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Graph Classification', 'api_name': 'graphormer-base-pcqm4mv2', 'api_call': \"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers', 'example_code': 'See the Graph Classification with Transformers tutorial.', 'performance': {'dataset': 'PCQM4M-LSCv2', 'accuracy': 'Not provided'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling.' </s>"
"<s>[INST] تطوير تطبيق يستخدم تقنيات الذكاء الاصطناعي لتوليد صور فنية فريدة من نوعها.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Graph Classification', 'api_name': 'graphormer-base-pcqm4mv2', 'api_call': \"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers', 'example_code': 'See the Graph Classification with Transformers tutorial.', 'performance': {'dataset': 'PCQM4M-LSCv2', 'accuracy': 'Not provided'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling.' </s>"
"<s>[INST] إنشاء نظام تصنيف للمستندات القانونية باستخدام تقنيات تعلم الآلة.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Graph Classification', 'api_name': 'graphormer-base-pcqm4mv2', 'api_call': \"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers', 'example_code': 'See the Graph Classification with Transformers tutorial.', 'performance': {'dataset': 'PCQM4M-LSCv2', 'accuracy': 'Not provided'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling.' </s>"
"<s>[INST] تحليل بيانات العملاء لتوقع احتياجاتهم المستقبلية وتقديم خدمات مخصصة لهم.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Graph Classification', 'api_name': 'graphormer-base-pcqm4mv2', 'api_call': \"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers', 'example_code': 'See the Graph Classification with Transformers tutorial.', 'performance': {'dataset': 'PCQM4M-LSCv2', 'accuracy': 'Not provided'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling.' </s>"
"<s>[INST]إنشاء تطبيق يستخدم الـ API لتحليل وتحويل نصوص إلى صور توضح المحتوى بشكل بصري.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': \"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\", 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.' </s>"
"<s>[INST] يجب على فريق تطوير تطبيق الحجز الخاص بك استخدام الـ API لتحسين تجربة رؤية الحجوزات للمستخدمين.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': \"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\", 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.' </s>"
"<s>[INST] قم بتكامل الـ API لتوليد الصوت مع تطبيقك القائم لتحويل نصوص إلى ملفات صوتية بشكل أوتوماتيكي.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': \"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\", 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.' </s>"
"<s>[INST] سلّط الضوء على تصميم الجرافيكس والأنماط في تطبيقك باستخدام الـ API الخاص بتحليل الصور.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': \"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\", 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.' </s>"
"<s>[INST] قم بتطوير تطبيق لتوليد نصوص إبداعية باستخدام الـ API وتمكين المستخدمين من إنشاء محتوى فريد.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': \"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\", 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.' </s>"
"<s>[INST] أنشئ تجربة تفاعلية مبتكرة لمستخدمي تطبيقك عبر تكامل الـ API لتحويل النصوص إلى ملفات PDF سهلة الاستخدام والقراءة.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': \"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\", 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.' </s>"
"<s>[INST] استخدم الـ API لتحسين عرض البيانات في تطبيقك من خلال تحويل الأرقام والإحصائيات إلى رسوم بيانية بصرية.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': \"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\", 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.' </s>"
"<s>[INST] قم بتكامل الـ API لتحليل المحتوى الصوتي مع تطبيقك لاستخراج المعلومات الهامة من الملفات الصوتية وتحويلها إلى نصوص.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': \"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\", 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.' </s>"
"<s>[INST] قم بتطوير نظام توصيات شخصية لتجربة التسوق في تطبيقك باستخدام الـ API لتحليل سلوك المستخدمين واقتراح المنتجات المناسبة.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': \"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\", 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.' </s>"
"<s>[INST] ساهم في تحسين واجهة تطبيقك من خلال دمج الـ API لتحليل المشاعر وتفاعلات المستخدمين مع المحتوى للحصول على ردود فعل دقيقة ومعلومات قيمة.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': \"AutoModel.from_pretrained('graphormer-base-pcqm4mv1')\", 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.' </s>"
"<s>[INST]انشئ تطبيق يسمح للمستخدمين برفع صورة لملف نصي والحصول على تلخيص تلقائي لمحتواه.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'CQI_Visual_Question_Awnser_PT_v0', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('microsoft/layoutlm-base-uncased'))\", 'api_arguments': ['url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': [\"nlp('https://templates.invoicehome.com/invoice-template-us-neat-750px.png', 'What is the invoice number?')\", \"nlp('https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg', 'What is the purchase amount?')\", \"nlp('https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png', 'What are the 2020 net sales?')\"], 'performance': {'dataset': [{'accuracy': 0.9943977}, {'accuracy': 0.9912159}, {'accuracy': 0.59147286}]}, 'description': 'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.' </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم خوارزمية تعلم الآلة لتحليل مستندات الصور الطبية وتوفير تشخيص تلقائي للأمراض المكتشفة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'CQI_Visual_Question_Awnser_PT_v0', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('microsoft/layoutlm-base-uncased'))\", 'api_arguments': ['url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': [\"nlp('https://templates.invoicehome.com/invoice-template-us-neat-750px.png', 'What is the invoice number?')\", \"nlp('https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg', 'What is the purchase amount?')\", \"nlp('https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png', 'What are the 2020 net sales?')\"], 'performance': {'dataset': [{'accuracy': 0.9943977}, {'accuracy': 0.9912159}, {'accuracy': 0.59147286}]}, 'description': 'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.' </s>"
"<s>[INST] أنشئ تطبيقًا يستخدم تقنيات تعلم الآلة لتحليل محتوى مقاطع الفيديو واكتشاف الأشياء والأماكن والأفراد به.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'CQI_Visual_Question_Awnser_PT_v0', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('microsoft/layoutlm-base-uncased'))\", 'api_arguments': ['url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': [\"nlp('https://templates.invoicehome.com/invoice-template-us-neat-750px.png', 'What is the invoice number?')\", \"nlp('https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg', 'What is the purchase amount?')\", \"nlp('https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png', 'What are the 2020 net sales?')\"], 'performance': {'dataset': [{'accuracy': 0.9943977}, {'accuracy': 0.9912159}, {'accuracy': 0.59147286}]}, 'description': 'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.' </s>"
"<s>[INST] قم بتطوير واجهة برمجية تسمح بإدراج صورة واستخراج نصوصها بدقة عالية باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'CQI_Visual_Question_Awnser_PT_v0', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('microsoft/layoutlm-base-uncased'))\", 'api_arguments': ['url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': [\"nlp('https://templates.invoicehome.com/invoice-template-us-neat-750px.png', 'What is the invoice number?')\", \"nlp('https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg', 'What is the purchase amount?')\", \"nlp('https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png', 'What are the 2020 net sales?')\"], 'performance': {'dataset': [{'accuracy': 0.9943977}, {'accuracy': 0.9912159}, {'accuracy': 0.59147286}]}, 'description': 'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.' </s>"
"<s>[INST] أنشئ واجهة برمجية تستخدم خوارزمية تعلم الآلة للكشف عن الإحتيال في المعاملات المالية الإلكترونية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'CQI_Visual_Question_Awnser_PT_v0', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('microsoft/layoutlm-base-uncased'))\", 'api_arguments': ['url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': [\"nlp('https://templates.invoicehome.com/invoice-template-us-neat-750px.png', 'What is the invoice number?')\", \"nlp('https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg', 'What is the purchase amount?')\", \"nlp('https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png', 'What are the 2020 net sales?')\"], 'performance': {'dataset': [{'accuracy': 0.9943977}, {'accuracy': 0.9912159}, {'accuracy': 0.59147286}]}, 'description': 'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.' </s>"
"<s>[INST] قم بتصميم تطبيق يستخدم خوارزميات التعلم العميق لتصنيف التعليقات النصية إلى إيجابية وسلبية.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'CQI_Visual_Question_Awnser_PT_v0', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('microsoft/layoutlm-base-uncased'))\", 'api_arguments': ['url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': [\"nlp('https://templates.invoicehome.com/invoice-template-us-neat-750px.png', 'What is the invoice number?')\", \"nlp('https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg', 'What is the purchase amount?')\", \"nlp('https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png', 'What are the 2020 net sales?')\"], 'performance': {'dataset': [{'accuracy': 0.9943977}, {'accuracy': 0.9912159}, {'accuracy': 0.59147286}]}, 'description': 'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.' </s>"
"<s>[INST] أنشيء واجهة برمجية تستخدم تقنيات التعلم الآلي لتحليل سلوك المستخدمين على الإنترنت وتوفير توصيات استنادًا إلى ذلك التحليل.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'CQI_Visual_Question_Awnser_PT_v0', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('microsoft/layoutlm-base-uncased'))\", 'api_arguments': ['url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': [\"nlp('https://templates.invoicehome.com/invoice-template-us-neat-750px.png', 'What is the invoice number?')\", \"nlp('https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg', 'What is the purchase amount?')\", \"nlp('https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png', 'What are the 2020 net sales?')\"], 'performance': {'dataset': [{'accuracy': 0.9943977}, {'accuracy': 0.9912159}, {'accuracy': 0.59147286}]}, 'description': 'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.' </s>"
"<s>[INST] قم بتطوير واجهة برمجية تستخدم خوارزميات التعلم الآلي لتحليل صور الأرض من الأقمار الصناعية وتوفير بيانات جغرافية دقيقة.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'CQI_Visual_Question_Awnser_PT_v0', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('microsoft/layoutlm-base-uncased'))\", 'api_arguments': ['url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': [\"nlp('https://templates.invoicehome.com/invoice-template-us-neat-750px.png', 'What is the invoice number?')\", \"nlp('https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg', 'What is the purchase amount?')\", \"nlp('https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png', 'What are the 2020 net sales?')\"], 'performance': {'dataset': [{'accuracy': 0.9943977}, {'accuracy': 0.9912159}, {'accuracy': 0.59147286}]}, 'description': 'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.' </s>"
"<s>[INST] أنشيء واجهة برمجية تستخدم تقنيات تعلم الآلة لتحليل بيانات العملاء وتوقع سلوكهم التالي لتحسين التسويق.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'CQI_Visual_Question_Awnser_PT_v0', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('microsoft/layoutlm-base-uncased'))\", 'api_arguments': ['url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': [\"nlp('https://templates.invoicehome.com/invoice-template-us-neat-750px.png', 'What is the invoice number?')\", \"nlp('https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg', 'What is the purchase amount?')\", \"nlp('https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png', 'What are the 2020 net sales?')\"], 'performance': {'dataset': [{'accuracy': 0.9943977}, {'accuracy': 0.9912159}, {'accuracy': 0.59147286}]}, 'description': 'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.' </s>"
"<s>[INST] قم بإنشاء واجهة برمجية تستخدم خوارزميات التعلم العميق لتحليل التعقيد الحسابي للمشروعات الهندسية وتوفير تقديرات دقيقة للتكاليف.? [/INST]'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Document Question Answering', 'api_name': 'CQI_Visual_Question_Awnser_PT_v0', 'api_call': \"pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('microsoft/layoutlm-base-uncased'))\", 'api_arguments': ['url', 'question'], 'python_environment_requirements': ['PIL', 'pytesseract', 'PyTorch', 'transformers'], 'example_code': [\"nlp('https://templates.invoicehome.com/invoice-template-us-neat-750px.png', 'What is the invoice number?')\", \"nlp('https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg', 'What is the purchase amount?')\", \"nlp('https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png', 'What are the 2020 net sales?')\"], 'performance': {'dataset': [{'accuracy': 0.9943977}, {'accuracy': 0.9912159}, {'accuracy': 0.59147286}]}, 'description': 'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.' </s>"
"<s>[INST]تُطلَب منك تطوير نظام تعليمي يستخدم تقنيات التعلم الآلي لتحليل أنماط الكتابات اليدوية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'tiny-random-DPTForDepthEstimation', 'api_call': \"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random DPT model for depth estimation using Hugging Face Transformers library.' </s>"
"<s>[INST] قم ببناء نظام يعتمد على تكنولوجيا معالجة اللغة الطبيعية لتحليل السلوكيات اللغوية في مناشط التعلم عبر الإنترنت.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'tiny-random-DPTForDepthEstimation', 'api_call': \"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random DPT model for depth estimation using Hugging Face Transformers library.' </s>"
"<s>[INST] قم بتطوير تطبيق قادر على استخراج المعلومات الرئيسية من مقاطع الفيديو الطبية باستخدام تقنيات الرؤية الحاسوبية المتقدمة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'tiny-random-DPTForDepthEstimation', 'api_call': \"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random DPT model for depth estimation using Hugging Face Transformers library.' </s>"
"<s>[INST] ابدأ في إنشاء نظام تحليل بيانات العملاء لتلخيص الاتجاهات والميول الشرائية باستخدام تقنيات تنقيب البيانات.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'tiny-random-DPTForDepthEstimation', 'api_call': \"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random DPT model for depth estimation using Hugging Face Transformers library.' </s>"
"<s>[INST] ابنِ نظاماً يقوم بتحليل التعليقات على منصات التواصل الاجتماعي واستخراج الأفكار الرئيسية والمشاعر العامة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'tiny-random-DPTForDepthEstimation', 'api_call': \"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random DPT model for depth estimation using Hugging Face Transformers library.' </s>"
"<s>[INST] قم بتنفيذ تطبيق يستخدم تقنيات تعلم الآلة لفهم نمط استخدام التطبيقات الذكية واقتراح تحسينات.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'tiny-random-DPTForDepthEstimation', 'api_call': \"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random DPT model for depth estimation using Hugging Face Transformers library.' </s>"
"<s>[INST] ابنِ نظام يستخدم تكنولوجيا البيانات الضخمة لتحليل سلوك المستخدمين على الإنترنت وتوقع الاهتمامات المستقبلية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'tiny-random-DPTForDepthEstimation', 'api_call': \"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random DPT model for depth estimation using Hugging Face Transformers library.' </s>"
"<s>[INST] قم ببناء تطبيق يستخدم تقنيات التعلم العميق لتصنيف الصور الطبية وتحديد تشخيصات طبية محتملة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'tiny-random-DPTForDepthEstimation', 'api_call': \"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random DPT model for depth estimation using Hugging Face Transformers library.' </s>"
"<s>[INST] قم بتطوير نظام يقوم بتحليل بيانات الأسهم وتوقع الاتجاهات السعرية باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'tiny-random-DPTForDepthEstimation', 'api_call': \"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random DPT model for depth estimation using Hugging Face Transformers library.' </s>"
"<s>[INST] تطلب المهمة بناء نظام يختص بمعالجة البيانات الصوتية وتحليلها لاستخراج معلومات قيمة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'tiny-random-DPTForDepthEstimation', 'api_call': \"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random DPT model for depth estimation using Hugging Face Transformers library.' </s>"
"<s>[INST]تصميم نظام يحلل الأخبار ويقوم بتصنيفها تلقائيًا باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dpt-large-redesign', 'api_call': \"AutoModelForDepthEstimation.from_pretrained('nielsr/dpt-large-redesign')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A depth estimation model based on the DPT architecture.' </s>"
"<s>[INST] إنشاء نظام توصيف يعتمد على الذكاء الاصطناعي لتحليل سلوك المستخدمين في التطبيقات الرقمية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dpt-large-redesign', 'api_call': \"AutoModelForDepthEstimation.from_pretrained('nielsr/dpt-large-redesign')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A depth estimation model based on the DPT architecture.' </s>"
"<s>[INST] بناء نظام توصيف يعتمد على الذكاء الاصطناعي لتحليل الصور الطبية وتقديم توصيات تشخيصية سريعة ودقيقة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dpt-large-redesign', 'api_call': \"AutoModelForDepthEstimation.from_pretrained('nielsr/dpt-large-redesign')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A depth estimation model based on the DPT architecture.' </s>"
"<s>[INST] تطوير نظام تقدير الأعمار للصور الفوتوغرافية باستخدام تقنيات تعلم الآلة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dpt-large-redesign', 'api_call': \"AutoModelForDepthEstimation.from_pretrained('nielsr/dpt-large-redesign')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A depth estimation model based on the DPT architecture.' </s>"
"<s>[INST] إنشاء نظام تحليل وتصنيف الموسيقى استنادًا إلى سماتها الصوتية باستخدام تقنيات معالجة الإشارات الصوتية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dpt-large-redesign', 'api_call': \"AutoModelForDepthEstimation.from_pretrained('nielsr/dpt-large-redesign')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A depth estimation model based on the DPT architecture.' </s>"
"<s>[INST] توظيف تقنيات تعلم الآلة لتطوير نظام توصيف يتنبأ بأداء الطلاب في المقاطع الدراسية المختلفة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dpt-large-redesign', 'api_call': \"AutoModelForDepthEstimation.from_pretrained('nielsr/dpt-large-redesign')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A depth estimation model based on the DPT architecture.' </s>"
"<s>[INST] بناء نظام تحليل السلوك الاستهلاكي للعملاء وتوفير توصيات شخصية في مجال التسويق.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dpt-large-redesign', 'api_call': \"AutoModelForDepthEstimation.from_pretrained('nielsr/dpt-large-redesign')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A depth estimation model based on the DPT architecture.' </s>"
"<s>[INST] تطوير نظام تحليل وتحويل الخط العربي إلى نصوص إلكترونية باستخدام نماذج التعلم العميق.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dpt-large-redesign', 'api_call': \"AutoModelForDepthEstimation.from_pretrained('nielsr/dpt-large-redesign')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A depth estimation model based on the DPT architecture.' </s>"
"<s>[INST] تصميم نظام يقوم بتحليل البيانات الصوتية وكشف الإشارات الصوتية لتقديم تشخيص سريع لاضطرابات النطق.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dpt-large-redesign', 'api_call': \"AutoModelForDepthEstimation.from_pretrained('nielsr/dpt-large-redesign')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A depth estimation model based on the DPT architecture.' </s>"
"<s>[INST] إنشاء نظام تصنيف للنصوص باللغة العربية يعتمد على تقنيات تعلم الآلة ومكتبة التحليل اللغوي الطبيعي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'dpt-large-redesign', 'api_call': \"AutoModelForDepthEstimation.from_pretrained('nielsr/dpt-large-redesign')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A depth estimation model based on the DPT architecture.' </s>"
"<s>[INST]لتحسين فعالية تسويق المنتجات، نحتاج إلى تحليل الصور الخاصة بالمنتجات وتصنيفها تلقائياً.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-GLPNForDepthEstimation', 'api_call': \"AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.' </s>"
"<s>[INST] تطوير تطبيق يمكنه تحديد أماكن السيارات الخالية في مواقف السيارات باستخدام تقنيات الذكاء الصناعي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-GLPNForDepthEstimation', 'api_call': \"AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.' </s>"
"<s>[INST] لتحسين أداء نماذج التعلم الآلي في التعرف على الكلمات العربية، يجب تحسين تطبيقات الـ NLP الخاصة بنا.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-GLPNForDepthEstimation', 'api_call': \"AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.' </s>"
"<s>[INST] إنشاء نظام لتحليل المشاهد في الوقت الحقيقي لتحسين أمن الفضاءات العامة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-GLPNForDepthEstimation', 'api_call': \"AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.' </s>"
"<s>[INST] التطوير الآلي لتحليل النصوص الإشهارية لتحسين استراتيجيات التسويق الرقمي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-GLPNForDepthEstimation', 'api_call': \"AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.' </s>"
"<s>[INST] تصميم نظام تحليل وتحديد الملامح الوجهية في الصور لتعزيز دقة التعرف على الأفراد.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-GLPNForDepthEstimation', 'api_call': \"AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.' </s>"
"<s>[INST] إنشاء نظام للكشف عن التشويه في الصور الطبية لتحسين دقة تحليل الأشعة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-GLPNForDepthEstimation', 'api_call': \"AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.' </s>"
"<s>[INST] تطوير نظام لتحليل التعابير الوجهية في الفيديوهات لفهم المشاعر والتفاعلات البشرية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-GLPNForDepthEstimation', 'api_call': \"AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.' </s>"
"<s>[INST] إنشاء واجهة برمجية لتوليد مخططات بيانية تفاعلية لتحليل بيانات الأداء في الشركات.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-GLPNForDepthEstimation', 'api_call': \"AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.' </s>"
"<s>[INST] تصميم نظام لتحليل الصوتيات وتحويلها إلى نصوص في المكالمات الهاتفية لتحسين خدمات خدمة العملاء.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'hf-tiny-model-private/tiny-random-GLPNForDepthEstimation', 'api_call': \"AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')\", 'api_arguments': [], 'python_environment_requirements': ['torch', 'transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.' </s>"
"<s>[INST]استخدم UniXcoder لتحويل سلسلة نصية إلى تنسيق Markdown واحفظ النتيجة في ملف HTML.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'KITTI', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] إنشاء تطبيق بسيط باستخدام API Google Maps لعرض المواقع الجغرافية على الخريطة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'KITTI', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] تصميم تطبيق ويب يستخدم API OpenWeatherMap لإظهار توقعات الطقس الحالية والمستقبلية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'KITTI', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] كتابة تقرير تحليلي يقوم باستخدام بيانات واستدعاء API Amazon S3 للوصول إلى مجموعة متنوعة من الملفات.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'KITTI', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] إنشاء تطبيق تواصل اجتماعي جديد باستخدام API Twitter للتغريد ومشاركة المحتوى.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'KITTI', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] ابنِ نموذج تعلم آلي يستخدم API TensorFlow للكشف عن الكائنات في صور.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'KITTI', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] تصميم تطبيق ويب يستخدم API YouTube لاقتراح مقاطع الفيديو استنادًا إلى اهتمامات المستخدم.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'KITTI', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] إنشاء أداة تحليل بيانات تدعم استرداد البيانات باستخدام API Google BigQuery.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'KITTI', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] تطوير تطبيق الهاتف الذكي الذي يستخدم API Spotify لتشغيل الموسيقى عبر الإنترنت.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'KITTI', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] إعداد نظام توصيات للمستخدمين يستخدم API Amazon Personalize لتوصيات المنتجات.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': 'transformers', 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'KITTI', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST]تحتاج إلى تنفيذ خوارزمية استشفاف شبكة عناصر نقاط الضعف لتحليل أمان الشبكة الخاصة بشركتك. استخدم إطار عمل Scapy لتنفيذ الخوارزمية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-230131-041708', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4425, 'Mae': 0.427, 'Rmse': 0.6196, 'Abs_Rel': 0.4543, 'Log_Mae': 0.1732, 'Log_Rmse': 0.2288, 'Delta1': 0.3787, 'Delta2': 0.6298, 'Delta3': 0.8083}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] قم بإنشاء تطبيق واجهة مستخدم لتصدير بيانات الاستشعار من جهاز IoT إلى قاعدة بيانات سحابية باستخدام تقنية RESTful API.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-230131-041708', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4425, 'Mae': 0.427, 'Rmse': 0.6196, 'Abs_Rel': 0.4543, 'Log_Mae': 0.1732, 'Log_Rmse': 0.2288, 'Delta1': 0.3787, 'Delta2': 0.6298, 'Delta3': 0.8083}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] أنشئ نظام للتعرف على وجوه الأشخاص في مقاطع الفيديو باستخدام مكتبة OpenCV لمعالجة الصور.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-230131-041708', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4425, 'Mae': 0.427, 'Rmse': 0.6196, 'Abs_Rel': 0.4543, 'Log_Mae': 0.1732, 'Log_Rmse': 0.2288, 'Delta1': 0.3787, 'Delta2': 0.6298, 'Delta3': 0.8083}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] إنشاء تطبيق يقوم بتوليد تقارير تحليلية مفصلة حول تدفق البيانات في نظام الشبكات الخاص بمؤسستك باستخدام تقنية الواجهة البينية RESTful API.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-230131-041708', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4425, 'Mae': 0.427, 'Rmse': 0.6196, 'Abs_Rel': 0.4543, 'Log_Mae': 0.1732, 'Log_Rmse': 0.2288, 'Delta1': 0.3787, 'Delta2': 0.6298, 'Delta3': 0.8083}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] تكليف بتصميم نظام استشعار يتفاعل مع البيانات البيئية ويقوم بإرسالها إلى خادم سحابي للتحليل الآني. يجب أن يتم تنفيذ النظام باستخدام تقنيات MQTT و RESTful.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-230131-041708', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4425, 'Mae': 0.427, 'Rmse': 0.6196, 'Abs_Rel': 0.4543, 'Log_Mae': 0.1732, 'Log_Rmse': 0.2288, 'Delta1': 0.3787, 'Delta2': 0.6298, 'Delta3': 0.8083}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] إنشاء تطبيق يقوم بتحليل بيانات الأمان والحماية في الشبكة الخاصة بشركتك باستخدام إطار عمل Splunk.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-230131-041708', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4425, 'Mae': 0.427, 'Rmse': 0.6196, 'Abs_Rel': 0.4543, 'Log_Mae': 0.1732, 'Log_Rmse': 0.2288, 'Delta1': 0.3787, 'Delta2': 0.6298, 'Delta3': 0.8083}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] يرغب العملاء في تطوير تطبيق لتتبع الشحنات عبر الإنترنت عن طريق مسح أو تعيين أرقام باركود. يمكنك تنفيذ هذا باستخدام مكتبة ZXing لمعالجة الباركود.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-230131-041708', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4425, 'Mae': 0.427, 'Rmse': 0.6196, 'Abs_Rel': 0.4543, 'Log_Mae': 0.1732, 'Log_Rmse': 0.2288, 'Delta1': 0.3787, 'Delta2': 0.6298, 'Delta3': 0.8083}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] تنفيذ نظام لتصنيف النصوص باللغة العربية باستخدام تقنية التعلم العميق. يمكنك استخدام مكتبة TensorFlow لتحقيق ذلك.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-230131-041708', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4425, 'Mae': 0.427, 'Rmse': 0.6196, 'Abs_Rel': 0.4543, 'Log_Mae': 0.1732, 'Log_Rmse': 0.2288, 'Delta1': 0.3787, 'Delta2': 0.6298, 'Delta3': 0.8083}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] بناء نظام توصيف آلي للمستحضرات الصيدلانية بناءً على التركيبة الكيميائية باستخدام إطار عمل RDKit.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-230131-041708', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4425, 'Mae': 0.427, 'Rmse': 0.6196, 'Abs_Rel': 0.4543, 'Log_Mae': 0.1732, 'Log_Rmse': 0.2288, 'Delta1': 0.3787, 'Delta2': 0.6298, 'Delta3': 0.8083}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] إنشاء تطبيق للتحليل اللغوي للنصوص باللغة الإنجليزية باستخدام مكتبة NLTK لمعالجة اللغة الطبيعية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-230131-041708', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4425, 'Mae': 0.427, 'Rmse': 0.6196, 'Abs_Rel': 0.4543, 'Log_Mae': 0.1732, 'Log_Rmse': 0.2288, 'Delta1': 0.3787, 'Delta2': 0.6298, 'Delta3': 0.8083}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST]يحتاج المستخدم إلى إضافة نظام تعرف على الكلام إلى تطبيقه لزيادة تفاعلية التجربة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Monocular Depth Estimation', 'api_name': 'Intel/dpt-large', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-large'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPTImageProcessor, DPTForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'MIX 6', 'accuracy': '10.82'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.' </s>"
"<s>[INST] تطلب الشركة تطوير تطبيق يتيح للمستخدمين استكشاف وحجز تحف فنية عبر الإنترنت.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Monocular Depth Estimation', 'api_name': 'Intel/dpt-large', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-large'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPTImageProcessor, DPTForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'MIX 6', 'accuracy': '10.82'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.' </s>"
"<s>[INST] صاحب المتجر يطلب تكامل نظام الدفع على تطبيقه مع خدمة دفع آمنة وموثوقة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Monocular Depth Estimation', 'api_name': 'Intel/dpt-large', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-large'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPTImageProcessor, DPTForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'MIX 6', 'accuracy': '10.82'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.' </s>"
"<s>[INST] المستخدم يرغب في تكامل نظام إشعارات فورية عبر التطبيق الخاص به.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Monocular Depth Estimation', 'api_name': 'Intel/dpt-large', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-large'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPTImageProcessor, DPTForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'MIX 6', 'accuracy': '10.82'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.' </s>"
"<s>[INST] الشركة تطلب تكامل نظام تحليل بيانات العملاء لتحسين استراتيجيات التسويق.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Monocular Depth Estimation', 'api_name': 'Intel/dpt-large', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-large'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPTImageProcessor, DPTForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'MIX 6', 'accuracy': '10.82'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.' </s>"
"<s>[INST] يرغب المستخدم في تحسين تجربة التسوق عبر تطبيقه بإضافة نظام توصيات شخصية للعملاء.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Monocular Depth Estimation', 'api_name': 'Intel/dpt-large', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-large'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPTImageProcessor, DPTForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'MIX 6', 'accuracy': '10.82'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.' </s>"
"<s>[INST] مطلوب تكامل نظام إدارة تذاكر الدعم الفني لتحسين استجابة الشركة لاستفسارات العملاء.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Monocular Depth Estimation', 'api_name': 'Intel/dpt-large', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-large'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPTImageProcessor, DPTForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'MIX 6', 'accuracy': '10.82'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.' </s>"
"<s>[INST] صاحب الموقع يرغب في تضمين نظام تقييم وتعليقات المستخدمين على صفحات منتجاته.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Monocular Depth Estimation', 'api_name': 'Intel/dpt-large', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-large'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPTImageProcessor, DPTForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'MIX 6', 'accuracy': '10.82'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.' </s>"
"<s>[INST] مطلوب تكامل نظام إرسال رسائل بريد إلكتروني تلقائية لمستخدمي التطبيق.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Monocular Depth Estimation', 'api_name': 'Intel/dpt-large', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-large'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPTImageProcessor, DPTForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'MIX 6', 'accuracy': '10.82'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.' </s>"
"<s>[INST] يرغب المستخدم في تكامل نظام معالجة اللغة الطبيعية لتحسين إمكانيات بحث التطبيق.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Monocular Depth Estimation', 'api_name': 'Intel/dpt-large', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-large'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import DPTImageProcessor, DPTForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'MIX 6', 'accuracy': '10.82'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.' </s>"
"<s>[INST]لإضافة ميزة تمييز الأشخاص على الصور في تطبيقنا الجديد، تحتاج إلى استخدام خدمة تحليل الوجوه المبتكرة. اقترح API لتحديد مواقع الوجوه في الصور.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-nyu')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': ['transformers', 'torch', 'numpy', 'PIL', 'requests'], 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-nyu)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=bicubic, align_corners=False,)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'NYUv2', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] لتحليل محتوى النصوص في موقعنا واقتراح المحتوى المناسب لكل مستخدم، يجب استخدام خدمة توليد اللغات الطبيعية. اقترح API لتوليد النصوص بشكل تلقائي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-nyu')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': ['transformers', 'torch', 'numpy', 'PIL', 'requests'], 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-nyu)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=bicubic, align_corners=False,)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'NYUv2', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] لتحسين تجربة تنقل المستخدمين في تطبيقنا، نريد إضافة خاصية الاقتراحات التلقائية لتحديد المواقع القريبة. اقترح API لتحديد المواقع على خريطة تفاعلية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-nyu')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': ['transformers', 'torch', 'numpy', 'PIL', 'requests'], 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-nyu)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=bicubic, align_corners=False,)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'NYUv2', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] لإثراء محتوى تطبيقنا بالعروض والتخفيضات، نريد دمج محرك بحث تلقائي لاقتراح العروض المناسبة لكل مستخدم. اقترح API لتنفيذ نظام البحث والتصفية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-nyu')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': ['transformers', 'torch', 'numpy', 'PIL', 'requests'], 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-nyu)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=bicubic, align_corners=False,)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'NYUv2', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] لدينا متجر إلكتروني ونحتاج إلى تحسين عمليات الدفع والتحقق من الهوية. اقترح خدمة إدارة المدفوعات والتحقق.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-nyu')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': ['transformers', 'torch', 'numpy', 'PIL', 'requests'], 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-nyu)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=bicubic, align_corners=False,)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'NYUv2', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] لتحسين تفاعل المستخدم مع منشوراتنا على وسائل التواصل الاجتماعي، نريد تقديم تحليلات مفصلة حول أداء المحتوى. اقترح API لتحليل تفاعلات المستخدمين على منصات التواصل الاجتماعي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-nyu')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': ['transformers', 'torch', 'numpy', 'PIL', 'requests'], 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-nyu)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=bicubic, align_corners=False,)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'NYUv2', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] يرغب فريق الدعم الفني في تحسين وقت الاستجابة للاستفسارات العملية. اقترح خدمة تخزين الأسئلة الشائعة والإجابات الآلية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-nyu')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': ['transformers', 'torch', 'numpy', 'PIL', 'requests'], 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-nyu)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=bicubic, align_corners=False,)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'NYUv2', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] لتنظيم عملية تقديم العروض الترويجية في تطبيقنا، نريد استخدام نظام إدارة العروض. اقترح API لإنشاء وتتبع العروض الترويجية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-nyu')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': ['transformers', 'torch', 'numpy', 'PIL', 'requests'], 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-nyu)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=bicubic, align_corners=False,)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'NYUv2', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] لتحسين تحليل البيانات في تطبيقنا، نريد تضمين نظام تحليل الأداء الذكي. اقترح API لتحليل البيانات وتقديم التوصيات.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-nyu')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': ['transformers', 'torch', 'numpy', 'PIL', 'requests'], 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-nyu)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=bicubic, align_corners=False,)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'NYUv2', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST] يرغب فريق التطوير في تشغيل اختبارات الوحدة بشكل أوتوماتيكي لضمان جودة الكود. اقترح أداة لتنفيذ اختبارات الوحدة تلقائيًا.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu', 'api_call': \"GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-nyu')\", 'api_arguments': 'images, return_tensors', 'python_environment_requirements': ['transformers', 'torch', 'numpy', 'PIL', 'requests'], 'example_code': 'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-nyu)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=bicubic, align_corners=False,)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)', 'performance': {'dataset': 'NYUv2', 'accuracy': 'Not provided'}, 'description': 'Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.' </s>"
"<s>[INST]استكشف كيف يمكن استخدام تقنية Computer Vision لتحديد المناطق الصعبة في صور الطبيعة. قم بتطبيق الـ API التالي للحصول على تقدير العمق:? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'Intel/dpt-hybrid-midas', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-hybrid-midas', low_cpu_mem_usage=True)\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-hybrid-midas', 'low_cpu_mem_usage': 'True'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'numpy', 'requests'], 'example_code': 'from PIL import Image\\nimport numpy as np\\nimport requests\\nimport torch\\nfrom transformers import DPTForDepthEstimation, DPTFeatureExtractor\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-hybrid-midas, low_cpu_mem_usage=True)\\nfeature_extractor = DPTFeatureExtractor.from_pretrained(Intel/dpt-hybrid-midas)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\\ndepth.show()', 'performance': {'dataset': 'MIX 6', 'accuracy': '11.06'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.' </s>"
"<s>[INST] قم بتعليم النموذج على تمييز الكائنات غير التقليدية في صور الفن الحديث باستخدام الـ API التالي:? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'Intel/dpt-hybrid-midas', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-hybrid-midas', low_cpu_mem_usage=True)\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-hybrid-midas', 'low_cpu_mem_usage': 'True'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'numpy', 'requests'], 'example_code': 'from PIL import Image\\nimport numpy as np\\nimport requests\\nimport torch\\nfrom transformers import DPTForDepthEstimation, DPTFeatureExtractor\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-hybrid-midas, low_cpu_mem_usage=True)\\nfeature_extractor = DPTFeatureExtractor.from_pretrained(Intel/dpt-hybrid-midas)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\\ndepth.show()', 'performance': {'dataset': 'MIX 6', 'accuracy': '11.06'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.' </s>"
"<s>[INST] أنشىء تطبيقًا يمكنه تحليل عمق الخلفية في الصورة بدقة باستخدام API:? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'Intel/dpt-hybrid-midas', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-hybrid-midas', low_cpu_mem_usage=True)\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-hybrid-midas', 'low_cpu_mem_usage': 'True'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'numpy', 'requests'], 'example_code': 'from PIL import Image\\nimport numpy as np\\nimport requests\\nimport torch\\nfrom transformers import DPTForDepthEstimation, DPTFeatureExtractor\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-hybrid-midas, low_cpu_mem_usage=True)\\nfeature_extractor = DPTFeatureExtractor.from_pretrained(Intel/dpt-hybrid-midas)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\\ndepth.show()', 'performance': {'dataset': 'MIX 6', 'accuracy': '11.06'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.' </s>"
"<s>[INST] قم بتطبيق طريقة تقدير العمق العالي الدقة على صور الطبيعة باستخدام الـ API:? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'Intel/dpt-hybrid-midas', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-hybrid-midas', low_cpu_mem_usage=True)\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-hybrid-midas', 'low_cpu_mem_usage': 'True'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'numpy', 'requests'], 'example_code': 'from PIL import Image\\nimport numpy as np\\nimport requests\\nimport torch\\nfrom transformers import DPTForDepthEstimation, DPTFeatureExtractor\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-hybrid-midas, low_cpu_mem_usage=True)\\nfeature_extractor = DPTFeatureExtractor.from_pretrained(Intel/dpt-hybrid-midas)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\\ndepth.show()', 'performance': {'dataset': 'MIX 6', 'accuracy': '11.06'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.' </s>"
"<s>[INST] قم بتطبيق تقنية التقدير الثلاثي البعد على صور الأطفال لتحليل الأبعاد باستخدام API:? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'Intel/dpt-hybrid-midas', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-hybrid-midas', low_cpu_mem_usage=True)\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-hybrid-midas', 'low_cpu_mem_usage': 'True'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'numpy', 'requests'], 'example_code': 'from PIL import Image\\nimport numpy as np\\nimport requests\\nimport torch\\nfrom transformers import DPTForDepthEstimation, DPTFeatureExtractor\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-hybrid-midas, low_cpu_mem_usage=True)\\nfeature_extractor = DPTFeatureExtractor.from_pretrained(Intel/dpt-hybrid-midas)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\\ndepth.show()', 'performance': {'dataset': 'MIX 6', 'accuracy': '11.06'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.' </s>"
"<s>[INST] قم بتشكيل الأشكال والهياكل الهندسية في صور العمارة باستخدام تكنولوجيا التحليل العميق المبني على هيكلية Hugging Face Transformers. استخدم الـ API التالي:? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'Intel/dpt-hybrid-midas', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-hybrid-midas', low_cpu_mem_usage=True)\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-hybrid-midas', 'low_cpu_mem_usage': 'True'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'numpy', 'requests'], 'example_code': 'from PIL import Image\\nimport numpy as np\\nimport requests\\nimport torch\\nfrom transformers import DPTForDepthEstimation, DPTFeatureExtractor\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-hybrid-midas, low_cpu_mem_usage=True)\\nfeature_extractor = DPTFeatureExtractor.from_pretrained(Intel/dpt-hybrid-midas)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\\ndepth.show()', 'performance': {'dataset': 'MIX 6', 'accuracy': '11.06'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.' </s>"
"<s>[INST] ابتكر أداة تساعد في استعراض العناصر غير المألوفة في صور السفر باستخدام تقنية التحليل العميق الموجودة في Hugging Face Transformers. استخدم API:? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'Intel/dpt-hybrid-midas', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-hybrid-midas', low_cpu_mem_usage=True)\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-hybrid-midas', 'low_cpu_mem_usage': 'True'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'numpy', 'requests'], 'example_code': 'from PIL import Image\\nimport numpy as np\\nimport requests\\nimport torch\\nfrom transformers import DPTForDepthEstimation, DPTFeatureExtractor\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-hybrid-midas, low_cpu_mem_usage=True)\\nfeature_extractor = DPTFeatureExtractor.from_pretrained(Intel/dpt-hybrid-midas)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\\ndepth.show()', 'performance': {'dataset': 'MIX 6', 'accuracy': '11.06'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.' </s>"
"<s>[INST] ابتكر آلية لكشف العوائق والتغيرات في الصور الطبيعية باستخدام خوارزمية تقدير العمق الموجودة في Hugging Face Transformers. استخدم الـ API:? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'Intel/dpt-hybrid-midas', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-hybrid-midas', low_cpu_mem_usage=True)\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-hybrid-midas', 'low_cpu_mem_usage': 'True'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'numpy', 'requests'], 'example_code': 'from PIL import Image\\nimport numpy as np\\nimport requests\\nimport torch\\nfrom transformers import DPTForDepthEstimation, DPTFeatureExtractor\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-hybrid-midas, low_cpu_mem_usage=True)\\nfeature_extractor = DPTFeatureExtractor.from_pretrained(Intel/dpt-hybrid-midas)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\\ndepth.show()', 'performance': {'dataset': 'MIX 6', 'accuracy': '11.06'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.' </s>"
"<s>[INST] قم بتحليل الظلال والعناصر الغامضة في صور المناظر الطبيعية باستخدام تقنية تقدير العمق مع Hugging Face Transformers. استخدم الـ API:? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'Intel/dpt-hybrid-midas', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-hybrid-midas', low_cpu_mem_usage=True)\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-hybrid-midas', 'low_cpu_mem_usage': 'True'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'numpy', 'requests'], 'example_code': 'from PIL import Image\\nimport numpy as np\\nimport requests\\nimport torch\\nfrom transformers import DPTForDepthEstimation, DPTFeatureExtractor\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-hybrid-midas, low_cpu_mem_usage=True)\\nfeature_extractor = DPTFeatureExtractor.from_pretrained(Intel/dpt-hybrid-midas)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\\ndepth.show()', 'performance': {'dataset': 'MIX 6', 'accuracy': '11.06'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.' </s>"
"<s>[INST] قم بتحويل صور الفضاء إلى صور ثلاثية الأبعاد لتحليل العناصر الغريبة باستخدام تقنية التقدير العميق المعتمدة على Hugging Face Transformers. استخدم الـ API:? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'Intel/dpt-hybrid-midas', 'api_call': \"DPTForDepthEstimation.from_pretrained('Intel/dpt-hybrid-midas', low_cpu_mem_usage=True)\", 'api_arguments': {'pretrained_model_name_or_path': 'Intel/dpt-hybrid-midas', 'low_cpu_mem_usage': 'True'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'numpy', 'requests'], 'example_code': 'from PIL import Image\\nimport numpy as np\\nimport requests\\nimport torch\\nfrom transformers import DPTForDepthEstimation, DPTFeatureExtractor\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-hybrid-midas, low_cpu_mem_usage=True)\\nfeature_extractor = DPTFeatureExtractor.from_pretrained(Intel/dpt-hybrid-midas)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\\ndepth.show()', 'performance': {'dataset': 'MIX 6', 'accuracy': '11.06'}, 'description': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.' </s>"
"<s>[INST]تنفيذ تحليل الصور لتصنيف الوجوه في ملفات الوسائط الاجتماعية واقتراح أو إخراج التسميات الخاصة بهم.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-093747', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\", 'api_arguments': [], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تطوير نظام للكشف عن التلاعب في الصور في منصة الوسائط الاجتماعية. اقتراح استخدام API لتحليل وتحديد أي تعديلات غير مصرح بها على الصور.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-093747', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\", 'api_arguments': [], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تطوير نظام لاستخراج المعلومات النصية من صور المنتجات لتحسين تصنيف السلع على المواقع التجارية. اقتراح استخدام خدمة تحليل النص من Google Cloud Vision API.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-093747', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\", 'api_arguments': [], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تنفيذ تحليل تصديرات الشركات وبناء نموذج لتوقع الاتجاهات الاقتصادية المستقبلية. استخدم API للتحليل الإحصائي والاقتصادي مثل بيانات الاقتصاد العالمي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-093747', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\", 'api_arguments': [], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تطوير نظام لمراقبة النظام اللحظي عن بُعد وتحليل البيانات للكشف عن مشكلات الأداء والتحسينات. اقتراح استخدام واجهة برمجة تطبيقات مانجو المتخصصة في الرصد والإبلاغ.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-093747', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\", 'api_arguments': [], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تنفيذ تحليل السيرة الذاتية لمساعدة الشركات في اختيار المرشحين المناسبين للوظائف الشاغرة. استخدم API لمعالجة اللغة الطبيعية لتحليل المهارات والخبرات.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-093747', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\", 'api_arguments': [], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تنفيذ نظام توصية للمستخدمين على منصة التجارة الإلكترونية استند إلى تاريخ التصفح والمشتريات السابقة. اقترح استخدام API لتطوير نماذج التوصية وتحليل سلوك المستخدمين.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-093747', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\", 'api_arguments': [], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تطوير نظام للتحليل الصوتي لمراقبة الاستماع عبر الفيديوهات التعليمية وتقديم تقارير حول فهم وتفاعل الطلاب. اقترح استخدام خدمة تحليل الصوت من Google Cloud Speech-to-Text API.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-093747', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\", 'api_arguments': [], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تنفيذ تحليل البيانات الطبية لتوفير تشخيص دقيق وتوجيه العلاج بناءً على بيانات المريض. اقترح استخدام API للتحليل الطبي واستخراج المعلومات الحيوية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-093747', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\", 'api_arguments': [], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تطوير نظام تحليل نغمات الموسيقى لتوليد قوائم تشغيل تلقائية وتوصيات بأغاني جديدة. اقترح API لتحليل الصوت واستخراج الخصائص الصوتية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-093747', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\", 'api_arguments': [], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST]تصميم نظام لتحديد مواقع الحيوانات البرية باستخدام تقنيات التعرف على الصور.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-092352', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-092352')\", 'api_arguments': {}, 'python_environment_requirements': {'huggingface_transformers': '4.13.0'}, 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] بناء نظام لتحليل الملامح الوجهية في الصور الطبية لتقديم التشخيص الأولي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-092352', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-092352')\", 'api_arguments': {}, 'python_environment_requirements': {'huggingface_transformers': '4.13.0'}, 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تطوير نظام لتحليل نمط التشخيصات الطبية من النصوص الطبية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-092352', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-092352')\", 'api_arguments': {}, 'python_environment_requirements': {'huggingface_transformers': '4.13.0'}, 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] إنشاء نظام لتحليل سلوك المركبات الذكية على الطرق السريعة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-092352', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-092352')\", 'api_arguments': {}, 'python_environment_requirements': {'huggingface_transformers': '4.13.0'}, 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تنفيذ نظام لتوقع تقنيات الحماية المستخدمة في التطبيقات السحابية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-092352', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-092352')\", 'api_arguments': {}, 'python_environment_requirements': {'huggingface_transformers': '4.13.0'}, 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تطوير نظام لتحليل السلوك الاستهلاكي للمستهلكين عبر وسائل التواصل الاجتماعي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-092352', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-092352')\", 'api_arguments': {}, 'python_environment_requirements': {'huggingface_transformers': '4.13.0'}, 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] بناء نظام لتصنيف الصور الفضائية لدراسة تغيرات المناخ.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-092352', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-092352')\", 'api_arguments': {}, 'python_environment_requirements': {'huggingface_transformers': '4.13.0'}, 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تنفيذ نظام لتحليل توزيع الأمراض في منطقة محددة باستخدام البيانات الطبية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-092352', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-092352')\", 'api_arguments': {}, 'python_environment_requirements': {'huggingface_transformers': '4.13.0'}, 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] إنشاء نظام لتحليل بيانات المرضى من ملفاتهم الطبية لاكتشاف أنماط جديدة للأمراض.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-092352', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-092352')\", 'api_arguments': {}, 'python_environment_requirements': {'huggingface_transformers': '4.13.0'}, 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تطوير نظام لتحليل تغيرات أسعار العملات المشفرة وتوقع حركتها السعرية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-092352', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-092352')\", 'api_arguments': {}, 'python_environment_requirements': {'huggingface_transformers': '4.13.0'}, 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST]إستخدم نموذج \"nshepperd/gpt-2\" لتوليد نصوص إبداعية بناءً على قصص ملحمية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-095508', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': 'DIODE', 'accuracy': None}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.' </s>"
"<s>[INST] أنشئ تطبيق يستخدم نموذج \"valhalla/t5-base-qg-hl\" لتحويل النصوص إلى أسئلة وإجابات.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-095508', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': 'DIODE', 'accuracy': None}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.' </s>"
"<s>[INST] إستخدم نموذج الذكاء الاصطناعي \"allenai/longformer-base-4096\" لفهم النصوص الطويلة واستخراج المعلومات الهامة منها.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-095508', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': 'DIODE', 'accuracy': None}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.' </s>"
"<s>[INST] أنشئ تطبيق يستخدم نموذج \"facebook/wav2vec2-base-960h\" لتحويل الكلام إلى نص مفهوم.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-095508', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': 'DIODE', 'accuracy': None}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.' </s>"
"<s>[INST] قم بتطوير تطبيق يستخدم نموذج \"bert-base-uncased\" لتحليل المشهاد من التغريدات واستخراج المشاعر والمواضيع الرئيسية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-095508', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': 'DIODE', 'accuracy': None}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.' </s>"
"<s>[INST] أنشئ تطبيق يستخدم نموذج \"camembert-base\" لتحويل النصوص إلى لغة فرنسية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-095508', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': 'DIODE', 'accuracy': None}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.' </s>"
"<s>[INST] إستخدم نموذج \"sshleifer/tiny_mbart\" لتحويل النصوص من الإنجليزية إلى العربية بدقة عالية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-095508', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': 'DIODE', 'accuracy': None}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.' </s>"
"<s>[INST] أنشئ تطبيق يستخدم نموذج \"mrm8488/t5-base-finetuned-question-generation-ap\" لتحويل النصوص إلى أسئلة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-095508', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': 'DIODE', 'accuracy': None}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.' </s>"
"<s>[INST] استخدم نموذج \"valhalla/convbert-base-cased\" لتنفيذ مهمة استدلال المعاني بدقة عالية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-095508', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': 'DIODE', 'accuracy': None}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.' </s>"
"<s>[INST] أنشئ تطبيق يستخدم نموذج \"mrm8488/mt5-arabic-base-iwslt\" لتحويل النصوص من العربية إلى إنجليزية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-095508', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\", 'api_arguments': None, 'python_environment_requirements': ['transformers'], 'example_code': None, 'performance': {'dataset': 'DIODE', 'accuracy': None}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.' </s>"
"<s>[INST]إنشاء نظام يستخدم تكنولوجيا تعلم الآلة لتحليل نمط الكتابة اليدوية وتحويله إلى نصوص قابلة للتحرير بواسطة API محدد.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-112116', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تدريب نموذج متقدم للتعرف على أصوات الحيوانات البرية باستخدام API متخصص.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-112116', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تطوير تطبيق يستخدم خدمة API لتحليل سلوك المستخدمين عبر منصة ويب.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-112116', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] بناء نظام يستخدم تقنية الاستشعار الحيوي للمساعدة في تحقيق التحقق الثنائي للمستخدمين.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-112116', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تصميم حل لتحليل بيانات الصوت واستخراج الأنماط الصوتية الرئيسية باستخدام واجهة برمجة التطبيقات المحددة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-112116', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] بناء تطبيق يستخدم خدمة API لإجراء تقدير دقيق لأحجام الأشياء في الصور باستخدام تقنيات تعلم الآلة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-112116', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تصميم نظام يستخدم خدمة API لتحليل وتصنيف الملفات الصوتية بناءً على محتواها.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-112116', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] بناء تطبيق يستخدم خدمة متخصصة للكشف عن الأشخاص في الصور وتحديد هويتهم.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-112116', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] إنشاء نظام يقوم بتحليل بيانات النصوص واستخراج المعلومات الرئيسية باستخدام واجهة برمجة التطبيقات المخصصة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-112116', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST] تدريب نموذج لتحديد الرسائل الإلكترونية المزعجة باستخدام بيانات التدريب وواجهة برمجة التطبيقات المعينة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221215-112116', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'DIODE', 'accuracy': ''}, 'description': 'A depth estimation model fine-tuned on the DIODE dataset.' </s>"
"<s>[INST]\"إجراء تحليل المشاعر على النص العربي باستخدام نموذج مُدرب مسبقًا.\"، \"إنشاء ملخصات نصية باللغة العربية باستخدام نماذج المحولات.\"، \"اكتشاف الكائنات في الصور العربية باستخدام واجهات برمجة التطبيقات للرؤية الحاسوبية.، \"ترجمة النص الإنجليزي إلى اللغة العربية باستخدام نماذج المحولات. .، إنشاء روبوت دردشة لدعم العملاء العرب باستخدام نماذج المحولات.، تحليل مشاعر محتوى الوسائط الاجتماعية العربية باستخدام نماذج البرمجة اللغوية العصبية.، تحديد الكيانات المسماة في النص العربي باستخدام نماذج المحولات.، تحليل بيانات الكلام العربي بحثًا عن المشاعر. الكشف باستخدام نماذج المحولات.، تنفيذ توليد النص العربي مع التركيز على الشعر باستخدام نماذج المحولات.، اكتشاف أنماط النص العربي لتحليل المشاعر باستخدام نماذج المحولات.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-030603', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221122-030603')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3597, 'Mae': 0.3054, 'Rmse': 0.4481, 'Abs Rel': 0.3462, 'Log Mae': 0.1256, 'Log Rmse': 0.1798, 'Delta1': 0.5278, 'Delta2': 0.8055, 'Delta3': 0.9191}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST]Create an application that allows users to upload images for detecting weeds in fields.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti-finetuned-diode', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-kitti-finetuned-diode')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers==4.24.0, torch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': 'N/A', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.5845, 'Rmse': 0.6175}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset.' </s>"
"<s>[INST] تطوير خوارزمية تمكن من إكتشاف الإحتيال في المعاملات المصرفية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti-finetuned-diode', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-kitti-finetuned-diode')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers==4.24.0, torch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': 'N/A', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.5845, 'Rmse': 0.6175}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset.' </s>"
"<s>[INST] إنشاء نظام توصية للمستخدمين بالأفلام بناءً على تفضيلاتهم السابقة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti-finetuned-diode', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-kitti-finetuned-diode')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers==4.24.0, torch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': 'N/A', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.5845, 'Rmse': 0.6175}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset.' </s>"
"<s>[INST] تطوير نظام يقوم بتحليل نغمة صوت العميل ويحدد مزاجه لتوصية الموسيقى المناسبة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti-finetuned-diode', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-kitti-finetuned-diode')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers==4.24.0, torch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': 'N/A', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.5845, 'Rmse': 0.6175}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset.' </s>"
"<s>[INST] إضافة قدرة التعرف على اللهجات العربية المختلفة إلى تطبيقك الحالي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti-finetuned-diode', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-kitti-finetuned-diode')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers==4.24.0, torch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': 'N/A', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.5845, 'Rmse': 0.6175}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset.' </s>"
"<s>[INST] تطوير نظام يتعقب تصنيفات الأنماط الزخرفية في الأزياء الشرقية التقليدية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti-finetuned-diode', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-kitti-finetuned-diode')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers==4.24.0, torch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': 'N/A', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.5845, 'Rmse': 0.6175}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset.' </s>"
"<s>[INST] إدراج وظيفة للتعرف على النصوص اليدوية العربية في تطبيقك الحالي لتحسين القراءة الضوئية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti-finetuned-diode', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-kitti-finetuned-diode')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers==4.24.0, torch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': 'N/A', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.5845, 'Rmse': 0.6175}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset.' </s>"
"<s>[INST] تطوير نظام يحلل النصوص الطبية لتوفير تشخيص دقيق للمرضى.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti-finetuned-diode', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-kitti-finetuned-diode')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers==4.24.0, torch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': 'N/A', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.5845, 'Rmse': 0.6175}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset.' </s>"
"<s>[INST] إضافة إمكانية التعرف على السيارات والشاحنات في صور الطرق إلى تطبيق الذكاء الاصطناعي لديك.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti-finetuned-diode', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-kitti-finetuned-diode')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers==4.24.0, torch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': 'N/A', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.5845, 'Rmse': 0.6175}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset.' </s>"
"<s>[INST] تقديم تقنية الترجمة الآلية لتطبيق الدردشة لديك لتعزيز التواصل بين المستخدمين من جنسيات مختلفة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-kitti-finetuned-diode', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-kitti-finetuned-diode')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers==4.24.0, torch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': 'N/A', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.5845, 'Rmse': 0.6175}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset.' </s>"
"<s>[INST]كيف يمكنني استخدام API لتحسين تحديد الكائنات في صورة فوتوغرافية؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-054332', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\", 'api_arguments': {'model_name': 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'}, 'python_environment_requirements': {'transformers': '4.24.0', 'pytorch': '1.13.0+cu117', 'tokenizers': '0.13.2'}, 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.6028, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحويل نص إلى كلام مسموع؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-054332', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\", 'api_arguments': {'model_name': 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'}, 'python_environment_requirements': {'transformers': '4.24.0', 'pytorch': '1.13.0+cu117', 'tokenizers': '0.13.2'}, 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.6028, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] استخدم API لتصنيف النصوص الطبية والتحقق من التشخيص الذاتي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-054332', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\", 'api_arguments': {'model_name': 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'}, 'python_environment_requirements': {'transformers': '4.24.0', 'pytorch': '1.13.0+cu117', 'tokenizers': '0.13.2'}, 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.6028, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحويل صورة إلى نص؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-054332', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\", 'api_arguments': {'model_name': 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'}, 'python_environment_requirements': {'transformers': '4.24.0', 'pytorch': '1.13.0+cu117', 'tokenizers': '0.13.2'}, 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.6028, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحليل مشاعر النصوص الاجتماعية؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-054332', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\", 'api_arguments': {'model_name': 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'}, 'python_environment_requirements': {'transformers': '4.24.0', 'pytorch': '1.13.0+cu117', 'tokenizers': '0.13.2'}, 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.6028, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] قم بتحليل النصوص الإخبارية باستخدام API لتصنيف المواضيع والمواجهات.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-054332', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\", 'api_arguments': {'model_name': 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'}, 'python_environment_requirements': {'transformers': '4.24.0', 'pytorch': '1.13.0+cu117', 'tokenizers': '0.13.2'}, 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.6028, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحليل الصور الطبية وتحديد التشخيص؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-054332', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\", 'api_arguments': {'model_name': 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'}, 'python_environment_requirements': {'transformers': '4.24.0', 'pytorch': '1.13.0+cu117', 'tokenizers': '0.13.2'}, 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.6028, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] قم بتصميم نظام للكشف التلقائي عن الحوادث المرورية باستخدام API.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-054332', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\", 'api_arguments': {'model_name': 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'}, 'python_environment_requirements': {'transformers': '4.24.0', 'pytorch': '1.13.0+cu117', 'tokenizers': '0.13.2'}, 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.6028, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحليل نغمات الصوت وتقديم مواجهة موسيقية؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-054332', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\", 'api_arguments': {'model_name': 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'}, 'python_environment_requirements': {'transformers': '4.24.0', 'pytorch': '1.13.0+cu117', 'tokenizers': '0.13.2'}, 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.6028, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] قم بتحويل صور الأقمار الصناعية إلى تحليل بيانات جغرافية باستخدام API.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-054332', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\", 'api_arguments': {'model_name': 'sayakpaul/glpn-nyu-finetuned-diode-221116-054332'}, 'python_environment_requirements': {'transformers': '4.24.0', 'pytorch': '1.13.0+cu117', 'tokenizers': '0.13.2'}, 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.6028, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST]قم بتطوير تطبيق يعتمد على تقنيات الذكاء الاصطناعي لتحليل نمط الكتابة والتعرف على النصوص الخلافية.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'GTA5 AI model', 'api_name': 'GTA5_PROCESS_LEARNING_AI', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\", 'api_arguments': {'model': 'NanoCircuit', 'data_loader': 'train_loader', 'criterion': 'nn.CrossEntropyLoss', 'optimizer': 'optim.SGD', 'device': 'torch.device', 'data_cap_gb': 10}, 'python_environment_requirements': ['contextlib', 'os', 'matplotlib', 'numpy', 'torch', 'torch.nn', 'torch.optim', 'requests', 'torchvision', 'psutil', 'time', 'subprocess', 'onnxruntime', 'numexpr', 'transformers'], 'example_code': {'import_libraries': ['import contextlib', 'import os', 'from matplotlib import pyplot as plt', 'import numpy as np', 'import torch', 'import torch.nn as nn', 'import torch.optim as optim', 'import requests', 'from torchvision import datasets, transforms', 'import psutil', 'import time', 'import subprocess', 'import onnxruntime as ort', 'import matplotlib.pyplot as plt', 'import numpy as np', 'import numexpr as ne', 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM'], 'define_neural_network': ['class NanoCircuit(nn.Module):', ' def init(self):', ' super(NanoCircuit, self).init()', ' self.fc1 = nn.Linear(784, 128)', ' self.fc2 = nn.Linear(128, 10)', 'def forward(self, x):', ' x = x.view(-1, 784)', ' x = torch.relu(self.fc1(x))', ' x = self.fc2(x)', ' return x'], 'train_with_data_cap': ['def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):', ' data_processed = 0', ' data_cap_bytes = data_cap_gb * (1024 ** 3)', ' epoch = 0', 'while data_processed < data_cap_bytes:', ' running_loss = 0.0', ' for i, data in enumerate(data_loader, 0):', ' inputs, labels = data', ' inputs, labels = inputs.to(device), labels.to(device)', ' data_processed += inputs.nelement() * inputs.element_size()', ' if data_processed >= data_cap_bytes:', ' break', ' optimizer.zero_grad()', ' outputs = model(inputs.view(-1, 28 * 28))', ' loss = criterion(outputs, labels)', ' loss.backward()', ' optimizer.step()', ' running_loss += loss.item()', 'epoch += 1', 'print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})', 'print(fData processed: {data_processed / (1024 ** 3):.2f} GB)', 'return model']}, 'performance': {'dataset': 'MNIST', 'accuracy': 'Not specified'}, 'description': 'This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available.' </s>"
"<s>[INST] أكتب برنامج يستخدم الذكاء الاصطناعي لإنشاء نموذج تنبؤي لتطور الأسعار في سوق الأسهم.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'GTA5 AI model', 'api_name': 'GTA5_PROCESS_LEARNING_AI', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\", 'api_arguments': {'model': 'NanoCircuit', 'data_loader': 'train_loader', 'criterion': 'nn.CrossEntropyLoss', 'optimizer': 'optim.SGD', 'device': 'torch.device', 'data_cap_gb': 10}, 'python_environment_requirements': ['contextlib', 'os', 'matplotlib', 'numpy', 'torch', 'torch.nn', 'torch.optim', 'requests', 'torchvision', 'psutil', 'time', 'subprocess', 'onnxruntime', 'numexpr', 'transformers'], 'example_code': {'import_libraries': ['import contextlib', 'import os', 'from matplotlib import pyplot as plt', 'import numpy as np', 'import torch', 'import torch.nn as nn', 'import torch.optim as optim', 'import requests', 'from torchvision import datasets, transforms', 'import psutil', 'import time', 'import subprocess', 'import onnxruntime as ort', 'import matplotlib.pyplot as plt', 'import numpy as np', 'import numexpr as ne', 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM'], 'define_neural_network': ['class NanoCircuit(nn.Module):', ' def init(self):', ' super(NanoCircuit, self).init()', ' self.fc1 = nn.Linear(784, 128)', ' self.fc2 = nn.Linear(128, 10)', 'def forward(self, x):', ' x = x.view(-1, 784)', ' x = torch.relu(self.fc1(x))', ' x = self.fc2(x)', ' return x'], 'train_with_data_cap': ['def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):', ' data_processed = 0', ' data_cap_bytes = data_cap_gb * (1024 ** 3)', ' epoch = 0', 'while data_processed < data_cap_bytes:', ' running_loss = 0.0', ' for i, data in enumerate(data_loader, 0):', ' inputs, labels = data', ' inputs, labels = inputs.to(device), labels.to(device)', ' data_processed += inputs.nelement() * inputs.element_size()', ' if data_processed >= data_cap_bytes:', ' break', ' optimizer.zero_grad()', ' outputs = model(inputs.view(-1, 28 * 28))', ' loss = criterion(outputs, labels)', ' loss.backward()', ' optimizer.step()', ' running_loss += loss.item()', 'epoch += 1', 'print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})', 'print(fData processed: {data_processed / (1024 ** 3):.2f} GB)', 'return model']}, 'performance': {'dataset': 'MNIST', 'accuracy': 'Not specified'}, 'description': 'This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available.' </s>"
"<s>[INST] عمل برنامج يعتمد على نموذج الشبكات العصبية لتوجيه السيارات بشكل ذاتي في بيئة افتراضية.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'GTA5 AI model', 'api_name': 'GTA5_PROCESS_LEARNING_AI', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\", 'api_arguments': {'model': 'NanoCircuit', 'data_loader': 'train_loader', 'criterion': 'nn.CrossEntropyLoss', 'optimizer': 'optim.SGD', 'device': 'torch.device', 'data_cap_gb': 10}, 'python_environment_requirements': ['contextlib', 'os', 'matplotlib', 'numpy', 'torch', 'torch.nn', 'torch.optim', 'requests', 'torchvision', 'psutil', 'time', 'subprocess', 'onnxruntime', 'numexpr', 'transformers'], 'example_code': {'import_libraries': ['import contextlib', 'import os', 'from matplotlib import pyplot as plt', 'import numpy as np', 'import torch', 'import torch.nn as nn', 'import torch.optim as optim', 'import requests', 'from torchvision import datasets, transforms', 'import psutil', 'import time', 'import subprocess', 'import onnxruntime as ort', 'import matplotlib.pyplot as plt', 'import numpy as np', 'import numexpr as ne', 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM'], 'define_neural_network': ['class NanoCircuit(nn.Module):', ' def init(self):', ' super(NanoCircuit, self).init()', ' self.fc1 = nn.Linear(784, 128)', ' self.fc2 = nn.Linear(128, 10)', 'def forward(self, x):', ' x = x.view(-1, 784)', ' x = torch.relu(self.fc1(x))', ' x = self.fc2(x)', ' return x'], 'train_with_data_cap': ['def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):', ' data_processed = 0', ' data_cap_bytes = data_cap_gb * (1024 ** 3)', ' epoch = 0', 'while data_processed < data_cap_bytes:', ' running_loss = 0.0', ' for i, data in enumerate(data_loader, 0):', ' inputs, labels = data', ' inputs, labels = inputs.to(device), labels.to(device)', ' data_processed += inputs.nelement() * inputs.element_size()', ' if data_processed >= data_cap_bytes:', ' break', ' optimizer.zero_grad()', ' outputs = model(inputs.view(-1, 28 * 28))', ' loss = criterion(outputs, labels)', ' loss.backward()', ' optimizer.step()', ' running_loss += loss.item()', 'epoch += 1', 'print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})', 'print(fData processed: {data_processed / (1024 ** 3):.2f} GB)', 'return model']}, 'performance': {'dataset': 'MNIST', 'accuracy': 'Not specified'}, 'description': 'This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available.' </s>"
"<s>[INST] قم بتصميم تطبيق يستخدم الذكاء الاصطناعي للكشف عن التلاعب بالصور وتحديد الصور المعدلة.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'GTA5 AI model', 'api_name': 'GTA5_PROCESS_LEARNING_AI', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\", 'api_arguments': {'model': 'NanoCircuit', 'data_loader': 'train_loader', 'criterion': 'nn.CrossEntropyLoss', 'optimizer': 'optim.SGD', 'device': 'torch.device', 'data_cap_gb': 10}, 'python_environment_requirements': ['contextlib', 'os', 'matplotlib', 'numpy', 'torch', 'torch.nn', 'torch.optim', 'requests', 'torchvision', 'psutil', 'time', 'subprocess', 'onnxruntime', 'numexpr', 'transformers'], 'example_code': {'import_libraries': ['import contextlib', 'import os', 'from matplotlib import pyplot as plt', 'import numpy as np', 'import torch', 'import torch.nn as nn', 'import torch.optim as optim', 'import requests', 'from torchvision import datasets, transforms', 'import psutil', 'import time', 'import subprocess', 'import onnxruntime as ort', 'import matplotlib.pyplot as plt', 'import numpy as np', 'import numexpr as ne', 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM'], 'define_neural_network': ['class NanoCircuit(nn.Module):', ' def init(self):', ' super(NanoCircuit, self).init()', ' self.fc1 = nn.Linear(784, 128)', ' self.fc2 = nn.Linear(128, 10)', 'def forward(self, x):', ' x = x.view(-1, 784)', ' x = torch.relu(self.fc1(x))', ' x = self.fc2(x)', ' return x'], 'train_with_data_cap': ['def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):', ' data_processed = 0', ' data_cap_bytes = data_cap_gb * (1024 ** 3)', ' epoch = 0', 'while data_processed < data_cap_bytes:', ' running_loss = 0.0', ' for i, data in enumerate(data_loader, 0):', ' inputs, labels = data', ' inputs, labels = inputs.to(device), labels.to(device)', ' data_processed += inputs.nelement() * inputs.element_size()', ' if data_processed >= data_cap_bytes:', ' break', ' optimizer.zero_grad()', ' outputs = model(inputs.view(-1, 28 * 28))', ' loss = criterion(outputs, labels)', ' loss.backward()', ' optimizer.step()', ' running_loss += loss.item()', 'epoch += 1', 'print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})', 'print(fData processed: {data_processed / (1024 ** 3):.2f} GB)', 'return model']}, 'performance': {'dataset': 'MNIST', 'accuracy': 'Not specified'}, 'description': 'This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available.' </s>"
"<s>[INST] أنشيء تطبيق يستخدم تقنيات الذكاء الاصطناعي لتوقع الظروف الجوية القادمة بناءً على البيانات التاريخية.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'GTA5 AI model', 'api_name': 'GTA5_PROCESS_LEARNING_AI', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\", 'api_arguments': {'model': 'NanoCircuit', 'data_loader': 'train_loader', 'criterion': 'nn.CrossEntropyLoss', 'optimizer': 'optim.SGD', 'device': 'torch.device', 'data_cap_gb': 10}, 'python_environment_requirements': ['contextlib', 'os', 'matplotlib', 'numpy', 'torch', 'torch.nn', 'torch.optim', 'requests', 'torchvision', 'psutil', 'time', 'subprocess', 'onnxruntime', 'numexpr', 'transformers'], 'example_code': {'import_libraries': ['import contextlib', 'import os', 'from matplotlib import pyplot as plt', 'import numpy as np', 'import torch', 'import torch.nn as nn', 'import torch.optim as optim', 'import requests', 'from torchvision import datasets, transforms', 'import psutil', 'import time', 'import subprocess', 'import onnxruntime as ort', 'import matplotlib.pyplot as plt', 'import numpy as np', 'import numexpr as ne', 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM'], 'define_neural_network': ['class NanoCircuit(nn.Module):', ' def init(self):', ' super(NanoCircuit, self).init()', ' self.fc1 = nn.Linear(784, 128)', ' self.fc2 = nn.Linear(128, 10)', 'def forward(self, x):', ' x = x.view(-1, 784)', ' x = torch.relu(self.fc1(x))', ' x = self.fc2(x)', ' return x'], 'train_with_data_cap': ['def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):', ' data_processed = 0', ' data_cap_bytes = data_cap_gb * (1024 ** 3)', ' epoch = 0', 'while data_processed < data_cap_bytes:', ' running_loss = 0.0', ' for i, data in enumerate(data_loader, 0):', ' inputs, labels = data', ' inputs, labels = inputs.to(device), labels.to(device)', ' data_processed += inputs.nelement() * inputs.element_size()', ' if data_processed >= data_cap_bytes:', ' break', ' optimizer.zero_grad()', ' outputs = model(inputs.view(-1, 28 * 28))', ' loss = criterion(outputs, labels)', ' loss.backward()', ' optimizer.step()', ' running_loss += loss.item()', 'epoch += 1', 'print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})', 'print(fData processed: {data_processed / (1024 ** 3):.2f} GB)', 'return model']}, 'performance': {'dataset': 'MNIST', 'accuracy': 'Not specified'}, 'description': 'This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available.' </s>"
"<s>[INST] قم بكتابة تطبيق يستخدم الذكاء الاصطناعي لزيادة فعالية التواصل مع العملاء عبر الدردشة الآلية.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'GTA5 AI model', 'api_name': 'GTA5_PROCESS_LEARNING_AI', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\", 'api_arguments': {'model': 'NanoCircuit', 'data_loader': 'train_loader', 'criterion': 'nn.CrossEntropyLoss', 'optimizer': 'optim.SGD', 'device': 'torch.device', 'data_cap_gb': 10}, 'python_environment_requirements': ['contextlib', 'os', 'matplotlib', 'numpy', 'torch', 'torch.nn', 'torch.optim', 'requests', 'torchvision', 'psutil', 'time', 'subprocess', 'onnxruntime', 'numexpr', 'transformers'], 'example_code': {'import_libraries': ['import contextlib', 'import os', 'from matplotlib import pyplot as plt', 'import numpy as np', 'import torch', 'import torch.nn as nn', 'import torch.optim as optim', 'import requests', 'from torchvision import datasets, transforms', 'import psutil', 'import time', 'import subprocess', 'import onnxruntime as ort', 'import matplotlib.pyplot as plt', 'import numpy as np', 'import numexpr as ne', 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM'], 'define_neural_network': ['class NanoCircuit(nn.Module):', ' def init(self):', ' super(NanoCircuit, self).init()', ' self.fc1 = nn.Linear(784, 128)', ' self.fc2 = nn.Linear(128, 10)', 'def forward(self, x):', ' x = x.view(-1, 784)', ' x = torch.relu(self.fc1(x))', ' x = self.fc2(x)', ' return x'], 'train_with_data_cap': ['def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):', ' data_processed = 0', ' data_cap_bytes = data_cap_gb * (1024 ** 3)', ' epoch = 0', 'while data_processed < data_cap_bytes:', ' running_loss = 0.0', ' for i, data in enumerate(data_loader, 0):', ' inputs, labels = data', ' inputs, labels = inputs.to(device), labels.to(device)', ' data_processed += inputs.nelement() * inputs.element_size()', ' if data_processed >= data_cap_bytes:', ' break', ' optimizer.zero_grad()', ' outputs = model(inputs.view(-1, 28 * 28))', ' loss = criterion(outputs, labels)', ' loss.backward()', ' optimizer.step()', ' running_loss += loss.item()', 'epoch += 1', 'print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})', 'print(fData processed: {data_processed / (1024 ** 3):.2f} GB)', 'return model']}, 'performance': {'dataset': 'MNIST', 'accuracy': 'Not specified'}, 'description': 'This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available.' </s>"
"<s>[INST] أنشئ تطبيقًا يعتمد على تقنيات الذكاء الاصطناعي لتحليل سلوك المستخدمين على الإنترنت وتقديم توصيات شخصية.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'GTA5 AI model', 'api_name': 'GTA5_PROCESS_LEARNING_AI', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\", 'api_arguments': {'model': 'NanoCircuit', 'data_loader': 'train_loader', 'criterion': 'nn.CrossEntropyLoss', 'optimizer': 'optim.SGD', 'device': 'torch.device', 'data_cap_gb': 10}, 'python_environment_requirements': ['contextlib', 'os', 'matplotlib', 'numpy', 'torch', 'torch.nn', 'torch.optim', 'requests', 'torchvision', 'psutil', 'time', 'subprocess', 'onnxruntime', 'numexpr', 'transformers'], 'example_code': {'import_libraries': ['import contextlib', 'import os', 'from matplotlib import pyplot as plt', 'import numpy as np', 'import torch', 'import torch.nn as nn', 'import torch.optim as optim', 'import requests', 'from torchvision import datasets, transforms', 'import psutil', 'import time', 'import subprocess', 'import onnxruntime as ort', 'import matplotlib.pyplot as plt', 'import numpy as np', 'import numexpr as ne', 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM'], 'define_neural_network': ['class NanoCircuit(nn.Module):', ' def init(self):', ' super(NanoCircuit, self).init()', ' self.fc1 = nn.Linear(784, 128)', ' self.fc2 = nn.Linear(128, 10)', 'def forward(self, x):', ' x = x.view(-1, 784)', ' x = torch.relu(self.fc1(x))', ' x = self.fc2(x)', ' return x'], 'train_with_data_cap': ['def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):', ' data_processed = 0', ' data_cap_bytes = data_cap_gb * (1024 ** 3)', ' epoch = 0', 'while data_processed < data_cap_bytes:', ' running_loss = 0.0', ' for i, data in enumerate(data_loader, 0):', ' inputs, labels = data', ' inputs, labels = inputs.to(device), labels.to(device)', ' data_processed += inputs.nelement() * inputs.element_size()', ' if data_processed >= data_cap_bytes:', ' break', ' optimizer.zero_grad()', ' outputs = model(inputs.view(-1, 28 * 28))', ' loss = criterion(outputs, labels)', ' loss.backward()', ' optimizer.step()', ' running_loss += loss.item()', 'epoch += 1', 'print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})', 'print(fData processed: {data_processed / (1024 ** 3):.2f} GB)', 'return model']}, 'performance': {'dataset': 'MNIST', 'accuracy': 'Not specified'}, 'description': 'This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available.' </s>"
"<s>[INST] ابن تطبيق يستخدم الذكاء الاصطناعي لتحليل المحتوى الصوتي وتحويل النصوص الصوتية إلى نص.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'GTA5 AI model', 'api_name': 'GTA5_PROCESS_LEARNING_AI', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\", 'api_arguments': {'model': 'NanoCircuit', 'data_loader': 'train_loader', 'criterion': 'nn.CrossEntropyLoss', 'optimizer': 'optim.SGD', 'device': 'torch.device', 'data_cap_gb': 10}, 'python_environment_requirements': ['contextlib', 'os', 'matplotlib', 'numpy', 'torch', 'torch.nn', 'torch.optim', 'requests', 'torchvision', 'psutil', 'time', 'subprocess', 'onnxruntime', 'numexpr', 'transformers'], 'example_code': {'import_libraries': ['import contextlib', 'import os', 'from matplotlib import pyplot as plt', 'import numpy as np', 'import torch', 'import torch.nn as nn', 'import torch.optim as optim', 'import requests', 'from torchvision import datasets, transforms', 'import psutil', 'import time', 'import subprocess', 'import onnxruntime as ort', 'import matplotlib.pyplot as plt', 'import numpy as np', 'import numexpr as ne', 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM'], 'define_neural_network': ['class NanoCircuit(nn.Module):', ' def init(self):', ' super(NanoCircuit, self).init()', ' self.fc1 = nn.Linear(784, 128)', ' self.fc2 = nn.Linear(128, 10)', 'def forward(self, x):', ' x = x.view(-1, 784)', ' x = torch.relu(self.fc1(x))', ' x = self.fc2(x)', ' return x'], 'train_with_data_cap': ['def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):', ' data_processed = 0', ' data_cap_bytes = data_cap_gb * (1024 ** 3)', ' epoch = 0', 'while data_processed < data_cap_bytes:', ' running_loss = 0.0', ' for i, data in enumerate(data_loader, 0):', ' inputs, labels = data', ' inputs, labels = inputs.to(device), labels.to(device)', ' data_processed += inputs.nelement() * inputs.element_size()', ' if data_processed >= data_cap_bytes:', ' break', ' optimizer.zero_grad()', ' outputs = model(inputs.view(-1, 28 * 28))', ' loss = criterion(outputs, labels)', ' loss.backward()', ' optimizer.step()', ' running_loss += loss.item()', 'epoch += 1', 'print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})', 'print(fData processed: {data_processed / (1024 ** 3):.2f} GB)', 'return model']}, 'performance': {'dataset': 'MNIST', 'accuracy': 'Not specified'}, 'description': 'This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available.' </s>"
"<s>[INST] أنشئ تطبيق يعتمد على الذكاء الاصطناعي لتحليل نمط التسوق للمستخدمين وتقديم توصيات المنتجات.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'GTA5 AI model', 'api_name': 'GTA5_PROCESS_LEARNING_AI', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\", 'api_arguments': {'model': 'NanoCircuit', 'data_loader': 'train_loader', 'criterion': 'nn.CrossEntropyLoss', 'optimizer': 'optim.SGD', 'device': 'torch.device', 'data_cap_gb': 10}, 'python_environment_requirements': ['contextlib', 'os', 'matplotlib', 'numpy', 'torch', 'torch.nn', 'torch.optim', 'requests', 'torchvision', 'psutil', 'time', 'subprocess', 'onnxruntime', 'numexpr', 'transformers'], 'example_code': {'import_libraries': ['import contextlib', 'import os', 'from matplotlib import pyplot as plt', 'import numpy as np', 'import torch', 'import torch.nn as nn', 'import torch.optim as optim', 'import requests', 'from torchvision import datasets, transforms', 'import psutil', 'import time', 'import subprocess', 'import onnxruntime as ort', 'import matplotlib.pyplot as plt', 'import numpy as np', 'import numexpr as ne', 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM'], 'define_neural_network': ['class NanoCircuit(nn.Module):', ' def init(self):', ' super(NanoCircuit, self).init()', ' self.fc1 = nn.Linear(784, 128)', ' self.fc2 = nn.Linear(128, 10)', 'def forward(self, x):', ' x = x.view(-1, 784)', ' x = torch.relu(self.fc1(x))', ' x = self.fc2(x)', ' return x'], 'train_with_data_cap': ['def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):', ' data_processed = 0', ' data_cap_bytes = data_cap_gb * (1024 ** 3)', ' epoch = 0', 'while data_processed < data_cap_bytes:', ' running_loss = 0.0', ' for i, data in enumerate(data_loader, 0):', ' inputs, labels = data', ' inputs, labels = inputs.to(device), labels.to(device)', ' data_processed += inputs.nelement() * inputs.element_size()', ' if data_processed >= data_cap_bytes:', ' break', ' optimizer.zero_grad()', ' outputs = model(inputs.view(-1, 28 * 28))', ' loss = criterion(outputs, labels)', ' loss.backward()', ' optimizer.step()', ' running_loss += loss.item()', 'epoch += 1', 'print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})', 'print(fData processed: {data_processed / (1024 ** 3):.2f} GB)', 'return model']}, 'performance': {'dataset': 'MNIST', 'accuracy': 'Not specified'}, 'description': 'This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available.' </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم الذكاء الاصطناعي لتحليل البيانات الطبية وتوقع الإصابة بأمراض معينة استنادًا إلى العوامل المرتبطة.? [/INST]'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'GTA5 AI model', 'api_name': 'GTA5_PROCESS_LEARNING_AI', 'api_call': \"AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')\", 'api_arguments': {'model': 'NanoCircuit', 'data_loader': 'train_loader', 'criterion': 'nn.CrossEntropyLoss', 'optimizer': 'optim.SGD', 'device': 'torch.device', 'data_cap_gb': 10}, 'python_environment_requirements': ['contextlib', 'os', 'matplotlib', 'numpy', 'torch', 'torch.nn', 'torch.optim', 'requests', 'torchvision', 'psutil', 'time', 'subprocess', 'onnxruntime', 'numexpr', 'transformers'], 'example_code': {'import_libraries': ['import contextlib', 'import os', 'from matplotlib import pyplot as plt', 'import numpy as np', 'import torch', 'import torch.nn as nn', 'import torch.optim as optim', 'import requests', 'from torchvision import datasets, transforms', 'import psutil', 'import time', 'import subprocess', 'import onnxruntime as ort', 'import matplotlib.pyplot as plt', 'import numpy as np', 'import numexpr as ne', 'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM'], 'define_neural_network': ['class NanoCircuit(nn.Module):', ' def init(self):', ' super(NanoCircuit, self).init()', ' self.fc1 = nn.Linear(784, 128)', ' self.fc2 = nn.Linear(128, 10)', 'def forward(self, x):', ' x = x.view(-1, 784)', ' x = torch.relu(self.fc1(x))', ' x = self.fc2(x)', ' return x'], 'train_with_data_cap': ['def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):', ' data_processed = 0', ' data_cap_bytes = data_cap_gb * (1024 ** 3)', ' epoch = 0', 'while data_processed < data_cap_bytes:', ' running_loss = 0.0', ' for i, data in enumerate(data_loader, 0):', ' inputs, labels = data', ' inputs, labels = inputs.to(device), labels.to(device)', ' data_processed += inputs.nelement() * inputs.element_size()', ' if data_processed >= data_cap_bytes:', ' break', ' optimizer.zero_grad()', ' outputs = model(inputs.view(-1, 28 * 28))', ' loss = criterion(outputs, labels)', ' loss.backward()', ' optimizer.step()', ' running_loss += loss.item()', 'epoch += 1', 'print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})', 'print(fData processed: {data_processed / (1024 ** 3):.2f} GB)', 'return model']}, 'performance': {'dataset': 'MNIST', 'accuracy': 'Not specified'}, 'description': 'This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available.' </s>"
"<s>[INST]تطوير نظام لإنشاء ملخصات للاتفاقيات القانونية أو العقود تلقائيًا استنادًا إلى معايير محددة.\\n -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-062619', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\", 'api_arguments': 'None', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.13.0+cu117, Tokenizers 0.13.2', 'example_code': 'None', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.548, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تنفيذ ميزة يمكنها اكتشاف وترجمة اللغة المسيئة في المحتوى الذي ينشئه المستخدم.\\n -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-062619', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\", 'api_arguments': 'None', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.13.0+cu117, Tokenizers 0.13.2', 'example_code': 'None', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.548, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تطوير أداة لـ تحليل المشاعر في تعليقات الوسائط الاجتماعية لفهم آراء المستخدمين ومشاعرهم.\\n -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-062619', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\", 'api_arguments': 'None', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.13.0+cu117, Tokenizers 0.13.2', 'example_code': 'None', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.548, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] إنشاء نظام توصيات يقترح منتجات مخصصة للعملاء استنادًا إلى مشترياتهم وتفضيلاتهم السابقة.\\n -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-062619', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\", 'api_arguments': 'None', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.13.0+cu117, Tokenizers 0.13.2', 'example_code': 'None', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.548, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تصميم أداة للتعرف على الكيانات في مستندات قانونية لتحديد المصطلحات والكيانات الرئيسية للتحليل.\\n -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-062619', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\", 'api_arguments': 'None', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.13.0+cu117, Tokenizers 0.13.2', 'example_code': 'None', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.548, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تطوير أداة يمكنها تصنيف تذاكر الدعم إلى فئات مختلفة تلقائيًا استنادًا إلى محتوى التذكرة.\\n -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-062619', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\", 'api_arguments': 'None', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.13.0+cu117, Tokenizers 0.13.2', 'example_code': 'None', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.548, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] إنشاء أداة لإنشاء أوصاف المنتج من المنتج الصور باستخدام تقنيات التعليق على الصور.\\n -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-062619', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\", 'api_arguments': 'None', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.13.0+cu117, Tokenizers 0.13.2', 'example_code': 'None', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.548, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تنفيذ أداة لتحديد الأخبار المزيفة والمعلومات الخاطئة من المقالات الإخبارية ومنشورات وسائل التواصل الاجتماعي.\\n -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-062619', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\", 'api_arguments': 'None', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.13.0+cu117, Tokenizers 0.13.2', 'example_code': 'None', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.548, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] إنشاء أداة لاستخراج الكلمات الرئيسية والعبارات المهمة من أوراق البحث الأكاديمية لتسهيل الأمر استرداد المعلومات.\\n -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-062619', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\", 'api_arguments': 'None', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.13.0+cu117, Tokenizers 0.13.2', 'example_code': 'None', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.548, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تطوير أداة لترجمة اللغة في الوقت الحقيقي للاتصال عبر الإنترنت والدعم متعدد اللغات.\\n -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-062619', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\", 'api_arguments': 'None', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.13.0+cu117, Tokenizers 0.13.2', 'example_code': 'None', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.548, 'Rmse': 'nan'}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST]تطوير تقنية لإكتشاف الكواكب خارج المجموعة الشمسية باستخدام بيانات التلسكوبات الفضائية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-104421', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3736, 'Mae': 0.3079, 'Rmse': 0.4321, 'Abs Rel': 0.3666, 'Log Mae': 0.1288, 'Log Rmse': 0.1794, 'Delta1': 0.4929, 'Delta2': 0.7934, 'Delta3': 0.9234}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] إنشاء نظام لمراقبة وتحليل حشرات الزراعية باستخدام تقنيات التعرف على الصور.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-104421', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3736, 'Mae': 0.3079, 'Rmse': 0.4321, 'Abs Rel': 0.3666, 'Log Mae': 0.1288, 'Log Rmse': 0.1794, 'Delta1': 0.4929, 'Delta2': 0.7934, 'Delta3': 0.9234}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تطوير نظام لتحديد تصنيفات النباتات من الصور باستخدام الذكاء الاصطناعي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-104421', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3736, 'Mae': 0.3079, 'Rmse': 0.4321, 'Abs Rel': 0.3666, 'Log Mae': 0.1288, 'Log Rmse': 0.1794, 'Delta1': 0.4929, 'Delta2': 0.7934, 'Delta3': 0.9234}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تطوير تقنية للتعرف على التعبيرات الوجهية من الصور باستخدام شبكات عصبية عميقة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-104421', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3736, 'Mae': 0.3079, 'Rmse': 0.4321, 'Abs Rel': 0.3666, 'Log Mae': 0.1288, 'Log Rmse': 0.1794, 'Delta1': 0.4929, 'Delta2': 0.7934, 'Delta3': 0.9234}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] إنشاء نظام لتحليل بيانات السوق المالية وتوقع اتجاهات الأسهم باستخدام تقنيات الذكاء الاصطناعي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-104421', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3736, 'Mae': 0.3079, 'Rmse': 0.4321, 'Abs Rel': 0.3666, 'Log Mae': 0.1288, 'Log Rmse': 0.1794, 'Delta1': 0.4929, 'Delta2': 0.7934, 'Delta3': 0.9234}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تطوير تقنية للكشف عن الأشكال الهندسية الأساسية في الصور باستخدام تعلم الآلة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-104421', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3736, 'Mae': 0.3079, 'Rmse': 0.4321, 'Abs Rel': 0.3666, 'Log Mae': 0.1288, 'Log Rmse': 0.1794, 'Delta1': 0.4929, 'Delta2': 0.7934, 'Delta3': 0.9234}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] إنشاء تطبيق للتعرف على الأشخاص باستخدام بيانات الوجه وتقنيات الذكاء الاصطناعي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-104421', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3736, 'Mae': 0.3079, 'Rmse': 0.4321, 'Abs Rel': 0.3666, 'Log Mae': 0.1288, 'Log Rmse': 0.1794, 'Delta1': 0.4929, 'Delta2': 0.7934, 'Delta3': 0.9234}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تطوير نظام لكشف السيارات القادمة في الطريق باستخدام تقنيات الرؤية الحاسوبية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-104421', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3736, 'Mae': 0.3079, 'Rmse': 0.4321, 'Abs Rel': 0.3666, 'Log Mae': 0.1288, 'Log Rmse': 0.1794, 'Delta1': 0.4929, 'Delta2': 0.7934, 'Delta3': 0.9234}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] إنشاء تقنية لتحليل نصوص العملاء وتصنيف مشترياتهم بناءً على التحليل اللغوي بالاستخدام الذكاء الاصطناعي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-104421', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3736, 'Mae': 0.3079, 'Rmse': 0.4321, 'Abs Rel': 0.3666, 'Log Mae': 0.1288, 'Log Rmse': 0.1794, 'Delta1': 0.4929, 'Delta2': 0.7934, 'Delta3': 0.9234}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تطوير نظام لتحديد سلوك المركبات على الطرق باستخدام بيانات الفيديو والتعلم العميق.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-221116-104421', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3736, 'Mae': 0.3079, 'Rmse': 0.4321, 'Abs Rel': 0.3666, 'Log Mae': 0.1288, 'Log Rmse': 0.1794, 'Delta1': 0.4929, 'Delta2': 0.7934, 'Delta3': 0.9234}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST]\"لنقم بتطبيق نموذج تحليل النصوص \"mbien/recipetext2image\" لإنشاء صور توضيحية للوصف الجغرافي للأكلات.\"? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-063504', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3533, 'Mae': 0.2668, 'Rmse': 0.3716, 'Abs Rel': 0.3427, 'Log Mae': 0.1167, 'Log Rmse': 0.1703, 'Delta1': 0.5522, 'Delta2': 0.8362, 'Delta3': 0.9382}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation.' </s>"
"<s>[INST] \"استخدم API الخاص بالتفاعل بين الجهاز والإنسان \"rasa/rasa-x\" لتطوير نموذج يتعلم من محادثات المستخدمين.\"? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-063504', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3533, 'Mae': 0.2668, 'Rmse': 0.3716, 'Abs Rel': 0.3427, 'Log Mae': 0.1167, 'Log Rmse': 0.1703, 'Delta1': 0.5522, 'Delta2': 0.8362, 'Delta3': 0.9382}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation.' </s>"
"<s>[INST] \"قم بإنشاء نموذج ML لتبسيط النصوص الطبية باستخدام API \"tanyayt/medicalqa-mini\" لتحليل المعلومات الطبية.\"? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-063504', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3533, 'Mae': 0.2668, 'Rmse': 0.3716, 'Abs Rel': 0.3427, 'Log Mae': 0.1167, 'Log Rmse': 0.1703, 'Delta1': 0.5522, 'Delta2': 0.8362, 'Delta3': 0.9382}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation.' </s>"
"<s>[INST] \"استخدم API الذكاء الاصطناعي \"googleapis/google-cloud\" لتطوير نظام يحول الكتب الصوتية إلى نصوص كتابية.\"? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-063504', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3533, 'Mae': 0.2668, 'Rmse': 0.3716, 'Abs Rel': 0.3427, 'Log Mae': 0.1167, 'Log Rmse': 0.1703, 'Delta1': 0.5522, 'Delta2': 0.8362, 'Delta3': 0.9382}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation.' </s>"
"<s>[INST] \"نقوم بتنفيذ أمر \"docker run\" باستخدام API \"databrickslabs/koalas\" لمعالجة البيانات الضخمة بيئة بيثونية.\"? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-063504', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3533, 'Mae': 0.2668, 'Rmse': 0.3716, 'Abs Rel': 0.3427, 'Log Mae': 0.1167, 'Log Rmse': 0.1703, 'Delta1': 0.5522, 'Delta2': 0.8362, 'Delta3': 0.9382}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation.' </s>"
"<s>[INST] \"استخدم API \"aylien/news-api\" لجمع أحدث الأخبار حول الرياضات الأولمبية وتحليلها تلقائيًا.\"? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-063504', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3533, 'Mae': 0.2668, 'Rmse': 0.3716, 'Abs Rel': 0.3427, 'Log Mae': 0.1167, 'Log Rmse': 0.1703, 'Delta1': 0.5522, 'Delta2': 0.8362, 'Delta3': 0.9382}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation.' </s>"
"<s>[INST] \"لإستكشاف وتصنيف النصوص الطبية، قم بتحليلها باستخدام API \"vinai/bertweet-base\" لتصنيف التغريدات الطبية.\"? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-063504', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3533, 'Mae': 0.2668, 'Rmse': 0.3716, 'Abs Rel': 0.3427, 'Log Mae': 0.1167, 'Log Rmse': 0.1703, 'Delta1': 0.5522, 'Delta2': 0.8362, 'Delta3': 0.9382}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation.' </s>"
"<s>[INST] \"استخدم تقنية التعلم العميق باستخدام API \"clarifai/clarifai\" لتصنيف الصور بناء على المحتوى البصري.\"? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-063504', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3533, 'Mae': 0.2668, 'Rmse': 0.3716, 'Abs Rel': 0.3427, 'Log Mae': 0.1167, 'Log Rmse': 0.1703, 'Delta1': 0.5522, 'Delta2': 0.8362, 'Delta3': 0.9382}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation.' </s>"
"<s>[INST] \"انشئ نظام تشتمل على واجهة برمجة تطبيقات تعمل بتقنية البلوكتشين باستخدام API \"ethereum/web3\" للاتصال بمحفظة Ethereum.\"? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-063504', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3533, 'Mae': 0.2668, 'Rmse': 0.3716, 'Abs Rel': 0.3427, 'Log Mae': 0.1167, 'Log Rmse': 0.1703, 'Delta1': 0.5522, 'Delta2': 0.8362, 'Delta3': 0.9382}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation.' </s>"
"<s>[INST] \"استخدم API \"openai/gpt-3.5\" لتوليد خطط تسويقية إبداعية ومحتوى جذاب لوسائل التواصل الاجتماعي.\"? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-063504', 'api_call': \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3533, 'Mae': 0.2668, 'Rmse': 0.3716, 'Abs Rel': 0.3427, 'Log Mae': 0.1167, 'Log Rmse': 0.1703, 'Delta1': 0.5522, 'Delta2': 0.8362, 'Delta3': 0.9382}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation.' </s>"
"<s>[INST]تطوير تطبيق يستخدم تقنيات الذكاء الصناعي لتوفير توصيات فريدة للمستخدمين استنادًا إلى تفضيلاتهم الشخصية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221116-110652', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4018, 'Mae': 0.3272, 'Rmse': 0.4546, 'Abs Rel': 0.3934, 'Log Mae': 0.138, 'Log Rmse': 0.1907, 'Delta1': 0.4598, 'Delta2': 0.7659, 'Delta3': 0.9082}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] تكامل واجهة برمجية للتعرف على الكلام في تطبيقك الحالي لتمكين التفاعل الصوتي مع المستخدمين.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221116-110652', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4018, 'Mae': 0.3272, 'Rmse': 0.4546, 'Abs Rel': 0.3934, 'Log Mae': 0.138, 'Log Rmse': 0.1907, 'Delta1': 0.4598, 'Delta2': 0.7659, 'Delta3': 0.9082}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] إضافة إمكانية تحويل النص إلى كلام مسموع في تطبيقك لدعم مستخدمي الصم وضعاف السمع.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221116-110652', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4018, 'Mae': 0.3272, 'Rmse': 0.4546, 'Abs Rel': 0.3934, 'Log Mae': 0.138, 'Log Rmse': 0.1907, 'Delta1': 0.4598, 'Delta2': 0.7659, 'Delta3': 0.9082}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] تطوير واجهة برمجية لإدارة عمليات البحث والفرز الذكية في تطبيق التجارة الإلكترونية الخاص بك.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221116-110652', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4018, 'Mae': 0.3272, 'Rmse': 0.4546, 'Abs Rel': 0.3934, 'Log Mae': 0.138, 'Log Rmse': 0.1907, 'Delta1': 0.4598, 'Delta2': 0.7659, 'Delta3': 0.9082}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] تكامل وظيفة التعرف على الوجوه في تطبيق الهوية الرقمية الخاص بك باستخدام التكنولوجيا البيومترية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221116-110652', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4018, 'Mae': 0.3272, 'Rmse': 0.4546, 'Abs Rel': 0.3934, 'Log Mae': 0.138, 'Log Rmse': 0.1907, 'Delta1': 0.4598, 'Delta2': 0.7659, 'Delta3': 0.9082}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] إنشاء واجهة برمجية لتحليل نغمات الموسيقى واكتشاف التقارير الصوتية في تطبيقك للموسيقى.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221116-110652', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4018, 'Mae': 0.3272, 'Rmse': 0.4546, 'Abs Rel': 0.3934, 'Log Mae': 0.138, 'Log Rmse': 0.1907, 'Delta1': 0.4598, 'Delta2': 0.7659, 'Delta3': 0.9082}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] تكامل واجهة برمجية لإجراء تحليل سلوك المستخدم وتوصيات مستندة إلى السلوك في تطبيقك.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221116-110652', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4018, 'Mae': 0.3272, 'Rmse': 0.4546, 'Abs Rel': 0.3934, 'Log Mae': 0.138, 'Log Rmse': 0.1907, 'Delta1': 0.4598, 'Delta2': 0.7659, 'Delta3': 0.9082}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] تضمين واجهة برمجية لرصد وتحليل الأداء المالي في تطبيق الأعمال الخاص بك.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221116-110652', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4018, 'Mae': 0.3272, 'Rmse': 0.4546, 'Abs Rel': 0.3934, 'Log Mae': 0.138, 'Log Rmse': 0.1907, 'Delta1': 0.4598, 'Delta2': 0.7659, 'Delta3': 0.9082}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] تكامل واجهة برمجية لإضافة خدمات الدفع الآمنة والموثوقة في تطبيق التجارة الإلكترونية الخاص بك.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221116-110652', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4018, 'Mae': 0.3272, 'Rmse': 0.4546, 'Abs Rel': 0.3934, 'Log Mae': 0.138, 'Log Rmse': 0.1907, 'Delta1': 0.4598, 'Delta2': 0.7659, 'Delta3': 0.9082}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] إنشاء واجهة برمجية لدمج خدمات الوسائط الاجتماعية في تطبيق التواصل الاجتماعي الخاص بك.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221116-110652', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4018, 'Mae': 0.3272, 'Rmse': 0.4546, 'Abs Rel': 0.3934, 'Log Mae': 0.138, 'Log Rmse': 0.1907, 'Delta1': 0.4598, 'Delta2': 0.7659, 'Delta3': 0.9082}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST]قم بتطوير تطبيق يستخدم تقنيات التعلم الآلي لتحليل وتصنيف الصور الطبية. كيف يمكن استخدام تطبيق Hugging Face Transformers لتحقيق هذا الهدف؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-113853', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3384, 'Mae': 0.2739, 'Rmse': 0.3959, 'Abs Rel': 0.323, 'Log Mae': 0.1148, 'Log Rmse': 0.1651, 'Delta1': 0.5576, 'Delta2': 0.8345, 'Delta3': 0.9398}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] قم بإنشاء نموذج يستخدم تحليل النصوص لفهم مشكلات العملاء وتوجيههم للحلول الصحيحة. ما هي أفضل استراتيجية لتحقيق هذا الهدف؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-113853', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3384, 'Mae': 0.2739, 'Rmse': 0.3959, 'Abs Rel': 0.323, 'Log Mae': 0.1148, 'Log Rmse': 0.1651, 'Delta1': 0.5576, 'Delta2': 0.8345, 'Delta3': 0.9398}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] أرغب في تطوير نظام يستخدم تكنولوجيا التعلم الآلي لتنبؤ بأداء الأسهم. كيف يمكن استخدام API Alpha Vantage للوصول إلى البيانات المالية اللازمة؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-113853', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3384, 'Mae': 0.2739, 'Rmse': 0.3959, 'Abs Rel': 0.323, 'Log Mae': 0.1148, 'Log Rmse': 0.1651, 'Delta1': 0.5576, 'Delta2': 0.8345, 'Delta3': 0.9398}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] قم ببناء نظام يستخدم تعلم الآلة لتحليل استجابات المستخدمين وتقديم توصيات شخصية. كيف يمكن استخدام API IBM Watson Assistant لتطوير هذا النوع من الأنظمة؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-113853', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3384, 'Mae': 0.2739, 'Rmse': 0.3959, 'Abs Rel': 0.323, 'Log Mae': 0.1148, 'Log Rmse': 0.1651, 'Delta1': 0.5576, 'Delta2': 0.8345, 'Delta3': 0.9398}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] قم بتطبيق نظام يستخدم تقنيات التعلم الآلي للتعرف على الأمراض النفسية من الصور الطبية. كيف يمكن استخدام API TensorFlow لتنفيذ ذلك؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-113853', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3384, 'Mae': 0.2739, 'Rmse': 0.3959, 'Abs Rel': 0.323, 'Log Mae': 0.1148, 'Log Rmse': 0.1651, 'Delta1': 0.5576, 'Delta2': 0.8345, 'Delta3': 0.9398}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] أنشئ نظام يستخدم تحليل البيانات وتقنيات التعلم العميق لتوقع سلوك المستهلكين. كيف يمكن استخدام API Amazon SageMaker لتنفيذ هذه المهمة؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-113853', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3384, 'Mae': 0.2739, 'Rmse': 0.3959, 'Abs Rel': 0.323, 'Log Mae': 0.1148, 'Log Rmse': 0.1651, 'Delta1': 0.5576, 'Delta2': 0.8345, 'Delta3': 0.9398}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] قم بتنفيذ نظام يستخدم تعلم الآلة لتحليل التوجيهات المرورية وتوجيه السائقين. كيف يمكن استخدام API OpenCV لتنفيذ هذه المهمة؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-113853', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3384, 'Mae': 0.2739, 'Rmse': 0.3959, 'Abs Rel': 0.323, 'Log Mae': 0.1148, 'Log Rmse': 0.1651, 'Delta1': 0.5576, 'Delta2': 0.8345, 'Delta3': 0.9398}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] قم بتصميم نظام يستخدم تقنيات الذكاء الاصطناعي لتحسين تجربة المستخدم في التسوق عبر الإنترنت. كيف يمكن استخدام API Shopify لتحقيق هذا الهدف؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-113853', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3384, 'Mae': 0.2739, 'Rmse': 0.3959, 'Abs Rel': 0.323, 'Log Mae': 0.1148, 'Log Rmse': 0.1651, 'Delta1': 0.5576, 'Delta2': 0.8345, 'Delta3': 0.9398}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] أنشئ تطبيقاً يستخدم تعلم الآلة لتحليل نمط الدماغ والكشف عن الأمراض المتعلقة بالدماغ. كيف يمكن استخدام API NeuroAI لتنفيذ هذه المهمة؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-113853', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3384, 'Mae': 0.2739, 'Rmse': 0.3959, 'Abs Rel': 0.323, 'Log Mae': 0.1148, 'Log Rmse': 0.1651, 'Delta1': 0.5576, 'Delta2': 0.8345, 'Delta3': 0.9398}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] قم ببناء نظام يستخدم تحليل البيانات وتقنيات التعلم الآلي لتوقع الطلب على المنتجات. كيف يمكن استخدام API Microsoft Azure Machine Learning لتنفيذ هذا الهدف؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221121-113853', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3384, 'Mae': 0.2739, 'Rmse': 0.3959, 'Abs Rel': 0.323, 'Log Mae': 0.1148, 'Log Rmse': 0.1651, 'Delta1': 0.5576, 'Delta2': 0.8345, 'Delta3': 0.9398}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST]قم بتطوير تطبيق يستخدم تقنية الواقع المعزز لعرض معلومات تفصيلية حول الوجوه الذي تظهر في الصور.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-014502', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3476, 'Mae': 0.2763, 'Rmse': 0.4088, 'Abs Rel': 0.3308, 'Log Mae': 0.1161, 'Log Rmse': 0.17, 'Delta1': 0.5682, 'Delta2': 0.8301, 'Delta3': 0.9279}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics.' </s>"
"<s>[INST] ابنِ نظام جديد لتحليل النصوص الطبية واستخراج المعلومات الحيوية الرئيسية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-014502', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3476, 'Mae': 0.2763, 'Rmse': 0.4088, 'Abs Rel': 0.3308, 'Log Mae': 0.1161, 'Log Rmse': 0.17, 'Delta1': 0.5682, 'Delta2': 0.8301, 'Delta3': 0.9279}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics.' </s>"
"<s>[INST] قم ببناء نظام تنبؤ آلي يستند إلى بيانات سابقة لتوقع الاحتياجات اللازمة في سلسلة التوريد.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-014502', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3476, 'Mae': 0.2763, 'Rmse': 0.4088, 'Abs Rel': 0.3308, 'Log Mae': 0.1161, 'Log Rmse': 0.17, 'Delta1': 0.5682, 'Delta2': 0.8301, 'Delta3': 0.9279}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics.' </s>"
"<s>[INST] انشئ تطبيقاً يستخدم تقنيات الذكاء الاصطناعي لتحليل الأصوات واكتشاف الانفعالات في التحدث.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-014502', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3476, 'Mae': 0.2763, 'Rmse': 0.4088, 'Abs Rel': 0.3308, 'Log Mae': 0.1161, 'Log Rmse': 0.17, 'Delta1': 0.5682, 'Delta2': 0.8301, 'Delta3': 0.9279}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics.' </s>"
"<s>[INST] ابنِ تطبيق يستخدم خوارزميات التعلم الآلي لتوقع أسعار العقارات في السوق العقارية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-014502', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3476, 'Mae': 0.2763, 'Rmse': 0.4088, 'Abs Rel': 0.3308, 'Log Mae': 0.1161, 'Log Rmse': 0.17, 'Delta1': 0.5682, 'Delta2': 0.8301, 'Delta3': 0.9279}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics.' </s>"
"<s>[INST] قم بتطوير نظام يستخدم تقنيات تعلم الآلة لتحليل السلوكيات على الإنترنت والكشف عن أنماط غير عادية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-014502', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3476, 'Mae': 0.2763, 'Rmse': 0.4088, 'Abs Rel': 0.3308, 'Log Mae': 0.1161, 'Log Rmse': 0.17, 'Delta1': 0.5682, 'Delta2': 0.8301, 'Delta3': 0.9279}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics.' </s>"
"<s>[INST] بنِ منظومة لتحليل بيانات مبيعات التجزئة وتوليد تقارير محددة لأداء المنتجات.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-014502', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3476, 'Mae': 0.2763, 'Rmse': 0.4088, 'Abs Rel': 0.3308, 'Log Mae': 0.1161, 'Log Rmse': 0.17, 'Delta1': 0.5682, 'Delta2': 0.8301, 'Delta3': 0.9279}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics.' </s>"
"<s>[INST] ابنِ تطبيق يدعم تقنية التعرف على الوجوه لضمان الأمان والوصول الآمن للأماكن المحمية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-014502', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3476, 'Mae': 0.2763, 'Rmse': 0.4088, 'Abs Rel': 0.3308, 'Log Mae': 0.1161, 'Log Rmse': 0.17, 'Delta1': 0.5682, 'Delta2': 0.8301, 'Delta3': 0.9279}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics.' </s>"
"<s>[INST] نفذ نظام يستخدم تقنيات تحليل البيانات لتحديد الاتجاهات وتوقع المستقبل في سوق الأسهم.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-014502', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3476, 'Mae': 0.2763, 'Rmse': 0.4088, 'Abs Rel': 0.3308, 'Log Mae': 0.1161, 'Log Rmse': 0.17, 'Delta1': 0.5682, 'Delta2': 0.8301, 'Delta3': 0.9279}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics.' </s>"
"<s>[INST] ابنِ تطبيق يستخدم تقنيات الذكاء الصناعي لتحليل بيانات العملاء وتنبؤ سلوكهم المستقبلي لدعم اتخاذ القرارات التسويقية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-014502', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3476, 'Mae': 0.2763, 'Rmse': 0.4088, 'Abs Rel': 0.3308, 'Log Mae': 0.1161, 'Log Rmse': 0.17, 'Delta1': 0.5682, 'Delta2': 0.8301, 'Delta3': 0.9279}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics.' </s>"
"<s>[INST]\"كيف يمكننا الاستفادة من نماذج معالجة اللغات الطبيعية (NLP) لأتمتة تلخيص المستندات القانونية لعمليات مراجعة أسرع؟\"، \"تطوير نظام توصية باستخدام تقنيات التصفية التعاونية لاقتراح الأفلام ذات الصلة للمستخدمين استنادًا إلى سجل المشاهدة الخاص بهم.\" استخدم تقنية التعرف على الكلام لنسخ الملفات الصوتية إلى نص لتسهيل التحليل وإمكانية الوصول إليها.، إنشاء أداة تحليل المشاعر باستخدام نموذج BERT المُدرب مسبقًا لفهم النغمة العاطفية لمنشورات وسائل التواصل الاجتماعي.، تنفيذ خوارزميات التعلم العميق للكشف عن الحالات الشاذة. الكشف في بيانات مستشعر إنترنت الأشياء الصناعية لتحديد أعطال المعدات المحتملة والتخفيف منها.، تعزيز عمليات خدمة العملاء من خلال نشر روبوت محادثة مدعوم بنموذج GPT-3 مُدرب مسبقًا للتفاعلات والدعم الشخصي.، إنشاء محرك توصيات باستخدام المصفوفة. التخصيم لاقتراح المنتجات ذات الصلة للمتسوقين عبر الإنترنت بناءً على سجل الشراء الخاص بهم.، استخدم نماذج اكتشاف الكائنات لتحليل لقطات الكاميرا الأمنية وتحديد الأنشطة المشبوهة في الوقت الفعلي لتعزيز المراقبة.، تطوير أداة تلخيص النص باستخدام GPT- 2 نموذج لغة لتكثيف المقالات والتقارير المطولة للاستهلاك المبسط.، تنفيذ نظام تركيب الكلام باستخدام تقنية WaveNet لتوليد كلام يشبه الإنسان من إدخال النص لإنتاج الصوت الطبيعي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-044810', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221122-044810')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, torch==1.12.1, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.369, 'Mae': 0.2909, 'Rmse': 0.4208, 'Abs Rel': 0.3635, 'Log Mae': 0.1224, 'Log Rmse': 0.1793, 'Delta1': 0.5323, 'Delta2': 0.8179, 'Delta3': 0.9258}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST]تصميم نظام لتحويل النصوص العربية إلى كلام مفهوم باستخدام تقنيات متقدمة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-082237', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3421, 'Mae': 0.27, 'Rmse': 0.4042, 'Abs Rel': 0.3279, 'Log Mae': 0.1132, 'Log Rmse': 0.1688, 'Delta1': 0.5839, 'Delta2': 0.8408, 'Delta3': 0.9309}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] إنشاء أداة تستخدم تقنيات التعلم العميق لفهم محتوى النصوص العربية وتوليدها بشكل تلقائي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-082237', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3421, 'Mae': 0.27, 'Rmse': 0.4042, 'Abs Rel': 0.3279, 'Log Mae': 0.1132, 'Log Rmse': 0.1688, 'Delta1': 0.5839, 'Delta2': 0.8408, 'Delta3': 0.9309}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] تحسين واجهة البرنامج الخاص بك من خلال تطبيق تقنيات تحليل النصوص العربية بواسطة مكتبة Hugging Face Transformers.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-082237', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3421, 'Mae': 0.27, 'Rmse': 0.4042, 'Abs Rel': 0.3279, 'Log Mae': 0.1132, 'Log Rmse': 0.1688, 'Delta1': 0.5839, 'Delta2': 0.8408, 'Delta3': 0.9309}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] تطوير نظام يستخدم تقنيات تحليل الصوتيات للكشف عن الأنماط الصوتية العربية بدقة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-082237', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3421, 'Mae': 0.27, 'Rmse': 0.4042, 'Abs Rel': 0.3279, 'Log Mae': 0.1132, 'Log Rmse': 0.1688, 'Delta1': 0.5839, 'Delta2': 0.8408, 'Delta3': 0.9309}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] إنشاء نظام يقوم بتحليل الأشكال الهندسية في الصور العربية واستخراج المعلومات ذات الصلة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-082237', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3421, 'Mae': 0.27, 'Rmse': 0.4042, 'Abs Rel': 0.3279, 'Log Mae': 0.1132, 'Log Rmse': 0.1688, 'Delta1': 0.5839, 'Delta2': 0.8408, 'Delta3': 0.9309}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] تصميم نظام يستخدم تقنيات التعلم العميق لتوليد النصوص الشعرية باللغة العربية بشكل إبداعي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-082237', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3421, 'Mae': 0.27, 'Rmse': 0.4042, 'Abs Rel': 0.3279, 'Log Mae': 0.1132, 'Log Rmse': 0.1688, 'Delta1': 0.5839, 'Delta2': 0.8408, 'Delta3': 0.9309}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] تطوير أداة لتحليل الأحاديث العربية واستخراج المعلومات الهامة باستخدام مكتبة Hugging Face Transformers.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-082237', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3421, 'Mae': 0.27, 'Rmse': 0.4042, 'Abs Rel': 0.3279, 'Log Mae': 0.1132, 'Log Rmse': 0.1688, 'Delta1': 0.5839, 'Delta2': 0.8408, 'Delta3': 0.9309}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] تحليل وتصنيف النصوص العربية تلقائياً باستخدام تقنيات التعلم العميق ومكتبة Hugging Face Transformers.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-082237', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3421, 'Mae': 0.27, 'Rmse': 0.4042, 'Abs Rel': 0.3279, 'Log Mae': 0.1132, 'Log Rmse': 0.1688, 'Delta1': 0.5839, 'Delta2': 0.8408, 'Delta3': 0.9309}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] إنشاء نظام يستخدم تقنيات التعلم العميق لتحليل الموسيقى العربية واقتراح أعمال موسيقية مماثلة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-082237', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3421, 'Mae': 0.27, 'Rmse': 0.4042, 'Abs Rel': 0.3279, 'Log Mae': 0.1132, 'Log Rmse': 0.1688, 'Delta1': 0.5839, 'Delta2': 0.8408, 'Delta3': 0.9309}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST] تصميم أداة تستخدم تقنيات التحليل الصوتي لتحويل الكتب الصوتية العربية إلى نصوص قابلة للتحرير.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221122-082237', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\", 'api_arguments': 'pretrained_model_name', 'python_environment_requirements': 'transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3421, 'Mae': 0.27, 'Rmse': 0.4042, 'Abs Rel': 0.3279, 'Log Mae': 0.1132, 'Log Rmse': 0.1688, 'Delta1': 0.5839, 'Delta2': 0.8408, 'Delta3': 0.9309}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.' </s>"
"<s>[INST]تنفيذ نظام استدعاء صوتي لإدارة المهام اليومية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-kitti-finetuned-diode-221214-123047', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3497, 'Mae': 0.2847, 'Rmse': 0.3977, 'Abs Rel': 0.3477, 'Log Mae': 0.1203, 'Log Rmse': 0.1726, 'Delta1': 0.5217, 'Delta2': 0.8246, 'Delta3': 0.9436}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.' </s>"
"<s>[INST] تطوير نظام تعلم آلي لتوصيف الأطعمة من الصور بدقة عالية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-kitti-finetuned-diode-221214-123047', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3497, 'Mae': 0.2847, 'Rmse': 0.3977, 'Abs Rel': 0.3477, 'Log Mae': 0.1203, 'Log Rmse': 0.1726, 'Delta1': 0.5217, 'Delta2': 0.8246, 'Delta3': 0.9436}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.' </s>"
"<s>[INST] إنشاء نظام تصنيف تلقائي للرسائل الإلكترونية الواردة لتحسين إدارة البريد الإلكتروني.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-kitti-finetuned-diode-221214-123047', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3497, 'Mae': 0.2847, 'Rmse': 0.3977, 'Abs Rel': 0.3477, 'Log Mae': 0.1203, 'Log Rmse': 0.1726, 'Delta1': 0.5217, 'Delta2': 0.8246, 'Delta3': 0.9436}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.' </s>"
"<s>[INST] تكامل نظام التحقق الثنائي لتعزيز أمان تطبيق الهاتف المحمول.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-kitti-finetuned-diode-221214-123047', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3497, 'Mae': 0.2847, 'Rmse': 0.3977, 'Abs Rel': 0.3477, 'Log Mae': 0.1203, 'Log Rmse': 0.1726, 'Delta1': 0.5217, 'Delta2': 0.8246, 'Delta3': 0.9436}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.' </s>"
"<s>[INST] تصميم نظام ترجمة تلقائية لزيادة إمكانية التواصل بين اللغات المختلفة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-kitti-finetuned-diode-221214-123047', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3497, 'Mae': 0.2847, 'Rmse': 0.3977, 'Abs Rel': 0.3477, 'Log Mae': 0.1203, 'Log Rmse': 0.1726, 'Delta1': 0.5217, 'Delta2': 0.8246, 'Delta3': 0.9436}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.' </s>"
"<s>[INST] إعداد نظام تحليل الانفعالات لتقييم مشاعر المستخدمين في التطبيقات الاجتماعية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-kitti-finetuned-diode-221214-123047', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3497, 'Mae': 0.2847, 'Rmse': 0.3977, 'Abs Rel': 0.3477, 'Log Mae': 0.1203, 'Log Rmse': 0.1726, 'Delta1': 0.5217, 'Delta2': 0.8246, 'Delta3': 0.9436}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.' </s>"
"<s>[INST] تطوير نظام تعرف على الوجوه لزيادة مستوى الأمان في أنظمة التحكم بالوصول.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-kitti-finetuned-diode-221214-123047', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3497, 'Mae': 0.2847, 'Rmse': 0.3977, 'Abs Rel': 0.3477, 'Log Mae': 0.1203, 'Log Rmse': 0.1726, 'Delta1': 0.5217, 'Delta2': 0.8246, 'Delta3': 0.9436}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.' </s>"
"<s>[INST] إنشاء نظام تعقب المواقع لتحسين خدمات توصيل الطرود.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-kitti-finetuned-diode-221214-123047', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3497, 'Mae': 0.2847, 'Rmse': 0.3977, 'Abs Rel': 0.3477, 'Log Mae': 0.1203, 'Log Rmse': 0.1726, 'Delta1': 0.5217, 'Delta2': 0.8246, 'Delta3': 0.9436}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.' </s>"
"<s>[INST] تكامل نظام التعرف على النصوص لاستخراج المعلومات الرئيسية من الوثائق القانونية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-kitti-finetuned-diode-221214-123047', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3497, 'Mae': 0.2847, 'Rmse': 0.3977, 'Abs Rel': 0.3477, 'Log Mae': 0.1203, 'Log Rmse': 0.1726, 'Delta1': 0.5217, 'Delta2': 0.8246, 'Delta3': 0.9436}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.' </s>"
"<s>[INST] تنفيذ نظام فلترة الصور لتحسين جودة الصور الرقمية.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-kitti-finetuned-diode-221214-123047', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\", 'api_arguments': [], 'python_environment_requirements': ['transformers==4.24.0', 'torch==1.12.1+cu116', 'tokenizers==0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.3497, 'Mae': 0.2847, 'Rmse': 0.3977, 'Abs Rel': 0.3477, 'Log Mae': 0.1203, 'Log Rmse': 0.1726, 'Delta1': 0.5217, 'Delta2': 0.8246, 'Delta3': 0.9436}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.' </s>"
"<s>[INST]ابتكار تطبيق يستخدم تقنيات الذكاء الاصطناعي لتحليل نمط الكتابة اليدوية في الصور وتحويلها إلى نصوص قابلة للتحرير.\\n   -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221221-102136', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\", 'api_arguments': [], 'python_environment_requirements': ['Transformers 4.24.0', 'Pytorch 1.12.1+cu116', 'Datasets 2.8.0', 'Tokenizers 0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4222, 'Mae': 0.411, 'Rmse': 0.6292, 'Abs Rel': 0.3778, 'Log Mae': 0.1636, 'Log Rmse': 0.224, 'Delta1': 0.432, 'Delta2': 0.6806, 'Delta3': 0.8068}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تطوير تطبيق يستخدم الذكاء الاصطناعي لتحليل موضوعات النصوص وتوليد ملخصات نصية احترافية.\\n   -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221221-102136', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\", 'api_arguments': [], 'python_environment_requirements': ['Transformers 4.24.0', 'Pytorch 1.12.1+cu116', 'Datasets 2.8.0', 'Tokenizers 0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4222, 'Mae': 0.411, 'Rmse': 0.6292, 'Abs Rel': 0.3778, 'Log Mae': 0.1636, 'Log Rmse': 0.224, 'Delta1': 0.432, 'Delta2': 0.6806, 'Delta3': 0.8068}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم تقنيات التعلم العميق لتصنيف الفيديوهات بناءً على محتواها.\\n   -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221221-102136', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\", 'api_arguments': [], 'python_environment_requirements': ['Transformers 4.24.0', 'Pytorch 1.12.1+cu116', 'Datasets 2.8.0', 'Tokenizers 0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4222, 'Mae': 0.411, 'Rmse': 0.6292, 'Abs Rel': 0.3778, 'Log Mae': 0.1636, 'Log Rmse': 0.224, 'Delta1': 0.432, 'Delta2': 0.6806, 'Delta3': 0.8068}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] بناء نظام يستخدم APIs لتحليل مواقع التواصل الاجتماعي واستخراج الاتجاهات والمعلومات الهامة.\\n   -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221221-102136', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\", 'api_arguments': [], 'python_environment_requirements': ['Transformers 4.24.0', 'Pytorch 1.12.1+cu116', 'Datasets 2.8.0', 'Tokenizers 0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4222, 'Mae': 0.411, 'Rmse': 0.6292, 'Abs Rel': 0.3778, 'Log Mae': 0.1636, 'Log Rmse': 0.224, 'Delta1': 0.432, 'Delta2': 0.6806, 'Delta3': 0.8068}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تطوير تطبيق يستخدم خوارزميات التعلم الآلي لتوصيف منتجات التجزئة استنادًا إلى تفضيلات المستهلكين.\\n   -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221221-102136', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\", 'api_arguments': [], 'python_environment_requirements': ['Transformers 4.24.0', 'Pytorch 1.12.1+cu116', 'Datasets 2.8.0', 'Tokenizers 0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4222, 'Mae': 0.411, 'Rmse': 0.6292, 'Abs Rel': 0.3778, 'Log Mae': 0.1636, 'Log Rmse': 0.224, 'Delta1': 0.432, 'Delta2': 0.6806, 'Delta3': 0.8068}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] برمجة تطبيق يستخدم خوارزميات التعلم الآلي للتعرف على الأشكال الهندسية في الصور.\\n   -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221221-102136', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\", 'api_arguments': [], 'python_environment_requirements': ['Transformers 4.24.0', 'Pytorch 1.12.1+cu116', 'Datasets 2.8.0', 'Tokenizers 0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4222, 'Mae': 0.411, 'Rmse': 0.6292, 'Abs Rel': 0.3778, 'Log Mae': 0.1636, 'Log Rmse': 0.224, 'Delta1': 0.432, 'Delta2': 0.6806, 'Delta3': 0.8068}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم خوارزميات التعلم الآلي لتحليل الشبكات الاجتماعية واكتشاف العلاقات بين المستخدمين.\\n   -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221221-102136', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\", 'api_arguments': [], 'python_environment_requirements': ['Transformers 4.24.0', 'Pytorch 1.12.1+cu116', 'Datasets 2.8.0', 'Tokenizers 0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4222, 'Mae': 0.411, 'Rmse': 0.6292, 'Abs Rel': 0.3778, 'Log Mae': 0.1636, 'Log Rmse': 0.224, 'Delta1': 0.432, 'Delta2': 0.6806, 'Delta3': 0.8068}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تطوير نظام يقوم بمراقبة وتحليل بيانات الصحة الإلكترونية باستخدام تقنيات التعلم الآلي.\\n   -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221221-102136', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\", 'api_arguments': [], 'python_environment_requirements': ['Transformers 4.24.0', 'Pytorch 1.12.1+cu116', 'Datasets 2.8.0', 'Tokenizers 0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4222, 'Mae': 0.411, 'Rmse': 0.6292, 'Abs Rel': 0.3778, 'Log Mae': 0.1636, 'Log Rmse': 0.224, 'Delta1': 0.432, 'Delta2': 0.6806, 'Delta3': 0.8068}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] إعداد تطبيق يستخدم تقنيات الذكاء الاصطناعي لتحليل سلوك المستخدمين عبر المواقع الإلكترونية.\\n   -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221221-102136', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\", 'api_arguments': [], 'python_environment_requirements': ['Transformers 4.24.0', 'Pytorch 1.12.1+cu116', 'Datasets 2.8.0', 'Tokenizers 0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4222, 'Mae': 0.411, 'Rmse': 0.6292, 'Abs Rel': 0.3778, 'Log Mae': 0.1636, 'Log Rmse': 0.224, 'Delta1': 0.432, 'Delta2': 0.6806, 'Delta3': 0.8068}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تنفيذ نظام يعتمد على APIs لتحليل بيانات المبيعات والإيرادات وتوفير توصيات لزيادة الربحية.\\n   -? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221221-102136', 'api_call': \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')\", 'api_arguments': [], 'python_environment_requirements': ['Transformers 4.24.0', 'Pytorch 1.12.1+cu116', 'Datasets 2.8.0', 'Tokenizers 0.13.2'], 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4222, 'Mae': 0.411, 'Rmse': 0.6292, 'Abs Rel': 0.3778, 'Log Mae': 0.1636, 'Log Rmse': 0.224, 'Delta1': 0.432, 'Delta2': 0.6806, 'Delta3': 0.8068}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST]قم بتطوير خدمة تقديم توصيات للموسيقى باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221228-072509', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4012, 'Mae': 0.403, 'Rmse': 0.6173, 'Abs Rel': 0.3487, 'Log Mae': 0.1574, 'Log Rmse': 0.211, 'Delta1': 0.4308, 'Delta2': 0.6997, 'Delta3': 0.8249}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] إنشاء نظام تحليل السلوك للمشتريات عبر الإنترنت باستخدام خوارزميات تعلم الآلة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221228-072509', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4012, 'Mae': 0.403, 'Rmse': 0.6173, 'Abs Rel': 0.3487, 'Log Mae': 0.1574, 'Log Rmse': 0.211, 'Delta1': 0.4308, 'Delta2': 0.6997, 'Delta3': 0.8249}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] إعداد نظام تجارة إلكترونية يستخدم واجهة برمجة لتوليد رموز خصم.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221228-072509', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4012, 'Mae': 0.403, 'Rmse': 0.6173, 'Abs Rel': 0.3487, 'Log Mae': 0.1574, 'Log Rmse': 0.211, 'Delta1': 0.4308, 'Delta2': 0.6997, 'Delta3': 0.8249}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] إنشاء نظام تصنيف للأفلام باستخدام تقنيات تعلم الآلة لاقتراح الأفلام المناسبة للمستخدمين.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221228-072509', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4012, 'Mae': 0.403, 'Rmse': 0.6173, 'Abs Rel': 0.3487, 'Log Mae': 0.1574, 'Log Rmse': 0.211, 'Delta1': 0.4308, 'Delta2': 0.6997, 'Delta3': 0.8249}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تنفيذ نظام تحليل النصوص لفهم مشاعر المستخدمين عبر وسائل التواصل الاجتماعي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221228-072509', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4012, 'Mae': 0.403, 'Rmse': 0.6173, 'Abs Rel': 0.3487, 'Log Mae': 0.1574, 'Log Rmse': 0.211, 'Delta1': 0.4308, 'Delta2': 0.6997, 'Delta3': 0.8249}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] إعداد منصة تقديم استشارات صحية عبر الإنترنت باستخدام تقنيات الذكاء الاصطناعي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221228-072509', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4012, 'Mae': 0.403, 'Rmse': 0.6173, 'Abs Rel': 0.3487, 'Log Mae': 0.1574, 'Log Rmse': 0.211, 'Delta1': 0.4308, 'Delta2': 0.6997, 'Delta3': 0.8249}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تطوير نظام تعرف على الصوت باستخدام تقنيات تعلم الآلة لتحليل الصوتيات وتحويلها إلى نص.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221228-072509', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4012, 'Mae': 0.403, 'Rmse': 0.6173, 'Abs Rel': 0.3487, 'Log Mae': 0.1574, 'Log Rmse': 0.211, 'Delta1': 0.4308, 'Delta2': 0.6997, 'Delta3': 0.8249}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] إنشاء نظام للتعرف على الوجوه في الصور باستخدام تقنيات شبكات العصب الاصطناعي.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221228-072509', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4012, 'Mae': 0.403, 'Rmse': 0.6173, 'Abs Rel': 0.3487, 'Log Mae': 0.1574, 'Log Rmse': 0.211, 'Delta1': 0.4308, 'Delta2': 0.6997, 'Delta3': 0.8249}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تطوير نظام تحليل البيانات لتوفير إحصائيات دقيقة حول أداء التطبيقات الجديدة.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221228-072509', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4012, 'Mae': 0.403, 'Rmse': 0.6173, 'Abs Rel': 0.3487, 'Log Mae': 0.1574, 'Log Rmse': 0.211, 'Delta1': 0.4308, 'Delta2': 0.6997, 'Delta3': 0.8249}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST] تنفيذ نظام توصيف للكتب باستخدام تقنيات تعلم الآلة لاقتراح الكتب المناسبة للقراء.? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'glpn-nyu-finetuned-diode-221228-072509', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\", 'api_arguments': '', 'python_environment_requirements': 'Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.4012, 'Mae': 0.403, 'Rmse': 0.6173, 'Abs Rel': 0.3487, 'Log Mae': 0.1574, 'Log Rmse': 0.211, 'Delta1': 0.4308, 'Delta2': 0.6997, 'Delta3': 0.8249}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.' </s>"
"<s>[INST]كيف يمكنني تحسين جودة صور الطبيعة باستخدام التقنيات الحديثة؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-230103-091356', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, datasets==2.8.0, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.436, 'Mae': 0.4251, 'Rmse': 0.6169, 'Abs Rel': 0.45, 'Log Mae': 0.1721, 'Log Rmse': 0.2269, 'Delta1': 0.3828, 'Delta2': 0.6326, 'Delta3': 0.8051}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] كيف يمكنني تحسين تصنيف النصوص العربية في تطبيقي؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-230103-091356', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, datasets==2.8.0, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.436, 'Mae': 0.4251, 'Rmse': 0.6169, 'Abs Rel': 0.45, 'Log Mae': 0.1721, 'Log Rmse': 0.2269, 'Delta1': 0.3828, 'Delta2': 0.6326, 'Delta3': 0.8051}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] كيف يمكنني تطوير نموذج تعلم آلي لتحليل العواطف في النصوص؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-230103-091356', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, datasets==2.8.0, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.436, 'Mae': 0.4251, 'Rmse': 0.6169, 'Abs Rel': 0.45, 'Log Mae': 0.1721, 'Log Rmse': 0.2269, 'Delta1': 0.3828, 'Delta2': 0.6326, 'Delta3': 0.8051}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] كيف يمكنني تحويل النصوص العربية إلى نصوص صوتية بجودة عالية؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-230103-091356', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, datasets==2.8.0, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.436, 'Mae': 0.4251, 'Rmse': 0.6169, 'Abs Rel': 0.45, 'Log Mae': 0.1721, 'Log Rmse': 0.2269, 'Delta1': 0.3828, 'Delta2': 0.6326, 'Delta3': 0.8051}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] كيف يمكنني تطوير نظام إدراك الكلام للغات الشرق أوسطية؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-230103-091356', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, datasets==2.8.0, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.436, 'Mae': 0.4251, 'Rmse': 0.6169, 'Abs Rel': 0.45, 'Log Mae': 0.1721, 'Log Rmse': 0.2269, 'Delta1': 0.3828, 'Delta2': 0.6326, 'Delta3': 0.8051}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] كيف يمكنني تحويل الملفات الصوتية إلى نصوص مكتوبة باللغة العربية؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-230103-091356', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, datasets==2.8.0, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.436, 'Mae': 0.4251, 'Rmse': 0.6169, 'Abs Rel': 0.45, 'Log Mae': 0.1721, 'Log Rmse': 0.2269, 'Delta1': 0.3828, 'Delta2': 0.6326, 'Delta3': 0.8051}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] كيف يمكنني تحسين كفاءة نظام التصنيف الآلي في تطبيق الذكاء الاصطناعي؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-230103-091356', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, datasets==2.8.0, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.436, 'Mae': 0.4251, 'Rmse': 0.6169, 'Abs Rel': 0.45, 'Log Mae': 0.1721, 'Log Rmse': 0.2269, 'Delta1': 0.3828, 'Delta2': 0.6326, 'Delta3': 0.8051}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] كيف يمكنني تحليل نصوص كبيرة واستخراج المعلومات المهمة منها بشكل ذكي؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-230103-091356', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, datasets==2.8.0, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.436, 'Mae': 0.4251, 'Rmse': 0.6169, 'Abs Rel': 0.45, 'Log Mae': 0.1721, 'Log Rmse': 0.2269, 'Delta1': 0.3828, 'Delta2': 0.6326, 'Delta3': 0.8051}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] كيف يمكنني تحسين جودة الصور الطبية بتقنيات تعزيز الصور؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-230103-091356', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, datasets==2.8.0, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.436, 'Mae': 0.4251, 'Rmse': 0.6169, 'Abs Rel': 0.45, 'Log Mae': 0.1721, 'Log Rmse': 0.2269, 'Delta1': 0.3828, 'Delta2': 0.6326, 'Delta3': 0.8051}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST] كيف يمكنني تحليل البيانات الضخمة سريعًا باستخدام تقنيات التعلم الآلي؟? [/INST]'domain': 'Computer Vision Depth Estimation', 'framework': 'Hugging Face Transformers', 'functionality': 'Depth Estimation', 'api_name': 'glpn-nyu-finetuned-diode-230103-091356', 'api_call': \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\", 'api_arguments': '', 'python_environment_requirements': 'transformers==4.24.0, pytorch==1.12.1+cu116, datasets==2.8.0, tokenizers==0.13.2', 'example_code': '', 'performance': {'dataset': 'diode-subset', 'accuracy': {'Loss': 0.436, 'Mae': 0.4251, 'Rmse': 0.6169, 'Abs Rel': 0.45, 'Log Mae': 0.1721, 'Log Rmse': 0.2269, 'Delta1': 0.3828, 'Delta2': 0.6326, 'Delta3': 0.8051}}, 'description': 'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.' </s>"
"<s>[INST]قم بتصميم تطبيق لمشاركة الصور الفوتوغرافية وتريد إضافة خاصية التعرف التلقائي على أوضاع الصور. استخدم الـ API التالي للتعرف على الصور بشكل دقيق.\\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-50', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\", 'api_arguments': {'from_pretrained': 'microsoft/resnet-50'}, 'python_environment_requirements': {'transformers': 'AutoImageProcessor, ResNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoImageProcessor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/resnet-50)\\nmodel = ResNetForImageClassification.from_pretrained(microsoft/resnet-50)\\ninputs = processor(image, return_tensors=pt)\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': '~0.5% top1'}, 'description': 'ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback.' </s>"
"<s>[INST] إنشاء نظام يسمح لمستخدمي التطبيق بتحديد ملامح الوجه في الصور وتلوينها بشكل أوتوماتيكي. يجب استخدام الـ API التالي لتحقيق ذلك.\\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-50', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\", 'api_arguments': {'from_pretrained': 'microsoft/resnet-50'}, 'python_environment_requirements': {'transformers': 'AutoImageProcessor, ResNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoImageProcessor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/resnet-50)\\nmodel = ResNetForImageClassification.from_pretrained(microsoft/resnet-50)\\ninputs = processor(image, return_tensors=pt)\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': '~0.5% top1'}, 'description': 'ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback.' </s>"
"<s>[INST] ترغب شركة التسوق الإلكتروني في تحسين تجربة المستخدم عبر توفير ميزة التعرف على المنتجات في الصور التي يلتقطها المستخدم. قدم لهم الـ API التالي للمساعدة في تنفيذ هذه الميزة.\\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-50', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\", 'api_arguments': {'from_pretrained': 'microsoft/resnet-50'}, 'python_environment_requirements': {'transformers': 'AutoImageProcessor, ResNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoImageProcessor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/resnet-50)\\nmodel = ResNetForImageClassification.from_pretrained(microsoft/resnet-50)\\ninputs = processor(image, return_tensors=pt)\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': '~0.5% top1'}, 'description': 'ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback.' </s>"
"<s>[INST] ينوي فريق التطوير إنشاء تطبيق لتحليل المزاج من الصور الشخصية التي يتم تحميلها. قدم لهم الـ API التالي لتحقيق هذا الهدف.\\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-50', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\", 'api_arguments': {'from_pretrained': 'microsoft/resnet-50'}, 'python_environment_requirements': {'transformers': 'AutoImageProcessor, ResNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoImageProcessor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/resnet-50)\\nmodel = ResNetForImageClassification.from_pretrained(microsoft/resnet-50)\\ninputs = processor(image, return_tensors=pt)\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': '~0.5% top1'}, 'description': 'ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback.' </s>"
"<s>[INST] ترغب منصة التواصل الاجتماعي في توفير ميزة التعرف على النصوص في الصور المشاركة. استخدم الـ API التالي لمساعدتهم في تحقيق هذا الهدف.\\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-50', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\", 'api_arguments': {'from_pretrained': 'microsoft/resnet-50'}, 'python_environment_requirements': {'transformers': 'AutoImageProcessor, ResNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoImageProcessor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/resnet-50)\\nmodel = ResNetForImageClassification.from_pretrained(microsoft/resnet-50)\\ninputs = processor(image, return_tensors=pt)\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': '~0.5% top1'}, 'description': 'ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback.' </s>"
"<s>[INST] قم بتصميم نظام لتحليل الرسوم البيانية والمخططات بشكل آلي من الصور. اقترح استخدام الـ API التالي للتحليل البصري للبيانات الرقمية.\\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-50', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\", 'api_arguments': {'from_pretrained': 'microsoft/resnet-50'}, 'python_environment_requirements': {'transformers': 'AutoImageProcessor, ResNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoImageProcessor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/resnet-50)\\nmodel = ResNetForImageClassification.from_pretrained(microsoft/resnet-50)\\ninputs = processor(image, return_tensors=pt)\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': '~0.5% top1'}, 'description': 'ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback.' </s>"
"<s>[INST] يهدف فريق التطوير إلى إنشاء تطبيق لتحسين جودة الصور الطبية وتشخيص الأمراض بشكل دقيق. استخدم الـ API التالي لتقديم ميزة التشخيص التلقائي للصور الطبية.\\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-50', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\", 'api_arguments': {'from_pretrained': 'microsoft/resnet-50'}, 'python_environment_requirements': {'transformers': 'AutoImageProcessor, ResNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoImageProcessor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/resnet-50)\\nmodel = ResNetForImageClassification.from_pretrained(microsoft/resnet-50)\\ninputs = processor(image, return_tensors=pt)\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': '~0.5% top1'}, 'description': 'ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback.' </s>"
"<s>[INST] تريد شركة الأمان السيبراني تحليل توقيعات تحميل الملفات لتحديد الملفات الضارة الناشئة. استخدم الـ API التالي لتطوير نظام الكشف التلقائي عن البرمجيات الضارة.\\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-50', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\", 'api_arguments': {'from_pretrained': 'microsoft/resnet-50'}, 'python_environment_requirements': {'transformers': 'AutoImageProcessor, ResNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoImageProcessor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/resnet-50)\\nmodel = ResNetForImageClassification.from_pretrained(microsoft/resnet-50)\\ninputs = processor(image, return_tensors=pt)\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': '~0.5% top1'}, 'description': 'ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback.' </s>"
"<s>[INST] قم بإنشاء تطبيق لفحص الوثائق واستخراج المعلومات الرئيسية منها بشكل تلقائي. استخدم الـ API التالي لتحقيق هذا الهدف.\\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-50', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\", 'api_arguments': {'from_pretrained': 'microsoft/resnet-50'}, 'python_environment_requirements': {'transformers': 'AutoImageProcessor, ResNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoImageProcessor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/resnet-50)\\nmodel = ResNetForImageClassification.from_pretrained(microsoft/resnet-50)\\ninputs = processor(image, return_tensors=pt)\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': '~0.5% top1'}, 'description': 'ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback.' </s>"
"<s>[INST] يرغب فريق البحث العلمي في تنفيذ تحليل الصور الطبية الشاملة والتفصيلية. قدم لهم الـ API التالي لمزيد من التطور في تحليل الصور الطبية وتشخيص الأمراض بدقة عالية.\\n    -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-50', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\", 'api_arguments': {'from_pretrained': 'microsoft/resnet-50'}, 'python_environment_requirements': {'transformers': 'AutoImageProcessor, ResNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoImageProcessor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/resnet-50)\\nmodel = ResNetForImageClassification.from_pretrained(microsoft/resnet-50)\\ninputs = processor(image, return_tensors=pt)\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': '~0.5% top1'}, 'description': 'ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback.' </s>"
"<s>[INST]إنشاء تطبيق لمراقبة وتحليل سلوك المستخدمين على منصة التواصل الاجتماعي الخاصة بشركتك. يمكنك استخدام الواجهة البرمجية التالية لتطوير نظام فهم البيانات لهذا الغرض:? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-large-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-large-224'}, 'python_environment_requirements': {'transformers': 'Hugging Face Transformers', 'torch': 'PyTorch', 'datasets': 'Hugging Face Datasets'}, 'example_code': {'import': ['from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification', 'import torch', 'from datasets import load_dataset'], 'load_dataset': \"dataset = load_dataset('huggingface/cats-image')\", 'image': \"image = dataset['test']['image'][0]\", 'feature_extractor': \"feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-large-224')\", 'model': \"model = ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'inputs': \"inputs = feature_extractor(image, return_tensors='pt')\", 'logits': 'with torch.no_grad():\\n  logits = model(**inputs).logits', 'predicted_label': 'predicted_label = logits.argmax(-1).item()', 'print': 'print(model.config.id2label[predicted_label])'}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration.\" </s>"
"<s>[INST] قم بتطوير نظام توصية ذكي لتحسين تجربة تصفح مستخدمي متجر التجزئة الخاص بشركتك. يمكنك استخدام API التالي لبناء نموذج توصية معتمد على التعلم الآلي وتحليل سلوك المستخدمين:? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-large-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-large-224'}, 'python_environment_requirements': {'transformers': 'Hugging Face Transformers', 'torch': 'PyTorch', 'datasets': 'Hugging Face Datasets'}, 'example_code': {'import': ['from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification', 'import torch', 'from datasets import load_dataset'], 'load_dataset': \"dataset = load_dataset('huggingface/cats-image')\", 'image': \"image = dataset['test']['image'][0]\", 'feature_extractor': \"feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-large-224')\", 'model': \"model = ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'inputs': \"inputs = feature_extractor(image, return_tensors='pt')\", 'logits': 'with torch.no_grad():\\n  logits = model(**inputs).logits', 'predicted_label': 'predicted_label = logits.argmax(-1).item()', 'print': 'print(model.config.id2label[predicted_label])'}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration.\" </s>"
"<s>[INST] إنشاء تطبيق لتوفير حلاً ذكيًا لمشكلة التنبؤ بحركة الأسهم باستخدام تقنيات التعلم الآلي والبيانات المالية التاريخية. يمكنك استخدام API التالي لتنفيذ نموذج التنبؤ:? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-large-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-large-224'}, 'python_environment_requirements': {'transformers': 'Hugging Face Transformers', 'torch': 'PyTorch', 'datasets': 'Hugging Face Datasets'}, 'example_code': {'import': ['from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification', 'import torch', 'from datasets import load_dataset'], 'load_dataset': \"dataset = load_dataset('huggingface/cats-image')\", 'image': \"image = dataset['test']['image'][0]\", 'feature_extractor': \"feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-large-224')\", 'model': \"model = ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'inputs': \"inputs = feature_extractor(image, return_tensors='pt')\", 'logits': 'with torch.no_grad():\\n  logits = model(**inputs).logits', 'predicted_label': 'predicted_label = logits.argmax(-1).item()', 'print': 'print(model.config.id2label[predicted_label])'}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration.\" </s>"
"<s>[INST] قم بتنفيذ نظام تحليل محتوى النصوص الكبيرة لإيجاد الأنماط والاختلافات الرئيسية باستخدام تقنيات تجميع البيانات والتعلم الآلي. يمكنك استخدام API التالي لتنفيذ هذه المهمة:? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-large-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-large-224'}, 'python_environment_requirements': {'transformers': 'Hugging Face Transformers', 'torch': 'PyTorch', 'datasets': 'Hugging Face Datasets'}, 'example_code': {'import': ['from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification', 'import torch', 'from datasets import load_dataset'], 'load_dataset': \"dataset = load_dataset('huggingface/cats-image')\", 'image': \"image = dataset['test']['image'][0]\", 'feature_extractor': \"feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-large-224')\", 'model': \"model = ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'inputs': \"inputs = feature_extractor(image, return_tensors='pt')\", 'logits': 'with torch.no_grad():\\n  logits = model(**inputs).logits', 'predicted_label': 'predicted_label = logits.argmax(-1).item()', 'print': 'print(model.config.id2label[predicted_label])'}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration.\" </s>"
"<s>[INST] قم بتطوير تطبيق لتحليل وتصنيف الصور الطبية لتقديم تشخيص دقيق باستخدام تقنيات الرؤية الحاسوبية المتقدمة. يمكنك استخدام API التالي لهذا الغرض:? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-large-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-large-224'}, 'python_environment_requirements': {'transformers': 'Hugging Face Transformers', 'torch': 'PyTorch', 'datasets': 'Hugging Face Datasets'}, 'example_code': {'import': ['from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification', 'import torch', 'from datasets import load_dataset'], 'load_dataset': \"dataset = load_dataset('huggingface/cats-image')\", 'image': \"image = dataset['test']['image'][0]\", 'feature_extractor': \"feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-large-224')\", 'model': \"model = ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'inputs': \"inputs = feature_extractor(image, return_tensors='pt')\", 'logits': 'with torch.no_grad():\\n  logits = model(**inputs).logits', 'predicted_label': 'predicted_label = logits.argmax(-1).item()', 'print': 'print(model.config.id2label[predicted_label])'}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration.\" </s>"
"<s>[INST] إنشاء نظام توصية شخصية لتقديم توصيات مخصصة للمستخدمين استنادًا إلى تفاعلاتهم السابقة باستخدام تقنيات التعلم الآلي. يمكنك استخدام API التالي لبناء نظام التوصية:? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-large-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-large-224'}, 'python_environment_requirements': {'transformers': 'Hugging Face Transformers', 'torch': 'PyTorch', 'datasets': 'Hugging Face Datasets'}, 'example_code': {'import': ['from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification', 'import torch', 'from datasets import load_dataset'], 'load_dataset': \"dataset = load_dataset('huggingface/cats-image')\", 'image': \"image = dataset['test']['image'][0]\", 'feature_extractor': \"feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-large-224')\", 'model': \"model = ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'inputs': \"inputs = feature_extractor(image, return_tensors='pt')\", 'logits': 'with torch.no_grad():\\n  logits = model(**inputs).logits', 'predicted_label': 'predicted_label = logits.argmax(-1).item()', 'print': 'print(model.config.id2label[predicted_label])'}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration.\" </s>"
"<s>[INST] قم بتنفيذ نظام تحليل للصوتيات لفهم المحتوى الصوتي وتحليله باستخدام تقنيات التعلم الآلي. يمكنك استخدام API التالي لهذا الغرض:? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-large-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-large-224'}, 'python_environment_requirements': {'transformers': 'Hugging Face Transformers', 'torch': 'PyTorch', 'datasets': 'Hugging Face Datasets'}, 'example_code': {'import': ['from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification', 'import torch', 'from datasets import load_dataset'], 'load_dataset': \"dataset = load_dataset('huggingface/cats-image')\", 'image': \"image = dataset['test']['image'][0]\", 'feature_extractor': \"feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-large-224')\", 'model': \"model = ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'inputs': \"inputs = feature_extractor(image, return_tensors='pt')\", 'logits': 'with torch.no_grad():\\n  logits = model(**inputs).logits', 'predicted_label': 'predicted_label = logits.argmax(-1).item()', 'print': 'print(model.config.id2label[predicted_label])'}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration.\" </s>"
"<s>[INST] أنشئ تطبيقًا لتحليل وتحويل النصوص من اللغة العربية إلى الإنجليزية باستخدام تقنيات الترجمة الآلية. يمكنك استخدام API التالي لهذا الغرض:? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-large-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-large-224'}, 'python_environment_requirements': {'transformers': 'Hugging Face Transformers', 'torch': 'PyTorch', 'datasets': 'Hugging Face Datasets'}, 'example_code': {'import': ['from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification', 'import torch', 'from datasets import load_dataset'], 'load_dataset': \"dataset = load_dataset('huggingface/cats-image')\", 'image': \"image = dataset['test']['image'][0]\", 'feature_extractor': \"feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-large-224')\", 'model': \"model = ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'inputs': \"inputs = feature_extractor(image, return_tensors='pt')\", 'logits': 'with torch.no_grad():\\n  logits = model(**inputs).logits', 'predicted_label': 'predicted_label = logits.argmax(-1).item()', 'print': 'print(model.config.id2label[predicted_label])'}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration.\" </s>"
"<s>[INST] قم بتنفيذ تطبيق لتحليل المشاهد في مقاطع الفيديو للتعرف على الأشياء والأحداث الرئيسية باستخدام تقنيات الرؤية الحاسوبية. يمكنك استخدام API التالي لأغراض التحليل:? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-large-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-large-224'}, 'python_environment_requirements': {'transformers': 'Hugging Face Transformers', 'torch': 'PyTorch', 'datasets': 'Hugging Face Datasets'}, 'example_code': {'import': ['from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification', 'import torch', 'from datasets import load_dataset'], 'load_dataset': \"dataset = load_dataset('huggingface/cats-image')\", 'image': \"image = dataset['test']['image'][0]\", 'feature_extractor': \"feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-large-224')\", 'model': \"model = ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'inputs': \"inputs = feature_extractor(image, return_tensors='pt')\", 'logits': 'with torch.no_grad():\\n  logits = model(**inputs).logits', 'predicted_label': 'predicted_label = logits.argmax(-1).item()', 'print': 'print(model.config.id2label[predicted_label])'}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration.\" </s>"
"<s>[INST] إنشاء نظام توصية ذكي لتحسين تجربة المستخدمين في تطبيق البث المباشر على منصتك. يمكنك استخدام API التالي لتطوير نموذج توصية مخصص:? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-large-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-large-224'}, 'python_environment_requirements': {'transformers': 'Hugging Face Transformers', 'torch': 'PyTorch', 'datasets': 'Hugging Face Datasets'}, 'example_code': {'import': ['from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification', 'import torch', 'from datasets import load_dataset'], 'load_dataset': \"dataset = load_dataset('huggingface/cats-image')\", 'image': \"image = dataset['test']['image'][0]\", 'feature_extractor': \"feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-large-224')\", 'model': \"model = ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", 'inputs': \"inputs = feature_extractor(image, return_tensors='pt')\", 'logits': 'with torch.no_grad():\\n  logits = model(**inputs).logits', 'predicted_label': 'predicted_label = logits.argmax(-1).item()', 'print': 'print(model.config.id2label[predicted_label])'}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration.\" </s>"
"<s>[INST]قم بتنفيذ إعداد متقدم لتحليل اللغة الطبيعية لتصنيف النصوص الطبية. هل يمكنك الاستفادة من واجهة برمجة التطبيقات المقدمة من Hugging Face Transformers لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-18', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import AutoFeatureExtractor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/resnet-18')\\nmodel = ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n    logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k'}, 'description': 'ResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository. ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.' </s>"
"<s>[INST] قم ببناء نظام يستخدم تقنيات التعلم العميق لتحليل المخطوطات اليدوية والتعرف عليها. هل يمكنك استخدام واجهة برمجة التطبيقات المقدمة من Tesseract OCR لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-18', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import AutoFeatureExtractor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/resnet-18')\\nmodel = ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n    logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k'}, 'description': 'ResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository. ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.' </s>"
"<s>[INST] قم بتطوير نظام يعتمد على الذكاء الاصطناعي للتعرف على أعمال الفنانين وتصنيفها. ما هي الخطوات اللازمة للاستفادة من واجهة برمجة التطبيقات التي يقدمها TensorFlow لمهام التصنيف الفني؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-18', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import AutoFeatureExtractor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/resnet-18')\\nmodel = ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n    logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k'}, 'description': 'ResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository. ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.' </s>"
"<s>[INST] قم ببناء نظام يستخدم تعلم الآلة لتحليل البيانات السكانية وتوقع الاتجاهات الديموغرافية. هل يمكنك توجيه الاستفادة من واجهة برمجة التطبيقات التي توفرها Scikit-Learn لبناء نظام توقعي ديموغرافي؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-18', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import AutoFeatureExtractor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/resnet-18')\\nmodel = ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n    logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k'}, 'description': 'ResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository. ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.' </s>"
"<s>[INST] قم بإنشاء نظام للتعرف على البيئة الطبيعية من الصور الجوية باستخدام تقنيات الذكاء الاصطناعي. كيف يمكن استخدام واجهة برمجة التطبيقات المقدمة من OpenCV لهذا النوع من الأنظمة؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-18', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import AutoFeatureExtractor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/resnet-18')\\nmodel = ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n    logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k'}, 'description': 'ResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository. ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.' </s>"
"<s>[INST] اكتب تطبيق يقوم بتحليل المستندات القانونية واستخراج المفاهيم القانونية الرئيسية. هل يمكن استخدام واجهة برمجة التطبيقات المقدمة من SpaCy لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-18', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import AutoFeatureExtractor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/resnet-18')\\nmodel = ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n    logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k'}, 'description': 'ResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository. ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.' </s>"
"<s>[INST] قم بتطوير أداة لتقدير التكاليف المالية لمشروع قائم على الذكاء الاصطناعي. هل يتوفر واجهة برمجة التطبيقات توفرها AWS SageMaker لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-18', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import AutoFeatureExtractor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/resnet-18')\\nmodel = ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n    logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k'}, 'description': 'ResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository. ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.' </s>"
"<s>[INST] أنشئ نظامًا يستخدم تعلم الآلة للتنبؤ بأسعار العقارات في السوق الحالية. ما هي الخطوات اللازمة للاستفادة من واجهة برمجة التطبيقات التي تقدمها Zillow لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-18', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import AutoFeatureExtractor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/resnet-18')\\nmodel = ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n    logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k'}, 'description': 'ResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository. ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.' </s>"
"<s>[INST] قم ببناء نظام يستند إلى التعلم الآلي للتعرف على أنماط الاحتيال في المعاملات المالية. هل يمكن تحقيق ذلك باستخدام واجهة برمجة التطبيقات المقدمة من TensorFlow لتصنيف البيانات؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-18', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import AutoFeatureExtractor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/resnet-18')\\nmodel = ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n    logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k'}, 'description': 'ResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository. ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.' </s>"
"<s>[INST] أنشئ نظامًا يستخدم تقنيات الذكاء الاصطناعي لتحديد أساليب تحسين تجربة المستخدم على موقع الويب. كيف يمكن توظيف واجهة برمجة التطبيقات Google Analytics لتحقيق هذا الهدف؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/resnet-18', 'api_call': \"ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import AutoFeatureExtractor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/resnet-18')\\nmodel = ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n    logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k'}, 'description': 'ResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository. ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.' </s>"
"<s>[INST]\"قم بتنفيذ دالة استخراج الميزات من الصور باستخدام Transformers لتحليل الصور وتصنيفها. ابدأ باستيراد الدالات والمكتبات اللازمة وثم قم بتحميل الصورة وتنفيذ الدالة ConvNextFeatureExtractor.from_pretrained(facebook/convnext-base-224).\"? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-base-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\" </s>"
"<s>[INST] \"اكتب برنامج Python يستخدم API ConvNextForImageClassification لتصنيف صورة معينة. يجب عليك تحديد الصورة المطلوبة واستخدام النموذج المدرب لتصنيفها باستخدام الدالة ConvNextForImageClassification.from_pretrained(facebook/convnext-base-224).\"? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-base-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\" </s>"
"<s>[INST] \"استخدم API ConvNextForImageClassification من Hugging Face Transformers لتقديم تحليل تصنيف صورة مدخلة. يجب استخدام النموذج المدرب مسبقًا وتحديد الدالة ConvNextForImageClassification.from_pretrained(facebook/convnext-base-224) لتقديم التحليل.\"? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-base-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\" </s>"
"<s>[INST] \"اكتب برنامج Python يقوم بتصنيف صورة مدخلة باستخدام API ConvNextForImageClassification. يجب تكوين الصورة للتحليل واستخدام النموذج ConvNextForImageClassification.from_pretrained(facebook/convnext-base-224) في التصنيف.\"? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-base-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\" </s>"
"<s>[INST] \"أعد صياغة صورة معينة باستخدام API Hugging Face Transformers للتصنيف الضوئي. تأكد من استخدام دالة ConvNextForImageClassification.from_pretrained(facebook/convnext-base-224) لتحقيق النتيجة المرغوبة.\"? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-base-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\" </s>"
"<s>[INST] \"استخدم API ConvNextForImageClassification لتقديم تحليل الصور الضوئية باستخدام Transformers. قم بتحديد الصورة المراد تصنيفها وضبط الدالة ConvNextForImageClassification.from_pretrained(facebook/convnext-base-224) لتصنيف الصورة.\"? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-base-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\" </s>"
"<s>[INST] \"عين صورة محددة لتحليل تصنيفي باستخدام API ConvNextForImageClassification من Hugging Face Transformers. استخدم النموذج المحدد وقم بتصنيف الصورة باستخدام الدالة ConvNextForImageClassification.from_pretrained(facebook/convnext-base-224).\"? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-base-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\" </s>"
"<s>[INST] \"أعد صياغة صورة معينة باستخدام API ConvNextForImageClassification للتصنيف الضوئي. تأكد من استخدام دالة ConvNextForImageClassification.from_pretrained(facebook/convnext-base-224) لتحقيق النتيجة المرغوبة.\"? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-base-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\" </s>"
"<s>[INST] \"استخدم API ConvNextForImageClassification لتقديم تحليل الصور الضوئية باستخدام Transformers. قم بتحديد الصورة المراد تصنيفها وضبط الدالة ConvNextForImageClassification.from_pretrained(facebook/convnext-base-224) لتصنيف الصورة.\"? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-base-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\" </s>"
"<s>[INST] \"عين صورة محددة لتحليل تصنيفي باستخدام API ConvNextForImageClassification من Hugging Face Transformers. استخدم النموذج المحدد وقم بتصنيف الصورة باستخدام الدالة ConvNextForImageClassification.from_pretrained(facebook/convnext-base-224).\"? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-base-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': None}, 'description': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\" </s>"
"<s>[INST]كتابة برنامج يستخدم تقنيات الذكاء الاصطناعي لتحديد وتصنيف صور السماء الليلية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/beit-base-patch16-224-pt22k-ft22k', 'api_call': \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet-22k', 'accuracy': 'Not specified'}, 'description': 'BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.' </s>"
"<s>[INST] تطوير نظام يستخدم تقنيات الذكاء الاصطناعي لتحليل وتصنيف صور النباتات البرية النادرة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/beit-base-patch16-224-pt22k-ft22k', 'api_call': \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet-22k', 'accuracy': 'Not specified'}, 'description': 'BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.' </s>"
"<s>[INST] كتابة برنامج يعتمد على تقنيات تصنيف الصور بالذكاء الاصطناعي لتحليل التغيرات الجوية من خلال تصنيف السحب.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/beit-base-patch16-224-pt22k-ft22k', 'api_call': \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet-22k', 'accuracy': 'Not specified'}, 'description': 'BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.' </s>"
"<s>[INST] تطوير نظام يقوم بتحليل صور الأقمار الصناعية باستخدام تقنيات الذكاء الصناعي لتحديد طبيعة المناطق المحلية المعيشية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/beit-base-patch16-224-pt22k-ft22k', 'api_call': \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet-22k', 'accuracy': 'Not specified'}, 'description': 'BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.' </s>"
"<s>[INST] بناء نظام يستخدم الذكاء الاصطناعي لتحليل وتصنيف صور العمارة الحديثة في المدن.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/beit-base-patch16-224-pt22k-ft22k', 'api_call': \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet-22k', 'accuracy': 'Not specified'}, 'description': 'BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.' </s>"
"<s>[INST] كتابة برنامج يستخدم تقنيات تصنيف الصور بالذكاء الاصطناعي لتحليل وتصنيف الأشكال الجيولوجية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/beit-base-patch16-224-pt22k-ft22k', 'api_call': \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet-22k', 'accuracy': 'Not specified'}, 'description': 'BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.' </s>"
"<s>[INST] بناء نظام يعتمد على تقنيات التصنيف بالذكاء الاصطناعي لتحليل وتحديد انواع النباتات في الغابات الاستوائية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/beit-base-patch16-224-pt22k-ft22k', 'api_call': \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet-22k', 'accuracy': 'Not specified'}, 'description': 'BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.' </s>"
"<s>[INST] كتابة برنامج يعتمد على تقنيات تصنيف الصور بالذكاء الاصطناعي لتحليل وتصنيف الحيوانات البرية في المناطق الجغرافية المتنوعة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/beit-base-patch16-224-pt22k-ft22k', 'api_call': \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet-22k', 'accuracy': 'Not specified'}, 'description': 'BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.' </s>"
"<s>[INST] بناء نظام يستخدم تقنيات الذكاء الاصطناعي لتحليل وتصنيف الصور الفضائية للمريخ الملتقطة من المسبارات الفضائية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/beit-base-patch16-224-pt22k-ft22k', 'api_call': \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet-22k', 'accuracy': 'Not specified'}, 'description': 'BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.' </s>"
"<s>[INST] تطوير نظام يستخدم تقنيات الذكاء الصناعي لتحليل وتصنيف صور المراقبة الأمنية وتحديد المواقف الاحتمالية لتعزيز الأمان.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/beit-base-patch16-224-pt22k-ft22k', 'api_call': \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet-22k', 'accuracy': 'Not specified'}, 'description': 'BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.' </s>"
"<s>[INST]تحتاج إلى نظام لتحليل بيانات التغذية والصحة وتوليد تقارير شهرية للمستخدمين.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-224', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224', 'from_tf': 'False', 'config': 'None', 'cache_dir': 'None', 'revision': 'None', 'use_auth_token': 'False'}, 'python_environment_requirements': {'transformers': '4.0.0', 'torch': '1.9.0', 'PIL': '8.3.2', 'requests': '2.26.0'}, 'example_code': {'1': 'from transformers import ViTImageProcessor, ViTForImageClassification', '2': 'from PIL import Image', '3': 'import requests', '4': \"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\", '5': 'image = Image.open(requests.get(url, stream=True).raw)', '6': \"processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\", '7': \"model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", '8': \"inputs = processor(images=image, return_tensors='pt')\", '9': 'outputs = model(**inputs)', '10': 'logits = outputs.logits', '11': 'predicted_class_idx = logits.argmax(-1).item()', '12': \"print('Predicted class:', model.config.id2label[predicted_class_idx])\"}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.' </s>"
"<s>[INST] أنا بحاجة إلى منصة تعليم آلي لتوقع توجيهات الأسهم في سوق الأوراق المالية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-224', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224', 'from_tf': 'False', 'config': 'None', 'cache_dir': 'None', 'revision': 'None', 'use_auth_token': 'False'}, 'python_environment_requirements': {'transformers': '4.0.0', 'torch': '1.9.0', 'PIL': '8.3.2', 'requests': '2.26.0'}, 'example_code': {'1': 'from transformers import ViTImageProcessor, ViTForImageClassification', '2': 'from PIL import Image', '3': 'import requests', '4': \"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\", '5': 'image = Image.open(requests.get(url, stream=True).raw)', '6': \"processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\", '7': \"model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", '8': \"inputs = processor(images=image, return_tensors='pt')\", '9': 'outputs = model(**inputs)', '10': 'logits = outputs.logits', '11': 'predicted_class_idx = logits.argmax(-1).item()', '12': \"print('Predicted class:', model.config.id2label[predicted_class_idx])\"}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.' </s>"
"<s>[INST] بناء نظام لتحليل البيانات الطبية وتقديم توصيات للعلاجات الفعالة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-224', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224', 'from_tf': 'False', 'config': 'None', 'cache_dir': 'None', 'revision': 'None', 'use_auth_token': 'False'}, 'python_environment_requirements': {'transformers': '4.0.0', 'torch': '1.9.0', 'PIL': '8.3.2', 'requests': '2.26.0'}, 'example_code': {'1': 'from transformers import ViTImageProcessor, ViTForImageClassification', '2': 'from PIL import Image', '3': 'import requests', '4': \"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\", '5': 'image = Image.open(requests.get(url, stream=True).raw)', '6': \"processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\", '7': \"model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", '8': \"inputs = processor(images=image, return_tensors='pt')\", '9': 'outputs = model(**inputs)', '10': 'logits = outputs.logits', '11': 'predicted_class_idx = logits.argmax(-1).item()', '12': \"print('Predicted class:', model.config.id2label[predicted_class_idx])\"}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.' </s>"
"<s>[INST] بناء نظام تتبع المبيعات والمخزون لمتجر إلكتروني.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-224', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224', 'from_tf': 'False', 'config': 'None', 'cache_dir': 'None', 'revision': 'None', 'use_auth_token': 'False'}, 'python_environment_requirements': {'transformers': '4.0.0', 'torch': '1.9.0', 'PIL': '8.3.2', 'requests': '2.26.0'}, 'example_code': {'1': 'from transformers import ViTImageProcessor, ViTForImageClassification', '2': 'from PIL import Image', '3': 'import requests', '4': \"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\", '5': 'image = Image.open(requests.get(url, stream=True).raw)', '6': \"processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\", '7': \"model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", '8': \"inputs = processor(images=image, return_tensors='pt')\", '9': 'outputs = model(**inputs)', '10': 'logits = outputs.logits', '11': 'predicted_class_idx = logits.argmax(-1).item()', '12': \"print('Predicted class:', model.config.id2label[predicted_class_idx])\"}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.' </s>"
"<s>[INST] تصميم نظام يمكنه تحديد غش المحتوى عبر الإنترنت.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-224', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224', 'from_tf': 'False', 'config': 'None', 'cache_dir': 'None', 'revision': 'None', 'use_auth_token': 'False'}, 'python_environment_requirements': {'transformers': '4.0.0', 'torch': '1.9.0', 'PIL': '8.3.2', 'requests': '2.26.0'}, 'example_code': {'1': 'from transformers import ViTImageProcessor, ViTForImageClassification', '2': 'from PIL import Image', '3': 'import requests', '4': \"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\", '5': 'image = Image.open(requests.get(url, stream=True).raw)', '6': \"processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\", '7': \"model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", '8': \"inputs = processor(images=image, return_tensors='pt')\", '9': 'outputs = model(**inputs)', '10': 'logits = outputs.logits', '11': 'predicted_class_idx = logits.argmax(-1).item()', '12': \"print('Predicted class:', model.config.id2label[predicted_class_idx])\"}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.' </s>"
"<s>[INST] بناء نظام لتحليل البيانات الجغرافية وتوصيات السفر.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-224', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224', 'from_tf': 'False', 'config': 'None', 'cache_dir': 'None', 'revision': 'None', 'use_auth_token': 'False'}, 'python_environment_requirements': {'transformers': '4.0.0', 'torch': '1.9.0', 'PIL': '8.3.2', 'requests': '2.26.0'}, 'example_code': {'1': 'from transformers import ViTImageProcessor, ViTForImageClassification', '2': 'from PIL import Image', '3': 'import requests', '4': \"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\", '5': 'image = Image.open(requests.get(url, stream=True).raw)', '6': \"processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\", '7': \"model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", '8': \"inputs = processor(images=image, return_tensors='pt')\", '9': 'outputs = model(**inputs)', '10': 'logits = outputs.logits', '11': 'predicted_class_idx = logits.argmax(-1).item()', '12': \"print('Predicted class:', model.config.id2label[predicted_class_idx])\"}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.' </s>"
"<s>[INST] تحتاج إلى تطوير نظام لمراقبة الأمان في العقارات الذكية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-224', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224', 'from_tf': 'False', 'config': 'None', 'cache_dir': 'None', 'revision': 'None', 'use_auth_token': 'False'}, 'python_environment_requirements': {'transformers': '4.0.0', 'torch': '1.9.0', 'PIL': '8.3.2', 'requests': '2.26.0'}, 'example_code': {'1': 'from transformers import ViTImageProcessor, ViTForImageClassification', '2': 'from PIL import Image', '3': 'import requests', '4': \"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\", '5': 'image = Image.open(requests.get(url, stream=True).raw)', '6': \"processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\", '7': \"model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", '8': \"inputs = processor(images=image, return_tensors='pt')\", '9': 'outputs = model(**inputs)', '10': 'logits = outputs.logits', '11': 'predicted_class_idx = logits.argmax(-1).item()', '12': \"print('Predicted class:', model.config.id2label[predicted_class_idx])\"}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.' </s>"
"<s>[INST] بناء نظام لتحليل الأداء الطلابي واقتراح خطط تعليمية فعالة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-224', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224', 'from_tf': 'False', 'config': 'None', 'cache_dir': 'None', 'revision': 'None', 'use_auth_token': 'False'}, 'python_environment_requirements': {'transformers': '4.0.0', 'torch': '1.9.0', 'PIL': '8.3.2', 'requests': '2.26.0'}, 'example_code': {'1': 'from transformers import ViTImageProcessor, ViTForImageClassification', '2': 'from PIL import Image', '3': 'import requests', '4': \"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\", '5': 'image = Image.open(requests.get(url, stream=True).raw)', '6': \"processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\", '7': \"model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", '8': \"inputs = processor(images=image, return_tensors='pt')\", '9': 'outputs = model(**inputs)', '10': 'logits = outputs.logits', '11': 'predicted_class_idx = logits.argmax(-1).item()', '12': \"print('Predicted class:', model.config.id2label[predicted_class_idx])\"}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.' </s>"
"<s>[INST] قم بتطوير منصة للكشف التلقائي عن الأخبار الزائفة عبر الإنترنت.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-224', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224', 'from_tf': 'False', 'config': 'None', 'cache_dir': 'None', 'revision': 'None', 'use_auth_token': 'False'}, 'python_environment_requirements': {'transformers': '4.0.0', 'torch': '1.9.0', 'PIL': '8.3.2', 'requests': '2.26.0'}, 'example_code': {'1': 'from transformers import ViTImageProcessor, ViTForImageClassification', '2': 'from PIL import Image', '3': 'import requests', '4': \"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\", '5': 'image = Image.open(requests.get(url, stream=True).raw)', '6': \"processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\", '7': \"model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", '8': \"inputs = processor(images=image, return_tensors='pt')\", '9': 'outputs = model(**inputs)', '10': 'logits = outputs.logits', '11': 'predicted_class_idx = logits.argmax(-1).item()', '12': \"print('Predicted class:', model.config.id2label[predicted_class_idx])\"}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.' </s>"
"<s>[INST] تصميم نظام لتحليل وتصنيف الإشارات الصوتية لتعزيز تجربة العملاء.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-224', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-224', 'from_tf': 'False', 'config': 'None', 'cache_dir': 'None', 'revision': 'None', 'use_auth_token': 'False'}, 'python_environment_requirements': {'transformers': '4.0.0', 'torch': '1.9.0', 'PIL': '8.3.2', 'requests': '2.26.0'}, 'example_code': {'1': 'from transformers import ViTImageProcessor, ViTForImageClassification', '2': 'from PIL import Image', '3': 'import requests', '4': \"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\", '5': 'image = Image.open(requests.get(url, stream=True).raw)', '6': \"processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\", '7': \"model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", '8': \"inputs = processor(images=image, return_tensors='pt')\", '9': 'outputs = model(**inputs)', '10': 'logits = outputs.logits', '11': 'predicted_class_idx = logits.argmax(-1).item()', '12': \"print('Predicted class:', model.config.id2label[predicted_class_idx])\"}, 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.' </s>"
"<s>[INST]تحتاج شبكة المتاجر إلى تطوير واجهة برمجة التطبيقات (API) لإدارة المخزون وتتبع الطلبات. هل يمكن استخدام واجهة برمجة التطبيقات (API) التالية لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy', 'api_call': \"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\", 'api_arguments': {'model_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy'}, 'python_environment_requirements': {'transformers': '4.28.1', 'pytorch': '2.0.0+cu118', 'datasets': '2.11.0', 'tokenizers': '0.13.3'}, 'example_code': \"from transformers import pipeline\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\nresult = image_classifier('path/to/image.jpg')\", 'performance': {'dataset': 'None', 'accuracy': 0.7744}, 'description': 'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.' </s>"
"<s>[INST] تتطلع منصة التواصل الاجتماعي لتخصيص واجهة برمجة التطبيقات (API) لتمكين المستخدمين من التفاعل مع المنشورات بواسطة تطبيقات الجوّال. هل يمكن استخدام الواجهة البرمجية التالية لتحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy', 'api_call': \"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\", 'api_arguments': {'model_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy'}, 'python_environment_requirements': {'transformers': '4.28.1', 'pytorch': '2.0.0+cu118', 'datasets': '2.11.0', 'tokenizers': '0.13.3'}, 'example_code': \"from transformers import pipeline\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\nresult = image_classifier('path/to/image.jpg')\", 'performance': {'dataset': 'None', 'accuracy': 0.7744}, 'description': 'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.' </s>"
"<s>[INST] ترغب منظمة السياحة في تطوير واجهة برمجة التطبيقات (API) للحصول على بيانات حالة حجوزات الفنادق في الوقت الحقيقي. هل يتوفر واجهة برمجة التطبيقات (API) مناسبة لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy', 'api_call': \"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\", 'api_arguments': {'model_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy'}, 'python_environment_requirements': {'transformers': '4.28.1', 'pytorch': '2.0.0+cu118', 'datasets': '2.11.0', 'tokenizers': '0.13.3'}, 'example_code': \"from transformers import pipeline\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\nresult = image_classifier('path/to/image.jpg')\", 'performance': {'dataset': 'None', 'accuracy': 0.7744}, 'description': 'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.' </s>"
"<s>[INST] يحتاج مطور التطبيقات إلى واجهة برمجة التطبيقات (API) تقدم خدمة متابعة البريد الإلكتروني وإدارة النشاطات البريدية. هل يُمكن الاعتماد على الواجهة البرمجية التالية لتحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy', 'api_call': \"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\", 'api_arguments': {'model_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy'}, 'python_environment_requirements': {'transformers': '4.28.1', 'pytorch': '2.0.0+cu118', 'datasets': '2.11.0', 'tokenizers': '0.13.3'}, 'example_code': \"from transformers import pipeline\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\nresult = image_classifier('path/to/image.jpg')\", 'performance': {'dataset': 'None', 'accuracy': 0.7744}, 'description': 'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.' </s>"
"<s>[INST] ترغب منظمة الأبحاث الطبية في تطوير واجهة برمجة التطبيقات (API) للحصول على بعض الإحصائيات الطبية الهامة من مجموعة البيانات الخاصة بها. هل يمكن الوثوق في الواجهة البرمجية التالية لتلبية هذه الاحتياجات؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy', 'api_call': \"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\", 'api_arguments': {'model_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy'}, 'python_environment_requirements': {'transformers': '4.28.1', 'pytorch': '2.0.0+cu118', 'datasets': '2.11.0', 'tokenizers': '0.13.3'}, 'example_code': \"from transformers import pipeline\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\nresult = image_classifier('path/to/image.jpg')\", 'performance': {'dataset': 'None', 'accuracy': 0.7744}, 'description': 'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.' </s>"
"<s>[INST] ترغب منصة التجارة الإلكترونية في دمج واجهة برمجة التطبيقات (API) لتحليل سلوك المستخدمين وتوجيه الإعلانات بشكل فعال. هل يمكن استخدام الواجهة البرمجية التالية لتنفيذ هذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy', 'api_call': \"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\", 'api_arguments': {'model_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy'}, 'python_environment_requirements': {'transformers': '4.28.1', 'pytorch': '2.0.0+cu118', 'datasets': '2.11.0', 'tokenizers': '0.13.3'}, 'example_code': \"from transformers import pipeline\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\nresult = image_classifier('path/to/image.jpg')\", 'performance': {'dataset': 'None', 'accuracy': 0.7744}, 'description': 'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.' </s>"
"<s>[INST] تود منصة الببلشير تحسين تجربة مشتركي خدمة البث الحي عبر توفير واجهة برمجة التطبيقات (API) للتحكم في تنسيق وعرض البث المباشر. هل تتوفر واجهة برمجة التطبيقات المثالية لتحقيق هذا الهدف؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy', 'api_call': \"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\", 'api_arguments': {'model_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy'}, 'python_environment_requirements': {'transformers': '4.28.1', 'pytorch': '2.0.0+cu118', 'datasets': '2.11.0', 'tokenizers': '0.13.3'}, 'example_code': \"from transformers import pipeline\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\nresult = image_classifier('path/to/image.jpg')\", 'performance': {'dataset': 'None', 'accuracy': 0.7744}, 'description': 'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.' </s>"
"<s>[INST] ترغب شركة الطيران في إنشاء واجهة برمجة التطبيقات (API) للحصول على بيانات المحطات الجوية الحالية والتنبؤات الجوية لتحسين تخطيط الرحلات. هل يتوفر API مناسب لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy', 'api_call': \"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\", 'api_arguments': {'model_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy'}, 'python_environment_requirements': {'transformers': '4.28.1', 'pytorch': '2.0.0+cu118', 'datasets': '2.11.0', 'tokenizers': '0.13.3'}, 'example_code': \"from transformers import pipeline\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\nresult = image_classifier('path/to/image.jpg')\", 'performance': {'dataset': 'None', 'accuracy': 0.7744}, 'description': 'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.' </s>"
"<s>[INST] تريد شركة التكنولوجيا تطوير واجهة برمجة التطبيقات (API) لذكاء اصطناعي يعتمد على الشبكات العصبية لتحليل البيانات الكبيرة بفعالية. هل يوفر API اللازم لهذا النوع من التحليل؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy', 'api_call': \"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\", 'api_arguments': {'model_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy'}, 'python_environment_requirements': {'transformers': '4.28.1', 'pytorch': '2.0.0+cu118', 'datasets': '2.11.0', 'tokenizers': '0.13.3'}, 'example_code': \"from transformers import pipeline\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\nresult = image_classifier('path/to/image.jpg')\", 'performance': {'dataset': 'None', 'accuracy': 0.7744}, 'description': 'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.' </s>"
"<s>[INST] تحتاج منصة اللعب الرقمية إلى واجهة برمجة التطبيقات (API) لتوفير خدمة تصور ثلاثي الأبعاد (3D) لمطوّري الألعاب. هل يمكن استخدام API التالية لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy', 'api_call': \"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\", 'api_arguments': {'model_name': 'martinezomg/vit-base-patch16-224-diabetic-retinopathy'}, 'python_environment_requirements': {'transformers': '4.28.1', 'pytorch': '2.0.0+cu118', 'datasets': '2.11.0', 'tokenizers': '0.13.3'}, 'example_code': \"from transformers import pipeline\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\nresult = image_classifier('path/to/image.jpg')\", 'performance': {'dataset': 'None', 'accuracy': 0.7744}, 'description': 'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.' </s>"
"<s>[INST]تحتاج إلى تطوير تطبيق يستخدم API لتحويل النصوص إلى كود برمجي قابل للتنفيذ.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Age Classification', 'api_name': 'nateraw/vit-age-classifier', 'api_call': \"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/vit-age-classifier'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\n\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\nim = Image.open(BytesIO(r.content))\\n\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\n\\ninputs = transforms(im, return_tensors='pt')\\noutput = model(**inputs)\\n\\nproba = output.logits.softmax(1)\\npreds = proba.argmax(1)\", 'performance': {'dataset': 'fairface', 'accuracy': None}, 'description': \"A vision transformer finetuned to classify the age of a given person's face.\" </s>"
"<s>[INST] يجب عليك إنشاء نظام يستخدم API لتحليل المشاعر من الصور الشخصية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Age Classification', 'api_name': 'nateraw/vit-age-classifier', 'api_call': \"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/vit-age-classifier'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\n\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\nim = Image.open(BytesIO(r.content))\\n\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\n\\ninputs = transforms(im, return_tensors='pt')\\noutput = model(**inputs)\\n\\nproba = output.logits.softmax(1)\\npreds = proba.argmax(1)\", 'performance': {'dataset': 'fairface', 'accuracy': None}, 'description': \"A vision transformer finetuned to classify the age of a given person's face.\" </s>"
"<s>[INST] تحتاج لتطوير تطبيق يستخدم API لتحليل المقابلات واستخراج المعلومات الرئيسية منها.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Age Classification', 'api_name': 'nateraw/vit-age-classifier', 'api_call': \"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/vit-age-classifier'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\n\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\nim = Image.open(BytesIO(r.content))\\n\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\n\\ninputs = transforms(im, return_tensors='pt')\\noutput = model(**inputs)\\n\\nproba = output.logits.softmax(1)\\npreds = proba.argmax(1)\", 'performance': {'dataset': 'fairface', 'accuracy': None}, 'description': \"A vision transformer finetuned to classify the age of a given person's face.\" </s>"
"<s>[INST] يجب عليك تنفيذ API لتحليل الوثائق القانونية واستخراج المفاهيم الرئيسية منها.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Age Classification', 'api_name': 'nateraw/vit-age-classifier', 'api_call': \"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/vit-age-classifier'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\n\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\nim = Image.open(BytesIO(r.content))\\n\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\n\\ninputs = transforms(im, return_tensors='pt')\\noutput = model(**inputs)\\n\\nproba = output.logits.softmax(1)\\npreds = proba.argmax(1)\", 'performance': {'dataset': 'fairface', 'accuracy': None}, 'description': \"A vision transformer finetuned to classify the age of a given person's face.\" </s>"
"<s>[INST] إنشاء نظام يستخدم API لتحليل السلوك العملي للموظفين داخل الشركة وتوليد تقارير استنتاجية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Age Classification', 'api_name': 'nateraw/vit-age-classifier', 'api_call': \"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/vit-age-classifier'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\n\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\nim = Image.open(BytesIO(r.content))\\n\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\n\\ninputs = transforms(im, return_tensors='pt')\\noutput = model(**inputs)\\n\\nproba = output.logits.softmax(1)\\npreds = proba.argmax(1)\", 'performance': {'dataset': 'fairface', 'accuracy': None}, 'description': \"A vision transformer finetuned to classify the age of a given person's face.\" </s>"
"<s>[INST] نود تنفيذ API لتحليل المخاطر المالية في الأسواق العالمية وتقديم توصيات للإستثمار.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Age Classification', 'api_name': 'nateraw/vit-age-classifier', 'api_call': \"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/vit-age-classifier'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\n\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\nim = Image.open(BytesIO(r.content))\\n\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\n\\ninputs = transforms(im, return_tensors='pt')\\noutput = model(**inputs)\\n\\nproba = output.logits.softmax(1)\\npreds = proba.argmax(1)\", 'performance': {'dataset': 'fairface', 'accuracy': None}, 'description': \"A vision transformer finetuned to classify the age of a given person's face.\" </s>"
"<s>[INST] يرغب العميل في تطوير تطبيق يستخدم API لمساعدة في تحليل وفهم الأخبار العالمية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Age Classification', 'api_name': 'nateraw/vit-age-classifier', 'api_call': \"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/vit-age-classifier'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\n\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\nim = Image.open(BytesIO(r.content))\\n\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\n\\ninputs = transforms(im, return_tensors='pt')\\noutput = model(**inputs)\\n\\nproba = output.logits.softmax(1)\\npreds = proba.argmax(1)\", 'performance': {'dataset': 'fairface', 'accuracy': None}, 'description': \"A vision transformer finetuned to classify the age of a given person's face.\" </s>"
"<s>[INST] يجب تنفيذ API للتعرف على الأشخاص من خلال صورهم واستخراج المعلومات الرئيسية عنهم.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Age Classification', 'api_name': 'nateraw/vit-age-classifier', 'api_call': \"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/vit-age-classifier'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\n\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\nim = Image.open(BytesIO(r.content))\\n\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\n\\ninputs = transforms(im, return_tensors='pt')\\noutput = model(**inputs)\\n\\nproba = output.logits.softmax(1)\\npreds = proba.argmax(1)\", 'performance': {'dataset': 'fairface', 'accuracy': None}, 'description': \"A vision transformer finetuned to classify the age of a given person's face.\" </s>"
"<s>[INST] يتعين عليك تنفيذ API لتحليل السلوك العملي في الشركات واستخراج البيانات الرئيسية لاتخاذ قرارات إدارية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Age Classification', 'api_name': 'nateraw/vit-age-classifier', 'api_call': \"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/vit-age-classifier'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\n\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\nim = Image.open(BytesIO(r.content))\\n\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\n\\ninputs = transforms(im, return_tensors='pt')\\noutput = model(**inputs)\\n\\nproba = output.logits.softmax(1)\\npreds = proba.argmax(1)\", 'performance': {'dataset': 'fairface', 'accuracy': None}, 'description': \"A vision transformer finetuned to classify the age of a given person's face.\" </s>"
"<s>[INST] تريد تطوير نظام يستخدم API لتحليل البيانات السوقية وتكوين توقعات دقيقة للتداول.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Age Classification', 'api_name': 'nateraw/vit-age-classifier', 'api_call': \"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\", 'api_arguments': {'pretrained_model_name_or_path': 'nateraw/vit-age-classifier'}, 'python_environment_requirements': ['requests', 'PIL', 'transformers'], 'example_code': \"import requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\n\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\nim = Image.open(BytesIO(r.content))\\n\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\n\\ninputs = transforms(im, return_tensors='pt')\\noutput = model(**inputs)\\n\\nproba = output.logits.softmax(1)\\npreds = proba.argmax(1)\", 'performance': {'dataset': 'fairface', 'accuracy': None}, 'description': \"A vision transformer finetuned to classify the age of a given person's face.\" </s>"
"<s>[INST]تريد إضافة خاصية البحث عن الصور بواسطة الوصف إلى تطبيقك.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-384', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-384'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.' </s>"
"<s>[INST] تحتاج إلى تطوير نظام يقوم بتصنيف العناصر الفنية استنادًا إلى أنماط التصميم.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-384', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-384'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.' </s>"
"<s>[INST] تحتاج إلى أداة تساعدك في تحليل النصوص الأدبية لاكتشاف التقنيات الأدبية المستخدمة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-384', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-384'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.' </s>"
"<s>[INST] يطلب منك تطوير نظام يقوم بتحليل سلوك المستخدمين على منصة التجارة الإلكترونية لتوجيه العروض والتوصيات.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-384', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-384'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.' </s>"
"<s>[INST] تتطلب منك مهمة تطوير نظام يستخدم الذكاء الاصطناعي لتحديد إذا كانت العملة الرقمية مريمار تحليلية أم خارجية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-384', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-384'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.' </s>"
"<s>[INST] طلب عميلك تطوير نظام لتصنيف الأزياء الرجالية عبر الصور المقدمة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-384', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-384'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.' </s>"
"<s>[INST] يرغب العميل في تصميم نظام لتحليل الصور الطبية لتشخيص الأمراض بدقة عالية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-384', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-384'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.' </s>"
"<s>[INST] تحتاج إلى تطوير نظام لتصنيف التعليقات العملاء الإلكترونية لفهم مستوى الرضا.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-384', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-384'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.' </s>"
"<s>[INST] تريد تحسين تطبيق البحث الخاص بك عن طريق تضمين خاصية الترجمة التلقائية للنصوص المسترجعة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-384', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-384'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.' </s>"
"<s>[INST] تريد تضمين وظيفة تحليل الشبكات الاجتماعية في تطبيقك لتصنيف العلاقات بين المستخدمين.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/vit-base-patch16-384', 'api_call': \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/vit-base-patch16-384'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import ViTFeatureExtractor, ViTForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet', 'accuracy': 'Refer to tables 2 and 5 of the original paper'}, 'description': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.' </s>"
"<s>[INST]قم بتطوير منصة لتحليل المشاعر بشأن مراجعات العملاء لتحديد التعليقات الإيجابية والسلبية بكفاءة. هل هناك أي واجهات برمجة التطبيقات (API) متاحة لهذه المهمة؟، قم بإنشاء نظام للترجمة التلقائية للغة لدعم التواصل متعدد اللغات داخل مؤسستك. استخدم واجهة برمجة التطبيقات التالية لتنفيذ هذه الميزة بشكل فعال.، ابحث عن حل لتلخيص المستندات لاستخراج المعلومات الأساسية من التقارير المطولة. قم بدمج واجهة برمجة التطبيقات التالية لتوفير هذه الخدمة بكفاءة لمؤسستك.، قم بتطوير نظام للتعرف على الكيانات في البيانات النصية لتحديد الكيانات المهمة مثل الأشخاص والمؤسسات والمواقع. استخدم واجهة برمجة التطبيقات (API) أدناه لدمج هذه الوظيفة بسلاسة.، قم بتنفيذ نظام chatbot لدعم العملاء لتوفير استجابات تلقائية للاستفسارات الشائعة. استخدم واجهة برمجة التطبيقات (API) أدناه لدمج إمكانات فهم اللغة الطبيعية في برنامج الدردشة الآلي الخاص بك.، قم ببناء نظام لتحليل المشاعر على منشورات وسائل التواصل الاجتماعي لقياس الآراء والاتجاهات العامة. استخدم واجهة برمجة التطبيقات المتوفرة أدناه لتمكين تحليل المشاعر على بيانات الوسائط الاجتماعية بكفاءة.، صمم نظامًا لإنشاء النص لإنشاء أوصاف المنتج تلقائيًا بناءً على مواصفات الإدخال. قم بدمج واجهة برمجة التطبيقات (API) أدناه لتسهيل إنشاء نص مخصص لمنتجاتك بشكل فعال.، إنشاء نظام آلي لتصحيح اللغة لتحسين جودة المحتوى المكتوب. قم بدمج واجهة برمجة التطبيقات (API) أدناه لتمكين وظيفة تصحيح اللغة ضمن مسار معالجة النص الخاص بك.، قم بتطوير نظام لاستخراج الكلمات الرئيسية من المستندات النصية لتحديد المصطلحات والموضوعات المهمة. استخدم واجهة برمجة التطبيقات (API) أدناه لدمج وظيفة استخراج الكلمات الرئيسية بسلاسة في سير عمل معالجة المستندات لديك.، نفذ نظامًا لتحليل المشاعر بشأن تفاعلات خدمة العملاء لمراقبة مستويات رضا العملاء. استخدم واجهة برمجة التطبيقات التالية لدمج تحليل المشاعر بشكل فعال في منصة خدمة العملاء الخاصة بك.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/beit-base-patch16-224', 'api_call': \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'microsoft/beit-base-patch16-224'}, 'python_environment_requirements': ['transformers'], 'example_code': \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')\\ninputs = processor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'ImageNet', 'accuracy': 'Refer to tables 1 and 2 of the original paper'}, 'description': 'BEiT model pre-trained in a self-supervised fashion on ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224.' </s>"
"<s>[INST]لتحديد أكثر السلع مبيعًا في موقع التجارة الإلكترونية الخاص بك، استخدم API لتصنيف الصور.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'abhishek/autotrain-dog-vs-food', 'api_call': \"pipeline('image-classification', model='abhishek/autotrain-dog-vs-food')\", 'api_arguments': 'image_path', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'sasha/dog-food', 'accuracy': 0.998}, 'description': \"A pre-trained model for classifying images as either dog or food using Hugging Face's AutoTrain framework.\" </s>"
"<s>[INST] قامت شركتك بإطلاق منتجات جديدة وتحتاج إلى تحليل تفاعل العملاء معها عبر وسائل التواصل الاجتماعي. اعتمد على API لتصنيف الصور لتوقع تفضيلات العملاء.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'abhishek/autotrain-dog-vs-food', 'api_call': \"pipeline('image-classification', model='abhishek/autotrain-dog-vs-food')\", 'api_arguments': 'image_path', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'sasha/dog-food', 'accuracy': 0.998}, 'description': \"A pre-trained model for classifying images as either dog or food using Hugging Face's AutoTrain framework.\" </s>"
"<s>[INST] تريد شركتك تحسين تجربة تسوق عبر الإنترنت من خلال توصيات شخصية. أنشئ واجهة برمجية تستخدم تصنيف الصور لتوقع تفضيلات العملاء وتوجيههم إلى المنتجات المناسبة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'abhishek/autotrain-dog-vs-food', 'api_call': \"pipeline('image-classification', model='abhishek/autotrain-dog-vs-food')\", 'api_arguments': 'image_path', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'sasha/dog-food', 'accuracy': 0.998}, 'description': \"A pre-trained model for classifying images as either dog or food using Hugging Face's AutoTrain framework.\" </s>"
"<s>[INST] إذا كنت تدير مطعمًا وترغب في تقديم قوائم طعام محسنة بالصور للزبائن، استخدم API لتصنيف الصور لتحليل وعرض الأطعمة بطريقة جذابة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'abhishek/autotrain-dog-vs-food', 'api_call': \"pipeline('image-classification', model='abhishek/autotrain-dog-vs-food')\", 'api_arguments': 'image_path', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'sasha/dog-food', 'accuracy': 0.998}, 'description': \"A pre-trained model for classifying images as either dog or food using Hugging Face's AutoTrain framework.\" </s>"
"<s>[INST] تخطط لإطلاق خط جديد من الملابس وترغب في توجيه استراتيجية التسويق نحو جمهورك المستهدف. استفد من API تصنيف الصور لاختيار تصاميم تلائم اهتمامات عملائك.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'abhishek/autotrain-dog-vs-food', 'api_call': \"pipeline('image-classification', model='abhishek/autotrain-dog-vs-food')\", 'api_arguments': 'image_path', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'sasha/dog-food', 'accuracy': 0.998}, 'description': \"A pre-trained model for classifying images as either dog or food using Hugging Face's AutoTrain framework.\" </s>"
"<s>[INST] تهدف إلى تحليل المشاهد في أشرطة الفيديو الخاصة بك لتحسين تصنيف المحتوى. استخدم API تصنيف الصور لضبط وتحسين عملية تحليل الأفلام الخاصة بك.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'abhishek/autotrain-dog-vs-food', 'api_call': \"pipeline('image-classification', model='abhishek/autotrain-dog-vs-food')\", 'api_arguments': 'image_path', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'sasha/dog-food', 'accuracy': 0.998}, 'description': \"A pre-trained model for classifying images as either dog or food using Hugging Face's AutoTrain framework.\" </s>"
"<s>[INST] يرغب فريق البيانات في شركتك في برمجة نموذج لتحليل تعابير الوجه لتقييم ردود فعل الزبائن. توظف API تصنيف الصور لبناء نظام تعرف على العواطف.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'abhishek/autotrain-dog-vs-food', 'api_call': \"pipeline('image-classification', model='abhishek/autotrain-dog-vs-food')\", 'api_arguments': 'image_path', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'sasha/dog-food', 'accuracy': 0.998}, 'description': \"A pre-trained model for classifying images as either dog or food using Hugging Face's AutoTrain framework.\" </s>"
"<s>[INST] يرغب فريق التسويق بمتجر الإلكتروني في تصميم حملة إعلانية تستهدف محبي الطيور والحيوانات الأليفة. ساهم في ذلك بتمكينهم من API تصنيف الصور لاختيار الصور المناسبة للحملة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'abhishek/autotrain-dog-vs-food', 'api_call': \"pipeline('image-classification', model='abhishek/autotrain-dog-vs-food')\", 'api_arguments': 'image_path', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'sasha/dog-food', 'accuracy': 0.998}, 'description': \"A pre-trained model for classifying images as either dog or food using Hugging Face's AutoTrain framework.\" </s>"
"<s>[INST] لتعزيز جودة خدمة عملاء موقعك، يمكنك استخدام API تصنيف الصور لتحليل تعليقات العملاء وفهم احتياجاتهم. بناء واجهة برمجية تستخدم هذا النموذج.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'abhishek/autotrain-dog-vs-food', 'api_call': \"pipeline('image-classification', model='abhishek/autotrain-dog-vs-food')\", 'api_arguments': 'image_path', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'sasha/dog-food', 'accuracy': 0.998}, 'description': \"A pre-trained model for classifying images as either dog or food using Hugging Face's AutoTrain framework.\" </s>"
"<s>[INST] يود فريق البحث والتطوير في شركتك تصميم نموذج لتمييز بين أنواع مختلفة من المركبات عبر الصور. باستخدام API تصنيف الصور، بناء نظام لتعريف السيارات والشاحنات والدراجات النارية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'abhishek/autotrain-dog-vs-food', 'api_call': \"pipeline('image-classification', model='abhishek/autotrain-dog-vs-food')\", 'api_arguments': 'image_path', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'sasha/dog-food', 'accuracy': 0.998}, 'description': \"A pre-trained model for classifying images as either dog or food using Hugging Face's AutoTrain framework.\" </s>"
"<s>[INST]\"قم ببناء نظام تحليل المشاعر لفهم مشاعر المستخدمين من مدخلاتهم اللغوية. استخدم واجهة برمجة التطبيقات التالية لإعداد نموذج معالجة اللغة الطبيعية.\"، \"عزز واجهة التطبيق الخاصة بك عن طريق دمج واجهة برمجة التطبيقات لتحليل مشاعر المستخدم وتفاعلاته مع المحتوى احصل على تعليقات دقيقة ورؤى قيمة.، أنشئ قصة خيالية حول رحلة السفر عبر الزمن إلى عالم موازي.، طوّر روبوت دردشة يمكنه مساعدة المستخدمين في العثور على معلومات حول توقعات الطقس في مواقع متعددة.، تنفيذ صوت نظام التعرف في التطبيق الخاص بك لتمكين المستخدمين من التفاعل عن طريق الأوامر الصوتية.، قم بإنشاء محرك توصيات لمنصة التجارة الإلكترونية الخاصة بك لاقتراح توصيات منتجات مخصصة للمستخدمين بناءً على سجل التصفح الخاص بهم.، عزز منصة الوسائط الاجتماعية الخاصة بك عن طريق الدمج واجهة برمجة التطبيقات (API) لتحليل المشاعر لتحليل وتصنيف تعليقات ومنشورات المستخدمين بشكل فعال.، قم ببناء ميزة ترجمة اللغة في تطبيقك للسماح للمستخدمين بترجمة النص بين اللغات المختلفة بسهولة.، قم بتطوير نظام توصيات الأخبار لتوفير مقالات إخبارية مخصصة للمستخدمين بناءً على اهتماماتهم وعادات القراءة الخاصة بهم.، تنفيذ نظام التعرف على الوجوه في التطبيق الخاص بك لمصادقة المستخدم والتحكم في الوصول.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'lysandre/tiny-vit-random', 'api_call': \"ViTForImageClassification.from_pretrained('lysandre/tiny-vit-random')\", 'api_arguments': 'image_path', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A tiny-vit-random model for image classification using Hugging Face Transformers.' </s>"
"<s>[INST]تطلب شركة السياحة تطبيقًا يتيح للمستخدمين استعراض وحجز الفنادق بسهولة. ابحث عن واجهة برمجية توفر خدمات حجز الفنادق.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'fxmarty/resnet-tiny-beans', 'api_call': \"pipeline('image-classification', model='fxmarty/resnet-tiny-beans')\", 'api_arguments': {'model': 'fxmarty/resnet-tiny-beans'}, 'python_environment_requirements': {'transformers': 'latest'}, 'example_code': \"from transformers import pipeline; classifier = pipeline('image-classification', model='fxmarty/resnet-tiny-beans'); results = classifier('path/to/image.jpg')\", 'performance': {'dataset': 'beans', 'accuracy': 'Not provided'}, 'description': 'A model trained on the beans dataset, just for testing and having a really tiny model.' </s>"
"<s>[INST] تريد شركة التسوق عبر الإنترنت إضافة ميزة توصية المنتجات للعملاء استنادًا إلى تفضيلاتهم السابقة. ابحث عن واجهة برمجية لتوصية المنتجات.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'fxmarty/resnet-tiny-beans', 'api_call': \"pipeline('image-classification', model='fxmarty/resnet-tiny-beans')\", 'api_arguments': {'model': 'fxmarty/resnet-tiny-beans'}, 'python_environment_requirements': {'transformers': 'latest'}, 'example_code': \"from transformers import pipeline; classifier = pipeline('image-classification', model='fxmarty/resnet-tiny-beans'); results = classifier('path/to/image.jpg')\", 'performance': {'dataset': 'beans', 'accuracy': 'Not provided'}, 'description': 'A model trained on the beans dataset, just for testing and having a really tiny model.' </s>"
"<s>[INST] يسعى فريق التطوير إلى إنشاء تطبيق لتحسين ذكاء الأطفال. ابحث عن واجهة برمجية تقدم خدمات تعليمية تفاعلية للأطفال.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'fxmarty/resnet-tiny-beans', 'api_call': \"pipeline('image-classification', model='fxmarty/resnet-tiny-beans')\", 'api_arguments': {'model': 'fxmarty/resnet-tiny-beans'}, 'python_environment_requirements': {'transformers': 'latest'}, 'example_code': \"from transformers import pipeline; classifier = pipeline('image-classification', model='fxmarty/resnet-tiny-beans'); results = classifier('path/to/image.jpg')\", 'performance': {'dataset': 'beans', 'accuracy': 'Not provided'}, 'description': 'A model trained on the beans dataset, just for testing and having a really tiny model.' </s>"
"<s>[INST] ترغب شركة السيارات في تقديم خدمة تخصيص سيارات العملاء بألوان وتصميمات مخصصة. ابحث عن واجهة برمجية لتخصيص السيارات.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'fxmarty/resnet-tiny-beans', 'api_call': \"pipeline('image-classification', model='fxmarty/resnet-tiny-beans')\", 'api_arguments': {'model': 'fxmarty/resnet-tiny-beans'}, 'python_environment_requirements': {'transformers': 'latest'}, 'example_code': \"from transformers import pipeline; classifier = pipeline('image-classification', model='fxmarty/resnet-tiny-beans'); results = classifier('path/to/image.jpg')\", 'performance': {'dataset': 'beans', 'accuracy': 'Not provided'}, 'description': 'A model trained on the beans dataset, just for testing and having a really tiny model.' </s>"
"<s>[INST] تحتاج شركة التأمين إلى طريقة لتحقيق تحليل وتقييم سلوك القيادة للعملاء. ابحث عن واجهة برمجية للتحليل السلوكي للقيادة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'fxmarty/resnet-tiny-beans', 'api_call': \"pipeline('image-classification', model='fxmarty/resnet-tiny-beans')\", 'api_arguments': {'model': 'fxmarty/resnet-tiny-beans'}, 'python_environment_requirements': {'transformers': 'latest'}, 'example_code': \"from transformers import pipeline; classifier = pipeline('image-classification', model='fxmarty/resnet-tiny-beans'); results = classifier('path/to/image.jpg')\", 'performance': {'dataset': 'beans', 'accuracy': 'Not provided'}, 'description': 'A model trained on the beans dataset, just for testing and having a really tiny model.' </s>"
"<s>[INST] تهدف منصة التوصيل السريع إلى تحسين تجربة العملاء عن طريق توفير خدمة تتبع الطلبات في الوقت الفعلي. ابحث عن واجهة برمجية لخدمة تتبع الطلبات.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'fxmarty/resnet-tiny-beans', 'api_call': \"pipeline('image-classification', model='fxmarty/resnet-tiny-beans')\", 'api_arguments': {'model': 'fxmarty/resnet-tiny-beans'}, 'python_environment_requirements': {'transformers': 'latest'}, 'example_code': \"from transformers import pipeline; classifier = pipeline('image-classification', model='fxmarty/resnet-tiny-beans'); results = classifier('path/to/image.jpg')\", 'performance': {'dataset': 'beans', 'accuracy': 'Not provided'}, 'description': 'A model trained on the beans dataset, just for testing and having a really tiny model.' </s>"
"<s>[INST] تحتاج شركة الصحة إلى تطبيق يتيح للمستخدمين متابعة نشاطهم الرياضي وتسجيل السعرات الحرارية المستهلكة. ابحث عن واجهة برمجية لتتبع الأنشطة الرياضية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'fxmarty/resnet-tiny-beans', 'api_call': \"pipeline('image-classification', model='fxmarty/resnet-tiny-beans')\", 'api_arguments': {'model': 'fxmarty/resnet-tiny-beans'}, 'python_environment_requirements': {'transformers': 'latest'}, 'example_code': \"from transformers import pipeline; classifier = pipeline('image-classification', model='fxmarty/resnet-tiny-beans'); results = classifier('path/to/image.jpg')\", 'performance': {'dataset': 'beans', 'accuracy': 'Not provided'}, 'description': 'A model trained on the beans dataset, just for testing and having a really tiny model.' </s>"
"<s>[INST] ترغب شركة السياحة في توفير تطبيق يعرض معلومات حول الوجهات السياحية والفعاليات المحلية. ابحث عن واجهة برمجية لتوفير محتوى الوجهات السياحية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'fxmarty/resnet-tiny-beans', 'api_call': \"pipeline('image-classification', model='fxmarty/resnet-tiny-beans')\", 'api_arguments': {'model': 'fxmarty/resnet-tiny-beans'}, 'python_environment_requirements': {'transformers': 'latest'}, 'example_code': \"from transformers import pipeline; classifier = pipeline('image-classification', model='fxmarty/resnet-tiny-beans'); results = classifier('path/to/image.jpg')\", 'performance': {'dataset': 'beans', 'accuracy': 'Not provided'}, 'description': 'A model trained on the beans dataset, just for testing and having a really tiny model.' </s>"
"<s>[INST] تخطط شركة التجزئة لتطوير تطبيق لتحسين تجربة التسوق عبر الإنترنت بواسطة تقديم خدمة الدردشة الآلية. ابحث عن واجهة برمجية لدمج الدردشة الآلية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'fxmarty/resnet-tiny-beans', 'api_call': \"pipeline('image-classification', model='fxmarty/resnet-tiny-beans')\", 'api_arguments': {'model': 'fxmarty/resnet-tiny-beans'}, 'python_environment_requirements': {'transformers': 'latest'}, 'example_code': \"from transformers import pipeline; classifier = pipeline('image-classification', model='fxmarty/resnet-tiny-beans'); results = classifier('path/to/image.jpg')\", 'performance': {'dataset': 'beans', 'accuracy': 'Not provided'}, 'description': 'A model trained on the beans dataset, just for testing and having a really tiny model.' </s>"
"<s>[INST] تحتاج شركة التسويق الرقمي إلى أداة تساعدها على تحليل وفهم تفاعل الجمهور مع حملاتها الإعلانية. ابحث عن واجهة برمجية لتحليل تفاعل الجمهور.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'fxmarty/resnet-tiny-beans', 'api_call': \"pipeline('image-classification', model='fxmarty/resnet-tiny-beans')\", 'api_arguments': {'model': 'fxmarty/resnet-tiny-beans'}, 'python_environment_requirements': {'transformers': 'latest'}, 'example_code': \"from transformers import pipeline; classifier = pipeline('image-classification', model='fxmarty/resnet-tiny-beans'); results = classifier('path/to/image.jpg')\", 'performance': {'dataset': 'beans', 'accuracy': 'Not provided'}, 'description': 'A model trained on the beans dataset, just for testing and having a really tiny model.' </s>"
"<s>[INST]تنفيذ نموذج لتحليل الصور الطبية للكشف عن الأمراض. يمكنك استخدام ال API التالي لتطوير نظام الكشف التلقائي عن الأمراض من الصور.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/mobilenet_v1_0.75_192', 'api_call': \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/mobilenet_v1_0.75_192'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.' </s>"
"<s>[INST] تنفيذ نظام لتصنيف النباتات باستخدام تحليل الصور. يمكنك استخدام ال API التالي لتصنيف النباتات استنادًا إلى الصور النباتية المقدمة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/mobilenet_v1_0.75_192', 'api_call': \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/mobilenet_v1_0.75_192'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.' </s>"
"<s>[INST] إنشاء نظام لتحليل الصور الجوية للكشف عن التغيرات في البيئة. يمكنك استخدام ال API التالي لتحميل واستخدام نموذج الكشف عن التغيرات في البيئة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/mobilenet_v1_0.75_192', 'api_call': \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/mobilenet_v1_0.75_192'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.' </s>"
"<s>[INST] تطوير نظام لتحليل وتصنيف لقطات الفيديو الطبية. يمكنك استخدام ال API التالي لتصنيف لقطات الفيديو الطبية استنادًا إلى المحتوى الطبي فيها.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/mobilenet_v1_0.75_192', 'api_call': \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/mobilenet_v1_0.75_192'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.' </s>"
"<s>[INST] تطبيق نظام لتحليل أشكال السحب السحابية في الصور الجوية. يمكنك استخدام ال API التالي لتحليل أشكال السحاب في صور السحب الجوي.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/mobilenet_v1_0.75_192', 'api_call': \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/mobilenet_v1_0.75_192'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.' </s>"
"<s>[INST] تنفيذ نظام لتحليل الصور السينمائية للتعرف على الأفلام والممثلين. يمكنك استخدام ال API التالي لتحليل الصور السينمائية وتصنيفها تلقائيًا.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/mobilenet_v1_0.75_192', 'api_call': \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/mobilenet_v1_0.75_192'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.' </s>"
"<s>[INST] تنفيذ نظام لتحليل الصور الفضائية للاستشعار عن بُعد. يمكنك استخدام ال API التالي لمعالجة الصور الفضائية واستخراج المعلومات الضرورية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/mobilenet_v1_0.75_192', 'api_call': \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/mobilenet_v1_0.75_192'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.' </s>"
"<s>[INST] تنفيذ نظام للتعرف على الموسيقى من الصور السمعية. يمكنك استخدام ال API التالي لاستخراج البيانات السمعية من الصور للتعرف على الموسيقى.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/mobilenet_v1_0.75_192', 'api_call': \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/mobilenet_v1_0.75_192'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.' </s>"
"<s>[INST] تطوير نظام لتحليل الصور الطبية الشعاعية للتشخيص التلقائي. يمكنك استخدام ال API التالي لتطبيق تحليل الصور الطبية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/mobilenet_v1_0.75_192', 'api_call': \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/mobilenet_v1_0.75_192'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.' </s>"
"<s>[INST] تنفيذ نظام لتحليل الصور الفضائية للتعرف على المواقع الجغرافية. يمكنك استخدام ال API التالي لاستخراج المعلومات الجيومكانية من الصور الفضائية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/mobilenet_v1_0.75_192', 'api_call': \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\", 'api_arguments': {'pretrained_model_name_or_path': 'google/mobilenet_v1_0.75_192'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.' </s>"
"<s>[INST]كيف يمكن استخدام API لتحديد وتصنيف أنواع الحيوانات في الصور؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'nvidia/mit-b0', 'api_call': \"SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/mit-b0'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/mit-b0')\\nmodel = SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'imagenet_1k', 'accuracy': 'Not provided'}, 'description': 'SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes.' </s>"
"<s>[INST] كيفية استخدام API لتحليل صور السيارات وتحديد العلامات التجارية؟\\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'nvidia/mit-b0', 'api_call': \"SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/mit-b0'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/mit-b0')\\nmodel = SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'imagenet_1k', 'accuracy': 'Not provided'}, 'description': 'SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes.' </s>"
"<s>[INST] كيف يمكن إنشاء تطبيق يستخدم API لتحويل النصوص إلى صور فنية؟ \\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'nvidia/mit-b0', 'api_call': \"SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/mit-b0'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/mit-b0')\\nmodel = SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'imagenet_1k', 'accuracy': 'Not provided'}, 'description': 'SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes.' </s>"
"<s>[INST] كيفية استخدام API لتصنيف صور الطعام بناءً على الأنواع والمكونات؟\\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'nvidia/mit-b0', 'api_call': \"SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/mit-b0'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/mit-b0')\\nmodel = SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'imagenet_1k', 'accuracy': 'Not provided'}, 'description': 'SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes.' </s>"
"<s>[INST] نريد تطوير نظام للتعرف على أعلام الدول في الصور، كيف يمكننا استخدام الـ API المعطا؟\\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'nvidia/mit-b0', 'api_call': \"SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/mit-b0'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/mit-b0')\\nmodel = SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'imagenet_1k', 'accuracy': 'Not provided'}, 'description': 'SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes.' </s>"
"<s>[INST] كيفية استخدام API لتحليل الصور الطبية واكتشاف الأمراض؟\\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'nvidia/mit-b0', 'api_call': \"SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/mit-b0'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/mit-b0')\\nmodel = SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'imagenet_1k', 'accuracy': 'Not provided'}, 'description': 'SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes.' </s>"
"<s>[INST] كيف يمكن استخدام API لتصنيف الطيور في الصور؟\\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'nvidia/mit-b0', 'api_call': \"SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/mit-b0'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/mit-b0')\\nmodel = SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'imagenet_1k', 'accuracy': 'Not provided'}, 'description': 'SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes.' </s>"
"<s>[INST] كيف يمكن استخدام API لتحليل الوجوه وتحديد الانفعالات في الصور؟\\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'nvidia/mit-b0', 'api_call': \"SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/mit-b0'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/mit-b0')\\nmodel = SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'imagenet_1k', 'accuracy': 'Not provided'}, 'description': 'SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes.' </s>"
"<s>[INST] كيف يمكن استخدام API لتحليل الصور الطبية بغرض تشخيص الأمراض؟\\n   -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'nvidia/mit-b0', 'api_call': \"SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/mit-b0'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/mit-b0')\\nmodel = SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'imagenet_1k', 'accuracy': 'Not provided'}, 'description': 'SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes.' </s>"
"<s>[INST] كيفية استخدام API لتصنيف العناصر المنزلية في الصور مثل الأثاث والديكور؟\\n    -? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'nvidia/mit-b0', 'api_call': \"SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/mit-b0'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/mit-b0')\\nmodel = SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", 'performance': {'dataset': 'imagenet_1k', 'accuracy': 'Not provided'}, 'description': 'SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes.' </s>"
"<s>[INST]تحليل النصوص المكتوبة بخط اليد باستخدام تقنيات التعلم الآلي لتحسين دقة التعرف على الحروف والكلمات.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-tiny-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-tiny-224'}, 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification.' </s>"
"<s>[INST] تنفيذ تحليل للمحتوى الصوتي لتحويل ملفات الصوت إلى نصوص مكتوبة باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-tiny-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-tiny-224'}, 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification.' </s>"
"<s>[INST] تطبيق تقنيات التعلم الآلي لتصنيف الأفلام استنادًا إلى تقييمات المستخدمين وتحقيق توصيات دقيقة للمستخدمين.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-tiny-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-tiny-224'}, 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification.' </s>"
"<s>[INST] إنشاء نموذج تعلم آلي للتعرف على الوجوه في الصور وتصنيفها استنادًا إلى المعالم الفريدة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-tiny-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-tiny-224'}, 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification.' </s>"
"<s>[INST] تصميم نظام تخطيط ذكي لتحسين توزيع الطاقة الكهربائية في الشبكات الذكية باستخدام خوارزميات تعلم الآلة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-tiny-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-tiny-224'}, 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification.' </s>"
"<s>[INST] تطوير نظام تصنيف للمستندات الطبية باستخدام تقنيات التعلم الآلي لتبسيط وتسريع عمليات البحث والتحليل.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-tiny-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-tiny-224'}, 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification.' </s>"
"<s>[INST] تطبيق تقنيات التعلم الآلي لتحليل نمط سلوك المستهلكين وتوقع السلوكيات المستقبلية لتحسين الخدمات والتسويق.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-tiny-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-tiny-224'}, 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification.' </s>"
"<s>[INST] تحليل السلوك العملياتي لأنظمة الإنتاج باستخدام تقنيات التعلم الآلي لتحسين كفاءة الإنتاج وإدارة المخزون.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-tiny-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-tiny-224'}, 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification.' </s>"
"<s>[INST] تنفيذ تحليل الشبكات الاجتماعية لفهم العلاقات بين الأفراد وتحديد التأثيرات الاجتماعية باستخدام تقنيات تعلم الآلة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-tiny-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-tiny-224'}, 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification.' </s>"
"<s>[INST] تنفيذ تحليل تنبؤي لما يمكن أن يحدث في المستقبل باستخدام تقنيات التعلم الآلي وتقديم توصيات مبنية على النتائج المتوقعة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/convnext-tiny-224', 'api_call': \"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/convnext-tiny-224'}, 'python_environment_requirements': ['transformers', 'torch', 'datasets'], 'example_code': \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification.' </s>"
"<s>[INST]تحتاج شركة تضع خطة لتقديم خدمة مراقبة التأثير الإعلامي البصري. استخدم ال API التالي للتعرف على المحتوى المرئي.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_base_patch16_224.augreg2_in21k_ft_in1k', 'api_call': \"ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\", 'api_arguments': {'pretrained_model_name_or_path': 'timm/vit_base_patch16_224.augreg2_in21k_ft_in1k'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k.' </s>"
"<s>[INST] مطلوب تطبيق يقوم بتحليل السلوك البصري لمواقع التجارة الإلكترونية، استخدم API التالي لإنشاء النموذج.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_base_patch16_224.augreg2_in21k_ft_in1k', 'api_call': \"ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\", 'api_arguments': {'pretrained_model_name_or_path': 'timm/vit_base_patch16_224.augreg2_in21k_ft_in1k'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k.' </s>"
"<s>[INST] تخطط شركة لتطوير تطبيق يقدم تحليلات الصور لصناعة السيارات، استخدم API التالي لبناء النموذج.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_base_patch16_224.augreg2_in21k_ft_in1k', 'api_call': \"ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\", 'api_arguments': {'pretrained_model_name_or_path': 'timm/vit_base_patch16_224.augreg2_in21k_ft_in1k'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k.' </s>"
"<s>[INST] طلبت شركة تجهيز تطبيق لتحليل الأنماط البصرية في تصاميم الأزياء، يمكنك استخدام API التالي لتطوير الأداة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_base_patch16_224.augreg2_in21k_ft_in1k', 'api_call': \"ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\", 'api_arguments': {'pretrained_model_name_or_path': 'timm/vit_base_patch16_224.augreg2_in21k_ft_in1k'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k.' </s>"
"<s>[INST] مطوري تطبيق يرغبون في تحليل صور الأفراد لتقديم توصيات لتحسين الصحة، استخدم API التالي لتنفيذ النموذج.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_base_patch16_224.augreg2_in21k_ft_in1k', 'api_call': \"ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\", 'api_arguments': {'pretrained_model_name_or_path': 'timm/vit_base_patch16_224.augreg2_in21k_ft_in1k'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k.' </s>"
"<s>[INST] يهدف مشروع تطوير نظام الذكاء الصناعي إلى استخدام موديل لتحليل الصور الفضائية، استخدم ال API التالي لبناء النموذج? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_base_patch16_224.augreg2_in21k_ft_in1k', 'api_call': \"ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\", 'api_arguments': {'pretrained_model_name_or_path': 'timm/vit_base_patch16_224.augreg2_in21k_ft_in1k'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k.' </s>"
"<s>[INST] ترغب منظمة في تحليل الرسوم البيانية لتوليد تقارير مبسطة، يمكنك استخدام ال API التالي لتصميم الأداة المناسبة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_base_patch16_224.augreg2_in21k_ft_in1k', 'api_call': \"ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\", 'api_arguments': {'pretrained_model_name_or_path': 'timm/vit_base_patch16_224.augreg2_in21k_ft_in1k'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k.' </s>"
"<s>[INST] تحتاج مؤسسة تنمية الويب لبناء تطبيق يقوم بتحديد المحتوى السلبي في الصور، استخدم ال API التالي لتطوير النموذج.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_base_patch16_224.augreg2_in21k_ft_in1k', 'api_call': \"ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\", 'api_arguments': {'pretrained_model_name_or_path': 'timm/vit_base_patch16_224.augreg2_in21k_ft_in1k'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k.' </s>"
"<s>[INST] تطوير تطبيق لتحليل الصور الطبية واكتشاف الأمراض، يمكنك استخدام ال API التالي لبناء النموذج اللازم.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_base_patch16_224.augreg2_in21k_ft_in1k', 'api_call': \"ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\", 'api_arguments': {'pretrained_model_name_or_path': 'timm/vit_base_patch16_224.augreg2_in21k_ft_in1k'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k.' </s>"
"<s>[INST] شركة تقنية تريد تطوير تطبيق لتحليل الأوضاع البيئية من الصور الجوية، يمكنك استخدام ال API التالي لبناء النموذج المناسب.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_base_patch16_224.augreg2_in21k_ft_in1k', 'api_call': \"ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\", 'api_arguments': {'pretrained_model_name_or_path': 'timm/vit_base_patch16_224.augreg2_in21k_ft_in1k'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k.' </s>"
"<s>[INST]تحتاج إلى تطوير نظام لتحديد الأطعمة في الصور وتقديم توصيات غذائية استنادًا إلى تحليل التغذية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/mobilenet_v2_1.0_224', 'api_call': \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v2_1.0_224)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v2_1.0_224)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'MobileNet V2 model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in MobileNetV2: Inverted Residuals and Linear Bottlenecks by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.' </s>"
"<s>[INST] يرغب العميل في تطوير نظام يمكنه تحديد الرموز التعبيرية في النصوص وتقديم توصيات لتحسين التواصل الرقمي.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/mobilenet_v2_1.0_224', 'api_call': \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v2_1.0_224)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v2_1.0_224)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'MobileNet V2 model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in MobileNetV2: Inverted Residuals and Linear Bottlenecks by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.' </s>"
"<s>[INST] يجب تطوير نظام لتحديد الأماكن السياحية من الصور وتقديم معلومات سياحية مفصلة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'google/mobilenet_v2_1.0_224', 'api_call': \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v2_1.0_224)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v2_1.0_224)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'MobileNet V2 model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in MobileNetV2: Inverted Residuals and Linear Bottlenecks by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.' </s>"
"<s>[INST]ترغب في بناء تطبيق يعتمد على تقنية اللغة الطبيعية لتحليل المحادثات وفهم مشاعر المستخدمين.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'api_call': \"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\", 'api_arguments': {'model': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'framework': 'pt'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.' </s>"
"<s>[INST] تريد تطوير نظام يمكنه استشراف البيانات واكتشاف النماذج ذات الصلة في قواعد البيانات الكبيرة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'api_call': \"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\", 'api_arguments': {'model': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'framework': 'pt'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.' </s>"
"<s>[INST] تود إنشاء تطبيق يعتمد على تقنية البعد الزمني لتحليل السلوك الزبوني وتوقع العمليات المستقبلية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'api_call': \"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\", 'api_arguments': {'model': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'framework': 'pt'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.' </s>"
"<s>[INST] تخطط لإنشاء نظام يمكنه تحليل البقع الجلدية وتقديم تقرير عن الأمراض المحتملة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'api_call': \"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\", 'api_arguments': {'model': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'framework': 'pt'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.' </s>"
"<s>[INST] تود إنشاء تطبيق يقيس مدى تقارب الصور وتوافقها بناءً على المحتوى البصري.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'api_call': \"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\", 'api_arguments': {'model': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'framework': 'pt'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.' </s>"
"<s>[INST] ترغب في إنشاء تطبيق يقوم بتحليل النصوص الطبية واكتشاف الكيانات الطبية فيها.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'api_call': \"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\", 'api_arguments': {'model': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'framework': 'pt'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.' </s>"
"<s>[INST] تود بناء نظام يمكنه من تحديد المشاهد الخطرة في مجموعة من مقاطع الفيديو.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'api_call': \"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\", 'api_arguments': {'model': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'framework': 'pt'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.' </s>"
"<s>[INST] تريد بناء تطبيق يقوم بتصنيف الفواتير المالية تلقائياً بناءً على المحتوى النصي.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'api_call': \"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\", 'api_arguments': {'model': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'framework': 'pt'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.' </s>"
"<s>[INST] ترغب في تطوير نظام يعتمد على تقنية الإدراك الحاسوبي لتحليل توجيهات الحركة للمشاة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'api_call': \"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\", 'api_arguments': {'model': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'framework': 'pt'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.' </s>"
"<s>[INST] تخطط لإنشاء تطبيق لتصنيف تعليقات المستخدمين على منصة التواصل الاجتماعي بناءً على المشاعر المعبر عنها.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'api_call': \"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\", 'api_arguments': {'model': 'timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'framework': 'pt'}, 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.' </s>"
"<s>[INST]ابنِ تطبيقًا للتعرف على النباتات باستخدام واجهة برمجة التطبيقات Plant Recognition API.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_tiny_patch16_224.augreg_in21k_ft_in1k', 'api_call': \"timm.create_model('hf_hub:timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k')\", 'api_arguments': 'pretrained', 'python_environment_requirements': 'timm', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم API Weather Forecast لعرض تنبؤات حالة الطقس.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_tiny_patch16_224.augreg_in21k_ft_in1k', 'api_call': \"timm.create_model('hf_hub:timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k')\", 'api_arguments': 'pretrained', 'python_environment_requirements': 'timm', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization.' </s>"
"<s>[INST] قم بتطوير تطبيق يستخدم API Flight Tracking لتتبع حركة الرحلات الجوية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_tiny_patch16_224.augreg_in21k_ft_in1k', 'api_call': \"timm.create_model('hf_hub:timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k')\", 'api_arguments': 'pretrained', 'python_environment_requirements': 'timm', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization.' </s>"
"<s>[INST] استخدام API Recipe Search لبناء تطبيق يقدم وصفات طعام متنوعة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_tiny_patch16_224.augreg_in21k_ft_in1k', 'api_call': \"timm.create_model('hf_hub:timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k')\", 'api_arguments': 'pretrained', 'python_environment_requirements': 'timm', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization.' </s>"
"<s>[INST] تطوير تطبيق يستخدم API Music Recommendations لاقتراح أغاني وقوائم تشغيل موسيقية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_tiny_patch16_224.augreg_in21k_ft_in1k', 'api_call': \"timm.create_model('hf_hub:timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k')\", 'api_arguments': 'pretrained', 'python_environment_requirements': 'timm', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization.' </s>"
"<s>[INST] بناء تطبيق يستخدم API Stock Market Data لعرض وتحليل أسعار الأسهم والتغيرات في السوق المالية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_tiny_patch16_224.augreg_in21k_ft_in1k', 'api_call': \"timm.create_model('hf_hub:timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k')\", 'api_arguments': 'pretrained', 'python_environment_requirements': 'timm', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization.' </s>"
"<s>[INST] تطوير تطبيق يستخدم API News Headlines لعرض آخر الأخبار والعناوين الرئيسية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_tiny_patch16_224.augreg_in21k_ft_in1k', 'api_call': \"timm.create_model('hf_hub:timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k')\", 'api_arguments': 'pretrained', 'python_environment_requirements': 'timm', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم API Language Translation لترجمة نصوص بين لغات مختلفة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_tiny_patch16_224.augreg_in21k_ft_in1k', 'api_call': \"timm.create_model('hf_hub:timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k')\", 'api_arguments': 'pretrained', 'python_environment_requirements': 'timm', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization.' </s>"
"<s>[INST] ابنِ تطبيقًا يستخدم API Health Monitoring لقياس مؤشرات الصحة ومراقبتها.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_tiny_patch16_224.augreg_in21k_ft_in1k', 'api_call': \"timm.create_model('hf_hub:timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k')\", 'api_arguments': 'pretrained', 'python_environment_requirements': 'timm', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization.' </s>"
"<s>[INST] استخدام API Location Services لبناء تطبيق يوفر معلومات موقعية وخرائط للمستخدمين.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'vit_tiny_patch16_224.augreg_in21k_ft_in1k', 'api_call': \"timm.create_model('hf_hub:timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k')\", 'api_arguments': 'pretrained', 'python_environment_requirements': 'timm', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization.' </s>"
"<s>[INST]تحسين جودة الصور المعروضة على موقعك عبر تطبيق تقنيات تحسين الصور.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swin-tiny-patch4-window7-224', 'api_call': \"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.' </s>"
"<s>[INST] إضافة ميزة تقطيع الصور إلى قصاصات صغيرة لتسهيل عرض الصور الكبيرة على موقعك.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swin-tiny-patch4-window7-224', 'api_call': \"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.' </s>"
"<s>[INST] تحليل شفافية الصور لتحديد درجة الشفافية في الصور المقدمة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swin-tiny-patch4-window7-224', 'api_call': \"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.' </s>"
"<s>[INST] إنشاء نظام للكشف التلقائي عن وجوه الأشخاص في الصور المقدمة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swin-tiny-patch4-window7-224', 'api_call': \"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.' </s>"
"<s>[INST] تحليل درجة إضاءة الصور لتصنيفها إما كصور ذات إضاءة جيدة أو غير كافية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swin-tiny-patch4-window7-224', 'api_call': \"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.' </s>"
"<s>[INST] تحليل النمط اللوني للصور لتصنيفها إلى فئات مختلفة استنادًا إلى تشابه الألوان.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swin-tiny-patch4-window7-224', 'api_call': \"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.' </s>"
"<s>[INST] تقديم نظام للتعرف على النصوص في الصور لتسهيل استخراج المعلومات منها.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swin-tiny-patch4-window7-224', 'api_call': \"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.' </s>"
"<s>[INST] تطوير نظام لرصد تغييرات الألوان في الصور لاكتشاف تلاعب أو تعديلات غير مصرح بها.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swin-tiny-patch4-window7-224', 'api_call': \"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.' </s>"
"<s>[INST] تحليل النمط الهيكلي للصور لتحديد الأشكال الهندسية الموجودة فيها.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swin-tiny-patch4-window7-224', 'api_call': \"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.' </s>"
"<s>[INST] تحليل البيانات الجغرافية في الصور لاستخراج المواقع الجغرافية والمعلومات المكانية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swin-tiny-patch4-window7-224', 'api_call': \"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': 'from transformers import AutoFeatureExtractor, SwinForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not specified'}, 'description': 'Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.' </s>"
"<s>[INST]يتعين عليك تطوير تطبيق يستخدم تقنيات الذكاء الاصطناعي لتشخيص الأمراض الجلدية استنادًا إلى الصور الناتجة من الأشعة تحت الحمراء.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'julien-c/hotdog-not-hotdog', 'api_call': \"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': 0.825}, 'description': 'A model that classifies images as hotdog or not hotdog.' </s>"
"<s>[INST] انشئ تطبيقًا يقوم بتحليل تسجيلات الفيديو والكشف عن الحالات المرورية غير العادية باستخدام تقنيات الذكاء الاصطناعي.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'julien-c/hotdog-not-hotdog', 'api_call': \"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': 0.825}, 'description': 'A model that classifies images as hotdog or not hotdog.' </s>"
"<s>[INST] قم بتطوير تطبيق يعتمد على تقنيات خوارزميات التعلم الآلي لتحليل تغريدات المستخدمين وتقييم مدى رضاهم.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'julien-c/hotdog-not-hotdog', 'api_call': \"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': 0.825}, 'description': 'A model that classifies images as hotdog or not hotdog.' </s>"
"<s>[INST] على المطورين إنشاء تطبيق يستخدم الذكاء الاصطناعي للكشف عن نمط اختزال في الصور الطبية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'julien-c/hotdog-not-hotdog', 'api_call': \"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': 0.825}, 'description': 'A model that classifies images as hotdog or not hotdog.' </s>"
"<s>[INST] إنشاء تطبيق يقوم بتحديد الأشخاص في مقاطع الفيديو الأمنية باستخدام نماذج التعلم الآلي القائمة على الشبكات العصبية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'julien-c/hotdog-not-hotdog', 'api_call': \"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': 0.825}, 'description': 'A model that classifies images as hotdog or not hotdog.' </s>"
"<s>[INST] يتعين عليك تطوير تطبيق يستخدم تقنيات الذكاء الاصطناعي لتحليل سلوك الدراجات النارية في الطرق.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'julien-c/hotdog-not-hotdog', 'api_call': \"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': 0.825}, 'description': 'A model that classifies images as hotdog or not hotdog.' </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم الذكاء الاصطناعي لتحليل لغة الإشارة في مقاطع الفيديو.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'julien-c/hotdog-not-hotdog', 'api_call': \"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': 0.825}, 'description': 'A model that classifies images as hotdog or not hotdog.' </s>"
"<s>[INST] إنشاء تطبيق يستند إلى تقنيات الذكاء الاصطناعي لتحليل وتصنيف التقارير الطبية بدقة عالية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'julien-c/hotdog-not-hotdog', 'api_call': \"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': 0.825}, 'description': 'A model that classifies images as hotdog or not hotdog.' </s>"
"<s>[INST] يتوجب عليك تطوير تطبيق يستخدم تقنيات الذكاء الاصطناعي للتعرف على العناصر الغذائية في الصور الغذائية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'julien-c/hotdog-not-hotdog', 'api_call': \"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': 0.825}, 'description': 'A model that classifies images as hotdog or not hotdog.' </s>"
"<s>[INST] قم بتنفيذ تطبيق يعتمد على تقنيات الذكاء الاصطناعي للكشف عن الأشخاص المفتونين في النصوص الأدبية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'julien-c/hotdog-not-hotdog', 'api_call': \"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': 0.825}, 'description': 'A model that classifies images as hotdog or not hotdog.' </s>"
"<s>[INST]طلبت شركة تكنولوجيا إنشاء نظام لتحليل وتصنيف النصوص الطبية تلقائياً. كيف يمكن استخدام Hugging Face Transformers API لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swinv2-tiny-patch4-window8-256', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\", 'api_arguments': {'image': 'http://images.cocodataset.org/val2017/000000039769.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.' </s>"
"<s>[INST] تحتاج منظمة إلى نظام لتحليل سلوك المستخدمين على موقع الويب الخاص بها. كيف يمكن استخدام Hugging Face Transformers API لتحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swinv2-tiny-patch4-window8-256', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\", 'api_arguments': {'image': 'http://images.cocodataset.org/val2017/000000039769.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.' </s>"
"<s>[INST] تطلب شركة تأمين إنشاء نظام لتحليل الأضرار في صور الحوادث السيارية. كيف يمكن استخدام API من Hugging Face Transformers لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swinv2-tiny-patch4-window8-256', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\", 'api_arguments': {'image': 'http://images.cocodataset.org/val2017/000000039769.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.' </s>"
"<s>[INST] يرغب مُطوّر تطبيقات في إضافة ميزة تحويل الخط اليد إلى نص إليه تطبيقه. كيف يمكن استخدام Hugging Face Transformers API لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swinv2-tiny-patch4-window8-256', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\", 'api_arguments': {'image': 'http://images.cocodataset.org/val2017/000000039769.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.' </s>"
"<s>[INST] شركة تجارة إلكترونية تود تطوير نظام لتصنيف المنتجات استنادًا إلى الصور. كيف يمكن استخدام API من Hugging Face Transformers لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swinv2-tiny-patch4-window8-256', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\", 'api_arguments': {'image': 'http://images.cocodataset.org/val2017/000000039769.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.' </s>"
"<s>[INST] مطلوب من مطور تطبيق إضافة خاصية الاستعراض الصوتي لحديث المستخدمين. كيف يمكن استخدام Hugging Face Transformers API لتحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swinv2-tiny-patch4-window8-256', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\", 'api_arguments': {'image': 'http://images.cocodataset.org/val2017/000000039769.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.' </s>"
"<s>[INST] تحتاج شركة لبناء نظام للتحقق من هوية المستخدمين باستخدام صورهم الوجهية. كيف يمكن استخدام Hugging Face Transformers API لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swinv2-tiny-patch4-window8-256', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\", 'api_arguments': {'image': 'http://images.cocodataset.org/val2017/000000039769.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.' </s>"
"<s>[INST] ترغب منصة تعليمية في تنفيذ نظام لتحليل تقييمات الطلاب بشكل آلي. كيف يمكن استخدام API من Hugging Face Transformers لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swinv2-tiny-patch4-window8-256', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\", 'api_arguments': {'image': 'http://images.cocodataset.org/val2017/000000039769.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.' </s>"
"<s>[INST] شركة تقنية ترغب في تطوير نظام لتوليد النصوص بشكل آلي. كيف يمكن استخدام Hugging Face Transformers API لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swinv2-tiny-patch4-window8-256', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\", 'api_arguments': {'image': 'http://images.cocodataset.org/val2017/000000039769.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.' </s>"
"<s>[INST] مُطوّر يرغب في بناء نظام يمكنه تحليل محتوى الفيديوهات الرقمية تلقائيًا. كيف يمكن استخدام Hugging Face Transformers API لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'microsoft/swinv2-tiny-patch4-window8-256', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\", 'api_arguments': {'image': 'http://images.cocodataset.org/val2017/000000039769.jpg'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.' </s>"
"<s>[INST]\"قم بتطوير أداة تحليل المشاعر باستخدام معالجة اللغة الطبيعية لتحليل مراجعات العملاء وتعليقاتهم على منتجك.\"، \"إنشاء نظام توصية لاقتراحات الأفلام المخصصة بناءً على تفضيلات المستخدم وسجل العرض.\"، \"قم ببناء روبوت محادثة مزود بقدرات الذكاء الاصطناعي للمحادثة من أجل توفير دعم العملاء والإجابة على الأسئلة الشائعة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'saltacc/anime-ai-detect', 'api_call': \"pipeline('image-classification', model='saltacc/anime-ai-detect')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'aibooru and imageboard sites', 'accuracy': '96%'}, 'description': 'A BEiT classifier to see if anime art was made by an AI or a human.' </s>"
"<s>[INST] قم بتطوير أداة تلخيص نص باستخدام تقنيات معالجة اللغة الطبيعية المتقدمة لتكثيف المستندات الطويلة في ملخصات موجزة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'saltacc/anime-ai-detect', 'api_call': \"pipeline('image-classification', model='saltacc/anime-ai-detect')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'aibooru and imageboard sites', 'accuracy': '96%'}, 'description': 'A BEiT classifier to see if anime art was made by an AI or a human.' </s>"
"<s>[INST] تنفيذ نظام كشف الاحتيال باستخدام خوارزميات التعلم الآلي لتحديد المعاملات الشاذة ومنع المعاملات المالية الخسائر.، تصميم نظام التعرف على الصور لتحديد الأنواع المختلفة من النباتات في الحدائق النباتية لمساعدة علماء النبات في أبحاثهم.، إنشاء أداة للتنبؤ بالطقس تستخدم خوارزميات التعلم الآلي للتنبؤ بأنماط درجات الحرارة وهطول الأمطار بدقة.،  أنشئ محرك توصيات لمنصات التجارة الإلكترونية لاقتراح المنتجات ذات الصلة للمستخدمين استنادًا إلى سجل التصفح وسلوك الشراء.، قم بتطوير مساعد افتراضي يتمتع بقدرات التعرف على الكلام لأداء المهام وتعيين التذكيرات وتوفير المعلومات من خلال الأوامر الصوتية. \"تنفيذ نظام تصنيف المستندات باستخدام نماذج التعلم العميق لتصنيف المستندات النصية إلى فئات محددة مسبقًا تلقائيًا.\"? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'saltacc/anime-ai-detect', 'api_call': \"pipeline('image-classification', model='saltacc/anime-ai-detect')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': 'aibooru and imageboard sites', 'accuracy': '96%'}, 'description': 'A BEiT classifier to see if anime art was made by an AI or a human.' </s>"
"<s>[INST]كتابة قصة خيالية قصيرة تدور حول مجموعة من الروبوتات ذات الوظائف المختلفة وتوليد رسوم توضيحية لهم باستخدام تقنيات التصنيف الصوتي.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'swin-tiny-patch4-window7-224-bottom_cleaned_data', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224-bottom_cleaned_data')\", 'api_arguments': ['learning_rate', 'train_batch_size', 'eval_batch_size', 'seed', 'gradient_accumulation_steps', 'total_train_batch_size', 'optimizer', 'lr_scheduler_type', 'lr_scheduler_warmup_ratio', 'num_epochs'], 'python_environment_requirements': ['Transformers 4.28.1', 'Pytorch 2.0.0+cu118', 'Datasets 2.11.0', 'Tokenizers 0.13.3'], 'example_code': '', 'performance': {'dataset': 'imagefolder', 'accuracy': 0.9726}, 'description': 'This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset.' </s>"
"<s>[INST] تطوير نظام تحليل الصور السحابية وتصنيف السماء بناءً على درجات السطوع والألوان. استخدم API لتطبيق تقنيات التصنيف الصوري.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'swin-tiny-patch4-window7-224-bottom_cleaned_data', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224-bottom_cleaned_data')\", 'api_arguments': ['learning_rate', 'train_batch_size', 'eval_batch_size', 'seed', 'gradient_accumulation_steps', 'total_train_batch_size', 'optimizer', 'lr_scheduler_type', 'lr_scheduler_warmup_ratio', 'num_epochs'], 'python_environment_requirements': ['Transformers 4.28.1', 'Pytorch 2.0.0+cu118', 'Datasets 2.11.0', 'Tokenizers 0.13.3'], 'example_code': '', 'performance': {'dataset': 'imagefolder', 'accuracy': 0.9726}, 'description': 'This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset.' </s>"
"<s>[INST] إعداد موقع ويب لعرض مجموعة من الصور الخيالية الملونة بحيث تكون قابلة للتصنيف من خلال تقنيات التصنيف البصري.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'swin-tiny-patch4-window7-224-bottom_cleaned_data', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224-bottom_cleaned_data')\", 'api_arguments': ['learning_rate', 'train_batch_size', 'eval_batch_size', 'seed', 'gradient_accumulation_steps', 'total_train_batch_size', 'optimizer', 'lr_scheduler_type', 'lr_scheduler_warmup_ratio', 'num_epochs'], 'python_environment_requirements': ['Transformers 4.28.1', 'Pytorch 2.0.0+cu118', 'Datasets 2.11.0', 'Tokenizers 0.13.3'], 'example_code': '', 'performance': {'dataset': 'imagefolder', 'accuracy': 0.9726}, 'description': 'This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset.' </s>"
"<s>[INST] تنفيذ نظام تصنيف لصور الطبيعة والحياة البرية باستخدام تقنيات التحليل البصري.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'swin-tiny-patch4-window7-224-bottom_cleaned_data', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224-bottom_cleaned_data')\", 'api_arguments': ['learning_rate', 'train_batch_size', 'eval_batch_size', 'seed', 'gradient_accumulation_steps', 'total_train_batch_size', 'optimizer', 'lr_scheduler_type', 'lr_scheduler_warmup_ratio', 'num_epochs'], 'python_environment_requirements': ['Transformers 4.28.1', 'Pytorch 2.0.0+cu118', 'Datasets 2.11.0', 'Tokenizers 0.13.3'], 'example_code': '', 'performance': {'dataset': 'imagefolder', 'accuracy': 0.9726}, 'description': 'This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset.' </s>"
"<s>[INST] إنشاء بيئة افتراضية تفاعلية تسمح للمستخدمين بالتفاعل مع أشكال هندسية ثلاثية الأبعاد وتطبيق تحليل بصري لتحديد مواقع الأشكال.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'swin-tiny-patch4-window7-224-bottom_cleaned_data', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224-bottom_cleaned_data')\", 'api_arguments': ['learning_rate', 'train_batch_size', 'eval_batch_size', 'seed', 'gradient_accumulation_steps', 'total_train_batch_size', 'optimizer', 'lr_scheduler_type', 'lr_scheduler_warmup_ratio', 'num_epochs'], 'python_environment_requirements': ['Transformers 4.28.1', 'Pytorch 2.0.0+cu118', 'Datasets 2.11.0', 'Tokenizers 0.13.3'], 'example_code': '', 'performance': {'dataset': 'imagefolder', 'accuracy': 0.9726}, 'description': 'This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset.' </s>"
"<s>[INST] ابتكار لعبة تعليمية تعتمد على التعرف على الصور وطلب من اللاعبين تصنيف الصور بناءً على المضمون الذي يحتوي عليه.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'swin-tiny-patch4-window7-224-bottom_cleaned_data', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224-bottom_cleaned_data')\", 'api_arguments': ['learning_rate', 'train_batch_size', 'eval_batch_size', 'seed', 'gradient_accumulation_steps', 'total_train_batch_size', 'optimizer', 'lr_scheduler_type', 'lr_scheduler_warmup_ratio', 'num_epochs'], 'python_environment_requirements': ['Transformers 4.28.1', 'Pytorch 2.0.0+cu118', 'Datasets 2.11.0', 'Tokenizers 0.13.3'], 'example_code': '', 'performance': {'dataset': 'imagefolder', 'accuracy': 0.9726}, 'description': 'This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset.' </s>"
"<s>[INST] تصميم وتطوير تطبيق محمول يستخدم التقنيات البصرية لمساعدة المستخدمين على تحديد أنواع الأشجار والنباتات من خلال صورها.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'swin-tiny-patch4-window7-224-bottom_cleaned_data', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224-bottom_cleaned_data')\", 'api_arguments': ['learning_rate', 'train_batch_size', 'eval_batch_size', 'seed', 'gradient_accumulation_steps', 'total_train_batch_size', 'optimizer', 'lr_scheduler_type', 'lr_scheduler_warmup_ratio', 'num_epochs'], 'python_environment_requirements': ['Transformers 4.28.1', 'Pytorch 2.0.0+cu118', 'Datasets 2.11.0', 'Tokenizers 0.13.3'], 'example_code': '', 'performance': {'dataset': 'imagefolder', 'accuracy': 0.9726}, 'description': 'This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset.' </s>"
"<s>[INST] إنشاء نظام تحليل وتصنيف اللوحات الفنية باستخدام تقنيات التحليل البصري الذكي.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'swin-tiny-patch4-window7-224-bottom_cleaned_data', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224-bottom_cleaned_data')\", 'api_arguments': ['learning_rate', 'train_batch_size', 'eval_batch_size', 'seed', 'gradient_accumulation_steps', 'total_train_batch_size', 'optimizer', 'lr_scheduler_type', 'lr_scheduler_warmup_ratio', 'num_epochs'], 'python_environment_requirements': ['Transformers 4.28.1', 'Pytorch 2.0.0+cu118', 'Datasets 2.11.0', 'Tokenizers 0.13.3'], 'example_code': '', 'performance': {'dataset': 'imagefolder', 'accuracy': 0.9726}, 'description': 'This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset.' </s>"
"<s>[INST] تطوير نظام يستخدم التصنيف الصوري لتحديد الأجزاء التالفة في الآلات وتوفير تقارير تفصيلية حول الأضرار المكتشفة.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'swin-tiny-patch4-window7-224-bottom_cleaned_data', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224-bottom_cleaned_data')\", 'api_arguments': ['learning_rate', 'train_batch_size', 'eval_batch_size', 'seed', 'gradient_accumulation_steps', 'total_train_batch_size', 'optimizer', 'lr_scheduler_type', 'lr_scheduler_warmup_ratio', 'num_epochs'], 'python_environment_requirements': ['Transformers 4.28.1', 'Pytorch 2.0.0+cu118', 'Datasets 2.11.0', 'Tokenizers 0.13.3'], 'example_code': '', 'performance': {'dataset': 'imagefolder', 'accuracy': 0.9726}, 'description': 'This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset.' </s>"
"<s>[INST] إعداد نظام لتحليل وتصنيف العناصر الغذائية في الأطعمة باستخدام تقنيات التصنيف الصوري لمساعدة الأشخاص على اتباع نظام غذائي صحي.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'swin-tiny-patch4-window7-224-bottom_cleaned_data', 'api_call': \"AutoModelForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224-bottom_cleaned_data')\", 'api_arguments': ['learning_rate', 'train_batch_size', 'eval_batch_size', 'seed', 'gradient_accumulation_steps', 'total_train_batch_size', 'optimizer', 'lr_scheduler_type', 'lr_scheduler_warmup_ratio', 'num_epochs'], 'python_environment_requirements': ['Transformers 4.28.1', 'Pytorch 2.0.0+cu118', 'Datasets 2.11.0', 'Tokenizers 0.13.3'], 'example_code': '', 'performance': {'dataset': 'imagefolder', 'accuracy': 0.9726}, 'description': 'This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset.' </s>"
"<s>[INST]إنشاء تطبيق لتحليل نمط الكتابة اليدوية باستخدام ال API التالي لتحويل النص إلى صورة واستخدام تقنيات التعرف على الكتابة اليدوية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-structure-recognition', 'api_call': \"pipeline('object-detection', model='microsoft/table-transformer-structure-recognition')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'PubTables1M', 'accuracy': ''}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.' </s>"
"<s>[INST] قم بتطوير نظام استشعار ذكي لكشف العقبات في الطريق بواسطة ال API التالي للكشف عن الأشياء.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-structure-recognition', 'api_call': \"pipeline('object-detection', model='microsoft/table-transformer-structure-recognition')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'PubTables1M', 'accuracy': ''}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.' </s>"
"<s>[INST] قم بإنشاء نظام فحص تلقائي للأعطال في أجهزة الكمبيوتر باستخدام ال API التالي للكشف عن المشاكل.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-structure-recognition', 'api_call': \"pipeline('object-detection', model='microsoft/table-transformer-structure-recognition')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'PubTables1M', 'accuracy': ''}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.' </s>"
"<s>[INST] إنشاء نموذج تنبؤ لتحديد نوع الملابس من صورة فساتين باستخدام ال API التالي للكشف عن الأشياء.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-structure-recognition', 'api_call': \"pipeline('object-detection', model='microsoft/table-transformer-structure-recognition')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'PubTables1M', 'accuracy': ''}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.' </s>"
"<s>[INST] قم بتصميم نظام تعرف على الإشارات المرورية باستخدام ال API التالي للكشف عن الأشياء.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-structure-recognition', 'api_call': \"pipeline('object-detection', model='microsoft/table-transformer-structure-recognition')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'PubTables1M', 'accuracy': ''}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.' </s>"
"<s>[INST] أنشئ تطبيقًا لتحليل الملامح الوجهية وتحديد العوامل الوراثية باستخدام ال API التالي للكشف عن الوجوه.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-structure-recognition', 'api_call': \"pipeline('object-detection', model='microsoft/table-transformer-structure-recognition')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'PubTables1M', 'accuracy': ''}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.' </s>"
"<s>[INST] قم بتطوير نظام لتحديد تصنيف الأطعمة من الصور باستخدام ال API التالي للكشف عن الأشياء.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-structure-recognition', 'api_call': \"pipeline('object-detection', model='microsoft/table-transformer-structure-recognition')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'PubTables1M', 'accuracy': ''}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.' </s>"
"<s>[INST] إنشاء نظام للكشف الطبي الذاتي لتحديد الأعراض الصحية باستخدام ال API التالي للكشف عن المشاكل.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-structure-recognition', 'api_call': \"pipeline('object-detection', model='microsoft/table-transformer-structure-recognition')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'PubTables1M', 'accuracy': ''}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.' </s>"
"<s>[INST] أنشئ نظام لتحليل صور الطائرات الجوية وتحديد نوع الطائرة باستخدام ال API التالي للكشف عن الأشياء.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-structure-recognition', 'api_call': \"pipeline('object-detection', model='microsoft/table-transformer-structure-recognition')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'PubTables1M', 'accuracy': ''}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.' </s>"
"<s>[INST] قم بتطوير نظام لتحليل الصور الجوية وتحديد نقاط الاهتمام الجغرافية باستخدام ال API التالي للكشف عن الأشياء.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-structure-recognition', 'api_call': \"pipeline('object-detection', model='microsoft/table-transformer-structure-recognition')\", 'api_arguments': '', 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': 'PubTables1M', 'accuracy': ''}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.' </s>"
"<s>[INST]عند تطوير تطبيق لتحليل نصوص العملاء لتوجيهها إلى الأقسام المناسبة، يمكنك استخدام Google Natural Language API.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/regnet-y-008', 'api_call': \"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\", 'api_arguments': {'pretrained_model_name_or_path': 'zuppif/regnet-y-040'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor, RegNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoFeatureExtractor, RegNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\ninputs = feature_extractor(image, return_tensors=pt)\\nwith torch.no_grad():\\n... logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.' </s>"
"<s>[INST] لمعالجة الدفعات الواردة والخارجة بسرعة وكفاءة، يمكنك استخدام Stripe API.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/regnet-y-008', 'api_call': \"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\", 'api_arguments': {'pretrained_model_name_or_path': 'zuppif/regnet-y-040'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor, RegNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoFeatureExtractor, RegNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\ninputs = feature_extractor(image, return_tensors=pt)\\nwith torch.no_grad():\\n... logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.' </s>"
"<s>[INST] لإنشاء نظام لاستعراض وتصفية البيانات الكبيرة، يمكنك الاعتماد على Elasticsearch REST API.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/regnet-y-008', 'api_call': \"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\", 'api_arguments': {'pretrained_model_name_or_path': 'zuppif/regnet-y-040'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor, RegNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoFeatureExtractor, RegNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\ninputs = feature_extractor(image, return_tensors=pt)\\nwith torch.no_grad():\\n... logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.' </s>"
"<s>[INST] لتنفيذ تحليل نصوص واستخراج المفاهيم والشخصيات أو الشركات الرئيسية، يمكنك استخدام IBM Watson Natural Language Understanding API.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/regnet-y-008', 'api_call': \"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\", 'api_arguments': {'pretrained_model_name_or_path': 'zuppif/regnet-y-040'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor, RegNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoFeatureExtractor, RegNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\ninputs = feature_extractor(image, return_tensors=pt)\\nwith torch.no_grad():\\n... logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.' </s>"
"<s>[INST] عند بحثك عن API للتعرف على الصوت وتحويله إلى نص، يمكنك استخدام IBM Watson Speech to Text API.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/regnet-y-008', 'api_call': \"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\", 'api_arguments': {'pretrained_model_name_or_path': 'zuppif/regnet-y-040'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor, RegNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoFeatureExtractor, RegNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\ninputs = feature_extractor(image, return_tensors=pt)\\nwith torch.no_grad():\\n... logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.' </s>"
"<s>[INST] لتطوير نظام لتصنيف الوثائق بناءً على مضمونها، يمكنك استخدام OpenAI API.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/regnet-y-008', 'api_call': \"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\", 'api_arguments': {'pretrained_model_name_or_path': 'zuppif/regnet-y-040'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor, RegNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoFeatureExtractor, RegNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\ninputs = feature_extractor(image, return_tensors=pt)\\nwith torch.no_grad():\\n... logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.' </s>"
"<s>[INST] عند تطوير تطبيق يتيح للمستخدمين تحليل الصور واستخراج بيانات منها، يمكنك استخدام Google Cloud Vision API.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/regnet-y-008', 'api_call': \"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\", 'api_arguments': {'pretrained_model_name_or_path': 'zuppif/regnet-y-040'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor, RegNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoFeatureExtractor, RegNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\ninputs = feature_extractor(image, return_tensors=pt)\\nwith torch.no_grad():\\n... logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.' </s>"
"<s>[INST] لتحويل النصوص إلى كلام مقروء في تطبيقاتك، يمكنك استخدام Amazon Polly API.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/regnet-y-008', 'api_call': \"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\", 'api_arguments': {'pretrained_model_name_or_path': 'zuppif/regnet-y-040'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor, RegNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoFeatureExtractor, RegNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\ninputs = feature_extractor(image, return_tensors=pt)\\nwith torch.no_grad():\\n... logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.' </s>"
"<s>[INST] لإنشاء نظام لتحديد مواقع الأشخاص عبر تحليل بيانات الجغرافيا، يمكنك استخدام Mapbox API.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/regnet-y-008', 'api_call': \"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\", 'api_arguments': {'pretrained_model_name_or_path': 'zuppif/regnet-y-040'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor, RegNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoFeatureExtractor, RegNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\ninputs = feature_extractor(image, return_tensors=pt)\\nwith torch.no_grad():\\n... logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.' </s>"
"<s>[INST] عند تطوير نظام لتصنيف الصور الطبية، يمكنك استخدام API Clarifai لهذا الغرض.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'facebook/regnet-y-008', 'api_call': \"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\", 'api_arguments': {'pretrained_model_name_or_path': 'zuppif/regnet-y-040'}, 'python_environment_requirements': {'transformers': 'AutoFeatureExtractor, RegNetForImageClassification', 'torch': 'torch', 'datasets': 'load_dataset'}, 'example_code': 'from transformers import AutoFeatureExtractor, RegNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\ninputs = feature_extractor(image, return_tensors=pt)\\nwith torch.no_grad():\\n... logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])', 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.' </s>"
"<s>[INST]يهدف مشروعك لتنفيذ نظام تصنيف فني يعتمد على تحليل النصوص. تستخدم واجهة برمجية التالية لتحليل النصوص:? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'convnextv2_huge.fcmae_ft_in1k', 'api_call': \"timm.create_model('convnextv2_huge.fcmae_ft_in1k')\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': ['timm'], 'example_code': \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 86.256}, 'description': 'A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k.' </s>"
"<s>[INST] تطور تطبيقًا يستخدم تقنيات تحليل الصوت لتحويل تسجيل صوتي إلى نص. يمكنك الاعتماد على الواجهة البرمجية التالية:? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'convnextv2_huge.fcmae_ft_in1k', 'api_call': \"timm.create_model('convnextv2_huge.fcmae_ft_in1k')\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': ['timm'], 'example_code': \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 86.256}, 'description': 'A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k.' </s>"
"<s>[INST] تعتزم تطوير حلقة عمل تقوم بتحويل نصوص من اللغة الإنجليزية إلى اللغة العربية باستخدام تقنيات الترجمة الآلية. يمكنك البدء باستخدام الإعدادات التالية:? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'convnextv2_huge.fcmae_ft_in1k', 'api_call': \"timm.create_model('convnextv2_huge.fcmae_ft_in1k')\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': ['timm'], 'example_code': \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 86.256}, 'description': 'A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k.' </s>"
"<s>[INST]يرغب مهندس برمجيات في تطوير نظام ذكي لتحليل البيانات من الصور الطبية لتشخيص الأمراض. هل يمكنك استخدام إطار عمل Hugging Face Transformers؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification, Feature Map Extraction, Image Embeddings', 'api_name': 'convnext_base.fb_in1k', 'api_call': \"timm.create_model('convnext_base.fb_in1k')\", 'api_arguments': {'pretrained': 'True', 'features_only': 'True', 'num_classes': '0'}, 'python_environment_requirements': ['timm'], 'example_code': ['from urllib.request import urlopen', 'from PIL import Image', 'import timm', \"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\", \"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\", 'model = model.eval()', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'output = model(transforms(img).unsqueeze(0))'], 'performance': {'dataset': 'imagenet-1k', 'accuracy': '83.82%'}, 'description': 'A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.' </s>"
"<s>[INST] كتب كاتب رواية خيالية تبدأ بجملة \"في عالم بعيد\". هل يمكنك استخدام تعلم الآلة لتحليل نمط الكتابة؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification, Feature Map Extraction, Image Embeddings', 'api_name': 'convnext_base.fb_in1k', 'api_call': \"timm.create_model('convnext_base.fb_in1k')\", 'api_arguments': {'pretrained': 'True', 'features_only': 'True', 'num_classes': '0'}, 'python_environment_requirements': ['timm'], 'example_code': ['from urllib.request import urlopen', 'from PIL import Image', 'import timm', \"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\", \"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\", 'model = model.eval()', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'output = model(transforms(img).unsqueeze(0))'], 'performance': {'dataset': 'imagenet-1k', 'accuracy': '83.82%'}, 'description': 'A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.' </s>"
"<s>[INST] تريد شركة الطيران تطوير نظام معالجة الصور لفحص المحركات الطائرات. يمكنك مساعدتهم باستخدام إطار عمل OpenCV؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification, Feature Map Extraction, Image Embeddings', 'api_name': 'convnext_base.fb_in1k', 'api_call': \"timm.create_model('convnext_base.fb_in1k')\", 'api_arguments': {'pretrained': 'True', 'features_only': 'True', 'num_classes': '0'}, 'python_environment_requirements': ['timm'], 'example_code': ['from urllib.request import urlopen', 'from PIL import Image', 'import timm', \"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\", \"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\", 'model = model.eval()', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'output = model(transforms(img).unsqueeze(0))'], 'performance': {'dataset': 'imagenet-1k', 'accuracy': '83.82%'}, 'description': 'A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.' </s>"
"<s>[INST] يرغب الباحث العلمي في تدريب نموذج تعلم آلي للتعرف على أنواع مختلفة من الأزهار. هل يمكنك استخدام مكتبة TensorFlow؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification, Feature Map Extraction, Image Embeddings', 'api_name': 'convnext_base.fb_in1k', 'api_call': \"timm.create_model('convnext_base.fb_in1k')\", 'api_arguments': {'pretrained': 'True', 'features_only': 'True', 'num_classes': '0'}, 'python_environment_requirements': ['timm'], 'example_code': ['from urllib.request import urlopen', 'from PIL import Image', 'import timm', \"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\", \"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\", 'model = model.eval()', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'output = model(transforms(img).unsqueeze(0))'], 'performance': {'dataset': 'imagenet-1k', 'accuracy': '83.82%'}, 'description': 'A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.' </s>"
"<s>[INST] نريد تطوير نظام لتحليل شخصيات الروبوتات في الأفلام والمسلسلات. كيف يمكن استخدام مكتبة PyTorch؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification, Feature Map Extraction, Image Embeddings', 'api_name': 'convnext_base.fb_in1k', 'api_call': \"timm.create_model('convnext_base.fb_in1k')\", 'api_arguments': {'pretrained': 'True', 'features_only': 'True', 'num_classes': '0'}, 'python_environment_requirements': ['timm'], 'example_code': ['from urllib.request import urlopen', 'from PIL import Image', 'import timm', \"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\", \"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\", 'model = model.eval()', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'output = model(transforms(img).unsqueeze(0))'], 'performance': {'dataset': 'imagenet-1k', 'accuracy': '83.82%'}, 'description': 'A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.' </s>"
"<s>[INST] يرغب الفنان في استخدام تقنيات الذكاء الاصطناعي لتوليف موسيقى جديدة. كيف يمكن تحقيق ذلك باستخدام مكتبة LibROSA؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification, Feature Map Extraction, Image Embeddings', 'api_name': 'convnext_base.fb_in1k', 'api_call': \"timm.create_model('convnext_base.fb_in1k')\", 'api_arguments': {'pretrained': 'True', 'features_only': 'True', 'num_classes': '0'}, 'python_environment_requirements': ['timm'], 'example_code': ['from urllib.request import urlopen', 'from PIL import Image', 'import timm', \"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\", \"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\", 'model = model.eval()', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'output = model(transforms(img).unsqueeze(0))'], 'performance': {'dataset': 'imagenet-1k', 'accuracy': '83.82%'}, 'description': 'A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.' </s>"
"<s>[INST] تريد الشركة الناشئة تطوير نظام لتحليل سلوك المستهلكين عبر الإنترنت. هل يمكن استخدام نموذج تعلم آلي لهذا الغرض باستخدام مكتبة Scikit-learn؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification, Feature Map Extraction, Image Embeddings', 'api_name': 'convnext_base.fb_in1k', 'api_call': \"timm.create_model('convnext_base.fb_in1k')\", 'api_arguments': {'pretrained': 'True', 'features_only': 'True', 'num_classes': '0'}, 'python_environment_requirements': ['timm'], 'example_code': ['from urllib.request import urlopen', 'from PIL import Image', 'import timm', \"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\", \"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\", 'model = model.eval()', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'output = model(transforms(img).unsqueeze(0))'], 'performance': {'dataset': 'imagenet-1k', 'accuracy': '83.82%'}, 'description': 'A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.' </s>"
"<s>[INST] أنشئ تصميمًا جرافيكيًا مميزًا يحتوي على نمط أنيمي باستخدام تقنيات معالجة الصور. كيف يمكن تطبيق هذا باستخدام إطار عمل PIL (Python Imaging Library)؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification, Feature Map Extraction, Image Embeddings', 'api_name': 'convnext_base.fb_in1k', 'api_call': \"timm.create_model('convnext_base.fb_in1k')\", 'api_arguments': {'pretrained': 'True', 'features_only': 'True', 'num_classes': '0'}, 'python_environment_requirements': ['timm'], 'example_code': ['from urllib.request import urlopen', 'from PIL import Image', 'import timm', \"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\", \"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\", 'model = model.eval()', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'output = model(transforms(img).unsqueeze(0))'], 'performance': {'dataset': 'imagenet-1k', 'accuracy': '83.82%'}, 'description': 'A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.' </s>"
"<s>[INST] تريد مدرسة تحفيز الطلاب على القراءة باستخدام تطبيق الهاتف المحمول. يمكنك مساعدتهم في تحقيق هذا باستخدام خدمات Firebase؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification, Feature Map Extraction, Image Embeddings', 'api_name': 'convnext_base.fb_in1k', 'api_call': \"timm.create_model('convnext_base.fb_in1k')\", 'api_arguments': {'pretrained': 'True', 'features_only': 'True', 'num_classes': '0'}, 'python_environment_requirements': ['timm'], 'example_code': ['from urllib.request import urlopen', 'from PIL import Image', 'import timm', \"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\", \"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\", 'model = model.eval()', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'output = model(transforms(img).unsqueeze(0))'], 'performance': {'dataset': 'imagenet-1k', 'accuracy': '83.82%'}, 'description': 'A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.' </s>"
"<s>[INST] يرغب الباحث في تحليل سلوك الأشخاص على وسائل التواصل الاجتماعي باستخدام تقنيات التعلم الآلي. هل يمكنك استخدام مكتبة SpaCy لتحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification, Feature Map Extraction, Image Embeddings', 'api_name': 'convnext_base.fb_in1k', 'api_call': \"timm.create_model('convnext_base.fb_in1k')\", 'api_arguments': {'pretrained': 'True', 'features_only': 'True', 'num_classes': '0'}, 'python_environment_requirements': ['timm'], 'example_code': ['from urllib.request import urlopen', 'from PIL import Image', 'import timm', \"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\", \"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\", 'model = model.eval()', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'output = model(transforms(img).unsqueeze(0))'], 'performance': {'dataset': 'imagenet-1k', 'accuracy': '83.82%'}, 'description': 'A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.' </s>"
"<s>[INST]لتحليل البيانات واستخراج الرؤى القيمة، يمكنك استخدام واجهة برمجة التطبيقات التالية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/mobilenetv3_large_100.ra_in1k', 'api_call': \"timm.create_model('mobilenetv3_large_100.ra_in1k')\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': {'timm': 'latest'}, 'example_code': \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen(\\n 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\\n))\\nmodel = timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup.' </s>"
"<s>[INST] لإضافة خاصية البحث الذكي إلى تطبيقك، يمكنك استخدام واجهة برمجة تطبيقات خاصة بمحرك البحث.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/mobilenetv3_large_100.ra_in1k', 'api_call': \"timm.create_model('mobilenetv3_large_100.ra_in1k')\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': {'timm': 'latest'}, 'example_code': \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen(\\n 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\\n))\\nmodel = timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup.' </s>"
"<s>[INST] لتحسين تفاعل مستخدمي تطبيق الهاتف الذكي، يمكنك استخدام واجهة برمجة التطبيقات التالية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/mobilenetv3_large_100.ra_in1k', 'api_call': \"timm.create_model('mobilenetv3_large_100.ra_in1k')\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': {'timm': 'latest'}, 'example_code': \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen(\\n 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\\n))\\nmodel = timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup.' </s>"
"<s>[INST] لتخزين وإدارة البليانات بشكل فعال، يمكنك استخدام واجهة برمجة بيانات قاعدة البيانات التالية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/mobilenetv3_large_100.ra_in1k', 'api_call': \"timm.create_model('mobilenetv3_large_100.ra_in1k')\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': {'timm': 'latest'}, 'example_code': \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen(\\n 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\\n))\\nmodel = timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup.' </s>"
"<s>[INST] لفحص أمان تطبيق الويب الخاص بك، يمكنك استخدام واجهة برمجة التطبيقات التالية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/mobilenetv3_large_100.ra_in1k', 'api_call': \"timm.create_model('mobilenetv3_large_100.ra_in1k')\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': {'timm': 'latest'}, 'example_code': \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen(\\n 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\\n))\\nmodel = timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup.' </s>"
"<s>[INST] لإختبار وتنظيم عمليات تسويق الكتروني، يمكنك استخدام واجهة برمجة التطبيقات التالية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/mobilenetv3_large_100.ra_in1k', 'api_call': \"timm.create_model('mobilenetv3_large_100.ra_in1k')\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': {'timm': 'latest'}, 'example_code': \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen(\\n 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\\n))\\nmodel = timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup.' </s>"
"<s>[INST] لتنفيذ عمليات معالجة اللغات الطبيعية البسيطة، يمكنك استخدام واجهة برمجة التطبيقات التالية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/mobilenetv3_large_100.ra_in1k', 'api_call': \"timm.create_model('mobilenetv3_large_100.ra_in1k')\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': {'timm': 'latest'}, 'example_code': \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen(\\n 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\\n))\\nmodel = timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup.' </s>"
"<s>[INST] لتنظيم ومتابعة البيانات المالية، يمكنك استخدام واجهة برمجة التطبيقات التالية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/mobilenetv3_large_100.ra_in1k', 'api_call': \"timm.create_model('mobilenetv3_large_100.ra_in1k')\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': {'timm': 'latest'}, 'example_code': \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen(\\n 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\\n))\\nmodel = timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup.' </s>"
"<s>[INST] لتنفيذ عمليات التشفير وفك الشفرة، يمكنك استخدام واجهة برمجة التطبيقات التالية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/mobilenetv3_large_100.ra_in1k', 'api_call': \"timm.create_model('mobilenetv3_large_100.ra_in1k')\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': {'timm': 'latest'}, 'example_code': \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen(\\n 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\\n))\\nmodel = timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup.' </s>"
"<s>[INST] لإضافة خاصية إعادة البيانات إلى نص، يمكنك استخدام واجهة برمجة التطبيقات التالية.? [/INST]'domain': 'Computer Vision Image Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Classification', 'api_name': 'timm/mobilenetv3_large_100.ra_in1k', 'api_call': \"timm.create_model('mobilenetv3_large_100.ra_in1k')\", 'api_arguments': {'pretrained': 'True'}, 'python_environment_requirements': {'timm': 'latest'}, 'example_code': \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen(\\n 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\\n))\\nmodel = timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\", 'performance': {'dataset': 'imagenet-1k', 'accuracy': 'Not provided'}, 'description': 'A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup.' </s>"
"<s>[INST]تحتاج إلى إنشاء نظام قادر على تحليل النصوص واستخراج المعلومات الرئيسية منها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-detection', 'api_call': \"TableTransformerDetrModel.from_pretrained('microsoft/table-transformer-detection')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; table_detector = pipeline('object-detection', model='microsoft/table-transformer-detection'); results = table_detector(image)\", 'performance': {'dataset': 'PubTables1M', 'accuracy': 'Not provided'}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.' </s>"
"<s>[INST] نحتاج لإنشاء نظام قادر على تحليل نمط الكتابة اليدوية وتحويله إلى نص قابل للتحرير.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-detection', 'api_call': \"TableTransformerDetrModel.from_pretrained('microsoft/table-transformer-detection')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; table_detector = pipeline('object-detection', model='microsoft/table-transformer-detection'); results = table_detector(image)\", 'performance': {'dataset': 'PubTables1M', 'accuracy': 'Not provided'}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.' </s>"
"<s>[INST] تصميم نظام قادر على تحليل وتصنيف المقالات الطبية باستخدام تقنيات تعلم الآلة ومعالجة اللغة الطبيعية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-detection', 'api_call': \"TableTransformerDetrModel.from_pretrained('microsoft/table-transformer-detection')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; table_detector = pipeline('object-detection', model='microsoft/table-transformer-detection'); results = table_detector(image)\", 'performance': {'dataset': 'PubTables1M', 'accuracy': 'Not provided'}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.' </s>"
"<s>[INST] تحتاج لإنشاء نظام قادر على توليد وتحليل عبارات استفسار عن البيانات المكتسبة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-detection', 'api_call': \"TableTransformerDetrModel.from_pretrained('microsoft/table-transformer-detection')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; table_detector = pipeline('object-detection', model='microsoft/table-transformer-detection'); results = table_detector(image)\", 'performance': {'dataset': 'PubTables1M', 'accuracy': 'Not provided'}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.' </s>"
"<s>[INST] نريد تصميم نظامًا يتيح استخدام تحليل المشاعر في مواقع التواصل الاجتماعي لفهم ردود الفعل والمشاعر للنصوص.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-detection', 'api_call': \"TableTransformerDetrModel.from_pretrained('microsoft/table-transformer-detection')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; table_detector = pipeline('object-detection', model='microsoft/table-transformer-detection'); results = table_detector(image)\", 'performance': {'dataset': 'PubTables1M', 'accuracy': 'Not provided'}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.' </s>"
"<s>[INST] إنشاء نظام قادر على تحليل وتحويل الصوت إلى نص باللغة العربية باستخدام تقنيات تحويل النطق إلى نص.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-detection', 'api_call': \"TableTransformerDetrModel.from_pretrained('microsoft/table-transformer-detection')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; table_detector = pipeline('object-detection', model='microsoft/table-transformer-detection'); results = table_detector(image)\", 'performance': {'dataset': 'PubTables1M', 'accuracy': 'Not provided'}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.' </s>"
"<s>[INST] نريد تصميم نظامًا قادرًا على تحليل شبكات الكلام واستخراج العلاقات والمعلومات الرئيسية من النصوص المنطوقة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-detection', 'api_call': \"TableTransformerDetrModel.from_pretrained('microsoft/table-transformer-detection')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; table_detector = pipeline('object-detection', model='microsoft/table-transformer-detection'); results = table_detector(image)\", 'performance': {'dataset': 'PubTables1M', 'accuracy': 'Not provided'}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.' </s>"
"<s>[INST] تحتاج لإنشاء نظام يستخدم تقنيات تحليل الصور لتقدير العناصر البصرية الرئيسية وتصنيفها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-detection', 'api_call': \"TableTransformerDetrModel.from_pretrained('microsoft/table-transformer-detection')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; table_detector = pipeline('object-detection', model='microsoft/table-transformer-detection'); results = table_detector(image)\", 'performance': {'dataset': 'PubTables1M', 'accuracy': 'Not provided'}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.' </s>"
"<s>[INST] نريد تصميم نظاماً يقوم بتحليل وتصنيف الرسائل البريدية والتعرف على النمط اللغوي والموضوع الرئيسي.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-detection', 'api_call': \"TableTransformerDetrModel.from_pretrained('microsoft/table-transformer-detection')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; table_detector = pipeline('object-detection', model='microsoft/table-transformer-detection'); results = table_detector(image)\", 'performance': {'dataset': 'PubTables1M', 'accuracy': 'Not provided'}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.' </s>"
"<s>[INST] تحتاج لإنشاء نظام قادر على تحليل التغريدات عبر وسائل التواصل الاجتماعي لفهم الاتجاهات والمشاعر العامة المعبر عنها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'microsoft/table-transformer-detection', 'api_call': \"TableTransformerDetrModel.from_pretrained('microsoft/table-transformer-detection')\", 'api_arguments': 'image', 'python_environment_requirements': 'transformers', 'example_code': \"from transformers import pipeline; table_detector = pipeline('object-detection', model='microsoft/table-transformer-detection'); results = table_detector(image)\", 'performance': {'dataset': 'PubTables1M', 'accuracy': 'Not provided'}, 'description': 'Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.' </s>"
"<s>[INST]كيف يمكنني استخدام API للتحليل التلقائي للنصوص وتحويلها إلى بيانات كمية؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-50', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\", 'api_arguments': {'pretrained_model_name': 'facebook/detr-resnet-50'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-50)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-50)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '42.0 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحليل وتصنيف الصور بناءً على محتواها؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-50', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\", 'api_arguments': {'pretrained_model_name': 'facebook/detr-resnet-50'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-50)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-50)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '42.0 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحليل وفهم لغة الإشارة في الصور؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-50', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\", 'api_arguments': {'pretrained_model_name': 'facebook/detr-resnet-50'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-50)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-50)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '42.0 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] كيف يمكنني تطبيق تحليل غرف العمليات الطبية باستخدام API؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-50', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\", 'api_arguments': {'pretrained_model_name': 'facebook/detr-resnet-50'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-50)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-50)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '42.0 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] كيف يمكنني تحليل الموجات الصوتية باستخدام API وتحويلها إلى نص؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-50', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\", 'api_arguments': {'pretrained_model_name': 'facebook/detr-resnet-50'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-50)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-50)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '42.0 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتقدير سن الأشخاص من الصور؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-50', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\", 'api_arguments': {'pretrained_model_name': 'facebook/detr-resnet-50'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-50)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-50)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '42.0 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] كيف يمكنني تحليل نقاط الضعف في الكود باستخدام API؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-50', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\", 'api_arguments': {'pretrained_model_name': 'facebook/detr-resnet-50'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-50)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-50)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '42.0 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتوليد نصوص إبداعية من الصور؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-50', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\", 'api_arguments': {'pretrained_model_name': 'facebook/detr-resnet-50'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-50)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-50)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '42.0 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] كيف يمكنني تطبيق تحليل وتصنيف النصوص الطبية باستخدام API؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-50', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\", 'api_arguments': {'pretrained_model_name': 'facebook/detr-resnet-50'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-50)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-50)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '42.0 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحليل الشعور في النصوص وتصنيفها إلى إيجابية أو سلبية؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-50', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\", 'api_arguments': {'pretrained_model_name': 'facebook/detr-resnet-50'}, 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-50)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-50)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '42.0 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST]تطوير تطبيق يقوم بتحليل النصوص العربية واستخراج المعلومات الهامة منها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-tiny', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '28.7 AP'}, 'description': 'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.' </s>"
"<s>[INST] تصميم تطبيق لتحويل النصوص العربية إلى نصوص صوتية فائقة الجودة لدعم فصل القراءة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-tiny', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '28.7 AP'}, 'description': 'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.' </s>"
"<s>[INST] إنشاء تطبيق يتيح للمستخدمين تحليل الصور الطبية لتشخيص الأمراض بدقة عالية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-tiny', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '28.7 AP'}, 'description': 'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.' </s>"
"<s>[INST] تطوير نظام يستخدم تقنيات الذكاء الاصطناعي لترجمة النصوص العربية إلى لغات متعددة بدقة عالية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-tiny', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '28.7 AP'}, 'description': 'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.' </s>"
"<s>[INST] تصميم تطبيق يستخدم تقنيات التعلم الآلي لتحليل نمط الخطوط العربية للتعرف على الكتابة اليدوية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-tiny', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '28.7 AP'}, 'description': 'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.' </s>"
"<s>[INST] تطوير تطبيق يستخدم تقنيات الذكاء الاصطناعي لتوليد نصوص أدبية باللغة العربية بناءً على مدخلات المستخدم.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-tiny', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '28.7 AP'}, 'description': 'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.' </s>"
"<s>[INST] تصميم نظام يستخدم تقنيات الذكاء الاصطناعي لتحليل الموضوعات والانطباعات في النصوص الاجتماعية العربية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-tiny', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '28.7 AP'}, 'description': 'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.' </s>"
"<s>[INST] إعداد تطبيق باستخدام تقنيات الذكاء الاصطناعي للتعرف على الوجوه العربية وتصنيفها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-tiny', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '28.7 AP'}, 'description': 'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.' </s>"
"<s>[INST] بناء تطبيق يستخدم تقنيات التعلم الآلي لفهم النصوص العربية واستخراج المفاهيم الرئيسية منها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-tiny', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '28.7 AP'}, 'description': 'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.' </s>"
"<s>[INST] تصميم تطبيق يستخدم تقنيات الذكاء الاصطناعي للكشف عن الاحتيال في العمليات المالية العربية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-tiny', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '28.7 AP'}, 'description': 'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.' </s>"
"<s>[INST]قم بتطوير تطبيق يستخدم API لتحليل النصوص الطبية العربية واستخراج المعلومات الصحية الضرورية للمستخدمين.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-101)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-101)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017', 'accuracy': '43.5 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] إنشاء نظام يستخدم API لتحليل الموسيقى والتعرف على الأغاني وفنانيها من التسجيلات الصوتية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-101)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-101)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017', 'accuracy': '43.5 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] تطوير تطبيق يستخدم API للبحث واقتراح الوجبات الغذائية الصحية وفقًا لاحتياجات المستخدم.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-101)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-101)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017', 'accuracy': '43.5 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] انشاء تطبيق يستخدم API لتقديم توصيات لأفلام ومسلسلات جديدة وشهيرة وفقًا لاهتمامات المستخدم.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-101)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-101)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017', 'accuracy': '43.5 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] تطوير تطبيق يستخدم API لمراقبة وتحليل الأداء الرياضي للاعبي كرة القدم على مدى فترة زمنية محددة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-101)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-101)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017', 'accuracy': '43.5 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] إنشاء نظام يستخدم API لتحليل السلوك الاجتماعي عبر وسائل التواصل الاجتماعي وتوليد تقارير حول الاتجاهات والتفاعلات.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-101)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-101)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017', 'accuracy': '43.5 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] انشاء تطبيق يستخدم API لتحليل البيانات الجغرافية وتوفير معلومات حول المواقع والمسارات للمستخدمين.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-101)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-101)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017', 'accuracy': '43.5 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] تطوير تطبيق يستخدم API لتقديم النصائح والمقالات الصحية الشخصية وفقًا لاحتياجات وأهداف المستخدمين.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-101)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-101)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017', 'accuracy': '43.5 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم API لتوفير المعلومات والإحصائيات حول الأحداث العالمية والتطورات السياسية من مصادر موثوقة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-101)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-101)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017', 'accuracy': '43.5 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST] تطوير تطبيق يستخدم API لتحليل السلوك عبر الإنترنت وتقديم توصيات تسويقية مستهدفة للمستخدمين.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-101)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-101)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)', 'performance': {'dataset': 'COCO 2017', 'accuracy': '43.5 AP'}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST]يجب على المستخدم تحليل صورة للأطعمة وتوجيه النموذج للكشف عن المكونات الغذائية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-base-patch32', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32')\", 'api_arguments': {'texts': 'List of text queries', 'images': 'Image to be processed'}, 'python_environment_requirements': 'transformers', 'example_code': 'import requests\\nfrom PIL import Image\\nimport torch\\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\\nprocessor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch32)\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch32)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [[a photo of a cat, a photo of a dog]]\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'performance': {'dataset': 'COCO and OpenImages', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.' </s>"
"<s>[INST] قم بتحليل صورة لسيارة والتعرف على العلامة التجارية والموديل.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-base-patch32', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32')\", 'api_arguments': {'texts': 'List of text queries', 'images': 'Image to be processed'}, 'python_environment_requirements': 'transformers', 'example_code': 'import requests\\nfrom PIL import Image\\nimport torch\\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\\nprocessor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch32)\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch32)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [[a photo of a cat, a photo of a dog]]\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'performance': {'dataset': 'COCO and OpenImages', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.' </s>"
"<s>[INST] استخدم النموذج لتحديد نوع الحيوان في صورة معينة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-base-patch32', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32')\", 'api_arguments': {'texts': 'List of text queries', 'images': 'Image to be processed'}, 'python_environment_requirements': 'transformers', 'example_code': 'import requests\\nfrom PIL import Image\\nimport torch\\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\\nprocessor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch32)\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch32)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [[a photo of a cat, a photo of a dog]]\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'performance': {'dataset': 'COCO and OpenImages', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.' </s>"
"<s>[INST] قم بإدخال صورة يحتوي على عدة أشخاص واحصل على تحليل يدل على نشاطاتهم المحتملة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-base-patch32', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32')\", 'api_arguments': {'texts': 'List of text queries', 'images': 'Image to be processed'}, 'python_environment_requirements': 'transformers', 'example_code': 'import requests\\nfrom PIL import Image\\nimport torch\\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\\nprocessor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch32)\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch32)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [[a photo of a cat, a photo of a dog]]\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'performance': {'dataset': 'COCO and OpenImages', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.' </s>"
"<s>[INST] تحليل النص المكتوب على لوحة سيارة والكشف عن الرقم التسلسلي.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-base-patch32', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32')\", 'api_arguments': {'texts': 'List of text queries', 'images': 'Image to be processed'}, 'python_environment_requirements': 'transformers', 'example_code': 'import requests\\nfrom PIL import Image\\nimport torch\\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\\nprocessor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch32)\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch32)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [[a photo of a cat, a photo of a dog]]\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'performance': {'dataset': 'COCO and OpenImages', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.' </s>"
"<s>[INST] احصل على تحليل لصورة تحتوي على منظر طبيعي واستخراج التفاصيل المتعلقة بالأشجار والنباتات.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-base-patch32', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32')\", 'api_arguments': {'texts': 'List of text queries', 'images': 'Image to be processed'}, 'python_environment_requirements': 'transformers', 'example_code': 'import requests\\nfrom PIL import Image\\nimport torch\\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\\nprocessor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch32)\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch32)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [[a photo of a cat, a photo of a dog]]\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'performance': {'dataset': 'COCO and OpenImages', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.' </s>"
"<s>[INST] قم بتحليل صورة لمبنى وتوضيح أهم المعالم المرئية المتواجدة به.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-base-patch32', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32')\", 'api_arguments': {'texts': 'List of text queries', 'images': 'Image to be processed'}, 'python_environment_requirements': 'transformers', 'example_code': 'import requests\\nfrom PIL import Image\\nimport torch\\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\\nprocessor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch32)\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch32)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [[a photo of a cat, a photo of a dog]]\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'performance': {'dataset': 'COCO and OpenImages', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.' </s>"
"<s>[INST] استخدم النموذج لتحليل صورة لمأكولات وتقديم تفاصيل عن المكونات الرئيسية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-base-patch32', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32')\", 'api_arguments': {'texts': 'List of text queries', 'images': 'Image to be processed'}, 'python_environment_requirements': 'transformers', 'example_code': 'import requests\\nfrom PIL import Image\\nimport torch\\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\\nprocessor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch32)\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch32)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [[a photo of a cat, a photo of a dog]]\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'performance': {'dataset': 'COCO and OpenImages', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.' </s>"
"<s>[INST] قم بتحليل صورة لأثاث منزلي واستخراج التفاصيل حول الأنماط والألوان المستخدمة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-base-patch32', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32')\", 'api_arguments': {'texts': 'List of text queries', 'images': 'Image to be processed'}, 'python_environment_requirements': 'transformers', 'example_code': 'import requests\\nfrom PIL import Image\\nimport torch\\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\\nprocessor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch32)\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch32)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [[a photo of a cat, a photo of a dog]]\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'performance': {'dataset': 'COCO and OpenImages', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.' </s>"
"<s>[INST] قم بتحليل صورة لحديقة وتحديد أنواع النباتات والزهور المعروضة فيها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-base-patch32', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32')\", 'api_arguments': {'texts': 'List of text queries', 'images': 'Image to be processed'}, 'python_environment_requirements': 'transformers', 'example_code': 'import requests\\nfrom PIL import Image\\nimport torch\\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\\nprocessor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch32)\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch32)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [[a photo of a cat, a photo of a dog]]\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'performance': {'dataset': 'COCO and OpenImages', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.' </s>"
"<s>[INST]تطوير نظام تعرف على الوجوه باستخدام تقنيات التعرف الضوئي على وجوه الأشخاص.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Detect Bordered and Borderless tables in documents', 'api_name': 'TahaDouaji/detr-doc-table-detection', 'api_call': \"DetrForObjectDetection.from_pretrained('TahaDouaji/detr-doc-table-detection')\", 'api_arguments': ['images', 'return_tensors', 'threshold'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nimage = Image.open(IMAGE_PATH)\\nprocessor = DetrImageProcessor.from_pretrained(TahaDouaji/detr-doc-table-detection)\\nmodel = DetrForObjectDetection.from_pretrained(TahaDouaji/detr-doc-table-detection)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.tensor([image.size[::-1]])\\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\\nfor score, label, box in zip(results[scores], results[labels], results[boxes]):\\n box = [round(i, 2) for i in box.tolist()]\\n print(\\n fDetected {model.config.id2label[label.item()]} with confidence \\n f{round(score.item(), 3)} at location {box}\\n )', 'performance': {'dataset': 'ICDAR2019 Table Dataset', 'accuracy': 'Not provided'}, 'description': 'detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم تقنيات التعلم الآلي للتنبؤ بأعطال معدات الصيانة الصناعية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Detect Bordered and Borderless tables in documents', 'api_name': 'TahaDouaji/detr-doc-table-detection', 'api_call': \"DetrForObjectDetection.from_pretrained('TahaDouaji/detr-doc-table-detection')\", 'api_arguments': ['images', 'return_tensors', 'threshold'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nimage = Image.open(IMAGE_PATH)\\nprocessor = DetrImageProcessor.from_pretrained(TahaDouaji/detr-doc-table-detection)\\nmodel = DetrForObjectDetection.from_pretrained(TahaDouaji/detr-doc-table-detection)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.tensor([image.size[::-1]])\\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\\nfor score, label, box in zip(results[scores], results[labels], results[boxes]):\\n box = [round(i, 2) for i in box.tolist()]\\n print(\\n fDetected {model.config.id2label[label.item()]} with confidence \\n f{round(score.item(), 3)} at location {box}\\n )', 'performance': {'dataset': 'ICDAR2019 Table Dataset', 'accuracy': 'Not provided'}, 'description': 'detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.' </s>"
"<s>[INST] تطوير نظام يستخدم شبكات عصبية لتصنيف أنماط الأمراض الجلدية من الصور الطبية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Detect Bordered and Borderless tables in documents', 'api_name': 'TahaDouaji/detr-doc-table-detection', 'api_call': \"DetrForObjectDetection.from_pretrained('TahaDouaji/detr-doc-table-detection')\", 'api_arguments': ['images', 'return_tensors', 'threshold'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nimage = Image.open(IMAGE_PATH)\\nprocessor = DetrImageProcessor.from_pretrained(TahaDouaji/detr-doc-table-detection)\\nmodel = DetrForObjectDetection.from_pretrained(TahaDouaji/detr-doc-table-detection)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.tensor([image.size[::-1]])\\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\\nfor score, label, box in zip(results[scores], results[labels], results[boxes]):\\n box = [round(i, 2) for i in box.tolist()]\\n print(\\n fDetected {model.config.id2label[label.item()]} with confidence \\n f{round(score.item(), 3)} at location {box}\\n )', 'performance': {'dataset': 'ICDAR2019 Table Dataset', 'accuracy': 'Not provided'}, 'description': 'detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.' </s>"
"<s>[INST] بناء نظام يستخدم تقنيات التحليل الصوتي لتحديد أوامر المستخدم من الأصوات المسموعة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Detect Bordered and Borderless tables in documents', 'api_name': 'TahaDouaji/detr-doc-table-detection', 'api_call': \"DetrForObjectDetection.from_pretrained('TahaDouaji/detr-doc-table-detection')\", 'api_arguments': ['images', 'return_tensors', 'threshold'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nimage = Image.open(IMAGE_PATH)\\nprocessor = DetrImageProcessor.from_pretrained(TahaDouaji/detr-doc-table-detection)\\nmodel = DetrForObjectDetection.from_pretrained(TahaDouaji/detr-doc-table-detection)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.tensor([image.size[::-1]])\\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\\nfor score, label, box in zip(results[scores], results[labels], results[boxes]):\\n box = [round(i, 2) for i in box.tolist()]\\n print(\\n fDetected {model.config.id2label[label.item()]} with confidence \\n f{round(score.item(), 3)} at location {box}\\n )', 'performance': {'dataset': 'ICDAR2019 Table Dataset', 'accuracy': 'Not provided'}, 'description': 'detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.' </s>"
"<s>[INST] تصميم تطبيق يستخدم تقنيات التعلم الآلي للتنبؤ بأداء الطلاب في الامتحانات الدراسية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Detect Bordered and Borderless tables in documents', 'api_name': 'TahaDouaji/detr-doc-table-detection', 'api_call': \"DetrForObjectDetection.from_pretrained('TahaDouaji/detr-doc-table-detection')\", 'api_arguments': ['images', 'return_tensors', 'threshold'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nimage = Image.open(IMAGE_PATH)\\nprocessor = DetrImageProcessor.from_pretrained(TahaDouaji/detr-doc-table-detection)\\nmodel = DetrForObjectDetection.from_pretrained(TahaDouaji/detr-doc-table-detection)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.tensor([image.size[::-1]])\\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\\nfor score, label, box in zip(results[scores], results[labels], results[boxes]):\\n box = [round(i, 2) for i in box.tolist()]\\n print(\\n fDetected {model.config.id2label[label.item()]} with confidence \\n f{round(score.item(), 3)} at location {box}\\n )', 'performance': {'dataset': 'ICDAR2019 Table Dataset', 'accuracy': 'Not provided'}, 'description': 'detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.' </s>"
"<s>[INST] إنشاء نظام يستخدم تقنيات المحادثة الآلية لتقديم خدمات دعم العملاء عبر الدردشة النصية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Detect Bordered and Borderless tables in documents', 'api_name': 'TahaDouaji/detr-doc-table-detection', 'api_call': \"DetrForObjectDetection.from_pretrained('TahaDouaji/detr-doc-table-detection')\", 'api_arguments': ['images', 'return_tensors', 'threshold'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nimage = Image.open(IMAGE_PATH)\\nprocessor = DetrImageProcessor.from_pretrained(TahaDouaji/detr-doc-table-detection)\\nmodel = DetrForObjectDetection.from_pretrained(TahaDouaji/detr-doc-table-detection)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.tensor([image.size[::-1]])\\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\\nfor score, label, box in zip(results[scores], results[labels], results[boxes]):\\n box = [round(i, 2) for i in box.tolist()]\\n print(\\n fDetected {model.config.id2label[label.item()]} with confidence \\n f{round(score.item(), 3)} at location {box}\\n )', 'performance': {'dataset': 'ICDAR2019 Table Dataset', 'accuracy': 'Not provided'}, 'description': 'detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.' </s>"
"<s>[INST] تطوير تطبيق يستخدم تقنيات التحليل اللغوي لتحليل وتصنيف النصوص القانونية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Detect Bordered and Borderless tables in documents', 'api_name': 'TahaDouaji/detr-doc-table-detection', 'api_call': \"DetrForObjectDetection.from_pretrained('TahaDouaji/detr-doc-table-detection')\", 'api_arguments': ['images', 'return_tensors', 'threshold'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nimage = Image.open(IMAGE_PATH)\\nprocessor = DetrImageProcessor.from_pretrained(TahaDouaji/detr-doc-table-detection)\\nmodel = DetrForObjectDetection.from_pretrained(TahaDouaji/detr-doc-table-detection)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.tensor([image.size[::-1]])\\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\\nfor score, label, box in zip(results[scores], results[labels], results[boxes]):\\n box = [round(i, 2) for i in box.tolist()]\\n print(\\n fDetected {model.config.id2label[label.item()]} with confidence \\n f{round(score.item(), 3)} at location {box}\\n )', 'performance': {'dataset': 'ICDAR2019 Table Dataset', 'accuracy': 'Not provided'}, 'description': 'detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.' </s>"
"<s>[INST] إنشاء نظام يستخدم تقنيات التحليل السلوكي للتنبؤ بأنماط سلوك المستخدمين عبر التطبيقات الرقمية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Detect Bordered and Borderless tables in documents', 'api_name': 'TahaDouaji/detr-doc-table-detection', 'api_call': \"DetrForObjectDetection.from_pretrained('TahaDouaji/detr-doc-table-detection')\", 'api_arguments': ['images', 'return_tensors', 'threshold'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nimage = Image.open(IMAGE_PATH)\\nprocessor = DetrImageProcessor.from_pretrained(TahaDouaji/detr-doc-table-detection)\\nmodel = DetrForObjectDetection.from_pretrained(TahaDouaji/detr-doc-table-detection)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.tensor([image.size[::-1]])\\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\\nfor score, label, box in zip(results[scores], results[labels], results[boxes]):\\n box = [round(i, 2) for i in box.tolist()]\\n print(\\n fDetected {model.config.id2label[label.item()]} with confidence \\n f{round(score.item(), 3)} at location {box}\\n )', 'performance': {'dataset': 'ICDAR2019 Table Dataset', 'accuracy': 'Not provided'}, 'description': 'detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.' </s>"
"<s>[INST] تصميم تطبيق يستخدم تقنيات التعلم الألآي لتقديم توصيات شخصية للمستخدمين بناءً على سلوكهم التصفحي.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Detect Bordered and Borderless tables in documents', 'api_name': 'TahaDouaji/detr-doc-table-detection', 'api_call': \"DetrForObjectDetection.from_pretrained('TahaDouaji/detr-doc-table-detection')\", 'api_arguments': ['images', 'return_tensors', 'threshold'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nimage = Image.open(IMAGE_PATH)\\nprocessor = DetrImageProcessor.from_pretrained(TahaDouaji/detr-doc-table-detection)\\nmodel = DetrForObjectDetection.from_pretrained(TahaDouaji/detr-doc-table-detection)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.tensor([image.size[::-1]])\\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\\nfor score, label, box in zip(results[scores], results[labels], results[boxes]):\\n box = [round(i, 2) for i in box.tolist()]\\n print(\\n fDetected {model.config.id2label[label.item()]} with confidence \\n f{round(score.item(), 3)} at location {box}\\n )', 'performance': {'dataset': 'ICDAR2019 Table Dataset', 'accuracy': 'Not provided'}, 'description': 'detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.' </s>"
"<s>[INST] بناء نظام يستخدم تقنيات التعلم الآلي لتحديد سلوك العملاء وتصنيفهم في فئات لتحسين استراتيجيات التسويق.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Detect Bordered and Borderless tables in documents', 'api_name': 'TahaDouaji/detr-doc-table-detection', 'api_call': \"DetrForObjectDetection.from_pretrained('TahaDouaji/detr-doc-table-detection')\", 'api_arguments': ['images', 'return_tensors', 'threshold'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nimage = Image.open(IMAGE_PATH)\\nprocessor = DetrImageProcessor.from_pretrained(TahaDouaji/detr-doc-table-detection)\\nmodel = DetrForObjectDetection.from_pretrained(TahaDouaji/detr-doc-table-detection)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.tensor([image.size[::-1]])\\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\\nfor score, label, box in zip(results[scores], results[labels], results[boxes]):\\n box = [round(i, 2) for i in box.tolist()]\\n print(\\n fDetected {model.config.id2label[label.item()]} with confidence \\n f{round(score.item(), 3)} at location {box}\\n )', 'performance': {'dataset': 'ICDAR2019 Table Dataset', 'accuracy': 'Not provided'}, 'description': 'detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.' </s>"
"<s>[INST]تطوير تطبيق يستخدم تقنيات الذكاء الاصطناعي لتحليل الصور والكشف تلقائيا عن الكائنات داخلها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-small', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'api_arguments': {'model_name': 'hustvl/yolos-small'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': {'import': ['from transformers import YolosFeatureExtractor, YolosForObjectDetection', 'from PIL import Image', 'import requests'], 'url': 'http://images.cocodataset.org/val2017/000000039769.jpg', 'image': 'Image.open(requests.get(url, stream=True).raw)', 'feature_extractor': \"YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\", 'model': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'inputs': \"feature_extractor(images=image, return_tensors='pt')\", 'outputs': 'model(**inputs)', 'logits': 'outputs.logits', 'bboxes': 'outputs.pred_boxes'}, 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '36.1 AP'}, 'description': 'YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).' </s>"
"<s>[INST] إنشاء نظام يستخدم API للرؤية الحاسوبية لتعريف وبيان الأشياء التي تظهر في الصور المقدمة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-small', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'api_arguments': {'model_name': 'hustvl/yolos-small'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': {'import': ['from transformers import YolosFeatureExtractor, YolosForObjectDetection', 'from PIL import Image', 'import requests'], 'url': 'http://images.cocodataset.org/val2017/000000039769.jpg', 'image': 'Image.open(requests.get(url, stream=True).raw)', 'feature_extractor': \"YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\", 'model': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'inputs': \"feature_extractor(images=image, return_tensors='pt')\", 'outputs': 'model(**inputs)', 'logits': 'outputs.logits', 'bboxes': 'outputs.pred_boxes'}, 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '36.1 AP'}, 'description': 'YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).' </s>"
"<s>[INST] بناء تطبيق يستخدم تقنيات الذكاء الاصطناعي لتحليل وتعريف الأشياء في مقاطع الفيديو.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-small', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'api_arguments': {'model_name': 'hustvl/yolos-small'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': {'import': ['from transformers import YolosFeatureExtractor, YolosForObjectDetection', 'from PIL import Image', 'import requests'], 'url': 'http://images.cocodataset.org/val2017/000000039769.jpg', 'image': 'Image.open(requests.get(url, stream=True).raw)', 'feature_extractor': \"YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\", 'model': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'inputs': \"feature_extractor(images=image, return_tensors='pt')\", 'outputs': 'model(**inputs)', 'logits': 'outputs.logits', 'bboxes': 'outputs.pred_boxes'}, 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '36.1 AP'}, 'description': 'YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).' </s>"
"<s>[INST] إنشاء نظام يقوم بتحليل الصور وتسمية الكائنات فيها باستخدام API للرؤية الحاسوبية المحدثة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-small', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'api_arguments': {'model_name': 'hustvl/yolos-small'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': {'import': ['from transformers import YolosFeatureExtractor, YolosForObjectDetection', 'from PIL import Image', 'import requests'], 'url': 'http://images.cocodataset.org/val2017/000000039769.jpg', 'image': 'Image.open(requests.get(url, stream=True).raw)', 'feature_extractor': \"YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\", 'model': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'inputs': \"feature_extractor(images=image, return_tensors='pt')\", 'outputs': 'model(**inputs)', 'logits': 'outputs.logits', 'bboxes': 'outputs.pred_boxes'}, 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '36.1 AP'}, 'description': 'YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).' </s>"
"<s>[INST] بناء تطبيق يحلل الصور ويوفر وسائل لتحديد وتصنيف الكائنات داخلها، باستخدام حزمة Hugging Face Transformers.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-small', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'api_arguments': {'model_name': 'hustvl/yolos-small'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': {'import': ['from transformers import YolosFeatureExtractor, YolosForObjectDetection', 'from PIL import Image', 'import requests'], 'url': 'http://images.cocodataset.org/val2017/000000039769.jpg', 'image': 'Image.open(requests.get(url, stream=True).raw)', 'feature_extractor': \"YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\", 'model': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'inputs': \"feature_extractor(images=image, return_tensors='pt')\", 'outputs': 'model(**inputs)', 'logits': 'outputs.logits', 'bboxes': 'outputs.pred_boxes'}, 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '36.1 AP'}, 'description': 'YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).' </s>"
"<s>[INST] تطوير تطبيق يستخدم API لتحويل النصوص إلى فيديوهات توضيحية للمقالات، مع إمكانية تعريف العناصر المختلفة فيها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-small', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'api_arguments': {'model_name': 'hustvl/yolos-small'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': {'import': ['from transformers import YolosFeatureExtractor, YolosForObjectDetection', 'from PIL import Image', 'import requests'], 'url': 'http://images.cocodataset.org/val2017/000000039769.jpg', 'image': 'Image.open(requests.get(url, stream=True).raw)', 'feature_extractor': \"YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\", 'model': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'inputs': \"feature_extractor(images=image, return_tensors='pt')\", 'outputs': 'model(**inputs)', 'logits': 'outputs.logits', 'bboxes': 'outputs.pred_boxes'}, 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '36.1 AP'}, 'description': 'YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).' </s>"
"<s>[INST] بناء تطبيق يعتمد على تقنيات الذكاء الاصطناعي لتحليل وتسميم الصور بشكل دقيق وسريع.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-small', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'api_arguments': {'model_name': 'hustvl/yolos-small'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': {'import': ['from transformers import YolosFeatureExtractor, YolosForObjectDetection', 'from PIL import Image', 'import requests'], 'url': 'http://images.cocodataset.org/val2017/000000039769.jpg', 'image': 'Image.open(requests.get(url, stream=True).raw)', 'feature_extractor': \"YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\", 'model': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'inputs': \"feature_extractor(images=image, return_tensors='pt')\", 'outputs': 'model(**inputs)', 'logits': 'outputs.logits', 'bboxes': 'outputs.pred_boxes'}, 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '36.1 AP'}, 'description': 'YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).' </s>"
"<s>[INST] إنشاء تطبيق يقوم بتحليل النصوص العلمية وتحويلها إلى محتوى فيديو توضيحي يسهل فهمه ومشاركته.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-small', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'api_arguments': {'model_name': 'hustvl/yolos-small'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': {'import': ['from transformers import YolosFeatureExtractor, YolosForObjectDetection', 'from PIL import Image', 'import requests'], 'url': 'http://images.cocodataset.org/val2017/000000039769.jpg', 'image': 'Image.open(requests.get(url, stream=True).raw)', 'feature_extractor': \"YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\", 'model': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'inputs': \"feature_extractor(images=image, return_tensors='pt')\", 'outputs': 'model(**inputs)', 'logits': 'outputs.logits', 'bboxes': 'outputs.pred_boxes'}, 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '36.1 AP'}, 'description': 'YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).' </s>"
"<s>[INST] بناء تطبيق يستخدم تقنيات الذكاء الاصطناعي لتعريف خصائص الكائنات في الصور وتوضيحها بشكل تلقائي.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-small', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'api_arguments': {'model_name': 'hustvl/yolos-small'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': {'import': ['from transformers import YolosFeatureExtractor, YolosForObjectDetection', 'from PIL import Image', 'import requests'], 'url': 'http://images.cocodataset.org/val2017/000000039769.jpg', 'image': 'Image.open(requests.get(url, stream=True).raw)', 'feature_extractor': \"YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\", 'model': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'inputs': \"feature_extractor(images=image, return_tensors='pt')\", 'outputs': 'model(**inputs)', 'logits': 'outputs.logits', 'bboxes': 'outputs.pred_boxes'}, 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '36.1 AP'}, 'description': 'YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).' </s>"
"<s>[INST] تطوير تطبيق قادر على تعريف الأحداث في الفيديوهات وإضافة تسميات توضيحية باستخدام API للرؤية الحاسوبية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'hustvl/yolos-small', 'api_call': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'api_arguments': {'model_name': 'hustvl/yolos-small'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': {'import': ['from transformers import YolosFeatureExtractor, YolosForObjectDetection', 'from PIL import Image', 'import requests'], 'url': 'http://images.cocodataset.org/val2017/000000039769.jpg', 'image': 'Image.open(requests.get(url, stream=True).raw)', 'feature_extractor': \"YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\", 'model': \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", 'inputs': \"feature_extractor(images=image, return_tensors='pt')\", 'outputs': 'model(**inputs)', 'logits': 'outputs.logits', 'bboxes': 'outputs.pred_boxes'}, 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': '36.1 AP'}, 'description': 'YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).' </s>"
"<s>[INST]اكتشف الأشياء الموجودة في صور السفر الخاصة بك باستخدام API للكشف عن العناصر البصرية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101-dc5', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': 'AP 44.9'}, 'description': 'DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.' </s>"
"<s>[INST] استخدم API لاكتشاف الاختلافات بين صورتين وتحديد العناصر المختلفة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101-dc5', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': 'AP 44.9'}, 'description': 'DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.' </s>"
"<s>[INST] قم بتنفيذ تحليل تفصيلي للعناصر البصرية في الصور الفنية باستخدام API للكشف عن الأشياء في الصور.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101-dc5', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': 'AP 44.9'}, 'description': 'DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.' </s>"
"<s>[INST] قم بتصميم نموذج لتحليل تصنيفات مختلفة من الصور باستخدام API للكشف عن العناصر البصرية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101-dc5', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': 'AP 44.9'}, 'description': 'DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.' </s>"
"<s>[INST] استخدم API لتحليل الهيكل البصري في الصور الإعلانية وتحديد موضوعاتها الرئيسية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101-dc5', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': 'AP 44.9'}, 'description': 'DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.' </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم API لتحليل العناصر البصرية في تصاميم الويب الخاصة بك.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101-dc5', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': 'AP 44.9'}, 'description': 'DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.' </s>"
"<s>[INST] اكتشف التفاصيل الفنية في الصور الفوتوغرافية باستخدام API لاكتشاف العناصر البصرية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101-dc5', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': 'AP 44.9'}, 'description': 'DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.' </s>"
"<s>[INST] أنشئ تطبيقًا يستخدم الAPI للكشف عن الألوان السائدة في الصور الفنية الرقمية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101-dc5', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': 'AP 44.9'}, 'description': 'DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.' </s>"
"<s>[INST] ابتكر تطبيقًا يستخدم API لتحليل العناصر البصرية في الأعمال الفنية التقليدية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101-dc5', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': 'AP 44.9'}, 'description': 'DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.' </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم API لتقدير العناصر البصرية المختلفة في الصور الفوتوغرافية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'facebook/detr-resnet-101-dc5', 'api_call': \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\", 'api_arguments': {'image': 'Image.open(requests.get(url, stream=True).raw)', 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': 'AP 44.9'}, 'description': 'DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.' </s>"
"<s>[INST]إنشاء تطبيق يستخدم التقنيات الذكاء الاصطناعي لتحليل نمط استخدام وسائل التواصل الاجتماعي للمستخدمين. استخدم API لأدوات التحليل النصي للتعرف على نمط السلوك على الإنترنت.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'deformable-detr', 'api_call': \"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': \"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\ninputs = processor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\", 'performance': {'dataset': 'COCO 2017', 'accuracy': 'Not provided'}, 'description': 'Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.' </s>"
"<s>[INST] قم بتطوير تطبيق يقوم بالتعرف على الأشياء في صورة وتقديم تحليل شامل للمحتوى باستخدام الذكاء الاصطناعي. استخدم API للتعرف على الأشياء في الصور.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'deformable-detr', 'api_call': \"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': \"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\ninputs = processor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\", 'performance': {'dataset': 'COCO 2017', 'accuracy': 'Not provided'}, 'description': 'Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.' </s>"
"<s>[INST] قم بإنشاء نظام يستخدم تقنيات التعلم العميق لتنبؤ بالسلوك الاستهلاكي للمستخدمين على الإنترنت بناءً على أنماط التصفح. استخدم API لتحليل سلوك التصفح.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'deformable-detr', 'api_call': \"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': \"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\ninputs = processor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\", 'performance': {'dataset': 'COCO 2017', 'accuracy': 'Not provided'}, 'description': 'Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.' </s>"
"<s>[INST] قم بتطوير تطبيق يستخدم التعلم الآلي لتحليل نمط إنفاق المستخدمين عبر التطبيقات الرقمية وتوفير توصيات استنادًا إلى ذلك. استخدم API لتحليل أنماط الإنفاق.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'deformable-detr', 'api_call': \"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': \"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\ninputs = processor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\", 'performance': {'dataset': 'COCO 2017', 'accuracy': 'Not provided'}, 'description': 'Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم تحليل البيانات لفهم اهتمامات المستخدمين عبر منصات التواصل الاجتماعي وتقديم محتوى مخصص بناءً على ذلك. استخدم API لتحليل بيانات الاهتمام.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'deformable-detr', 'api_call': \"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': \"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\ninputs = processor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\", 'performance': {'dataset': 'COCO 2017', 'accuracy': 'Not provided'}, 'description': 'Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.' </s>"
"<s>[INST] قم بتطوير نظام يستخدم الذكاء الاصطناعي لتحليل التغييرات في الاتجاهات والثقافة عبر الإنترنت. استخدم API لتحليل التغيرات في العلاقات الثقافية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'deformable-detr', 'api_call': \"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': \"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\ninputs = processor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\", 'performance': {'dataset': 'COCO 2017', 'accuracy': 'Not provided'}, 'description': 'Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.' </s>"
"<s>[INST] إنشاء نظام يستخدم تقنيات التحليل العميق لتحليل أنماط التفاعل مع تطبيقات الهواتف الذكية من قبل المستخدمين. استخدم API لتحليل أنماط التفاعل بالهواتف الذكية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'deformable-detr', 'api_call': \"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': \"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\ninputs = processor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\", 'performance': {'dataset': 'COCO 2017', 'accuracy': 'Not provided'}, 'description': 'Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.' </s>"
"<s>[INST] قم بتطوير تطبيق يعتمد على تقنيات التعلم الآلي لتحليل السلوكيات الجماعية على الإنترنت واكتشاف الاتجاهات الاجتماعية الحديثة. استخدم API لتحليل السلوكيات الجماعية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'deformable-detr', 'api_call': \"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': \"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\ninputs = processor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\", 'performance': {'dataset': 'COCO 2017', 'accuracy': 'Not provided'}, 'description': 'Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.' </s>"
"<s>[INST] إنشاء تطبيق يسخدم التعلم العميق لتحليل نمط استخدام التطبيقات الذكية وتوفير توصيات شخصية للمستخدم. استخدم API لتحليل أنمطة استخدام التطبيقات الذكية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'deformable-detr', 'api_call': \"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': \"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\ninputs = processor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\", 'performance': {'dataset': 'COCO 2017', 'accuracy': 'Not provided'}, 'description': 'Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.' </s>"
"<s>[INST] قم بتطوير نظام يستخدم التحليل البياني لتقديم تقارير شاملة حول تفاعل المستخدمين مع المواقع الإلكترونية. استخدم API لتحليل تفاعل المستخدمين.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'deformable-detr', 'api_call': \"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': \"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\ninputs = processor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\", 'performance': {'dataset': 'COCO 2017', 'accuracy': 'Not provided'}, 'description': 'Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.' </s>"
"<s>[INST]كيف يمكنني استخدام API دراسات السوق لفهم احتياجات العملاء بشكل أفضل؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8m-hard-hat-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8m-hard-hat-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.811}, 'description': \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\" </s>"
"<s>[INST] أريد تطوير تطبيق يستخدم تقنية الترجمة الآلية، كيف يمكنني الاستفادة من API لتحقيق هذا الهدف؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8m-hard-hat-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8m-hard-hat-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.811}, 'description': \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\" </s>"
"<s>[INST] كيف يمكنني استخدام API البحث الجغرافي للعثور على أماكن مهمة في منطقتي؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8m-hard-hat-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8m-hard-hat-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.811}, 'description': \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\" </s>"
"<s>[INST] أريد بناء تطبيق يقوم بتحليل سلوك المستخدمين، ما نوع الAPI الذي يمكنني استخدامه لهذا الغرض؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8m-hard-hat-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8m-hard-hat-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.811}, 'description': \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\" </s>"
"<s>[INST] كيف يمكنني تنفيذ إعادة صياغة النصوص باستخدام تقنيات التوليف النصي؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8m-hard-hat-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8m-hard-hat-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.811}, 'description': \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\" </s>"
"<s>[INST] أريد تطوير تطبيق للتعرف على النصوص في الصور، كيف يمكنني استخدام API التعرف على النصوص لهذا الهدف؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8m-hard-hat-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8m-hard-hat-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.811}, 'description': \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\" </s>"
"<s>[INST] أحتاج إلى تطوير تطبيق لتحليل مشاعر النصوص، كيف يمكنني استخدام API تحليل المشاعر لهذا الغرض؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8m-hard-hat-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8m-hard-hat-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.811}, 'description': \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\" </s>"
"<s>[INST] كيف يمكنني تنفيذ استعلامات معقدة على قواعد البيانات باستخدام API قواعد البيانات؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8m-hard-hat-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8m-hard-hat-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.811}, 'description': \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\" </s>"
"<s>[INST] كيف يمكنني استخدام تقنيات التعلم الآلي لتنبؤ بالبيانات المالية؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8m-hard-hat-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8m-hard-hat-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.811}, 'description': \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\" </s>"
"<s>[INST] كيف يمكنني تطبيق تقنيات التعلم الآلي في إنشاء نظام توصيات لموقعي؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8m-hard-hat-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8m-hard-hat-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.811}, 'description': \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\" </s>"
"<s>[INST]تصميم وتطوير نظام يتيح للمستخدمين تحليل بيانات التسوق عبر الإنترنت وتوفير توصيات شخصية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5m-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5m-license-plate')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic': False, 'multi_label': False, 'max_det': 1000, 'img': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'size': 640, 'augment': True}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5m-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.988}, 'description': 'A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم تقنية التعلم الآلي لتحليل مشاعر المستخدمين من التغريدات على وسائل التواصل الاجتماعي.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5m-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5m-license-plate')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic': False, 'multi_label': False, 'max_det': 1000, 'img': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'size': 640, 'augment': True}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5m-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.988}, 'description': 'A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.' </s>"
"<s>[INST] بناء نظام يستخدم الذكاء الصناعي لتوفير توصيات فنية لفنانين مبتدئين.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5m-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5m-license-plate')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic': False, 'multi_label': False, 'max_det': 1000, 'img': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'size': 640, 'augment': True}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5m-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.988}, 'description': 'A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.' </s>"
"<s>[INST] تطوير تطبيق يستخدم تقنيات معالجة اللغة الطبيعية لتحليل وتصنيف تعليقات المستخدمين على منتجات.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5m-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5m-license-plate')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic': False, 'multi_label': False, 'max_det': 1000, 'img': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'size': 640, 'augment': True}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5m-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.988}, 'description': 'A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.' </s>"
"<s>[INST] إنشاء نظام يستخدم تقنيات التحليل البصري للتعرف على الأشخاص في مقاطع الفيديو الأمنية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5m-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5m-license-plate')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic': False, 'multi_label': False, 'max_det': 1000, 'img': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'size': 640, 'augment': True}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5m-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.988}, 'description': 'A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.' </s>"
"<s>[INST] تصميم وتطوير تطبيق يستخدم تقنيات الذكاء الصناعي لتحسين تجربة التسوق عبر الإنترنت.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5m-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5m-license-plate')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic': False, 'multi_label': False, 'max_det': 1000, 'img': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'size': 640, 'augment': True}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5m-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.988}, 'description': 'A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.' </s>"
"<s>[INST] تطوير تطبيق يستخدم تقنيات التعلم الآلي لتوليد نصوص توصيفية للصور الفوتوغرافية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5m-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5m-license-plate')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic': False, 'multi_label': False, 'max_det': 1000, 'img': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'size': 640, 'augment': True}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5m-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.988}, 'description': 'A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.' </s>"
"<s>[INST] بناء نظام يستخدم الذكاء الصناعي لتحليل السلوك التصفوي للمستخدمين في التطبيقات الرقمية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5m-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5m-license-plate')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic': False, 'multi_label': False, 'max_det': 1000, 'img': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'size': 640, 'augment': True}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5m-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.988}, 'description': 'A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.' </s>"
"<s>[INST] تحليل بيانات السفر للمساعدة في توفير توصيات شخصية للمسافرين.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5m-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5m-license-plate')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic': False, 'multi_label': False, 'max_det': 1000, 'img': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'size': 640, 'augment': True}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5m-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.988}, 'description': 'A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم تقنيات التعلم الآلي لتحليل سلوك العملاء في متاجر التجزئة الكبيرة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5m-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5m-license-plate')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic': False, 'multi_label': False, 'max_det': 1000, 'img': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'size': 640, 'augment': True}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5m-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.988}, 'description': 'A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.' </s>"
"<s>[INST]كيف يمكنني استخدام واجهة برمجية لتحويل النواتج الصوتية إلى تصاميم بصرية مبتكرة؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-valorant-detection', 'api_call': \"YOLO('keremberke/yolov8m-valorant-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'valorant-object-detection', 'accuracy': 0.965}, 'description': 'A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.' </s>"
"<s>[INST] كيف يمكنني استخدام واجهة برمجية لتوليف الصور الفوتوغرافية بناءً على النغمات الموسيقية؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-valorant-detection', 'api_call': \"YOLO('keremberke/yolov8m-valorant-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'valorant-object-detection', 'accuracy': 0.965}, 'description': 'A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.' </s>"
"<s>[INST] كيف يمكنني استخدام واجهة برمجية لتحليل أنماط الأصوات البيانية وتحويلها إلى تصاميم فنية؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-valorant-detection', 'api_call': \"YOLO('keremberke/yolov8m-valorant-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'valorant-object-detection', 'accuracy': 0.965}, 'description': 'A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.' </s>"
"<s>[INST] كيف يمكنني استخدام واجهة برمجية لتوسيع صور الفضاء من الأقمار الصناعية؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-valorant-detection', 'api_call': \"YOLO('keremberke/yolov8m-valorant-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'valorant-object-detection', 'accuracy': 0.965}, 'description': 'A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.' </s>"
"<s>[INST] كيف يمكنني استخدام واجهة برمجية لتحويل البيانات الصوتية إلى مقاطع فيديو توضيحية؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-valorant-detection', 'api_call': \"YOLO('keremberke/yolov8m-valorant-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'valorant-object-detection', 'accuracy': 0.965}, 'description': 'A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.' </s>"
"<s>[INST] كيف يمكنني استخدام واجهة برمجية لتحليل بيانات الفيديو ورسم تصورات بيانية دقيقة؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-valorant-detection', 'api_call': \"YOLO('keremberke/yolov8m-valorant-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'valorant-object-detection', 'accuracy': 0.965}, 'description': 'A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.' </s>"
"<s>[INST] كيف يمكنني استخدام واجهة برمجية لتحويل النصوص الشعرية إلى عروض تقديمية مرئية؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-valorant-detection', 'api_call': \"YOLO('keremberke/yolov8m-valorant-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'valorant-object-detection', 'accuracy': 0.965}, 'description': 'A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.' </s>"
"<s>[INST] كيف يمكنني استخدام واجهة برمجية لتوليف فنجان القهوة بناءً على نغمات صوت الطبيعة؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-valorant-detection', 'api_call': \"YOLO('keremberke/yolov8m-valorant-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'valorant-object-detection', 'accuracy': 0.965}, 'description': 'A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.' </s>"
"<s>[INST] كيف يمكنني استخدام واجهة برمجية لإنشاء ملصقات تفاعلية متأثرة بنغمات الموسيقى؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-valorant-detection', 'api_call': \"YOLO('keremberke/yolov8m-valorant-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'valorant-object-detection', 'accuracy': 0.965}, 'description': 'A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.' </s>"
"<s>[INST] كيف يمكنني استخدام واجهة برمجية لتحويل الأفكار المكتوبة إلى تصاميم جرافيكية ديناميكية؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-valorant-detection', 'api_call': \"YOLO('keremberke/yolov8m-valorant-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'valorant-object-detection', 'accuracy': 0.965}, 'description': 'A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.' </s>"
"<s>[INST]\"استخدم API \\AWS Polly\\ لتحويل النص إلى كلام تلقائي بصوت طبيعي.\"? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8m-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.892}, 'description': \"An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\" </s>"
"<s>[INST] \"استخدم API \\Google Cloud Vision\\ للكشف عن الأشياء والكائنات في الصور وتصنيفهم تلقائيًا.\"? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8m-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.892}, 'description': \"An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\" </s>"
"<s>[INST] \"قم بتطوير تطبيق يستخدم API \\Twitter API\\ لجلب التغريدات الأخيرة حول موضوع معين وعرضها للمستخدم.\"? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8m-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.892}, 'description': \"An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\" </s>"
"<s>[INST] \"استخدم API \\OpenWeatherMap\\ للحصول على معلومات الطقس الحالية لموقع معين.\"? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8m-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.892}, 'description': \"An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\" </s>"
"<s>[INST] \"استخدم API \\Wit.ai\\ لبناء نموذج للتعرف على الكلام الطبيعي وفهمه تلقائيًا.\"? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8m-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.892}, 'description': \"An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\" </s>"
"<s>[INST] \"قم بتنفيذ تحليل لبيانات الويب باستخدام API \\SerpApi\\ للحصول على تقارير حول الكلمات الرئيسية والنتائج ذات الصلة.\"? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8m-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.892}, 'description': \"An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\" </s>"
"<s>[INST] \"استخدم API \\IBM Watson Tone Analyzer\\ لتحليل مشاعر ونغمة النصوص المدخلة.\"? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8m-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.892}, 'description': \"An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\" </s>"
"<s>[INST] \"استخدم API \\NASA API\\ للحصول على أحدث الصور الفضائية والمعلومات حول الفضاء.\"? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8m-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.892}, 'description': \"An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\" </s>"
"<s>[INST] \"استخدم API \\RapidAPI\\ للوصول إلى مجموعة واسعة من الخدمات الشاملة عبر واجهة واحدة.\"? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8m-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.892}, 'description': \"An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\" </s>"
"<s>[INST] \"قم بتطبيق تحليل لغوي باستخدام API \\TextBlob\\ لفهم معاني النصوص والتعلم منها.\"? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8m-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.892}, 'description': \"An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\" </s>"
"<s>[INST]تطلب منصة موسيقى تحليلية API لتوليد مخطط بياني لنغمة صوتية بهدف تحليل مكوناتها وتوقع تأثيراتها النفسية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8s-table-extraction', 'api_call': \"YOLO('keremberke/yolov8s-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-table-extraction')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'table-extraction', 'accuracy': 0.984}, 'description': 'A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.' </s>"
"<s>[INST] ترغب في تطوير نظام تعلم آلي يتنبأ بالإجابة عن الأسئلة الطويلة باستخدام API.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8s-table-extraction', 'api_call': \"YOLO('keremberke/yolov8s-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-table-extraction')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'table-extraction', 'accuracy': 0.984}, 'description': 'A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.' </s>"
"<s>[INST] تريد تطوير نظام لتصنيف الوجوه في الصور باستخدام API لها تصميم دقيق.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8s-table-extraction', 'api_call': \"YOLO('keremberke/yolov8s-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-table-extraction')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'table-extraction', 'accuracy': 0.984}, 'description': 'A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.' </s>"
"<s>[INST] تريد تطوير تطبيق للكشف عن النصوص في الصور الطبية باستخدام API متخصص.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8s-table-extraction', 'api_call': \"YOLO('keremberke/yolov8s-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-table-extraction')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'table-extraction', 'accuracy': 0.984}, 'description': 'A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.' </s>"
"<s>[INST] تطلب نظامًا يستخدم تعلم آلي لتصنيف الرسوم البيانية في الوثائق العلمية باستخدام API متقدمة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8s-table-extraction', 'api_call': \"YOLO('keremberke/yolov8s-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-table-extraction')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'table-extraction', 'accuracy': 0.984}, 'description': 'A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.' </s>"
"<s>[INST] تحتاج لنظام يقوم بتحليل تغريدات المستخدمين ويصنفها وفقًا للمضمون باستخدام API لتعلم الآلة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8s-table-extraction', 'api_call': \"YOLO('keremberke/yolov8s-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-table-extraction')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'table-extraction', 'accuracy': 0.984}, 'description': 'A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.' </s>"
"<s>[INST] تريد تطوير نظام استشعار للحركة في غرفة العناية المركزة يستخدم تقنيات تحليل الصور باستخدام API.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8s-table-extraction', 'api_call': \"YOLO('keremberke/yolov8s-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-table-extraction')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'table-extraction', 'accuracy': 0.984}, 'description': 'A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.' </s>"
"<s>[INST] تحتاج منظمة لنظام يقوم بتحليل الصور لاكتشاف الأشخاص الغير مرغوب فيهم باستخدام API للتعرف على الوجوه.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8s-table-extraction', 'api_call': \"YOLO('keremberke/yolov8s-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-table-extraction')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'table-extraction', 'accuracy': 0.984}, 'description': 'A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.' </s>"
"<s>[INST] ترغب في تطوير نظام يقوم بتحليل المستندات الفنية للتعرف على الفنانين باستخدام واجهة برمجة التطبيقات.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8s-table-extraction', 'api_call': \"YOLO('keremberke/yolov8s-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-table-extraction')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'table-extraction', 'accuracy': 0.984}, 'description': 'A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.' </s>"
"<s>[INST] ترغب في تطوير نظام لتحليل نعومة الصوت في تسجيل المقابلات باستخدام API متخصصة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8s-table-extraction', 'api_call': \"YOLO('keremberke/yolov8s-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-table-extraction')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'table-extraction', 'accuracy': 0.984}, 'description': 'A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.' </s>"
"<s>[INST]قم بتحليل نصوص متعددة لتحديد المواضيع الرئيسية والأفكار الرئيسية في كل نص.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-large-patch14', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\", 'api_arguments': {'model_name': 'google/owlvit-large-patch14'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'import torch', 'from transformers import OwlViTProcessor, OwlViTForObjectDetection', 'processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)', 'model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)', 'url = http://images.cocodataset.org/val2017/000000039769.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'texts = [[a photo of a cat, a photo of a dog]', 'inputs = processor(text=texts, images=image, return_tensors=pt)', 'outputs = model(**inputs)', 'target_sizes = torch.Tensor([image.size[::-1]])', 'results = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'i = 0', 'text = texts[i]', 'boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]', 'score_threshold = 0.1', 'for box, score, label in zip(boxes, scores, labels):', ' box = [round(i, 2) for i in box.tolist()]', ' if score >= score_threshold:', ' print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})'], 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.' </s>"
"<s>[INST] استخدام الواجهة البرمجية التالية لتحديد الأشياء المفتاحية في موقع ويب معين.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-large-patch14', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\", 'api_arguments': {'model_name': 'google/owlvit-large-patch14'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'import torch', 'from transformers import OwlViTProcessor, OwlViTForObjectDetection', 'processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)', 'model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)', 'url = http://images.cocodataset.org/val2017/000000039769.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'texts = [[a photo of a cat, a photo of a dog]', 'inputs = processor(text=texts, images=image, return_tensors=pt)', 'outputs = model(**inputs)', 'target_sizes = torch.Tensor([image.size[::-1]])', 'results = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'i = 0', 'text = texts[i]', 'boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]', 'score_threshold = 0.1', 'for box, score, label in zip(boxes, scores, labels):', ' box = [round(i, 2) for i in box.tolist()]', ' if score >= score_threshold:', ' print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})'], 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.' </s>"
"<s>[INST] إنشاء نظام توصية لتصفح المدونات الطبية باستخدام الواجهة البرمجية التالية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-large-patch14', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\", 'api_arguments': {'model_name': 'google/owlvit-large-patch14'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'import torch', 'from transformers import OwlViTProcessor, OwlViTForObjectDetection', 'processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)', 'model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)', 'url = http://images.cocodataset.org/val2017/000000039769.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'texts = [[a photo of a cat, a photo of a dog]', 'inputs = processor(text=texts, images=image, return_tensors=pt)', 'outputs = model(**inputs)', 'target_sizes = torch.Tensor([image.size[::-1]])', 'results = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'i = 0', 'text = texts[i]', 'boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]', 'score_threshold = 0.1', 'for box, score, label in zip(boxes, scores, labels):', ' box = [round(i, 2) for i in box.tolist()]', ' if score >= score_threshold:', ' print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})'], 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.' </s>"
"<s>[INST] تنفيذ مقارنة بين مجموعات من البيانات النصية باستخدام الواجهة البرمجية التالية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-large-patch14', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\", 'api_arguments': {'model_name': 'google/owlvit-large-patch14'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'import torch', 'from transformers import OwlViTProcessor, OwlViTForObjectDetection', 'processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)', 'model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)', 'url = http://images.cocodataset.org/val2017/000000039769.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'texts = [[a photo of a cat, a photo of a dog]', 'inputs = processor(text=texts, images=image, return_tensors=pt)', 'outputs = model(**inputs)', 'target_sizes = torch.Tensor([image.size[::-1]])', 'results = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'i = 0', 'text = texts[i]', 'boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]', 'score_threshold = 0.1', 'for box, score, label in zip(boxes, scores, labels):', ' box = [round(i, 2) for i in box.tolist()]', ' if score >= score_threshold:', ' print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})'], 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.' </s>"
"<s>[INST] قم بتحليل نصوص التعليقات للمستخدمين واستخراج المشاعر الموجودة في كل تعليق باستخدام الواجهة البرمجية التالية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-large-patch14', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\", 'api_arguments': {'model_name': 'google/owlvit-large-patch14'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'import torch', 'from transformers import OwlViTProcessor, OwlViTForObjectDetection', 'processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)', 'model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)', 'url = http://images.cocodataset.org/val2017/000000039769.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'texts = [[a photo of a cat, a photo of a dog]', 'inputs = processor(text=texts, images=image, return_tensors=pt)', 'outputs = model(**inputs)', 'target_sizes = torch.Tensor([image.size[::-1]])', 'results = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'i = 0', 'text = texts[i]', 'boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]', 'score_threshold = 0.1', 'for box, score, label in zip(boxes, scores, labels):', ' box = [round(i, 2) for i in box.tolist()]', ' if score >= score_threshold:', ' print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})'], 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.' </s>"
"<s>[INST] قم بتصميم نظام تقديم قوائم التشغيل الموسيقية الذكية باستخدام الواجهة البرمجية التالية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-large-patch14', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\", 'api_arguments': {'model_name': 'google/owlvit-large-patch14'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'import torch', 'from transformers import OwlViTProcessor, OwlViTForObjectDetection', 'processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)', 'model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)', 'url = http://images.cocodataset.org/val2017/000000039769.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'texts = [[a photo of a cat, a photo of a dog]', 'inputs = processor(text=texts, images=image, return_tensors=pt)', 'outputs = model(**inputs)', 'target_sizes = torch.Tensor([image.size[::-1]])', 'results = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'i = 0', 'text = texts[i]', 'boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]', 'score_threshold = 0.1', 'for box, score, label in zip(boxes, scores, labels):', ' box = [round(i, 2) for i in box.tolist()]', ' if score >= score_threshold:', ' print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})'], 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.' </s>"
"<s>[INST] تنفيج محدد بقوائم الطعام من الصور الغذائية باستخدام الواجهة البرمجية التالية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-large-patch14', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\", 'api_arguments': {'model_name': 'google/owlvit-large-patch14'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'import torch', 'from transformers import OwlViTProcessor, OwlViTForObjectDetection', 'processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)', 'model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)', 'url = http://images.cocodataset.org/val2017/000000039769.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'texts = [[a photo of a cat, a photo of a dog]', 'inputs = processor(text=texts, images=image, return_tensors=pt)', 'outputs = model(**inputs)', 'target_sizes = torch.Tensor([image.size[::-1]])', 'results = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'i = 0', 'text = texts[i]', 'boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]', 'score_threshold = 0.1', 'for box, score, label in zip(boxes, scores, labels):', ' box = [round(i, 2) for i in box.tolist()]', ' if score >= score_threshold:', ' print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})'], 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.' </s>"
"<s>[INST] تنفيج التقاط صورة لكائن معين وتحديد معلومات محددة عنه باستخدام الواجهة البرمجية التالية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-large-patch14', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\", 'api_arguments': {'model_name': 'google/owlvit-large-patch14'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'import torch', 'from transformers import OwlViTProcessor, OwlViTForObjectDetection', 'processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)', 'model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)', 'url = http://images.cocodataset.org/val2017/000000039769.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'texts = [[a photo of a cat, a photo of a dog]', 'inputs = processor(text=texts, images=image, return_tensors=pt)', 'outputs = model(**inputs)', 'target_sizes = torch.Tensor([image.size[::-1]])', 'results = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'i = 0', 'text = texts[i]', 'boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]', 'score_threshold = 0.1', 'for box, score, label in zip(boxes, scores, labels):', ' box = [round(i, 2) for i in box.tolist()]', ' if score >= score_threshold:', ' print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})'], 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.' </s>"
"<s>[INST] إجراء تحليل لغوي لنص معين لتحديد الجوانب النحوية والصرفية به باستخدام الواجهة البرمجية التالية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-large-patch14', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\", 'api_arguments': {'model_name': 'google/owlvit-large-patch14'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'import torch', 'from transformers import OwlViTProcessor, OwlViTForObjectDetection', 'processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)', 'model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)', 'url = http://images.cocodataset.org/val2017/000000039769.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'texts = [[a photo of a cat, a photo of a dog]', 'inputs = processor(text=texts, images=image, return_tensors=pt)', 'outputs = model(**inputs)', 'target_sizes = torch.Tensor([image.size[::-1]])', 'results = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'i = 0', 'text = texts[i]', 'boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]', 'score_threshold = 0.1', 'for box, score, label in zip(boxes, scores, labels):', ' box = [round(i, 2) for i in box.tolist()]', ' if score >= score_threshold:', ' print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})'], 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.' </s>"
"<s>[INST] إنشاء نظام للتعرف على العناصر الغريبة في المشاهد السينمائية باستخدام الواجهة البرمجية التالية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-large-patch14', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\", 'api_arguments': {'model_name': 'google/owlvit-large-patch14'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': ['import requests', 'from PIL import Image', 'import torch', 'from transformers import OwlViTProcessor, OwlViTForObjectDetection', 'processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)', 'model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)', 'url = http://images.cocodataset.org/val2017/000000039769.jpg', 'image = Image.open(requests.get(url, stream=True).raw)', 'texts = [[a photo of a cat, a photo of a dog]', 'inputs = processor(text=texts, images=image, return_tensors=pt)', 'outputs = model(**inputs)', 'target_sizes = torch.Tensor([image.size[::-1]])', 'results = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'i = 0', 'text = texts[i]', 'boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]', 'score_threshold = 0.1', 'for box, score, label in zip(boxes, scores, labels):', ' box = [round(i, 2) for i in box.tolist()]', ' if score >= score_threshold:', ' print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})'], 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.' </s>"
"<s>[INST]تحتاج إلى تطوير نظام تقوم فيه بتحليل تغريدات العملاء للكشف عن مشاعرهم تجاه منتجاتنا باستخدام تقنيات معالجة اللغة الطبيعية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-nlf-head-detection', 'api_call': \"YOLO('keremberke/yolov8m-nlf-head-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.24 ultralytics==8.0.23', 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-nlf-head-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'nfl-object-detection', 'accuracy': 0.287}, 'description': 'A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.' </s>"
"<s>[INST] قم ببناء نظام يستخدم الذكاء الاصطناعي للتعرف على صور إشارات المرور وتحديد معناها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-nlf-head-detection', 'api_call': \"YOLO('keremberke/yolov8m-nlf-head-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.24 ultralytics==8.0.23', 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-nlf-head-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'nfl-object-detection', 'accuracy': 0.287}, 'description': 'A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.' </s>"
"<s>[INST] أنشىء نظام يعتمد على تقنيات المعالجة اللغوية الحاسوبية لتوليد نصوص تعليقية لصور المنتجات على متجرنا الإلكتروني.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-nlf-head-detection', 'api_call': \"YOLO('keremberke/yolov8m-nlf-head-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.24 ultralytics==8.0.23', 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-nlf-head-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'nfl-object-detection', 'accuracy': 0.287}, 'description': 'A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.' </s>"
"<s>[INST] تقوم بتطوير نظام يستخدم نموذج التعلم العميق لتحديد إذا كان نص البريد الإلكتروني يحتوي على تهديد أم لا.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-nlf-head-detection', 'api_call': \"YOLO('keremberke/yolov8m-nlf-head-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.24 ultralytics==8.0.23', 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-nlf-head-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'nfl-object-detection', 'accuracy': 0.287}, 'description': 'A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.' </s>"
"<s>[INST] قم بتنفيذ مهمة استرجاع الصور المشابهة باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-nlf-head-detection', 'api_call': \"YOLO('keremberke/yolov8m-nlf-head-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.24 ultralytics==8.0.23', 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-nlf-head-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'nfl-object-detection', 'accuracy': 0.287}, 'description': 'A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.' </s>"
"<s>[INST] تريد تصميم نظام يستخدم تقنيات الذكاء الاصطناعي لاقتراح المنتجات المناسبة للعملاء استنادًا إلى سجلات مشترياتهم السابقة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-nlf-head-detection', 'api_call': \"YOLO('keremberke/yolov8m-nlf-head-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.24 ultralytics==8.0.23', 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-nlf-head-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'nfl-object-detection', 'accuracy': 0.287}, 'description': 'A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.' </s>"
"<s>[INST] تحتاج إلى بناء نظام لتصنيف العملاء بناءً على استجابتهم للاستبيان الذي نقدمه، باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-nlf-head-detection', 'api_call': \"YOLO('keremberke/yolov8m-nlf-head-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.24 ultralytics==8.0.23', 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-nlf-head-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'nfl-object-detection', 'accuracy': 0.287}, 'description': 'A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.' </s>"
"<s>[INST] تطلب منك مهمة تجميع بيانات المرضى واكتشاف الأنماط السلوكية باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-nlf-head-detection', 'api_call': \"YOLO('keremberke/yolov8m-nlf-head-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.24 ultralytics==8.0.23', 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-nlf-head-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'nfl-object-detection', 'accuracy': 0.287}, 'description': 'A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.' </s>"
"<s>[INST] قم ببناء نظام يعتمد على الذكاء الاصطناعي لتحسين تجربة المستخدم في التواصل مع خدمة العملاء عبر الدردشة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-nlf-head-detection', 'api_call': \"YOLO('keremberke/yolov8m-nlf-head-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.24 ultralytics==8.0.23', 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-nlf-head-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'nfl-object-detection', 'accuracy': 0.287}, 'description': 'A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.' </s>"
"<s>[INST] أنشئ نظام يستخدم الذكاء الاصطناعي لتحليل بيانات السوق وتوقع اتجاهات الأسهم.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-nlf-head-detection', 'api_call': \"YOLO('keremberke/yolov8m-nlf-head-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000, 'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.24 ultralytics==8.0.23', 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-nlf-head-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'nfl-object-detection', 'accuracy': 0.287}, 'description': 'A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.' </s>"
"<s>[INST]طور تطبيق يعتمد على خدمة استشعار الوجوه للكشف عن المعالم الوجهية وتحليلها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-forklift-detection', 'api_call': \"YOLO('keremberke/yolov8m-forklift-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-forklift-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'forklift-object-detection', 'accuracy': 0.846}, 'description': 'A YOLOv8 model for detecting forklifts and persons in images.' </s>"
"<s>[INST] استخدم خدمة تعلم الآلة لتحليل نمط سلوك المستخدمين داخل تطبيقك.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-forklift-detection', 'api_call': \"YOLO('keremberke/yolov8m-forklift-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-forklift-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'forklift-object-detection', 'accuracy': 0.846}, 'description': 'A YOLOv8 model for detecting forklifts and persons in images.' </s>"
"<s>[INST] بناء نظام يقوم بتقدير عمر الأشخاص في الصور التي يتم تحميلها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-forklift-detection', 'api_call': \"YOLO('keremberke/yolov8m-forklift-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-forklift-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'forklift-object-detection', 'accuracy': 0.846}, 'description': 'A YOLOv8 model for detecting forklifts and persons in images.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم خدمة تعلم الآلة للتنبؤ بأسعار منتجات التجزئة عبر تحليل بيانات المبيعات السابقة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-forklift-detection', 'api_call': \"YOLO('keremberke/yolov8m-forklift-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-forklift-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'forklift-object-detection', 'accuracy': 0.846}, 'description': 'A YOLOv8 model for detecting forklifts and persons in images.' </s>"
"<s>[INST] تطوير تطبيق يستخدم خدمة تعلم الآلة لتصنيف التغريدات إلى فئات مختلفة مثل الأخبار والرياضة والتقنية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-forklift-detection', 'api_call': \"YOLO('keremberke/yolov8m-forklift-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-forklift-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'forklift-object-detection', 'accuracy': 0.846}, 'description': 'A YOLOv8 model for detecting forklifts and persons in images.' </s>"
"<s>[INST] بناء نظام يقوم بتحليل المشاهد المأخوذة من كاميرا المراقبة للكشف عن تجاوز السرعة للسيارات.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-forklift-detection', 'api_call': \"YOLO('keremberke/yolov8m-forklift-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-forklift-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'forklift-object-detection', 'accuracy': 0.846}, 'description': 'A YOLOv8 model for detecting forklifts and persons in images.' </s>"
"<s>[INST] تطوير تطبيق يستخدم خدمات تعلم الآلة لتحليل توجيهات العملاء لتحسين تجربة التسوق عبر الإنترنت.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-forklift-detection', 'api_call': \"YOLO('keremberke/yolov8m-forklift-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-forklift-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'forklift-object-detection', 'accuracy': 0.846}, 'description': 'A YOLOv8 model for detecting forklifts and persons in images.' </s>"
"<s>[INST] إنشاء نظام لتصنيف الأخبار عبر تحليل عناوين الأخبار للتعرف على المواضيع الشائعة والانتقالات الهامة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-forklift-detection', 'api_call': \"YOLO('keremberke/yolov8m-forklift-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-forklift-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'forklift-object-detection', 'accuracy': 0.846}, 'description': 'A YOLOv8 model for detecting forklifts and persons in images.' </s>"
"<s>[INST] تطوير تطبيق يستخدم خدمة توليد اللغة الطبيعية لإنشاء ملخصات للمقالات والمدونات.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-forklift-detection', 'api_call': \"YOLO('keremberke/yolov8m-forklift-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-forklift-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'forklift-object-detection', 'accuracy': 0.846}, 'description': 'A YOLOv8 model for detecting forklifts and persons in images.' </s>"
"<s>[INST] بناء نظام يستخدم خدمة تعلم الآلة للتنبؤ بالأمراض الجلدية من الصور الطبية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-forklift-detection', 'api_call': \"YOLO('keremberke/yolov8m-forklift-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-forklift-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'forklift-object-detection', 'accuracy': 0.846}, 'description': 'A YOLOv8 model for detecting forklifts and persons in images.' </s>"
"<s>[INST]? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'zero-shot-object-detection', 'api_name': 'google/owlvit-base-patch16', 'api_call': \"OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch16')\", 'api_arguments': ['texts', 'images'], 'python_environment_requirements': ['requests', 'PIL', 'torch', 'transformers'], 'example_code': 'processor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch16)\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch16)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [[a photo of a cat, a photo of a dog]]\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)', 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.' </s>"
"<s>[INST]لتحليل بيانات الفيديو واكتشاف الأحداث الشخصية في الفيديوهات، يمكنك استخدام API استخراج الميزات في Transformers.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-plane-detection', 'api_call': \"YOLO('keremberke/yolov8m-plane-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-plane-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'plane-detection', 'accuracy': '0.995'}, 'description': 'A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.' </s>"
"<s>[INST] ترغب في تحليل صور الطبيعة لتشخيص أمراض النباتات، يمكنك استخدام API التعرف على الصور في Transformers.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-plane-detection', 'api_call': \"YOLO('keremberke/yolov8m-plane-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-plane-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'plane-detection', 'accuracy': '0.995'}, 'description': 'A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.' </s>"
"<s>[INST] لتحليل الروايات وتجليات الشخصيات في الكتب، يمكنك استخدام API التحليل النصي في Transformers.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-plane-detection', 'api_call': \"YOLO('keremberke/yolov8m-plane-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-plane-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'plane-detection', 'accuracy': '0.995'}, 'description': 'A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.' </s>"
"<s>[INST] لتحليل السلوك العمراني في المدن، يمكنك استخدام API التحليل الصوري في Transformers.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-plane-detection', 'api_call': \"YOLO('keremberke/yolov8m-plane-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-plane-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'plane-detection', 'accuracy': '0.995'}, 'description': 'A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.' </s>"
"<s>[INST] تحتاج إلى تقييم الأثر البيئي لمناطق معينة، يمكنك استخدام API التحليل الجغرافي في Transformers.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-plane-detection', 'api_call': \"YOLO('keremberke/yolov8m-plane-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-plane-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'plane-detection', 'accuracy': '0.995'}, 'description': 'A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.' </s>"
"<s>[INST] لتحليل نمط الاستخدام في تطبيقات الهاتف المحمول وتوقع السلوك التفاعلي، يمكنك استخدام API التحليل السلوكي في Transformers.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-plane-detection', 'api_call': \"YOLO('keremberke/yolov8m-plane-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-plane-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'plane-detection', 'accuracy': '0.995'}, 'description': 'A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.' </s>"
"<s>[INST] ترغب في استخراج المعلومات الرئيسية من العروض التقديمية لتحسين محتوى العروض، يمكنك استخدام API الاستخراج من النصوص في Transformers.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-plane-detection', 'api_call': \"YOLO('keremberke/yolov8m-plane-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-plane-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'plane-detection', 'accuracy': '0.995'}, 'description': 'A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.' </s>"
"<s>[INST] لتحليل محتوى البريد الإلكتروني والكشف عن المعلومات الحاسمة، يمكنك استخدام API التحليل اللغوي في Transformers.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-plane-detection', 'api_call': \"YOLO('keremberke/yolov8m-plane-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-plane-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'plane-detection', 'accuracy': '0.995'}, 'description': 'A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.' </s>"
"<s>[INST] ترغب في تحليل الصور الطبية لتشخيص الأمراض، يمكنك استخدام API التشخيص الصوري في Transformers.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-plane-detection', 'api_call': \"YOLO('keremberke/yolov8m-plane-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-plane-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'plane-detection', 'accuracy': '0.995'}, 'description': 'A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.' </s>"
"<s>[INST] تحتاج إلى تحليل أداء تطبيقات الويب لتحسين تجربة المستخدم، يمكنك استخدام API التحليل السلوكي للمواقع الإلكترونية في Transformers.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-plane-detection', 'api_call': \"YOLO('keremberke/yolov8m-plane-detection')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['pip install ultralyticsplus==0.0.23 ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-plane-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'plane-detection', 'accuracy': '0.995'}, 'description': 'A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.' </s>"
"<s>[INST]قم بإنشاء تطبيق يستخدم API لتحليل المشاهد الطبيعية وتحديد المعالم البارزة بها مثل الجبال والبحيرات.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8s-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] يتوجب علينا إنشاء تطبيق يستفيد من API التعرف على النصوص لتحليل الوثائق القانونية وتصنيفها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8s-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] قم بتطوير تطبيق يستخدم API للتعرف على الجوانب الطبيعية في الصور وتسميتها بدقة مثل الغابات والشواطئ.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8s-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] قم ببناء تطبيق يستخدم API استشعار الحركة لتحليل أنماط الحركة للمستخدمين وتقديم تقارير تحليلية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8s-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم API لتحليل النصوص الطبية وتحديد الأمراض والأعراض الواردة في التقارير الطبية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8s-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] قم بتطوير تطبيق يستفيد من API التعرف على الصور للكشف عن التلاعب بالصور والتعديلات غير المصرح بها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8s-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم API التعرف على النصوص لمساعدة الطلاب في تحويل المواد المكتوبة إلى نصوص صوتية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8s-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] قم بتطوير تطبيق يستفيد من API التعرف على الوجوه لتصنيف المستخدمين وإعادة تطبيق التصنيفات على الصور الجديدة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8s-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] يتوجب علينا تطوير تطبيق يستفيد من API التعرف على النصوص لمراقبة تغييرات النصوص على الويب وتقديم تقارير حولها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8s-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم API التعرف على العناصر لتحليل صور المناظر الطبيعية واكتشاف الكائنات فيها بدقة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8s-csgo-player-detection')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.886}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST]تطوير تطبيق موسيقي يقدم توصيات شخصية للمستخدمين باستخدام واجهة برمجية Recommendation Engine.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-blood-cell-detection', 'api_call': \"YOLO('keremberke/yolov8m-blood-cell-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-blood-cell-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.927}, 'description': 'A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.' </s>"
"<s>[INST] تحليل بيانات التسوق للعملاء لتصميم نظام توصيات استنادًا إلى سلوكياتهم.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-blood-cell-detection', 'api_call': \"YOLO('keremberke/yolov8m-blood-cell-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-blood-cell-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.927}, 'description': 'A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.' </s>"
"<s>[INST] إنشاء نظام متقدم لتوصيات الأفلام باستخدام واجهة برمجية Recommendation Engine.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-blood-cell-detection', 'api_call': \"YOLO('keremberke/yolov8m-blood-cell-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-blood-cell-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.927}, 'description': 'A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.' </s>"
"<s>[INST] تطوير تطبيق تسويقي يستخدم تطبيق توصيات المنتجات لعرض منتجات مخصصة للعملاء.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-blood-cell-detection', 'api_call': \"YOLO('keremberke/yolov8m-blood-cell-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-blood-cell-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.927}, 'description': 'A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.' </s>"
"<s>[INST] تصميم نظام توجيه يعتمد على بيانات المرور باستخدام واجهة برمجية Navigation Engine.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-blood-cell-detection', 'api_call': \"YOLO('keremberke/yolov8m-blood-cell-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-blood-cell-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.927}, 'description': 'A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.' </s>"
"<s>[INST] بناء نظام ذكي لتوصيات الأزياء باستخدام واجهة برمجية Recommendation Engine.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-blood-cell-detection', 'api_call': \"YOLO('keremberke/yolov8m-blood-cell-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-blood-cell-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.927}, 'description': 'A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.' </s>"
"<s>[INST] إنشاء نظام توصيات للوجبات الصحية بناءً على تفضيلات المستخدم باستخدام واجهة برمجية Recommendation Engine.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-blood-cell-detection', 'api_call': \"YOLO('keremberke/yolov8m-blood-cell-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-blood-cell-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.927}, 'description': 'A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.' </s>"
"<s>[INST] تطوير تطبيق تعليمي يوفر توصيات ترفيهية للأطفال باستخدام واجهة برمجية Recommendation Engine.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-blood-cell-detection', 'api_call': \"YOLO('keremberke/yolov8m-blood-cell-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-blood-cell-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.927}, 'description': 'A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.' </s>"
"<s>[INST] بناء نظام إقتراحات للعروض السياحية يستخدم الذكاء الصناعي لتحليل تفضيلات العملاء.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-blood-cell-detection', 'api_call': \"YOLO('keremberke/yolov8m-blood-cell-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-blood-cell-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.927}, 'description': 'A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.' </s>"
"<s>[INST] تصميم منصة توصيات لعلم النفس تستخدم تحليل النصوص لفهم سلوك المستخدمين.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8m-blood-cell-detection', 'api_call': \"YOLO('keremberke/yolov8m-blood-cell-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-blood-cell-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.927}, 'description': 'A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.' </s>"
"<s>[INST]كيف يمكنني استخدام API لتحليل صور المناظر الطبيعية وتحديد العناصر الرئيسية؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8s-hard-hat-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-hard-hat-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.834}, 'description': 'An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحليل الصور الطبية واكتشاف الأورام الخبيثة بدقة؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8s-hard-hat-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-hard-hat-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.834}, 'description': 'An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحليل النصوص وتحديد المفردات الرئيسية في كتب الطهي؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8s-hard-hat-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-hard-hat-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.834}, 'description': 'An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.' </s>"
"<s>[INST] كيف يمكنني تطبيق تحليل الصوت باستخدام API لتحديد الموسيقى الشعبية؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8s-hard-hat-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-hard-hat-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.834}, 'description': 'An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.' </s>"
"<s>[INST] كيف يمكنني تطبيق تحليل النصوص الطبية باستخدام API لتحديد الأعراض الرئيسية في التقارير الطبية؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8s-hard-hat-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-hard-hat-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.834}, 'description': 'An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.' </s>"
"<s>[INST] كيف يمكنني تطبيق استخدام تقنية OCR باستخدام API لاستخراج النصوص من الصور الضوئية؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8s-hard-hat-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-hard-hat-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.834}, 'description': 'An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.' </s>"
"<s>[INST] كيف يمكنني تطبيق تحليل الأداء في الرياضات الإلكترونية بواسطة API لمعرفة اللاعبين البارزين؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8s-hard-hat-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-hard-hat-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.834}, 'description': 'An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتحليل النمط الرقمي في الصور الفنية وتحديد العناصر البارزة؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8s-hard-hat-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-hard-hat-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.834}, 'description': 'An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.' </s>"
"<s>[INST] كيف يمكنني تطبيق تحليل الانطباعات السياحية باستخدام API لفهم تفضيلات السياح؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8s-hard-hat-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-hard-hat-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.834}, 'description': 'An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.' </s>"
"<s>[INST] كيف يمكنني استخدام API لتطوير تطبيق تعليمي يقدم دروسًا مبتكرة باستخدام تقنيات الذكاء الصناعي؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8s-hard-hat-detection', 'api_call': \"YOLO('keremberke/yolov8s-hard-hat-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-hard-hat-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'hard-hat-detection', 'accuracy': 0.834}, 'description': 'An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.' </s>"
"<s>[INST]\"استخدم Google Cloud Vision API لاستخراج النص من صورة مستند مطبوع.\" تأكد من استدعاء الأسلوب document_text_detection باستخدام معلمات API المناسبة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Transformers', 'functionality': 'Object Detection', 'api_name': 'fcakyon/yolov5s-v7.0', 'api_call': \"yolov5.load('fcakyon/yolov5s-v7.0')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic': False, 'multi_label': False, 'max_det': 1000, 'img': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'size': 640, 'augment': True}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': \"import yolov5\\nmodel = yolov5.load('fcakyon/yolov5s-v7.0')\\nmodel.conf = 0.25\\nmodel.iou = 0.45\\nmodel.agnostic = False\\nmodel.multi_label = False\\nmodel.max_det = 1000\\nimg = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model(img)\\nresults = model(img, size=640)\\nresults = model(img, augment=True)\\npredictions = results.pred[0]\\nboxes = predictions[:, :4]\\nscores = predictions[:, 4]\\ncategories = predictions[:, 5]\\nresults.show()\\nresults.save(save_dir='results/')\", 'performance': {'dataset': 'detection-datasets/coco', 'accuracy': None}, 'description': 'Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.' </s>"
"<s>[INST] قم بتطوير ميزة تتيح ترجمة اللغة في الوقت الفعلي للتدفقات الصوتية باستخدام Microsoft Translator Speech API. استخدم وظيفة الترجمة المتدفقة المتوفرة في واجهة برمجة التطبيقات.، نفذ تحليل المشاعر على مجموعة من منشورات وسائل التواصل الاجتماعي باستخدام واجهة برمجة تطبيقات Hugging Face Transformers. استفد من نموذج تحليل المشاعر المُدرب مسبقًا للحصول على نتائج دقيقة.، استخدم OpenAI GPT-3 API لإنشاء أوصاف منتج إبداعية لموقع ويب للتجارة الإلكترونية. قم باستدعاء وظيفة إنشاء النص باستخدام معلمات API المناسبة.، قم بدمج IBM Watson Visual Recognition API لتصنيف الصور بناءً على فئات محددة مسبقًا. استخدم وظيفة تصنيف الصور للحصول على نتائج دقيقة.، عزز أمان تطبيق الويب الخاص بك عن طريق تنفيذ مصادقة المستخدم باستخدام Auth0 API. استخدم وظيفة المصادقة لتمكين التحكم في الوصول الآمن.، نفذ وضع علامات تلقائية على الصور باستخدام Clarifai Vision API لتعيين العلامات ذات الصلة للصور من أجل التصنيف الفعال. استخدم وظيفة وضع العلامات لتحسين تنظيم الصور.، استخدم Amazon Rekognition API لاكتشاف النص واستخراجه من الصور التي تحتوي على علامات وملصقات. استخدم وظيفة الكشف عن النص لاسترداد بيانات النص بدقة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Transformers', 'functionality': 'Object Detection', 'api_name': 'fcakyon/yolov5s-v7.0', 'api_call': \"yolov5.load('fcakyon/yolov5s-v7.0')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic': False, 'multi_label': False, 'max_det': 1000, 'img': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'size': 640, 'augment': True}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': \"import yolov5\\nmodel = yolov5.load('fcakyon/yolov5s-v7.0')\\nmodel.conf = 0.25\\nmodel.iou = 0.45\\nmodel.agnostic = False\\nmodel.multi_label = False\\nmodel.max_det = 1000\\nimg = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model(img)\\nresults = model(img, size=640)\\nresults = model(img, augment=True)\\npredictions = results.pred[0]\\nboxes = predictions[:, :4]\\nscores = predictions[:, 4]\\ncategories = predictions[:, 5]\\nresults.show()\\nresults.save(save_dir='results/')\", 'performance': {'dataset': 'detection-datasets/coco', 'accuracy': None}, 'description': 'Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.' </s>"
"<s>[INST] \"عزز قدرات المحادثة الخاصة ببرنامج chatbot من خلال دمج واجهة برمجة تطبيقات Dialogflow لفهم اللغة الطبيعية ومعالجتها. استخدم وظيفة التعرف على نوايا Dialogflow لتحسين استجابات chatbot.\"? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Transformers', 'functionality': 'Object Detection', 'api_name': 'fcakyon/yolov5s-v7.0', 'api_call': \"yolov5.load('fcakyon/yolov5s-v7.0')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic': False, 'multi_label': False, 'max_det': 1000, 'img': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'size': 640, 'augment': True}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': \"import yolov5\\nmodel = yolov5.load('fcakyon/yolov5s-v7.0')\\nmodel.conf = 0.25\\nmodel.iou = 0.45\\nmodel.agnostic = False\\nmodel.multi_label = False\\nmodel.max_det = 1000\\nimg = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model(img)\\nresults = model(img, size=640)\\nresults = model(img, augment=True)\\npredictions = results.pred[0]\\nboxes = predictions[:, :4]\\nscores = predictions[:, 4]\\ncategories = predictions[:, 5]\\nresults.show()\\nresults.save(save_dir='results/')\", 'performance': {'dataset': 'detection-datasets/coco', 'accuracy': None}, 'description': 'Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.' </s>"
"<s>[INST] استخدم واجهة برمجة تطبيقات Face++ لـ تنفيذ التعرف على الوجه للتحقق من الهوية في التطبيق الخاص بك. استفد من وظيفة التعرف على الوجه لاكتشاف الوجوه والتعرف عليها بدقة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Transformers', 'functionality': 'Object Detection', 'api_name': 'fcakyon/yolov5s-v7.0', 'api_call': \"yolov5.load('fcakyon/yolov5s-v7.0')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic': False, 'multi_label': False, 'max_det': 1000, 'img': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'size': 640, 'augment': True}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': \"import yolov5\\nmodel = yolov5.load('fcakyon/yolov5s-v7.0')\\nmodel.conf = 0.25\\nmodel.iou = 0.45\\nmodel.agnostic = False\\nmodel.multi_label = False\\nmodel.max_det = 1000\\nimg = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model(img)\\nresults = model(img, size=640)\\nresults = model(img, augment=True)\\npredictions = results.pred[0]\\nboxes = predictions[:, :4]\\nscores = predictions[:, 4]\\ncategories = predictions[:, 5]\\nresults.show()\\nresults.save(save_dir='results/')\", 'performance': {'dataset': 'detection-datasets/coco', 'accuracy': None}, 'description': 'Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.' </s>"
"<s>[INST]ترغب في تطوير تطبيق يستخدم تقنيات الذكاء الصناعي لتحليل نمط الكتابة اليدوية والتعرف على الحروف والأرقام.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8n-table-extraction', 'api_call': \"YOLO('keremberke/yolov8n-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8n-table-extraction')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'table-extraction', 'accuracy': 0.967}, 'description': \"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\" </s>"
"<s>[INST] تحتاج إلى استخدام خدمة API لتحليل الصور الطبية وتقديم توصيات طبية استناداً إلى النتائج.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8n-table-extraction', 'api_call': \"YOLO('keremberke/yolov8n-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8n-table-extraction')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'table-extraction', 'accuracy': 0.967}, 'description': \"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\" </s>"
"<s>[INST] تقوم ببناء تطبيق لتحليل النصوص الفنية والفهم منها باللغة العربية، تحتاج لاستخدام API تقوم بتمييز المصطلحات الفنية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8n-table-extraction', 'api_call': \"YOLO('keremberke/yolov8n-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8n-table-extraction')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'table-extraction', 'accuracy': 0.967}, 'description': \"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\" </s>"
"<s>[INST] تود بناء نظام يعتمد على الذكاء الاصطناعي لتعلم تمييز الأماكن السياحية من الصور.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8n-table-extraction', 'api_call': \"YOLO('keremberke/yolov8n-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8n-table-extraction')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'table-extraction', 'accuracy': 0.967}, 'description': \"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\" </s>"
"<s>[INST] ترغب في تصميم نظام لتصحيح النصوص العربية الكتابية، تحتاج إلى API تقوم بتصنيف الأخطاء الإملائية وتصويبها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8n-table-extraction', 'api_call': \"YOLO('keremberke/yolov8n-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8n-table-extraction')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'table-extraction', 'accuracy': 0.967}, 'description': \"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\" </s>"
"<s>[INST] تحتاج إلى تطوير تطبيق لتحليل مشاهد الفيديو وتنبؤ بالأحداث المحتملة، يجب استخدام API لتحديد الأشخاص والأشياء في الفيديو.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8n-table-extraction', 'api_call': \"YOLO('keremberke/yolov8n-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8n-table-extraction')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'table-extraction', 'accuracy': 0.967}, 'description': \"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\" </s>"
"<s>[INST] لديك مشروع لاختبار فعالية الإعلانات التجارية على وسائل التواصل الاجتماعي، تحتاج إلى API لتحديد الكائنات والنصوص الموجودة في الصور الإعلانية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8n-table-extraction', 'api_call': \"YOLO('keremberke/yolov8n-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8n-table-extraction')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'table-extraction', 'accuracy': 0.967}, 'description': \"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\" </s>"
"<s>[INST] تريد تطوير تطبيق يمكنه تحديد الموسيقى المُعزَزة بتقنيات الذكاء الاصطناعي، تحتاج إلى API تقوم بتصنيف الأدوات الموسيقية في الأغاني.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8n-table-extraction', 'api_call': \"YOLO('keremberke/yolov8n-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8n-table-extraction')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'table-extraction', 'accuracy': 0.967}, 'description': \"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\" </s>"
"<s>[INST] تخطط لتطوير تطبيق لمساعدة المكفوفين في التعرف على الأشياء من حولهم، تحتاج إلى API تقوم بتحديد الكائنات والمشاهد في العالم الواقعي.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8n-table-extraction', 'api_call': \"YOLO('keremberke/yolov8n-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8n-table-extraction')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'table-extraction', 'accuracy': 0.967}, 'description': \"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\" </s>"
"<s>[INST] تريد تحسين عملية الفحص الطبي عبر الإنترنت باستخدام التكنولوجيا، تحتاج إلى استخدام API لتحليل النصوص الطبية واستخراج المعلومات الأساسية منها.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Table Extraction', 'api_name': 'keremberke/yolov8n-table-extraction', 'api_call': \"YOLO('keremberke/yolov8n-table-extraction')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8n-table-extraction')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'table-extraction', 'accuracy': 0.967}, 'description': \"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\" </s>"
"<s>[INST]تحتاج إلى تطوير تطبيق يقدم خدمة تحليل الصور واستخراج النص منها. كيف يمكنك استخدام Hugging Face Transformers API لتحقيق هذا؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'clipseg-rd64-refined', 'api_call': \"pipeline('image-segmentation', model='CIDAS/clipseg-rd64-refined')\", 'api_arguments': {'model': 'CIDAS/clipseg-rd64-refined'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by Lüddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.' </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم تعلّم الآلة لتحليل مشاعر النصوص باللغة العربية. كيف يمكن استخدام TextBlob API لتحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'clipseg-rd64-refined', 'api_call': \"pipeline('image-segmentation', model='CIDAS/clipseg-rd64-refined')\", 'api_arguments': {'model': 'CIDAS/clipseg-rd64-refined'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by Lüddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.' </s>"
"<s>[INST] ابتكر تطبيقًا يستخدم تقنيات تعلم الآلة للتعرف على أشكال الوجوه في الصور. كيف يمكن استخدام OpenCV API لتحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'clipseg-rd64-refined', 'api_call': \"pipeline('image-segmentation', model='CIDAS/clipseg-rd64-refined')\", 'api_arguments': {'model': 'CIDAS/clipseg-rd64-refined'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by Lüddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.' </s>"
"<s>[INST] تحتاج إلى تطوير تطبيق يقوم بتحويل النص المكتوب بخط اليد إلى نص رقمي. كيف يمكنك استخدام Google Cloud Vision API لتحقيق هذا؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'clipseg-rd64-refined', 'api_call': \"pipeline('image-segmentation', model='CIDAS/clipseg-rd64-refined')\", 'api_arguments': {'model': 'CIDAS/clipseg-rd64-refined'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by Lüddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.' </s>"
"<s>[INST] أنشئ تطبيقًا يستخدم تقنيات تعلم الآلة لتوليد نصوص أدبية بشكل تلقائي. كيف يمكنك استخدام GPT-3 API لتحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'clipseg-rd64-refined', 'api_call': \"pipeline('image-segmentation', model='CIDAS/clipseg-rd64-refined')\", 'api_arguments': {'model': 'CIDAS/clipseg-rd64-refined'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by Lüddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.' </s>"
"<s>[INST] ابتكر تطبيقًا يستخدم تعلم الآلة لتنبؤ بسلوك مستخدمي موقع الويب. كيف يمكن استخدام Tensorflow API لتحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'clipseg-rd64-refined', 'api_call': \"pipeline('image-segmentation', model='CIDAS/clipseg-rd64-refined')\", 'api_arguments': {'model': 'CIDAS/clipseg-rd64-refined'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by Lüddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.' </s>"
"<s>[INST] أنشئ تطبيقًا يستخدم تقنيات تعلم الآلة لتصنيف الصور الفوتوغرافية. كيف يمكن استخدام Amazon Rekognition API لتحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'clipseg-rd64-refined', 'api_call': \"pipeline('image-segmentation', model='CIDAS/clipseg-rd64-refined')\", 'api_arguments': {'model': 'CIDAS/clipseg-rd64-refined'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by Lüddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.' </s>"
"<s>[INST] قم بتطوير تطبيق يستخدم تقنيات تعلم الآلة لاكتشاف التلاعب بالصور. كيف يمكنك استخدام DeepArt API لتحقيق هذا؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'clipseg-rd64-refined', 'api_call': \"pipeline('image-segmentation', model='CIDAS/clipseg-rd64-refined')\", 'api_arguments': {'model': 'CIDAS/clipseg-rd64-refined'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by Lüddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.' </s>"
"<s>[INST] أنشئ تطبيقًا يستخدم تقنيات تعلم الآلة لتوليد صور واقعية لأشخاص غير موجودين. كيف يمكنك استخدام StyleGAN API لتحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'clipseg-rd64-refined', 'api_call': \"pipeline('image-segmentation', model='CIDAS/clipseg-rd64-refined')\", 'api_arguments': {'model': 'CIDAS/clipseg-rd64-refined'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by Lüddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.' </s>"
"<s>[INST] تطوّر تطبيقًا يستخدم تقنيات تعلّم الآلة لتحليل نبرة الصوت في المواقع التسويقية. كيف يمكن استخدام Microsoft Azure Speech API لتحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'clipseg-rd64-refined', 'api_call': \"pipeline('image-segmentation', model='CIDAS/clipseg-rd64-refined')\", 'api_arguments': {'model': 'CIDAS/clipseg-rd64-refined'}, 'python_environment_requirements': 'transformers', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by Lüddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.' </s>"
"<s>[INST]قم بإنشاء تطبيق يستخدم تقنيات التعرف على الكائنات للكشف عن النباتات في الصور.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8n-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8n-csgo-player-detection')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.844}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] قم بتصميم أداة تستخدم تكنولوجيا تعرف العناصر لتدقيق صور السياحة عبر الإنترنت.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8n-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8n-csgo-player-detection')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.844}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] قم بتطوير أداة للكشف عن العوامل البيئية في الصور الجوية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8n-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8n-csgo-player-detection')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.844}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] قم بتحسين تطبيق الكشف عن مركبات النقل باستخدام تقنيات حديثة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8n-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8n-csgo-player-detection')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.844}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] قم بإنشاء أداة لتحليل الصور الطبية وكشف التغييرات في النسيج العضلي.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8n-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8n-csgo-player-detection')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.844}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] قم بتطوير تطبيق يستخدم الذكاء الاصطناعي للكشف عن الوجوه وتحديد هوياتهم.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8n-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8n-csgo-player-detection')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.844}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] قم بإنشاء تطبيق للكشف عن العناصر الضارة في الصور الطبية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8n-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8n-csgo-player-detection')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.844}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] قم بتصميم أداة تستخدم التطبيقات الذكية لتحديد المعالم السياحية في الصور الطبيعية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8n-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8n-csgo-player-detection')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.844}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] قم بتحديث تطبيق الكشف عن الكائنات البحرية باستخدام تقنيات متقدمة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8n-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8n-csgo-player-detection')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.844}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST] قم بتصميم أداة تستخدم الذكاء الاصطناعي لتحليل الصور الوبائية والتغطية الإخبارية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Object Detection', 'api_name': 'keremberke/yolov8n-csgo-player-detection', 'api_call': \"YOLO('keremberke/yolov8n-csgo-player-detection')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\n\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\n\\nresults = model.predict(image)\\n\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'csgo-object-detection', 'accuracy': 0.844}, 'description': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\" </s>"
"<s>[INST]تطوير تطبيق لتحليل البيانات اللغوية وتوفير احتماليات الترجمة الآلية بين العديد من اللغات المختلفة لتحسين التواصل.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5s-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5s-license-plate')\", 'api_arguments': {'img': 'image url or path', 'size': 'image resize dimensions', 'augment': 'optional, test time augmentation'}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5s-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.985}, 'description': 'A YOLOv5 based license plate detection model trained on a custom dataset.' </s>"
"<s>[INST] إعداد بيئة افتراضية لتطوير نظام توصية تجارية قائم على التعلم الآلي لمتجر إلكتروني.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5s-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5s-license-plate')\", 'api_arguments': {'img': 'image url or path', 'size': 'image resize dimensions', 'augment': 'optional, test time augmentation'}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5s-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.985}, 'description': 'A YOLOv5 based license plate detection model trained on a custom dataset.' </s>"
"<s>[INST] تنفيذ تحليل استرجاع النصوص لتعظيم التعرف على الكتابة اليدوية في صور الوثائق الرقمية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5s-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5s-license-plate')\", 'api_arguments': {'img': 'image url or path', 'size': 'image resize dimensions', 'augment': 'optional, test time augmentation'}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5s-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.985}, 'description': 'A YOLOv5 based license plate detection model trained on a custom dataset.' </s>"
"<s>[INST] تطوير تطبيق ذكاء اصطناعي لتحديد نوع الأزهار من الصور باستخدام الشبكات العصبية العميقة.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5s-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5s-license-plate')\", 'api_arguments': {'img': 'image url or path', 'size': 'image resize dimensions', 'augment': 'optional, test time augmentation'}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5s-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.985}, 'description': 'A YOLOv5 based license plate detection model trained on a custom dataset.' </s>"
"<s>[INST] بناء نموذج تعلم آلي يمكن تدريبه على مجموعة بيانات العاب الفيديو للتنبؤ بسلوك اللاعبين وتحسين تجربتهم.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5s-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5s-license-plate')\", 'api_arguments': {'img': 'image url or path', 'size': 'image resize dimensions', 'augment': 'optional, test time augmentation'}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5s-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.985}, 'description': 'A YOLOv5 based license plate detection model trained on a custom dataset.' </s>"
"<s>[INST] تجميع بيانات الجغرافيا والمواقع لإنشاء خدمة تتبع لمواقع السيارات عبر الإنترنت.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5s-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5s-license-plate')\", 'api_arguments': {'img': 'image url or path', 'size': 'image resize dimensions', 'augment': 'optional, test time augmentation'}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5s-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.985}, 'description': 'A YOLOv5 based license plate detection model trained on a custom dataset.' </s>"
"<s>[INST] استخدام تقنيات تعلم الآلة لتطوير نظام توصية للموسيقى يقترح قوائم تشغيل مخصصة لكل مستخدم بناءً على ذوقه الموسيقي.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5s-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5s-license-plate')\", 'api_arguments': {'img': 'image url or path', 'size': 'image resize dimensions', 'augment': 'optional, test time augmentation'}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5s-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.985}, 'description': 'A YOLOv5 based license plate detection model trained on a custom dataset.' </s>"
"<s>[INST] تصميم واجهة برمجة تطبيقات (API) لتسهيل التكامل مع خدمة الدفع الإلكتروني ومعالجة المعاملات المالية عبر الإنترنت.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5s-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5s-license-plate')\", 'api_arguments': {'img': 'image url or path', 'size': 'image resize dimensions', 'augment': 'optional, test time augmentation'}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5s-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.985}, 'description': 'A YOLOv5 based license plate detection model trained on a custom dataset.' </s>"
"<s>[INST] تنفيذ تحليل الشعور والمشاعر على نصوص الوسائط الاجتماعية لفهم تفاعل المستخدمين مع المحتوى الرقمي.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5s-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5s-license-plate')\", 'api_arguments': {'img': 'image url or path', 'size': 'image resize dimensions', 'augment': 'optional, test time augmentation'}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5s-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.985}, 'description': 'A YOLOv5 based license plate detection model trained on a custom dataset.' </s>"
"<s>[INST] تطوير تطبيق يُستخدم في مجال الطب لتشخيص أمراض الجلد عن طريق تحليل الصور الطبية واقتراح خطة علاجية.? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'License Plate Detection', 'api_name': 'keremberke/yolov5s-license-plate', 'api_call': \"yolov5.load('keremberke/yolov5s-license-plate')\", 'api_arguments': {'img': 'image url or path', 'size': 'image resize dimensions', 'augment': 'optional, test time augmentation'}, 'python_environment_requirements': 'pip install -U yolov5', 'example_code': ['import yolov5', \"model = yolov5.load('keremberke/yolov5s-license-plate')\", 'model.conf = 0.25', 'model.iou = 0.45', 'model.agnostic = False', 'model.multi_label = False', 'model.max_det = 1000', \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model(img, size=640)', 'results = model(img, augment=True)', 'predictions = results.pred[0]', 'boxes = predictions[:, :4]', 'scores = predictions[:, 4]', 'categories = predictions[:, 5]', 'results.show()', \"results.save(save_dir='results/')\"], 'performance': {'dataset': 'keremberke/license-plate-object-detection', 'accuracy': 0.985}, 'description': 'A YOLOv5 based license plate detection model trained on a custom dataset.' </s>"
"<s>[INST]كيف يمكنني تطبيق نظام لتحليل الصور الطبية باستخدام تقنيات الذكاء الاصطناعي؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'openmmlab/upernet-convnext-small', 'api_call': \"UperNetModel.from_pretrained('openmmlab/upernet-convnext-small')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.' </s>"
"<s>[INST] كيف يمكنني تطوير نظام لمعالجة اللغة الطبي؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'openmmlab/upernet-convnext-small', 'api_call': \"UperNetModel.from_pretrained('openmmlab/upernet-convnext-small')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.' </s>"
"<s>[INST] كيف يمكنني استخدام تقنيات التعلم العميق لتحسين أداء نظام الاستشعار عن بُعد؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'openmmlab/upernet-convnext-small', 'api_call': \"UperNetModel.from_pretrained('openmmlab/upernet-convnext-small')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.' </s>"
"<s>[INST] كيف يمكنني تطبيق نموذج توقعي لتصنيف الأخبار المالية؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'openmmlab/upernet-convnext-small', 'api_call': \"UperNetModel.from_pretrained('openmmlab/upernet-convnext-small')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.' </s>"
"<s>[INST] كيف يمكنني تحليل مشاهد الفيديو للكشف عن حركات غير طبيعية باستخدام تقنيات الذكاء الاصطناعي؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'openmmlab/upernet-convnext-small', 'api_call': \"UperNetModel.from_pretrained('openmmlab/upernet-convnext-small')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.' </s>"
"<s>[INST] كيف يمكنني تطوير نظام لتقدير عمق المحيطات باستخدام البيانات الجيومكانية؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'openmmlab/upernet-convnext-small', 'api_call': \"UperNetModel.from_pretrained('openmmlab/upernet-convnext-small')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.' </s>"
"<s>[INST] كيف يمكنني اقتراح تحسينات لأداء نظام التحليل اللغوي باستخدام تقنيات التوليد التلقائي؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'openmmlab/upernet-convnext-small', 'api_call': \"UperNetModel.from_pretrained('openmmlab/upernet-convnext-small')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.' </s>"
"<s>[INST] كيف يمكنني تصميم نظام لتحديد اتجاه السيارة باستخدام تقنيات تعرف الإشارات؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'openmmlab/upernet-convnext-small', 'api_call': \"UperNetModel.from_pretrained('openmmlab/upernet-convnext-small')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.' </s>"
"<s>[INST] كيف يمكنني استخدام تقنيات التعلم العميق لتحليل النصوص الطبية واكتشاف الأمراض؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'openmmlab/upernet-convnext-small', 'api_call': \"UperNetModel.from_pretrained('openmmlab/upernet-convnext-small')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.' </s>"
"<s>[INST] كيف يمكنني تطوير نموذج لتصنيف أنماط النباتات باستخدام الصور؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'openmmlab/upernet-convnext-small', 'api_call': \"UperNetModel.from_pretrained('openmmlab/upernet-convnext-small')\", 'api_arguments': 'N/A', 'python_environment_requirements': 'transformers', 'example_code': 'N/A', 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.' </s>"
"<s>[INST]تحتاج إلى تطبيق لتحليل وتصنيف النصوص باللغة العربية بشكل تلقائي. هل هناك API متخصص في ذلك؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Blood Cell Detection', 'api_name': 'keremberke/yolov8n-blood-cell-detection', 'api_call': \"YOLO('keremberke/yolov8n-blood-cell-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-blood-cell-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.893}, 'description': 'This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.' </s>"
"<s>[INST] ترغب في تطوير تطبيق لمساعدة المستخدمين في ترجمة النصوص الطبية إلى العربية. هل هناك API ترجمة متخصصة في مجال الطب؟? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Blood Cell Detection', 'api_name': 'keremberke/yolov8n-blood-cell-detection', 'api_call': \"YOLO('keremberke/yolov8n-blood-cell-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-blood-cell-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.893}, 'description': 'This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.' </s>"
"<s>[INST] ترغب في تطبيق يستطيع تحديد الأشخاص في صور العائلة بالزمن الحقيقي. هل هناك API ملائم لهذه المهمة؟\\n...\\n\\nFeel free to ask for more examples or specific domains!? [/INST]'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Blood Cell Detection', 'api_name': 'keremberke/yolov8n-blood-cell-detection', 'api_call': \"YOLO('keremberke/yolov8n-blood-cell-detection')\", 'api_arguments': {'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-blood-cell-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'blood-cell-object-detection', 'accuracy': 0.893}, 'description': 'This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.' </s>"
"<s>[INST]\"تطوير أداة لتحليل مشاعر وسائل التواصل الاجتماعي لتحليل اتجاهات الرأي العام.\"، \"إنشاء نظام توصيات للتجارة الإلكترونية استنادًا إلى تفضيلات المستخدم وسجل التصفح.\"، \"إنشاء روبوت محادثة يمكنه مساعدة المستخدمين في حجز تذاكر الطيران والتحقق من الرحلة.\" الجداول الزمنية.، تطوير أداة للتنبؤ بالطقس توفر تحديثات وتنبيهات كل ساعة لظروف الطقس القاسية.، تصميم نظام رؤية حاسوبية لاكتشاف الأشياء في لقطات كاميرات المراقبة وتوفير تنبيهات للتهديدات المحتملة.، تطوير سوق الأوراق المالية. أداة تنبؤ باستخدام خوارزميات التعلم الآلي لتحليل بيانات المخزون التاريخية.، \"إنشاء نظام مراقبة صحي يتتبع الأنشطة اليومية للمستخدمين ومعدل ضربات القلب وأنماط النوم باستخدام أجهزة استشعار يمكن ارتداؤها.\"، \"إنشاء نظام توصية إخباري مخصص يقدم مقالات إخبارية بناءً على اهتمامات المستخدمين وعادات القراءة.\"، \"تطوير نظام كشف الاحتيال للمعاملات المالية باستخدام نماذج التعلم الآلي لتحديد الأنشطة المشبوهة.\"، \"تصميم نظام تصنيف المستندات الذي يصنف المستندات النصية إلى موضوعات أو فئات محددة مسبقًا.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b0-finetuned-ade-512-512', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\", 'api_arguments': {'images': 'Image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'SegformerImageProcessor, SegformerForSemanticSegmentation', 'PIL': 'Image', 'requests': 'requests'}, 'example_code': 'from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nprocessor = SegformerImageProcessor.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST]\"تطوير نظام للتوصية باقتراحات الأفلام المخصصة بناءً على تفضيلات المستخدم باستخدام واجهة برمجة التطبيقات المحددة.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-ade-640-640', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20K', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] \"إنشاء روبوت محادثة يوفر تحديثات وتنبؤات الطقس في الوقت الفعلي باستخدام واجهة برمجة التطبيقات المخصصة لمعلومات الطقس.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-ade-640-640', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20K', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] \"تنفيذ نظام للإشراف على المحتوى لتصفية المحتوى غير المناسب ضمن البيانات التي ينشئها المستخدم باستخدام واجهة برمجة التطبيقات المحددة.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-ade-640-640', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20K', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] \"دمج إمكانات التعرف على الكلام في تطبيق مساعد افتراضي باستخدام واجهة برمجة التطبيقات المخصصة لتحويل الكلام إلى نص بشكل دقيق.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-ade-640-640', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20K', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] \" أنشئ نظامًا ينبه المستخدمين بالأحداث القادمة والمواعيد النهائية باستخدام واجهات برمجة التطبيقات المتاحة للجدولة والتذكيرات. تحويل النصوص التاريخية إلى تسلسلات صوتية لتحسين تجربة القراءة للأفراد ضعاف البصر.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-ade-640-640', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20K', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] \"تريد شركتك دمج خدمة التعرف الذكي على الكلام في تطبيقك الحالي.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-ade-640-640', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20K', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] \"إنشاء نظام لتحليل المشاعر لمنشورات وسائل التواصل الاجتماعي باستخدام واجهة برمجة التطبيقات المحددة لتحليل النص.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-ade-640-640', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20K', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] \"دمج ميزة ترجمة اللغة في تطبيقك باستخدام واجهة برمجة التطبيقات المخصصة لدعم متعدد اللغات.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-ade-640-640', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\", 'api_arguments': ['images', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'ADE20K', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST]قم بتطوير نظام محادثات آلي يستجيب لاستفسارات العملاء بشكل فوري وفعال.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b2-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] صمم نظام إدارة محتوى متقدم يتيح للمستخدمين إنشاء وتعديل المحتوى بسهولة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b2-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] أنشئ تطبيق ويب لعرض وإدارة البيانات الجغرافية باستخدام API جغرافية متقدمة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b2-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] قم ببناء نظام إدارة الأوراق المالية الشخصية الذي يتيح للمستخدمين تتبع وإدارة استثماراتهم.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b2-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] عرف نظام إنذار ذكي يراقب بيئة العمل الخاصة بشركتك وينبه عند اكتشاف أي تغيير غير مألوف.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b2-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] قم ببناء نظام إدارة العلاقات مع العملاء الذي يتيح للفرق التسويقية تتبع الاهتمامات والمعاملات لكل عميل.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b2-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] ألق نظرة على تصميم وتنفيذ نظام إدارة العمليات بشكل فعال يعتمد على API متكاملة للتواصل بين الأقسام المختلفة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b2-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] قم بتطوير نظام إدارة الإنتاج الذكي الذي يستخدم تقنيات الذكاء الصناعي لتحسين كفاءة عمليات التصنيع.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b2-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] أنشئ نظام توزيع لهواتفك الذكية يستخدم API لتتبع الطلبات وإدارة المخزون بكفاءة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b2-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] قم بتطوير نظام محرك بحث ذكي يستخدم تقنيات تعلم الآلة لتحسين دقة البحث وسرعته.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b2-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST]قم بتطوير نظام للكشف التلقائي عن الجراثيم في الصور الطبية بواسطة تحليل الصور.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] أنشئ تطبيق للتعرف على النصوص اليدوية في الصور وتحويلها إلى نصوص قابلة للتحرير.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] قم بتطوير نظام لتحديد الأشخاص في الصور الجماعية وتسميتهم تلقائيًا باستخدام تقنيات التعلم العميق.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] قم بتطوير تطبيق لتحديد النقاط الرئيسية في الصور الجيولوجية وتمييز وتعيينها بتقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] أنشئ تطبيقًا يستخدم تقنيات التعلم الآلي لتحليل الصور الطبية وتحديد الأمراض فيها.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] قم بإنشاء تطبيق لتحليل الوثائق القانونية من خلال تعريف النصوص وتصنيفها تلقائيًا باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] قم بتطوير تطبيق لتحليل الصور الفضائية وتحديد المناطق المهمة فيها باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] أنشئ تطبيقًا يستخدم تقنيات التعلم الآلي لتقييم جودة المنتجات في الصور التجارية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] قم بتطوير نظام لتحليل النصوص الطبية واستخراج المفاهيم الطبية الرئيسية لتسهيل التصفح والبحث.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] أنشئ تطبيقًا يستخدم تحليل النصوص والصور لتقديم توصيات فريدة للمستخدمين في مجال السفر والاستكشاف.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'pretrained_model_name_or_path': 'nvidia/segformer-b0-finetuned-cityscapes-1024-1024'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST]? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'facebook/detr-resnet-50-panoptic', 'api_call': \"DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\", 'api_arguments': ['image'], 'python_environment_requirements': ['torch', 'numpy', 'transformers', 'PIL', 'requests', 'io'], 'example_code': ['import io', 'import requests', 'from PIL import Image', 'import torch', 'import numpy', 'from transformers import DetrFeatureExtractor, DetrForSegmentation', 'from transformers.models.detr.feature_extraction_detr import rgb_to_id', \"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\", 'image = Image.open(requests.get(url, stream=True).raw)', \"feature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50-panoptic')\", \"model = DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\", \"inputs = feature_extractor(images=image, return_tensors='pt')\", 'outputs = model(**inputs)', \"processed_sizes = torch.as_tensor(inputs['pixel_values'].shape[-2:]).unsqueeze(0)\", 'result = feature_extractor.post_process_panoptic(outputs, processed_sizes)[0]', \"panoptic_seg = Image.open(io.BytesIO(result['png_string']))\", 'panoptic_seg = numpy.array(panoptic_seg, dtype=numpy.uint8)', 'panoptic_seg_id = rgb_to_id(panoptic_seg)'], 'performance': {'dataset': 'COCO 2017 validation', 'accuracy': {'box_AP': 38.8, 'segmentation_AP': 31.1, 'PQ': 43.4}}, 'description': 'DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.' </s>"
"<s>[INST]ترغب في بناء نظام تعرف على الصوت لتحليل وتحويل النص إلى نص مسموع. يمكنك استخدام API SpeechRecognition في Google Cloud لتحويل الصوت إلى نص.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\n\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] تخطط لتطوير تطبيق توقيع إلكتروني يمكنه تحويل التواقيع اليدوية إلى ملفات رقمية. يمكنك استخدام API التالي للتعامل مع تحويل الصور إلى نص.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\n\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] تريد تطوير تطبيق لتحليل المشاهد في أفلام الكرتون وتتابع الشخصيات. يمكنك استخدام API التالي للكشف عن الوجوه في مشاهد الفيديو.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\n\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] تخطط لتطوير تطبيق لتصنيف الصور بناءً على محتواها. يمكنك استخدام API التالي لتحويل الصور إلى تصنيفات نصية.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\n\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] تريد تطبيقًا يساعد المستخدمين في تحويل النصوص العربية إلى نطق صوتي. يمكنك استخدام API التالي لتوليد الصوت من النص.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\n\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] ترغب في تطوير تطبيق تحليل مواجهات الفريق في مباريات كرة القدم. يمكنك استخدام API التالي للكشف عن الوجوه وتحليل ملامحها.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\n\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] ترغب في تطوير تطبيق يمكنه تحليل نمط الكتابة اليدوية على الصور. يمكنك استخدام API التالي لتحديد النصوص على الصورة.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\n\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] تخطط لتطوير تطبيق للتعرف على الأشجار من الصور التي يقوم المستخدم بإلتقاطها. يمكنك استخدام API التالي لتحديد نوع الأشجار في الصور.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\n\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] تخطط لتطوير تطبيق يمكنه تحليل الصور الطبية لتشخيص الأمراض. يمكنك استخدام API التالي للكشف عن الأورام في الصور الطبية.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\n\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] ترغب في تطوير تطبيق لعرض المعالم السياحية والوصف الخاص بها بناءً على الصور. يمكنك استخدام API التالي لتحديد المعالم والتصنيفات في الصور.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\n\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST]اكتب رواية قصيرة تتحدث عن مغامرات مجموعة من المستكشفين في الفضاء الخارجي واستخدم خوارزمية Hugging Face Transformers لتوليد شخصية الروبوت الرئيسية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'mattmdjaga/segformer_b2_clothes', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'matplotlib', 'torch'], 'example_code': \"from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nimport matplotlib.pyplot as plt\\nimport torch.nn as nn\\nextractor = AutoFeatureExtractor.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nurl = 'https://plus.unsplash.com/premium_photo-1673210886161-bfcc40f54d1f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8cGVyc29uJTIwc3RhbmRpbmd8ZW58MHx8MHx8&amp;w=1000&amp;q=80'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits.cpu()\\nupsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode='bilinear', align_corners=False)\\npred_seg = upsampled_logits.argmax(dim=1)[0]\\nplt.imshow(pred_seg)\", 'performance': {'dataset': 'mattmdjaga/human_parsing_dataset', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ATR dataset for clothes segmentation.' </s>"
"<s>[INST] ابتكر تطبيقًا جديدًا للتعلم الآلي يستخدم خوارزمية تصنيف الصور البصرية المتقدمة المقدمة من Hugging Face Transformers.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'mattmdjaga/segformer_b2_clothes', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'matplotlib', 'torch'], 'example_code': \"from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nimport matplotlib.pyplot as plt\\nimport torch.nn as nn\\nextractor = AutoFeatureExtractor.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nurl = 'https://plus.unsplash.com/premium_photo-1673210886161-bfcc40f54d1f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8cGVyc29uJTIwc3RhbmRpbmd8ZW58MHx8MHx8&amp;w=1000&amp;q=80'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits.cpu()\\nupsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode='bilinear', align_corners=False)\\npred_seg = upsampled_logits.argmax(dim=1)[0]\\nplt.imshow(pred_seg)\", 'performance': {'dataset': 'mattmdjaga/human_parsing_dataset', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ATR dataset for clothes segmentation.' </s>"
"<s>[INST] كتب كتيبًا تعليميًا حول كيفية استخدام خوارزمية Hugging Face Transformers للمساعدة في إنشاء نصوص إعلانية فعالة عبر الإنترنت.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'mattmdjaga/segformer_b2_clothes', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'matplotlib', 'torch'], 'example_code': \"from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nimport matplotlib.pyplot as plt\\nimport torch.nn as nn\\nextractor = AutoFeatureExtractor.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nurl = 'https://plus.unsplash.com/premium_photo-1673210886161-bfcc40f54d1f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8cGVyc29uJTIwc3RhbmRpbmd8ZW58MHx8MHx8&amp;w=1000&amp;q=80'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits.cpu()\\nupsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode='bilinear', align_corners=False)\\npred_seg = upsampled_logits.argmax(dim=1)[0]\\nplt.imshow(pred_seg)\", 'performance': {'dataset': 'mattmdjaga/human_parsing_dataset', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ATR dataset for clothes segmentation.' </s>"
"<s>[INST] أنشئ نموذجاً جديداً لتعلم الآلة يستخدم خوارزمية Hugging Face Transformers لتحليل وتجميع بيانات المرضى وتوقع تشخيصاتهم الصحية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'mattmdjaga/segformer_b2_clothes', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'matplotlib', 'torch'], 'example_code': \"from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nimport matplotlib.pyplot as plt\\nimport torch.nn as nn\\nextractor = AutoFeatureExtractor.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nurl = 'https://plus.unsplash.com/premium_photo-1673210886161-bfcc40f54d1f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8cGVyc29uJTIwc3RhbmRpbmd8ZW58MHx8MHx8&amp;w=1000&amp;q=80'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits.cpu()\\nupsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode='bilinear', align_corners=False)\\npred_seg = upsampled_logits.argmax(dim=1)[0]\\nplt.imshow(pred_seg)\", 'performance': {'dataset': 'mattmdjaga/human_parsing_dataset', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ATR dataset for clothes segmentation.' </s>"
"<s>[INST] ابتكر ألعابًا تعليمية تفاعلية تستند إلى تقنيات الذكاء الاصطناعي باستخدام خوارزمية Hugging Face Transformers لتوليد الشخصيات والقصص.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'mattmdjaga/segformer_b2_clothes', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'matplotlib', 'torch'], 'example_code': \"from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nimport matplotlib.pyplot as plt\\nimport torch.nn as nn\\nextractor = AutoFeatureExtractor.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nurl = 'https://plus.unsplash.com/premium_photo-1673210886161-bfcc40f54d1f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8cGVyc29uJTIwc3RhbmRpbmd8ZW58MHx8MHx8&amp;w=1000&amp;q=80'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits.cpu()\\nupsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode='bilinear', align_corners=False)\\npred_seg = upsampled_logits.argmax(dim=1)[0]\\nplt.imshow(pred_seg)\", 'performance': {'dataset': 'mattmdjaga/human_parsing_dataset', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ATR dataset for clothes segmentation.' </s>"
"<s>[INST] قم بتطوير نظام توصية موسيقية شخصي يستخدم خوارزمية Hugging Face Transformers لتحليل تفضيلات المستخدم وتوليد قوائم التشغيل المخصصة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'mattmdjaga/segformer_b2_clothes', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'matplotlib', 'torch'], 'example_code': \"from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nimport matplotlib.pyplot as plt\\nimport torch.nn as nn\\nextractor = AutoFeatureExtractor.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nurl = 'https://plus.unsplash.com/premium_photo-1673210886161-bfcc40f54d1f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8cGVyc29uJTIwc3RhbmRpbmd8ZW58MHx8MHx8&amp;w=1000&amp;q=80'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits.cpu()\\nupsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode='bilinear', align_corners=False)\\npred_seg = upsampled_logits.argmax(dim=1)[0]\\nplt.imshow(pred_seg)\", 'performance': {'dataset': 'mattmdjaga/human_parsing_dataset', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ATR dataset for clothes segmentation.' </s>"
"<s>[INST] أنشئ تطبيقًا لتحليل النصوص والفهم اللغوي باستخدام خوارزمية Hugging Face Transformers لمساعدة المستخدمين في فهم محتوى النصوص الطويلة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'mattmdjaga/segformer_b2_clothes', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'matplotlib', 'torch'], 'example_code': \"from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nimport matplotlib.pyplot as plt\\nimport torch.nn as nn\\nextractor = AutoFeatureExtractor.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nurl = 'https://plus.unsplash.com/premium_photo-1673210886161-bfcc40f54d1f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8cGVyc29uJTIwc3RhbmRpbmd8ZW58MHx8MHx8&amp;w=1000&amp;q=80'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits.cpu()\\nupsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode='bilinear', align_corners=False)\\npred_seg = upsampled_logits.argmax(dim=1)[0]\\nplt.imshow(pred_seg)\", 'performance': {'dataset': 'mattmdjaga/human_parsing_dataset', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ATR dataset for clothes segmentation.' </s>"
"<s>[INST] ابتكر نظامًا ذكيًا لمراقبة الصحة العامة باستخدام البيانات الوطنية وتقنيات التعلم الآلي المبتكرة من Hugging Face Transformers.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'mattmdjaga/segformer_b2_clothes', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'matplotlib', 'torch'], 'example_code': \"from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nimport matplotlib.pyplot as plt\\nimport torch.nn as nn\\nextractor = AutoFeatureExtractor.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nurl = 'https://plus.unsplash.com/premium_photo-1673210886161-bfcc40f54d1f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8cGVyc29uJTIwc3RhbmRpbmd8ZW58MHx8MHx8&amp;w=1000&amp;q=80'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits.cpu()\\nupsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode='bilinear', align_corners=False)\\npred_seg = upsampled_logits.argmax(dim=1)[0]\\nplt.imshow(pred_seg)\", 'performance': {'dataset': 'mattmdjaga/human_parsing_dataset', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ATR dataset for clothes segmentation.' </s>"
"<s>[INST] أنشئ تطبيقًا لمساعدة في ترجمة النصوص بين لغات مختلفة باستخدام خوارزمية Hugging Face Transformers لترجمة اللغات.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'mattmdjaga/segformer_b2_clothes', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'matplotlib', 'torch'], 'example_code': \"from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nimport matplotlib.pyplot as plt\\nimport torch.nn as nn\\nextractor = AutoFeatureExtractor.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nurl = 'https://plus.unsplash.com/premium_photo-1673210886161-bfcc40f54d1f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8cGVyc29uJTIwc3RhbmRpbmd8ZW58MHx8MHx8&amp;w=1000&amp;q=80'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits.cpu()\\nupsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode='bilinear', align_corners=False)\\npred_seg = upsampled_logits.argmax(dim=1)[0]\\nplt.imshow(pred_seg)\", 'performance': {'dataset': 'mattmdjaga/human_parsing_dataset', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ATR dataset for clothes segmentation.' </s>"
"<s>[INST] قم بتطوير نظام ذكي للتعرف على أوجه الشخصيات وتحليل العواطف من الصور باستخدام البصريات المتقدمة المقدمة من Hugging Face Transformers.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'mattmdjaga/segformer_b2_clothes', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\", 'api_arguments': ['image'], 'python_environment_requirements': ['transformers', 'PIL', 'requests', 'matplotlib', 'torch'], 'example_code': \"from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nimport matplotlib.pyplot as plt\\nimport torch.nn as nn\\nextractor = AutoFeatureExtractor.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nurl = 'https://plus.unsplash.com/premium_photo-1673210886161-bfcc40f54d1f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8cGVyc29uJTIwc3RhbmRpbmd8ZW58MHx8MHx8&amp;w=1000&amp;q=80'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits.cpu()\\nupsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode='bilinear', align_corners=False)\\npred_seg = upsampled_logits.argmax(dim=1)[0]\\nplt.imshow(pred_seg)\", 'performance': {'dataset': 'mattmdjaga/human_parsing_dataset', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on ATR dataset for clothes segmentation.' </s>"
"<s>[INST]لإنشاء نظام لتحديد حالة الطرق والتنبؤ بحركة المرور، استخدم الـ API التالي:? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-base-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-base-coco-panoptic'}, 'python_environment_requirements': {'packages': ['requests', 'torch', 'PIL', 'transformers']}, 'example_code': \"import requests\\nimport torch\\nfrom PIL import Image\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\n\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': None}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] كيف يمكنني تطبيق تحليل الصوت لاستخراج المعلومات الدقيقة؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-base-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-base-coco-panoptic'}, 'python_environment_requirements': {'packages': ['requests', 'torch', 'PIL', 'transformers']}, 'example_code': \"import requests\\nimport torch\\nfrom PIL import Image\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\n\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': None}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] لإنشاء تطبيق لتحليل سلوك المستخدمين على المواقع الإلكترونية، استخدم الـ API التالي:? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-base-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-base-coco-panoptic'}, 'python_environment_requirements': {'packages': ['requests', 'torch', 'PIL', 'transformers']}, 'example_code': \"import requests\\nimport torch\\nfrom PIL import Image\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\n\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': None}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] كيف يمكن تطبيق التصنيف الآلي للصور لتحليل المضمون بدقة؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-base-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-base-coco-panoptic'}, 'python_environment_requirements': {'packages': ['requests', 'torch', 'PIL', 'transformers']}, 'example_code': \"import requests\\nimport torch\\nfrom PIL import Image\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\n\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': None}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] لتطبيق تحليل النصوص العربية للاستخلاص المعلوماتية، قم باستخدام الـ API التالي:? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-base-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-base-coco-panoptic'}, 'python_environment_requirements': {'packages': ['requests', 'torch', 'PIL', 'transformers']}, 'example_code': \"import requests\\nimport torch\\nfrom PIL import Image\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\n\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': None}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] كيف يمكن تصميم نظام لتحليل المشاهد في الأفلام باستخدام الـ API التالي:? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-base-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-base-coco-panoptic'}, 'python_environment_requirements': {'packages': ['requests', 'torch', 'PIL', 'transformers']}, 'example_code': \"import requests\\nimport torch\\nfrom PIL import Image\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\n\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': None}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] لتطبيق تحليل المشتريات ومراقبة سلوك العملاء، استخدم الـ API التالي:? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-base-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-base-coco-panoptic'}, 'python_environment_requirements': {'packages': ['requests', 'torch', 'PIL', 'transformers']}, 'example_code': \"import requests\\nimport torch\\nfrom PIL import Image\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\n\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': None}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] كيف يمكنني تطبيق محرك بحث نصي لتصنيف الوثائق؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-base-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-base-coco-panoptic'}, 'python_environment_requirements': {'packages': ['requests', 'torch', 'PIL', 'transformers']}, 'example_code': \"import requests\\nimport torch\\nfrom PIL import Image\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\n\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': None}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] لتنفيذ تحليل بيانات الوضع الحالي لسوق الأسهم، استخدم الـ API التالي:? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-base-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-base-coco-panoptic'}, 'python_environment_requirements': {'packages': ['requests', 'torch', 'PIL', 'transformers']}, 'example_code': \"import requests\\nimport torch\\nfrom PIL import Image\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\n\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': None}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] كيف يمكن تطبيق تحليل الأداء وتقييم العمليات بشكل شهري؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-base-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-base-coco-panoptic'}, 'python_environment_requirements': {'packages': ['requests', 'torch', 'PIL', 'transformers']}, 'example_code': \"import requests\\nimport torch\\nfrom PIL import Image\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\n\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\n\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': None}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST]لدي مجموعة من الصور الطبية وأرغب في تحليلها باستخدام تقنيات الرؤية الحاسوبية. هل يمكنني الاعتماد على API معين لتنفيذ هذه المهمة؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-cityscapes-semantic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-large-cityscapes-semantic'}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.' </s>"
"<s>[INST] أود إدراج تحليل الصور في تطبيق الذكاء الاصطناعي الخاص بي. هل يمكنك مساعدتي في العثور على API مناسب لتنفيذ هذه المهمة؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-cityscapes-semantic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-large-cityscapes-semantic'}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.' </s>"
"<s>[INST] كيف يمكنني إدراج تحليل بيانات الوجوه في تطبيقي التفاعلي؟ هل يمكنني استخدام API معين لتحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-cityscapes-semantic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-large-cityscapes-semantic'}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.' </s>"
"<s>[INST] أرغب في تحسين دقة نظام الاستشعار عن بُعد في تطبيقي. هل يمكنني استخدام API معين لتحقيق هذا الهدف؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-cityscapes-semantic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-large-cityscapes-semantic'}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.' </s>"
"<s>[INST] كيف يمكنني تنفيذ تحليل حقول النجوم باستخدام API في تطبيقي الحالي؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-cityscapes-semantic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-large-cityscapes-semantic'}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.' </s>"
"<s>[INST] كيف يمكنني تطبيق تحليل صور الأقمار الصناعية في تطبيقي الجديد؟ هل يوجد API محدد لهذا النوع من المهام؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-cityscapes-semantic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-large-cityscapes-semantic'}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.' </s>"
"<s>[INST] كيف يمكنني تحليل البيانات الصوتية باستخدام نماذج الذكاء الاصطناعي؟ هل هناك API معين يمكنني الاعتماد عليه؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-cityscapes-semantic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-large-cityscapes-semantic'}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.' </s>"
"<s>[INST] كيف يمكنني تنفيذ تحليل نصوص العملاء لتحسين تجربة المستخدم في تطبيقي؟ هل يوجد API موثوق لتنفيذ هذه المهمة؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-cityscapes-semantic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-large-cityscapes-semantic'}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.' </s>"
"<s>[INST] هل يمكنني استخدام نموذج معين لتحليل بيانات التسويق بشكل فعال؟ وهل هناك API يمكنني الاستفادة منه لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-cityscapes-semantic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-large-cityscapes-semantic'}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.' </s>"
"<s>[INST] أحتاج إلى تنفيذ تحليل النصوص الطبية في تطبيق الرعاية الصحية الذكي الذي أقوم بتطويره. هل هناك API يمكنني اللجوء إليه لتنفيذ هذه المهمة؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-cityscapes-semantic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-large-cityscapes-semantic'}, 'python_environment_requirements': ['torch', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", 'performance': {'dataset': 'Cityscapes', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.' </s>"
"<s>[INST]يمكنك استخدام واجهة برمجية Google Cloud Vision API للكشف عن الأشخاص في الصور واستخراج المعلومات الرئيسية عنهم.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_coco_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_coco_swin_large')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/coco.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_coco_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ydshieh/coco_dataset_script', 'accuracy': 'Not provided'}, 'description': 'OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] لتطوير منصة لتحليل بيانات المرضى، يمكن استخدام واجهة برمجية IBM Watson Health لاستخراج بيانات دقيقة عن حالتهم الصحية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_coco_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_coco_swin_large')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/coco.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_coco_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ydshieh/coco_dataset_script', 'accuracy': 'Not provided'}, 'description': 'OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] لتوليف موسيقى جديدة باستخدام LibROSA، يمكنك استخدام واجهة برمجية Spotify Web API للوصول إلى تشغيل الموسيقى والتلاعب بها.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_coco_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_coco_swin_large')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/coco.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_coco_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ydshieh/coco_dataset_script', 'accuracy': 'Not provided'}, 'description': 'OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] لتحليل سلوك المستهلكين، يمكنك استخدام واجهة برمجية Facebook Graph API للوصول إلى بيانات الاستخدام والتفاعل على منصة فيسبوك.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_coco_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_coco_swin_large')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/coco.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_coco_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ydshieh/coco_dataset_script', 'accuracy': 'Not provided'}, 'description': 'OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] لإنشاء تطبيق لتحليل نصوص العملاء، يمكنك استخدام واجهة Natural Language Processing API من Google Cloud لاستخراج المعلومات الهامة من النصوص.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_coco_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_coco_swin_large')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/coco.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_coco_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ydshieh/coco_dataset_script', 'accuracy': 'Not provided'}, 'description': 'OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] لتنفيذ تحليل البيانات الكبيرة، يمكنك استخدام واجهة Hadoop MapReduce API لتقسيم وتعليم أشجار البيانات ذات الحجم الكبير.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_coco_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_coco_swin_large')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/coco.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_coco_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ydshieh/coco_dataset_script', 'accuracy': 'Not provided'}, 'description': 'OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] لتحليل توجهات الأسواق، يمكن استخدام واجهة برمجية Google Trends API لاستخراج البيانات الحالية وتوقع الاتجاهات المستقبلية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_coco_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_coco_swin_large')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/coco.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_coco_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ydshieh/coco_dataset_script', 'accuracy': 'Not provided'}, 'description': 'OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] لإنشاء تطبيق للمراقبة الذكية، يمكنك استخدام واجهة Human Activity Recognition API لتحديد الأنشطة التي يقوم بها الأشخاص.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_coco_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_coco_swin_large')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/coco.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_coco_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ydshieh/coco_dataset_script', 'accuracy': 'Not provided'}, 'description': 'OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] يمكن استخدام واجهة برمجية TensorFlow Lite لتنفيذ نماذج التعلم الآلي على الأجهزة المحمولة وتحقيق أداء عالي للموديل.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_coco_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_coco_swin_large')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/coco.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_coco_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ydshieh/coco_dataset_script', 'accuracy': 'Not provided'}, 'description': 'OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] لتطوير حلول لذكاء الأعمال، يمكنك استخدام واجهة Power BI REST API لمزامنة واستعراض البيانات من مصادر متعددة بشكل مركزي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_coco_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_coco_swin_large')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/coco.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_coco_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ydshieh/coco_dataset_script', 'accuracy': 'Not provided'}, 'description': 'OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST]يحتاج عميلك إلى أداة لتحويل النص إلى صور معلومات مالية افتراضية. اشرح كيفية تحقيق ذلك باستخدام واجهة برمجة التطبيقات المتوفرة.، نحتاج إلى توفير أداة لإنشاء نصوص تعليمية لتدريب العملاء على استخدام تطبيقنا الجديد.، استخدم واجهة برمجة التطبيقات لإنشاء صور منظمة لاجتماع طلابي عبر الإنترنت.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-large-ade', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-large-ade')\", 'api_arguments': {'from_pretrained': 'facebook/maskformer-swin-large-ade'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': \"from transformers import MaskFormerImageProcessor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = MaskFormerImageProcessor.from_pretrained('facebook/maskformer-swin-large-ade')\\ninputs = processor(images=image, return_tensors='pt')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-large-ade')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.' </s>"
"<s>[INST]تطوير تطبيق يستخدم API لتحويل الصور إلى لوحات فنية بشكل تلقائي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\", 'api_arguments': ['images', 'task_inputs', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'scene_parse_150', 'accuracy': None}, 'description': 'OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم API للكشف عن الأشياء/المشاهد في الصور وتصنيفها تلقائياً.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\", 'api_arguments': ['images', 'task_inputs', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'scene_parse_150', 'accuracy': None}, 'description': 'OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم API للتعرف على النصوص في الصور واستخراجها بشكل تلقائي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\", 'api_arguments': ['images', 'task_inputs', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'scene_parse_150', 'accuracy': None}, 'description': 'OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] أنشئ تطبيق يستخدم API لتحليل الرسومات البيانية واستخراج البيانات المهمة منها.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\", 'api_arguments': ['images', 'task_inputs', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'scene_parse_150', 'accuracy': None}, 'description': 'OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] تحتاج إلى إنشاء تطبيق يستخدم API لتحويل الكلمات المنطوقة إلى نص كتابي بشكل تلقائي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\", 'api_arguments': ['images', 'task_inputs', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'scene_parse_150', 'accuracy': None}, 'description': 'OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم API لتحويل النصوص إلى ملفات صوتية بصيغة MP3.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\", 'api_arguments': ['images', 'task_inputs', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'scene_parse_150', 'accuracy': None}, 'description': 'OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] أنشئ تطبيق يستخدم API للتعرف على الأشخاص في الصور وتسمية الوجوه بشكل تلقائي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\", 'api_arguments': ['images', 'task_inputs', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'scene_parse_150', 'accuracy': None}, 'description': 'OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] تنفيذ نظام يستخدم API لتحليل النشرات الإخبارية واستخراج المعلومات الهامة منها.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\", 'api_arguments': ['images', 'task_inputs', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'scene_parse_150', 'accuracy': None}, 'description': 'OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] أنشئ تطبيق يستخدم API لتحليل المشاهد من ملفات الفيديو وتصنيفها تلقائياً.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\", 'api_arguments': ['images', 'task_inputs', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'scene_parse_150', 'accuracy': None}, 'description': 'OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] تحتاج إلى تطبيق يستخدم API لتحليل المسح الضوئي للمستندات واستخراج النصوص والصور منها بشكل تلقا\\u0602ي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_large', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_large')\", 'api_arguments': ['images', 'task_inputs', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'scene_parse_150', 'accuracy': None}, 'description': 'OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST]يسعى العميل لتطوير نظام لتحليل السلوك العملائي من خلال تقنيات التعلم الآلي. يمكنك استخدام الواجهة البرمجية التالية لاستدعاء النموذج المناسب:? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\", 'api_arguments': ['image'], 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': 'processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result[segmentation]', 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] قم بتصميم نظام توصيات شخصية لتطبيق الهاتف المحمول الجديد الذي تم تطويره. استخدم الواجهة البرمجية التالية للحصول على نموذج تعلم آلي لهذا الغرض:? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\", 'api_arguments': ['image'], 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': 'processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result[segmentation]', 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] لتحليل بيانات المبيعات والتنبؤ بالطلب، يتم استخدام ال API التالي:? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\", 'api_arguments': ['image'], 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': 'processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result[segmentation]', 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] بناء نظام استشعار ذكي للكشف عن حرائق في البيئات الحضرية باستخدام أحدث تقنيات التعلم العميق. يمكنك تحقيق ذلك باستخدام الواجهة البرمجية التالية:? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\", 'api_arguments': ['image'], 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': 'processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result[segmentation]', 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] لتطوير نظام تمييز الأصوات الأليفة باستخدام الشبكات العصبية العميقة، يمكنك استخدام ال API التالي:? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\", 'api_arguments': ['image'], 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': 'processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result[segmentation]', 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] ضع نموذج توقعي للإقبال على فندق معين باستخدام البيانات التاريخية وتقنيات التحليل التنبؤي. يمكنك الاعتماد على الواجهة البرمجية التالية:? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\", 'api_arguments': ['image'], 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': 'processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result[segmentation]', 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] قم بنشر نظام توصيات موسيقية مخصصة باستخدام تقنيات التعلم العميق. يمكنك اعتماد الواجهة البرمجية التالية للوصول إلى النموذج المناسب:? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\", 'api_arguments': ['image'], 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': 'processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result[segmentation]', 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] أنشىء نظام للتعرف على الحشرات الزراعية ضارة باستخدام تقنيات تعلم الآلة. يتوفر الواجهة البرمجية التالية لمساعدتك على تنفيذ الحل:? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\", 'api_arguments': ['image'], 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': 'processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result[segmentation]', 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] لتطوير نظام للتعرف على الطيور من الصور باستخدام الذكاء الصناعي، يمكنك الاستفادة من ال API التالي:? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\", 'api_arguments': ['image'], 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': 'processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result[segmentation]', 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] عند بناء نظام لتصنيف الأفلام بناءً على تفضيلات المستخدمين، يمكنك استخدام الواجهة البرمجية التالية لتكوين نموذج تعلم آلي مناسب:? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-large-coco-panoptic', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\", 'api_arguments': ['image'], 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': 'processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result[segmentation]', 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST]قم بتصميم نظام يقوم بتحليل ملامح الوجه في الصورة وتصنيفها وفقًا للعمر والنوع الجنسي باستخدام تقنيات التعلم الآلي. يمكنك استخدام API Face Recognition التالي لتنفيذ ذلك.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-small-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-small-coco-instance'}, 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] أنشئ نظاما يقوم بالكشف عن الأشياء الموجودة في الصورة وتحديد أماكن وجودها. يمكنك استخدام API Object Detection التالي لتحقيق هذا الهدف.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-small-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-small-coco-instance'}, 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] قم بتنفيذ نظام يستطيع تحليل نص اليدوية المكتوب بخط اليد وتحويله إلى نص مكتوب بواسطة آلة. يمكنك استخدام API Handwriting Recognition التالي لتحقيق ذلك.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-small-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-small-coco-instance'}, 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] إعداد نظام يستخدم تقنيات التعلم الآلي لتحليل المشاعر الواردة من تعبيرات الوجه في الصور. يمكنك استخدام ال API Emotion Recognition التالي لتحقيق هذا الهدف.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-small-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-small-coco-instance'}, 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] نريد بناء نظام قادر على التعرف على الأشخاص من الصور الواردة وتحديد هويتهم. يمكنك استخدام API Face Identification التالي لتطوير هذا النظام.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-small-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-small-coco-instance'}, 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] عندك مجموعة من البيانات الصوتية وترغب في تحويلها إلى نص مكتوب. قم بتطوير نظام لتحويل الصوت إلى نص باستخدام ال API Speech Recognition التالي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-small-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-small-coco-instance'}, 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] انشئ نظام يقوم بتحليل مقاطع الفيديو للكشف عن مواقع التصوير والأشخاص الموجودين باستخدام تقنيات التعلم الآلي. يمكنك استخدام API Video Analysis التالي لتحقيق هذا الهدف.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-small-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-small-coco-instance'}, 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] أعد تصميم جسم الإنسان في الصورة ليبدو وكأنه رسم كرتوني باستخدام تقنيات التعلم الآلي. يمكنك استخدام API Image Style Transfer التالي لتحقيق ذلك.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-small-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-small-coco-instance'}, 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] قم ببناء نظام يتمكن من تحليل الموسيقى الموجودة في النصوص وتوصيف أنماطها الموسيقية باستخدام تقنيات التعلم الآلي. يمكنك استخدام API Music Analysis التالي لتحقيق هذا الهدف.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-small-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-small-coco-instance'}, 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST] بناء نظام يتمكن من تصنيع نصوص خيالية جديدة استنادًا إلى مدخلات نصية معينة باستخدام تقنيات التعلم الآلي. يمكنك استخدام API Text Generation التالي لتطوير هذا النظام.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-small-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-small-coco-instance'}, 'python_environment_requirements': ['requests', 'torch', 'PIL', 'transformers'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not provided'}, 'description': 'Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.' </s>"
"<s>[INST]تطبيق البرمجة الخاص بالـ API لعمل نظام يولد النصوص التوضيحية للصور.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_tiny', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\n\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\ninstance_outputs = model(**instance_inputs)\\n\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\n\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\npanoptic_outputs = model(**panoptic_inputs)\\n\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم الـ API لتعلم آلي وتحليل النصوص الطبية لاكتشاف الأمراض المحتملة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_tiny', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\n\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\ninstance_outputs = model(**instance_inputs)\\n\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\n\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\npanoptic_outputs = model(**panoptic_inputs)\\n\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم الـ API لتوليد نصوص افتراضية توضح كيفية حل مشاكل برمجية مختلفة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_tiny', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\n\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\ninstance_outputs = model(**instance_inputs)\\n\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\n\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\npanoptic_outputs = model(**panoptic_inputs)\\n\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم الـ API لتحليل التعليقات على منصات التواصل الاجتماعي وتقديم تقرير بالمشاعر الموجودة فيها.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_tiny', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\n\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\ninstance_outputs = model(**instance_inputs)\\n\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\n\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\npanoptic_outputs = model(**panoptic_inputs)\\n\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم الـ API لتوليد توصيات لأفلام معينة استنادًا إلى أذواق المستخدم وتقييماته.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_tiny', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\n\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\ninstance_outputs = model(**instance_inputs)\\n\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\n\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\npanoptic_outputs = model(**panoptic_inputs)\\n\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم الـ API لتحليل الأسئلة والإجابات لتوليد ملخصات للمحتوى والمعلومات الهامة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_tiny', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\n\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\ninstance_outputs = model(**instance_inputs)\\n\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\n\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\npanoptic_outputs = model(**panoptic_inputs)\\n\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم الـ API لتحليل النصوص في الكتب الإلكترونية وتوليد ملخصات للفصول الهامة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_tiny', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\n\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\ninstance_outputs = model(**instance_inputs)\\n\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\n\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\npanoptic_outputs = model(**panoptic_inputs)\\n\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم الـ API للتعرف على الكائنات والأشخاص في الصور واستخراج معلومات محددة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_tiny', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\n\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\ninstance_outputs = model(**instance_inputs)\\n\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\n\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\npanoptic_outputs = model(**panoptic_inputs)\\n\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم الـ API لتوليد نصوص تعليمية مصورة توضح عمليات الحساب الرياضية الأساسية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_tiny', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\n\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\ninstance_outputs = model(**instance_inputs)\\n\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\n\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\npanoptic_outputs = model(**panoptic_inputs)\\n\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم الـ API لتوليد قصص قصيرة تصور مواقف مختلفة لتحسين الخيال الإبداعي للأطفال.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_tiny', 'api_call': \"OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_ade20k_swin_tiny')\", 'api_arguments': {'images': 'image', 'task_inputs': ['semantic', 'instance', 'panoptic'], 'return_tensors': 'pt'}, 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\n\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\n\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\n\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\ninstance_outputs = model(**instance_inputs)\\n\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\n\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\npanoptic_outputs = model(**panoptic_inputs)\\n\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.' </s>"
"<s>[INST]تحليل صور المأكولات وتقديم توصيات تحسين الأطباق. استخدم تقنيات التعرف على الصور لهذا الغرض.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-building-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-building-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.623, 'mAP@0.5(mask)': 0.613}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.' </s>"
"<s>[INST] تطوير نظام لتصنيف أماكن الجذب السياحي في مدينة معينة استنادًا إلى صور تم التقاطها بواسطة الزوار. استخدم تقنيات تفسير الصور لتنفيذ ذلك.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-building-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-building-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.623, 'mAP@0.5(mask)': 0.613}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.' </s>"
"<s>[INST] استكشاف وتحليل نمط العملاء في مواقع التجارة الإلكترونية. استخدم تقنيات التعرف على الوجوه لتحقيق هذا الهدف.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-building-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-building-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.623, 'mAP@0.5(mask)': 0.613}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.' </s>"
"<s>[INST] تصميم نظام لفحص جودة المنتجات في خط الإنتاج باستخدام الرؤية الحاسوبية. استخدم تقنيات التصنيف البصري للصور.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-building-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-building-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.623, 'mAP@0.5(mask)': 0.613}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.' </s>"
"<s>[INST] تحليل صور الطبيعة لتقديم توصيات بشأن الحفاظ على البيئة. استخدم تقنيات التعرف على النباتات والحيوانات.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-building-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-building-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.623, 'mAP@0.5(mask)': 0.613}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.' </s>"
"<s>[INST] تطوير نظام للكشف عن المشاهير في الفعاليات وتعريف الجمهور بالشخصيات المعروفة. استخدم تقنيات التعرف على الوجوه والبحث البصري.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-building-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-building-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.623, 'mAP@0.5(mask)': 0.613}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.' </s>"
"<s>[INST] إنشاء نظام يمكنه تحديد الأشخاص الذين لا يرتدون الكمامات في أماكن مزدحمة. استخدم تقنيات التعرف على الوجوه والتصنيف البصري.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-building-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-building-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.623, 'mAP@0.5(mask)': 0.613}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.' </s>"
"<s>[INST] تصميم نظام لتقدير عمر الأشخاص من خلال صورهم الشخصية. استخدم تقنيات التعرف على الوجوه والتقدير البصري.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-building-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-building-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.623, 'mAP@0.5(mask)': 0.613}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.' </s>"
"<s>[INST] بناء نموذج لتقدير مستوى السعادة في الصور الشخصية. استخدم تقنيات تحليل التعبيرات الوجهية والتصنيف البصري.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-building-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-building-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.623, 'mAP@0.5(mask)': 0.613}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.' </s>"
"<s>[INST] تطوير نظام للكشف عن العنف في مجموعات الصور. استخدم تقنيات التعرف على المواقف العنيفة لهذا الغرض.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-building-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-building-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': 'pip install ultralyticsplus==0.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.623, 'mAP@0.5(mask)': 0.613}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.' </s>"
"<s>[INST]تطوير تطبيق يستخدم تقنيات التعلم الآلي لتحليل الصور الطبية واكتشاف الأمراض المحتملة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] بناء نظام توصيف وتصنيف لوثائق اللغة العربية باستخدام تقنيات تحليل النصوص الذكية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] إنشاء تطبيق يقوم بتحليل توجيهات الدراجات الهوائية العربية المكتوبة باللغة العربية باستخدام تقنيات تحليل النصوص والتعلم الآلي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] تطوير نظام يستخدم تقنيات التعلم الآلي لتحليل الأنماط الزمنية في البيانات المالية العربية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم تقنيات التحليل الصوتي لتحويل النطق العربي إلى نص.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] بناء نظام تعلم آلي يستخدم شبكات عصبية لتحديد سلوك المستخدمين في تطبيقات الجوال باللغة العربية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] إنشاء أداة تستخدم تقنيات التحليل النصوص لفهم وتصنيف التعليقات على منصات التواصل الاجتماعي باللغة العربية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] بناء نظام استشعار ذكي يستخدم تقنيات الرؤية الحاسوبية للكشف عن الأشخاص داخل مبنى بناء عربي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] تصميم أداة تستخدم تقنيات التحليل الصوتي لتحويل الموسيقى العربية إلى تنغيمات موسيقية قابلة للتعديل.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST] تطوير أداة تستخدم تقنيات التعلم الآلي لتوقع العوامل الموقتة للازدحام في شبكات النقل العام بالمدن العربية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Semantic Segmentation', 'api_name': 'nvidia/segformer-b5-finetuned-cityscapes-1024-1024', 'api_call': \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')\", 'api_arguments': {'images': 'image', 'return_tensors': 'pt'}, 'python_environment_requirements': {'packages': ['transformers', 'PIL', 'requests']}, 'example_code': 'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits', 'performance': {'dataset': 'CityScapes', 'accuracy': 'Not provided'}, 'description': 'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.' </s>"
"<s>[INST]لتحسين تجربة المستخدمين في تطبيقك، ننصح باستخدام API لتحليل الصور وإضافة عناصر تفاعلية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-tiny-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-tiny-coco-instance'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.' </s>"
"<s>[INST] هل ترغب في إضافة قدرات تحليل الصور إلى منصتك الرقمية؟ يمكنك استخدام API لمعالجة الصور وتقديم بيانات موضوعية للعملاء.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-tiny-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-tiny-coco-instance'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.' </s>"
"<s>[INST] لتطوير منصة تفاعلية لعرض البيانات بطريقة مبسطة، ننصح باستخدام API لتحويل النصوص إلى صور توضح المفاهيم بشكل فعال.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-tiny-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-tiny-coco-instance'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.' </s>"
"<s>[INST] هل ترغب في تجديد تجربة متابعيك في منصات التواصل الاجتماعي؟ استخدم API لإنشاء صور مفهومية تعزز فهم المحتوى بشكل أفضل.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-tiny-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-tiny-coco-instance'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.' </s>"
"<s>[INST] لتعزيز فعالية التسويق الرقمي لمنتجاتك، يمكنك استخدام API لإضافة تفاصيل بصرية إلى المحتوى الإعلاني.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-tiny-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-tiny-coco-instance'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.' </s>"
"<s>[INST] تحتاج منصتك الرقمية إلى نظام لتوليد تقارير بيانية توضح البيانات والإحصاءات بشكل مبسط؟ استخدم API لتحويل البيانات إلى صور توضيحية قابلة للفهم.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-tiny-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-tiny-coco-instance'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.' </s>"
"<s>[INST] لإضافة بعد إبداعي إلى تطبيقك، استفد من API لتحويل النصوص المملة إلى صور ملهمة تشد انتباه المستخدمين.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-tiny-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-tiny-coco-instance'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.' </s>"
"<s>[INST] تطور الأداء البصري لتطبيقك من خلال API لتحليل الصور وإضافة تأثيرات بصرية مذهلة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-tiny-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-tiny-coco-instance'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.' </s>"
"<s>[INST] تهدف إلى تدعيم فعالية التواصل مع عملائك؟ استخدم API لإنشاء صور توضحية تسهل فهم خدماتك ومنتجاتك بشكل واضح.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-tiny-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-tiny-coco-instance'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.' </s>"
"<s>[INST] لتحقيق تجربة تفاعلية أفضل في تطبيقك، يُنصح باستخدام API لتحويل النصوص إلى صور ديناميكية تعزز تفاعل المستخدمين مع منصتك.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/mask2former-swin-tiny-coco-instance', 'api_call': \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\", 'api_arguments': {'pretrained_model_name_or_path': 'facebook/mask2former-swin-tiny-coco-instance'}, 'python_environment_requirements': ['torch', 'transformers', 'PIL', 'requests'], 'example_code': \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.' </s>"
"<s>[INST]إنشاء نظام لتحليل مشاعر العملاء من خلال تقنيات تعلم الآلة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-ade', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\", 'api_arguments': {'from_pretrained': 'facebook/maskformer-swin-base-ade'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': 'from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.' </s>"
"<s>[INST] بناء نظام تغذية إخبارية شخصية باستخدام تقنيات التجزئة الاجتماعية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-ade', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\", 'api_arguments': {'from_pretrained': 'facebook/maskformer-swin-base-ade'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': 'from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.' </s>"
"<s>[INST] تكامل برمجيات المحاسبة الخاصة بك مع مكتبة API توجيه المدفوعات لمعالجة المدفوعات الإلكترونية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-ade', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\", 'api_arguments': {'from_pretrained': 'facebook/maskformer-swin-base-ade'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': 'from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.' </s>"
"<s>[INST] إعداد نظام إدارة المشاريع الخاص بك باستخدام API إدارة المشاريع Jira.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-ade', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\", 'api_arguments': {'from_pretrained': 'facebook/maskformer-swin-base-ade'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': 'from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.' </s>"
"<s>[INST] تطوير واجهة برمجية لتوفير خدمة البحث الصوتي باستخدام تقنيات تحليل الصوت.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-ade', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\", 'api_arguments': {'from_pretrained': 'facebook/maskformer-swin-base-ade'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': 'from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.' </s>"
"<s>[INST] إنشاء نظام إدارة المحتوى الخاص بك باستخدام API إدارة المحتوى Contentful.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-ade', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\", 'api_arguments': {'from_pretrained': 'facebook/maskformer-swin-base-ade'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': 'from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.' </s>"
"<s>[INST] تكامل API خرائط Google مع نظام تتبع المواقع لزيادة دقة تحديد مواقع المستخدم.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-ade', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\", 'api_arguments': {'from_pretrained': 'facebook/maskformer-swin-base-ade'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': 'from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.' </s>"
"<s>[INST] بناء نظام إدارة المخزون الخاص بك باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-ade', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\", 'api_arguments': {'from_pretrained': 'facebook/maskformer-swin-base-ade'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': 'from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.' </s>"
"<s>[INST] تطوير واجهة برمجية لمعالحة بيانات استشعار الإنترنت الذكي لتوفير تحليل بيانات دقيق وموثوق.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-ade', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\", 'api_arguments': {'from_pretrained': 'facebook/maskformer-swin-base-ade'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': 'from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.' </s>"
"<s>[INST] إنشاء نظام لتوصيل الطلبات عبر تطبيق الهاتف الجوّال باستخدام خدمات توصيل الطلبات.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-base-ade', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\", 'api_arguments': {'from_pretrained': 'facebook/maskformer-swin-base-ade'}, 'python_environment_requirements': {'transformers': 'latest', 'PIL': 'latest', 'requests': 'latest'}, 'example_code': 'from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'ADE20k', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.' </s>"
"<s>[INST]\"إنشاء نظام chatbot يستخدم معالجة اللغة الطبيعية لفهم استعلامات المستخدم والرد عليها. يمكنك تحقيق ذلك من خلال الاستفادة من واجهة برمجة التطبيقات التي يوفرها Dialogflow من Google لبناء واجهات المحادثة.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'print(results[0].masks)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.568, 'mAP@0.5(mask)': 0.557}}, 'description': 'A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] تنفيذ نظام تحليل المشاعر لـ منشورات وسائل التواصل الاجتماعي باستخدام واجهة برمجة التطبيقات التي توفر نماذج مدربة مسبقًا لتصنيف المشاعر، مثل النموذج الذي يقدمه TextBlob لمهام معالجة النص واجهة برمجة التطبيقات المقدمة من LightFM لبناء نماذج التوصية بطريقة مختلطة.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'print(results[0].masks)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.568, 'mAP@0.5(mask)': 0.557}}, 'description': 'A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] \"قم بإنشاء أداة تلخيص نص تستخدم النماذج القائمة على المحولات لإنشاء ملخصات موجزة للمستندات الطويلة. يمكنك تحقيق ذلك من خلال الاستفادة من واجهة برمجة التطبيقات المقدمة من Hugging Face محولات لمهام تلخيص النص.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'print(results[0].masks)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.568, 'mAP@0.5(mask)': 0.557}}, 'description': 'A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] \"إنشاء نظام للتسميات التوضيحية التلقائية للصور التي تولد أوصافًا نصية للصور باستخدام نماذج التعلم العميق. استخدم واجهة برمجة تطبيقات Microsoft Azure Cognitive Services لتحليل الصور وإنشاء التسميات التوضيحية.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'print(results[0].masks)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.568, 'mAP@0.5(mask)': 0.557}}, 'description': 'A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] \"قم بتطوير نظام للتعرف على الكيانات المسماة في البيانات النصية باستخدام نموذج مُدرب مسبقًا من مكتبة spaCy. استخدم spaCy API للتعرف على الكيانات المسماة لاستخراج الكيانات مثل الأسماء والمواقع والمؤسسات من النص.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'print(results[0].masks)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.568, 'mAP@0.5(mask)': 0.557}}, 'description': 'A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] \"إنشاء نظام للكشف عن المشاعر لتحليل تعبيرات الوجه في الصور. استخدم مكتبة OpenCV جنبًا إلى جنب مع النماذج المدربة مسبقًا مثل مؤشر Dlib لمعالم الوجه للتعرف على مشاعر الوجه.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'print(results[0].masks)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.568, 'mAP@0.5(mask)': 0.557}}, 'description': 'A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] \"إنشاء نظام لإنشاء التعليمات البرمجية تلقائيًا استنادًا إلى أوصاف اللغة الطبيعية باستخدام Codex API بواسطة OpenAI. يجب أن يكون هذا النظام قادرًا على تحويل أوصاف اللغة البشرية إلى مقتطفات برمجية لمهام برمجة مختلفة.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'print(results[0].masks)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.568, 'mAP@0.5(mask)': 0.557}}, 'description': 'A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] \"تنفيذ نظام لاكتشاف الكائنات في الصور باستخدام نموذج مُدرب مسبقًا مثل Faster R-CNN المقدم من PyTorch. استخدم واجهة برمجة تطبيقات PyTorch لتحميل النموذج وإجراء اكتشاف الكائنات على الصور. \"\"، \"\"تطوير نظام لإنشاء النص باستخدام نموذج GPT-3 من OpenAI لإنشاء نص يشبه الإنسان استنادًا إلى المدخلات السريعة. استخدم OpenAI GPT-3 API لإنشاء نص ذي صلة بالسياق بناءً على مطالبات المستخدم.\"? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.24', 'ultralytics==8.0.23'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'print(results[0].masks)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.568, 'mAP@0.5(mask)': 0.557}}, 'description': 'A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST]استخدم الـ API التالي لتحليل نصوص العروض التقديمية واستخراج المعلومات الرئيسية منها.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-tiny-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] قم بتشغيل نموذج اللغة الطبيعية التالي لترجمة نصوص من الإنجليزية إلى العربية.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-tiny-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم الـ API التالي لتحويل النصوص إلى خطوط يدوية.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-tiny-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] قم بتصميم تطبيق يستخدم الـ API التالي لتحليل وتصنيف الصور الطبيعية.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-tiny-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] استخدم الـ API التالي لتحليل ومعالجة الصوتيات.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-tiny-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] قم بتطبيق الـ API التالي لتحسين جودة الصور الرقمية.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-tiny-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم الـ API التالي لتحليل الموسيقى والتعرف على الأغاني.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-tiny-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم الـ API التالي لتوليد نصوص إبداعية وجذابة.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-tiny-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] تطبيق الـ API التالي لتحليل الشخصيات في الروايات الأدبية.\\n   -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-tiny-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم الـ API التالي لتحليل وتصنيف الأطعمة في الصور.\\n    -? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'facebook/maskformer-swin-tiny-coco', 'api_call': \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\", 'api_arguments': ['image', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\n\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", 'performance': {'dataset': 'COCO panoptic segmentation', 'accuracy': 'Not provided'}, 'description': 'MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.' </s>"
"<s>[INST]تأتيني فكرة لتطوير تطبيق يستخدم تقنية تحليل الصور لتصنيف أنواع السمك. هل يمكن أن تقدم API لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local image path'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.858, 'mAP@0.5(mask)': 0.895}}, 'description': 'A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.' </s>"
"<s>[INST] أرغب في تصميم تطبيق يستخدم تقنية البصمة الصوتية للمصادقة على الهوية. هل هناك API متوفرة لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local image path'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.858, 'mAP@0.5(mask)': 0.895}}, 'description': 'A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.' </s>"
"<s>[INST] عندي مشروع لتطوير تطبيق لتحليل السلوك الصوتي للمستخدمين أثناء تفاعلهم مع التطبيقات التعليمية. ما ال API المناسب لهذه الغرض؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local image path'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.858, 'mAP@0.5(mask)': 0.895}}, 'description': 'A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.' </s>"
"<s>[INST] أرغب في تطوير نظام يستخدم تقنية معالجة اللغة الطبيعية لتحليل التغريدات الاجتماعية. ما هو ال API الذي يمكن استخدامه في هذا السياق؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local image path'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.858, 'mAP@0.5(mask)': 0.895}}, 'description': 'A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.' </s>"
"<s>[INST] أريد تطوير تطبيق لتقديم توصيات شخصية استنادًا إلى استخدام المستخدم لمواقع التواصل الاجتماعي. هل يمكن ال API مساعدتي في هذا؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local image path'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.858, 'mAP@0.5(mask)': 0.895}}, 'description': 'A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.' </s>"
"<s>[INST] أطمح إلى تطوير نظام يستخدم تقنية تحليل المحتوى لفهم الاتجاهات السياسية في النصوص الإخبارية. ما هي ال API المناسبة لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local image path'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.858, 'mAP@0.5(mask)': 0.895}}, 'description': 'A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.' </s>"
"<s>[INST] أحتاج إلى إنشاء تطبيق يستخدم تقنية التعرف على الكائنات من الصور لمساعدة المكفوفين في التنقل. هل يمكنني استخدام API لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local image path'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.858, 'mAP@0.5(mask)': 0.895}}, 'description': 'A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.' </s>"
"<s>[INST] لدي مشروع يهدف لتحليل نوع الفن في الرسومات واللوحات. هل يمكن ال API أن تساعدني في تحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local image path'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.858, 'mAP@0.5(mask)': 0.895}}, 'description': 'A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.' </s>"
"<s>[INST] أود تطوير تطبيق يستخدم تقنيات تعرف على الخط اليدوي من النصوص. هل يمكنني الاستعانة ب API لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local image path'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.858, 'mAP@0.5(mask)': 0.895}}, 'description': 'A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.' </s>"
"<s>[INST] أريد تطبيقًا يقوم بتحليل النصوص القوانينية لاستخراج المعلومات الرئيسية. هل يوجد API لذلك؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8m-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8m-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local image path'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.858, 'mAP@0.5(mask)': 0.895}}, 'description': 'A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.' </s>"
"<s>[INST]\"تطوير تطبيق يستخدم تقنيات الذكاء الاصطناعي لتحليل سلوك الدراجات النارية على الطرق.\"، \"لتلبية احتياجات شركتنا، نطلب واجهة برمجة تطبيقات تدعم تحليل النص السريع والفعال لمساعدتنا في مراجعة المحتوى الكبير.\"، \"مشروعنا يتضمن تطوير تطبيق للتعرف على الوجوه وتحليلها في مقاطع الفيديو باستخدام تقنيات الذكاء الاصطناعي. ما هي الأدوات والخطوات اللازمة لتحقيق ذلك؟، مشروع جامعي يتطلب تطبيق نظام تصنيف الصور المعتمد على خوارزميات التعلم العميق. ما هي واجهة برمجة التطبيقات والتقنيات التي توصي بها لهذه المهمة؟، \"نحن نتطلع إلى إنشاء نظام للكشف عن الأشياء في الوقت الفعلي لمراقبة تقاطعات حركة المرور.\" يرجى التوصية بواجهة برمجة التطبيقات (API) وتحديد خطوات التنفيذ اللازمة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-building-segmentation', 'api_call': \"YOLO('keremberke/yolov8s-building-segmentation')\", 'api_arguments': ['conf', 'iou', 'agnostic_nms', 'max_det', 'image'], 'python_environment_requirements': ['ultralyticsplus==0.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8s-building-segmentation')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'print(results[0].masks)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.661, 'mAP@0.5(mask)': 0.651}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.' </s>"
"<s>[INST] قم بتطوير أداة للإشراف على المحتوى تستخدم خوارزميات التعلم الآلي لاكتشاف المحتوى غير المناسب وتصفيته تلقائيًا. ما هي واجهة برمجة التطبيقات (API) والأساليب التي يجب استخدامها لهذا الغرض؟، \"يركز مشروعنا البحثي على تحليل المشاعر في منشورات وسائل التواصل الاجتماعي. اقترح واجهة برمجة تطبيقات وإطار عمل مناسبين لتحليل البيانات النصية واستخراج معلومات المشاعر.، إنشاء نظام توصيات لمنصة التجارة الإلكترونية التي تستخدم تقنيات التصفية التعاونية. ما هي واجهة برمجة التطبيقات والأدوات الأكثر ملاءمة لبناء هذا النظام؟، \"إنشاء تطبيق للتعرف على الكلام يمكنه نسخ المدخلات الصوتية بدقة.\" ما هي واجهة برمجة التطبيقات والمكتبات التي ستستخدمها لتطوير مثل هذا التطبيق؟، \"نحن بحاجة إلى تطوير نظام التعرف الضوئي على الحروف المخصص لاستخراج النص من الصور. هل يمكنك التوصية بواجهة برمجة التطبيقات للتعرف الضوئي على الحروف (OCR API) وتقديم إرشادات حول عملية التنفيذ؟? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-building-segmentation', 'api_call': \"YOLO('keremberke/yolov8s-building-segmentation')\", 'api_arguments': ['conf', 'iou', 'agnostic_nms', 'max_det', 'image'], 'python_environment_requirements': ['ultralyticsplus==0.0.21'], 'example_code': ['from ultralyticsplus import YOLO, render_result', \"model = YOLO('keremberke/yolov8s-building-segmentation')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", 'results = model.predict(image)', 'print(results[0].boxes)', 'print(results[0].masks)', 'render = render_result(model=model, image=image, result=results[0])', 'render.show()'], 'performance': {'dataset': 'satellite-building-segmentation', 'accuracy': {'mAP@0.5(box)': 0.661, 'mAP@0.5(mask)': 0.651}}, 'description': 'A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.' </s>"
"<s>[INST]تم العثور على فيروسات ضارة في بنية الشبكة الخاصة بشركتك، قم بتطوير نظام لتحليل البيانات لاكتشاف وإزالة البرامج الضارة باستخدام ال API التالي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8s-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.928, 'mAP@0.5(mask)': 0.928}}, 'description': 'A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.' </s>"
"<s>[INST] تحتاج مدرسة ابتدائية إلى نظام جديد لمتابعة وتحليل تقدم الطلاب، ابدأ في تنفيذ هذا المشروع باستخدام ال API التالي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8s-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.928, 'mAP@0.5(mask)': 0.928}}, 'description': 'A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.' </s>"
"<s>[INST] ستُنظم مؤتمرًا كبيرًا يتضمن مشاركين من مختلف البلدان، قم بتطوير نظام ترجمة فورية متقدم باستخدام ال API التالي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8s-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.928, 'mAP@0.5(mask)': 0.928}}, 'description': 'A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.' </s>"
"<s>[INST] يرغب مركز الأبحاث الطبية في تطوير نظام لتحليل بيانات المرضى وتوفير توصيات سريعة، بدايةً منافسة بوتينز تتيتوز? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8s-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.928, 'mAP@0.5(mask)': 0.928}}, 'description': 'A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.' </s>"
"<s>[INST] قم بتصميم تطبيق جديد للهواتف الذكية يوفر خدمة توصيل الوجبات السريعة، استخدم ال API التالي لتطوير نظام التتبع والتوصيل.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8s-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.928, 'mAP@0.5(mask)': 0.928}}, 'description': 'A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.' </s>"
"<s>[INST] أنشئ تطبيقًا للتواصل الاجتماعي يسمح بمشاركة الصور والتعليقات، قم بتضمين نظام تصنيف الصور باستخدام ال API التالي.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8s-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.928, 'mAP@0.5(mask)': 0.928}}, 'description': 'A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.' </s>"
"<s>[INST] تحتاج شركة التأمين لتحسين نظام التحليل لتقدير مخاطر التأمين، قم بتطوير نظام باستخدام ال API التالي لتحسين دقة التقدير.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8s-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.928, 'mAP@0.5(mask)': 0.928}}, 'description': 'A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.' </s>"
"<s>[INST] يرغب مختبر الروبوتات في تطوير نظام للرؤية الحاسوبية لتمكين الروبوتات من التعرف على الأشكال، استخدم ال API التالي لهذا الغرض.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8s-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.928, 'mAP@0.5(mask)': 0.928}}, 'description': 'A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.' </s>"
"<s>[INST] قم بتصميم تطبيق يسمح للمستخدمين بالبحث عن الأفلام والمسلسلات الشهيرة، استخدم ال API التالي للوصول إلى قاعدة بيانات الافلام.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8s-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.928, 'mAP@0.5(mask)': 0.928}}, 'description': 'A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.' </s>"
"<s>[INST] تحتاج مؤسسة الذكاء الاصطناعي إلى نظام لتصنيف النصوص الطويلة والقصيرة، استخدم ال API التالي لتطوير نظام التصنيف.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8s-pothole-segmentation')\", 'api_arguments': {'image': 'URL or local path to the image'}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.928, 'mAP@0.5(mask)': 0.928}}, 'description': 'A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.' </s>"
"<s>[INST]تطوير تطبيق لتحليل مشاعر العملاء عبر تحليل محتوى التعليقات والمراجعات. استخدم API كخدمة Microsoft Azure Text Analytics لتنفيذ هذه المهمة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pothole-segmentation')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.995, 'mAP@0.5(mask)': 0.995}}, 'description': 'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.' </s>"
"<s>[INST] إنشاء نظام لتحليل البيانات الكبيرة من مجموعة متنوعة من المصادر.استخدم API مثل Apache Hadoop لمعالجة وتحليل هذه البيانات.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pothole-segmentation')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.995, 'mAP@0.5(mask)': 0.995}}, 'description': 'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.' </s>"
"<s>[INST] تطوير نظام لتحديد الأشياء في الصور باستخدام تقنيات التعلم العميق.استخدم API مثل TensorFlow Object Detection API لتنفيذ هذه المهمة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pothole-segmentation')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.995, 'mAP@0.5(mask)': 0.995}}, 'description': 'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.' </s>"
"<s>[INST] أنشئ نظامًا للتعرف الصوتي على الكلام وتحويله إلى نص كتابي.استخدم API مثل Google Cloud Speech-to-Text لتحقيق هذه الوظيفة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pothole-segmentation')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.995, 'mAP@0.5(mask)': 0.995}}, 'description': 'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.' </s>"
"<s>[INST] إنشاء نظام للتصنيف التلقائي للصور إلى فئات مختلفة باستخدام تقنيات التعلم الآلي.استخدم API مثل Amazon Rekognition لتنفيذ هذه المهمة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pothole-segmentation')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.995, 'mAP@0.5(mask)': 0.995}}, 'description': 'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.' </s>"
"<s>[INST] تطوير نظام لاكتشاف الاحتيال في المعاملات المالية باستخدام تقنيات تعلم الآلة.استخدم API مثل TensorFlow Fraud Detection API لهذا الغرض.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pothole-segmentation')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.995, 'mAP@0.5(mask)': 0.995}}, 'description': 'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.' </s>"
"<s>[INST] بناء نظام لتحليل النصوص واستخراج المفاهيم الرئيسية باستخدام تقنيات معالجة اللغة الطبيعية.استخدم API مثل SpaCy NLP لهذه المهمة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pothole-segmentation')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.995, 'mAP@0.5(mask)': 0.995}}, 'description': 'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.' </s>"
"<s>[INST] تطوير نظام لتحسين تجربة المستخدم عبر تحليل سلوكه في التطبيق.استخدم API مثل Mixpanel لتنفيذ هذه الوظيفة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pothole-segmentation')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.995, 'mAP@0.5(mask)': 0.995}}, 'description': 'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.' </s>"
"<s>[INST] بناء نظام للتعرف على الوجوه في الصور وتوثيق الهوية باستخدام تقنيات التعرف على الوجوه.استخدم API مثل Kairos Face Recognition لهذه المهمة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pothole-segmentation')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.995, 'mAP@0.5(mask)': 0.995}}, 'description': 'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.' </s>"
"<s>[INST] إنشاء نظام لتوصيف المنتجات عبر تحليل الصور والنصوص لتحسين تصنيفها وعرضها.استخدم API مثل Amazon Comprehend لهذا الهدف.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pothole-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pothole-segmentation')\", 'api_arguments': {'image': 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg', 'conf': 0.25, 'iou': 0.45, 'agnostic_nms': False, 'max_det': 1000}, 'python_environment_requirements': {'ultralyticsplus': '0.0.23', 'ultralytics': '8.0.21'}, 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pothole-segmentation', 'accuracy': {'mAP@0.5(box)': 0.995, 'mAP@0.5(mask)': 0.995}}, 'description': 'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.' </s>"
"<s>[INST]الحصول على تحليل للنصوص والمرئيات لاكتشاف الإشارات السلبية أو الغير ملائمة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.512, 'mAP@0.5(mask)': 0.517}}, 'description': 'A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] تطبيق موديل ذكاء اصطناعي لتحليل سلوك العملاء داخل متاجر التجزئة لتحسين تجربة التسوق.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.512, 'mAP@0.5(mask)': 0.517}}, 'description': 'A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] تنفيذ تحليل شبكات الإجرام الجرافية باستخدام مكتبة محورة الحوار.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.512, 'mAP@0.5(mask)': 0.517}}, 'description': 'A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] استخدام تكنولوجيا الذكاء الاصطناعي للكشف عن الاحتيال في عمليات الدفع عبر الإنترنت.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.512, 'mAP@0.5(mask)': 0.517}}, 'description': 'A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] تحليل بيانات العملاء لتخصيص تجربة المستخدم في التطبيقات النقالة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.512, 'mAP@0.5(mask)': 0.517}}, 'description': 'A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] تنفيذ تقديرات نموذج الملف الشخصي للمستخدمين باستخدام واجهة برمجية للتعلم العميق.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.512, 'mAP@0.5(mask)': 0.517}}, 'description': 'A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] تطبيق موديل الذكاء الصناعي للكشف عن الأشياء المفقودة في الفيديوهات.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.512, 'mAP@0.5(mask)': 0.517}}, 'description': 'A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] تحليل البيانات الصوتية لتوصيف المشاعر والعواطف في المحادثات الهاتفية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.512, 'mAP@0.5(mask)': 0.517}}, 'description': 'A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] تطبيق تكنولوجيا الذكاء الاصطناعي لتحليل نمط الكتابة والإملاء في النصوص الطبية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.512, 'mAP@0.5(mask)': 0.517}}, 'description': 'A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] تشغيل نظام مراقبة للكشف عن التصرفات الغير ملائمة في الشبكات الاجتماعية.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8n-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8n-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': 'ultralyticsplus==0.0.23 ultralytics==8.0.21', 'example_code': \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.512, 'mAP@0.5(mask)': 0.517}}, 'description': 'A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST]\"إنشاء تطبيق جوال يستخدم واجهة برمجة تطبيقات الطقس لعرض أحوال الطقس والتنبؤات الحالية.\"، \"تطوير برنامج يستخدم واجهة برمجة تطبيقات خرائط Google لحساب المسافة بين موقعين محددين.\"، \"إنشاء تطبيق chatbot باستخدام واجهة برمجة تطبيقات Dialogflow للتعامل مع استفسارات المستخدم وتقديم الإجابات.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8s-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.515, 'mAP@0.5(mask)': 0.491}}, 'description': 'YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] إنشاء نظام توصية باستخدام واجهة برمجة تطبيقات IMDb لاقتراح أفلام بناءً على تفضيلات المستخدم.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8s-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.515, 'mAP@0.5(mask)': 0.491}}, 'description': 'YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST] تطوير أداة ترجمة لغة باستخدام واجهة برمجة تطبيقات الترجمة من Google لتحويل النص بين اللغات المختلفة.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8s-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.515, 'mAP@0.5(mask)': 0.491}}, 'description': 'YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST]  قم ببناء أداة تحليل المشاعر باستخدام Twitter API لتحليل التغريدات العامة وتحديد المشاعر.، إنشاء تطبيق للتعرف على الصور باستخدام Clarifai API لتحديد الكائنات في الصور.، تطوير أداة تجميع الأخبار باستخدام News API لجلب و عرض أحدث المقالات الإخبارية.، تنفيذ نظام توصية موسيقية باستخدام Spotify API لاقتراح الأغاني بناءً على تفضيلات المستخدم.، إنشاء نظام للتعرف على الوجوه باستخدام Azure Face API لاكتشاف الوجوه في الصور والتعرف عليها.? [/INST]'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Segmentation', 'api_name': 'keremberke/yolov8s-pcb-defect-segmentation', 'api_call': \"YOLO('keremberke/yolov8s-pcb-defect-segmentation')\", 'api_arguments': {'image': 'URL or local path to image'}, 'python_environment_requirements': ['ultralyticsplus==0.0.23', 'ultralytics==8.0.21'], 'example_code': \"from ultralyticsplus import YOLO, render_result\\n\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\n\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", 'performance': {'dataset': 'pcb-defect-segmentation', 'accuracy': {'mAP@0.5(box)': 0.515, 'mAP@0.5(mask)': 0.491}}, 'description': 'YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.' </s>"
"<s>[INST]قم بتطبيق تقنية استشعار الوجه لتحليل وتصنيف تعابير الوجه البشري.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Variations', 'api_name': 'lambdalabs/sd-image-variations-diffusers', 'api_call': \"StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')\", 'api_arguments': {'revision': 'v2.0'}, 'python_environment_requirements': 'Diffusers >=0.8.0', 'example_code': 'from diffusers import StableDiffusionImageVariationPipeline\\nfrom PIL import Image\\ndevice = cuda:0\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\n lambdalabs/sd-image-variations-diffusers,\\n revision=v2.0,\\n)\\nsd_pipe = sd_pipe.to(device)\\nim = Image.open(path/to/image.jpg)\\ntform = transforms.Compose([\\n transforms.ToTensor(),\\n transforms.Resize(\\n  (224, 224),\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\n  antialias=False,\\n ),\\n transforms.Normalize(\\n  [0.48145466, 0.4578275, 0.40821073],\\n  [0.26862954, 0.26130258, 0.27577711]),\\n])\\ninp = tform(im).to(device).unsqueeze(0)\\nout = sd_pipe(inp, guidance_scale=3)\\nout[images][0].save(result.jpg)', 'performance': {'dataset': 'ChristophSchuhmann/improved_aesthetics_6plus', 'accuracy': 'N/A'}, 'description': 'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.' </s>"
"<s>[INST] أنشئ برنامج يستخدم تقنية البصمة الصوتية لتحديد الهوية الصوتية للأفراد.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Variations', 'api_name': 'lambdalabs/sd-image-variations-diffusers', 'api_call': \"StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')\", 'api_arguments': {'revision': 'v2.0'}, 'python_environment_requirements': 'Diffusers >=0.8.0', 'example_code': 'from diffusers import StableDiffusionImageVariationPipeline\\nfrom PIL import Image\\ndevice = cuda:0\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\n lambdalabs/sd-image-variations-diffusers,\\n revision=v2.0,\\n)\\nsd_pipe = sd_pipe.to(device)\\nim = Image.open(path/to/image.jpg)\\ntform = transforms.Compose([\\n transforms.ToTensor(),\\n transforms.Resize(\\n  (224, 224),\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\n  antialias=False,\\n ),\\n transforms.Normalize(\\n  [0.48145466, 0.4578275, 0.40821073],\\n  [0.26862954, 0.26130258, 0.27577711]),\\n])\\ninp = tform(im).to(device).unsqueeze(0)\\nout = sd_pipe(inp, guidance_scale=3)\\nout[images][0].save(result.jpg)', 'performance': {'dataset': 'ChristophSchuhmann/improved_aesthetics_6plus', 'accuracy': 'N/A'}, 'description': 'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.' </s>"
"<s>[INST] قم بتطوير تطبيق يستخدم تقنية التعرف على الجسم لتحليل وتتبع حركة الأشخاص في مقاطع الفيديو.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Variations', 'api_name': 'lambdalabs/sd-image-variations-diffusers', 'api_call': \"StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')\", 'api_arguments': {'revision': 'v2.0'}, 'python_environment_requirements': 'Diffusers >=0.8.0', 'example_code': 'from diffusers import StableDiffusionImageVariationPipeline\\nfrom PIL import Image\\ndevice = cuda:0\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\n lambdalabs/sd-image-variations-diffusers,\\n revision=v2.0,\\n)\\nsd_pipe = sd_pipe.to(device)\\nim = Image.open(path/to/image.jpg)\\ntform = transforms.Compose([\\n transforms.ToTensor(),\\n transforms.Resize(\\n  (224, 224),\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\n  antialias=False,\\n ),\\n transforms.Normalize(\\n  [0.48145466, 0.4578275, 0.40821073],\\n  [0.26862954, 0.26130258, 0.27577711]),\\n])\\ninp = tform(im).to(device).unsqueeze(0)\\nout = sd_pipe(inp, guidance_scale=3)\\nout[images][0].save(result.jpg)', 'performance': {'dataset': 'ChristophSchuhmann/improved_aesthetics_6plus', 'accuracy': 'N/A'}, 'description': 'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.' </s>"
"<s>[INST] إنشاء نظام يستخدم تقنية التعرف على النص لتحليل وتصنيف المكونات النصية في مستندات PDF.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Variations', 'api_name': 'lambdalabs/sd-image-variations-diffusers', 'api_call': \"StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')\", 'api_arguments': {'revision': 'v2.0'}, 'python_environment_requirements': 'Diffusers >=0.8.0', 'example_code': 'from diffusers import StableDiffusionImageVariationPipeline\\nfrom PIL import Image\\ndevice = cuda:0\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\n lambdalabs/sd-image-variations-diffusers,\\n revision=v2.0,\\n)\\nsd_pipe = sd_pipe.to(device)\\nim = Image.open(path/to/image.jpg)\\ntform = transforms.Compose([\\n transforms.ToTensor(),\\n transforms.Resize(\\n  (224, 224),\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\n  antialias=False,\\n ),\\n transforms.Normalize(\\n  [0.48145466, 0.4578275, 0.40821073],\\n  [0.26862954, 0.26130258, 0.27577711]),\\n])\\ninp = tform(im).to(device).unsqueeze(0)\\nout = sd_pipe(inp, guidance_scale=3)\\nout[images][0].save(result.jpg)', 'performance': {'dataset': 'ChristophSchuhmann/improved_aesthetics_6plus', 'accuracy': 'N/A'}, 'description': 'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.' </s>"
"<s>[INST] أنشئ تطبيق يستخدم تقنية التعرف على الكائنات لتحليل وتصنيف المكونات البصرية في مجموعة من الصور.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Variations', 'api_name': 'lambdalabs/sd-image-variations-diffusers', 'api_call': \"StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')\", 'api_arguments': {'revision': 'v2.0'}, 'python_environment_requirements': 'Diffusers >=0.8.0', 'example_code': 'from diffusers import StableDiffusionImageVariationPipeline\\nfrom PIL import Image\\ndevice = cuda:0\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\n lambdalabs/sd-image-variations-diffusers,\\n revision=v2.0,\\n)\\nsd_pipe = sd_pipe.to(device)\\nim = Image.open(path/to/image.jpg)\\ntform = transforms.Compose([\\n transforms.ToTensor(),\\n transforms.Resize(\\n  (224, 224),\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\n  antialias=False,\\n ),\\n transforms.Normalize(\\n  [0.48145466, 0.4578275, 0.40821073],\\n  [0.26862954, 0.26130258, 0.27577711]),\\n])\\ninp = tform(im).to(device).unsqueeze(0)\\nout = sd_pipe(inp, guidance_scale=3)\\nout[images][0].save(result.jpg)', 'performance': {'dataset': 'ChristophSchuhmann/improved_aesthetics_6plus', 'accuracy': 'N/A'}, 'description': 'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.' </s>"
"<s>[INST] قم بتنفيذ تحليل الصور باستخدام تقنية التعرف على الإثارة لاكتشاف وتصنيف العواطف الإنسانية المعبرة عنها.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Variations', 'api_name': 'lambdalabs/sd-image-variations-diffusers', 'api_call': \"StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')\", 'api_arguments': {'revision': 'v2.0'}, 'python_environment_requirements': 'Diffusers >=0.8.0', 'example_code': 'from diffusers import StableDiffusionImageVariationPipeline\\nfrom PIL import Image\\ndevice = cuda:0\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\n lambdalabs/sd-image-variations-diffusers,\\n revision=v2.0,\\n)\\nsd_pipe = sd_pipe.to(device)\\nim = Image.open(path/to/image.jpg)\\ntform = transforms.Compose([\\n transforms.ToTensor(),\\n transforms.Resize(\\n  (224, 224),\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\n  antialias=False,\\n ),\\n transforms.Normalize(\\n  [0.48145466, 0.4578275, 0.40821073],\\n  [0.26862954, 0.26130258, 0.27577711]),\\n])\\ninp = tform(im).to(device).unsqueeze(0)\\nout = sd_pipe(inp, guidance_scale=3)\\nout[images][0].save(result.jpg)', 'performance': {'dataset': 'ChristophSchuhmann/improved_aesthetics_6plus', 'accuracy': 'N/A'}, 'description': 'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.' </s>"
"<s>[INST] إنشاء نظام يستخدم تقنية التعرف على الكتابة اليدوية لتحويل نصوص الكتابة اليدوية إلى نصوص ديجيتال قابلة للتحرير.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Variations', 'api_name': 'lambdalabs/sd-image-variations-diffusers', 'api_call': \"StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')\", 'api_arguments': {'revision': 'v2.0'}, 'python_environment_requirements': 'Diffusers >=0.8.0', 'example_code': 'from diffusers import StableDiffusionImageVariationPipeline\\nfrom PIL import Image\\ndevice = cuda:0\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\n lambdalabs/sd-image-variations-diffusers,\\n revision=v2.0,\\n)\\nsd_pipe = sd_pipe.to(device)\\nim = Image.open(path/to/image.jpg)\\ntform = transforms.Compose([\\n transforms.ToTensor(),\\n transforms.Resize(\\n  (224, 224),\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\n  antialias=False,\\n ),\\n transforms.Normalize(\\n  [0.48145466, 0.4578275, 0.40821073],\\n  [0.26862954, 0.26130258, 0.27577711]),\\n])\\ninp = tform(im).to(device).unsqueeze(0)\\nout = sd_pipe(inp, guidance_scale=3)\\nout[images][0].save(result.jpg)', 'performance': {'dataset': 'ChristophSchuhmann/improved_aesthetics_6plus', 'accuracy': 'N/A'}, 'description': 'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.' </s>"
"<s>[INST] تطبيق تقنية الاستشعار الصوتي لتحليل وتصنيف شكل الطيف الصوتي لمقاطع الصوت المقدمة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Variations', 'api_name': 'lambdalabs/sd-image-variations-diffusers', 'api_call': \"StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')\", 'api_arguments': {'revision': 'v2.0'}, 'python_environment_requirements': 'Diffusers >=0.8.0', 'example_code': 'from diffusers import StableDiffusionImageVariationPipeline\\nfrom PIL import Image\\ndevice = cuda:0\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\n lambdalabs/sd-image-variations-diffusers,\\n revision=v2.0,\\n)\\nsd_pipe = sd_pipe.to(device)\\nim = Image.open(path/to/image.jpg)\\ntform = transforms.Compose([\\n transforms.ToTensor(),\\n transforms.Resize(\\n  (224, 224),\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\n  antialias=False,\\n ),\\n transforms.Normalize(\\n  [0.48145466, 0.4578275, 0.40821073],\\n  [0.26862954, 0.26130258, 0.27577711]),\\n])\\ninp = tform(im).to(device).unsqueeze(0)\\nout = sd_pipe(inp, guidance_scale=3)\\nout[images][0].save(result.jpg)', 'performance': {'dataset': 'ChristophSchuhmann/improved_aesthetics_6plus', 'accuracy': 'N/A'}, 'description': 'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم تقنية الاستشعار الحركي لتتبع وتحليل حركة الأشخاص في أشرطة الفيديو.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Variations', 'api_name': 'lambdalabs/sd-image-variations-diffusers', 'api_call': \"StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')\", 'api_arguments': {'revision': 'v2.0'}, 'python_environment_requirements': 'Diffusers >=0.8.0', 'example_code': 'from diffusers import StableDiffusionImageVariationPipeline\\nfrom PIL import Image\\ndevice = cuda:0\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\n lambdalabs/sd-image-variations-diffusers,\\n revision=v2.0,\\n)\\nsd_pipe = sd_pipe.to(device)\\nim = Image.open(path/to/image.jpg)\\ntform = transforms.Compose([\\n transforms.ToTensor(),\\n transforms.Resize(\\n  (224, 224),\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\n  antialias=False,\\n ),\\n transforms.Normalize(\\n  [0.48145466, 0.4578275, 0.40821073],\\n  [0.26862954, 0.26130258, 0.27577711]),\\n])\\ninp = tform(im).to(device).unsqueeze(0)\\nout = sd_pipe(inp, guidance_scale=3)\\nout[images][0].save(result.jpg)', 'performance': {'dataset': 'ChristophSchuhmann/improved_aesthetics_6plus', 'accuracy': 'N/A'}, 'description': 'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.' </s>"
"<s>[INST] قم بتطوير نظام يستخدم تقنية التعرف على الكائنات لتحليل واستخراج المعلومات من ملفات الفيديو.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Variations', 'api_name': 'lambdalabs/sd-image-variations-diffusers', 'api_call': \"StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')\", 'api_arguments': {'revision': 'v2.0'}, 'python_environment_requirements': 'Diffusers >=0.8.0', 'example_code': 'from diffusers import StableDiffusionImageVariationPipeline\\nfrom PIL import Image\\ndevice = cuda:0\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\n lambdalabs/sd-image-variations-diffusers,\\n revision=v2.0,\\n)\\nsd_pipe = sd_pipe.to(device)\\nim = Image.open(path/to/image.jpg)\\ntform = transforms.Compose([\\n transforms.ToTensor(),\\n transforms.Resize(\\n  (224, 224),\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\n  antialias=False,\\n ),\\n transforms.Normalize(\\n  [0.48145466, 0.4578275, 0.40821073],\\n  [0.26862954, 0.26130258, 0.27577711]),\\n])\\ninp = tform(im).to(device).unsqueeze(0)\\nout = sd_pipe(inp, guidance_scale=3)\\nout[images][0].save(result.jpg)', 'performance': {'dataset': 'ChristophSchuhmann/improved_aesthetics_6plus', 'accuracy': 'N/A'}, 'description': 'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.' </s>"
"<s>[INST]طور نظامًا يقوم بتصنيف أنواع السيارات من الصور بدقة عالية باستخدام API Automotive Image Retrieval من Google Cloud Vision.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'opencv': 'pip install opencv-contrib-python', 'diffusers': 'pip install diffusers transformers accelerate'}, 'example_code': \"import cv2\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nimport numpy as np\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\\nimage = np.array(image)\\nlow_threshold = 100\\nhigh_threshold = 200\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(bird, image, num_inference_steps=20).images[0]\\nimage.save('images/bird_canny_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': '600 GPU-hours with Nvidia A100 80G'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تحتاج شركتنا إلى تطوير نظام تعلم آلي لكشف الوجوه في الصور بسرعة ودقة. كيف يمكن استخدام API Face Detection من OpenCV لتحقيق ذلك؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'opencv': 'pip install opencv-contrib-python', 'diffusers': 'pip install diffusers transformers accelerate'}, 'example_code': \"import cv2\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nimport numpy as np\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\\nimage = np.array(image)\\nlow_threshold = 100\\nhigh_threshold = 200\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(bird, image, num_inference_steps=20).images[0]\\nimage.save('images/bird_canny_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': '600 GPU-hours with Nvidia A100 80G'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] نرغب في بشكل كبير في تحسين كفاءة نظام استقبال الإشارات الرقمية لدينا. ما هي أفضل طريقة لاستخدام API من TensorFlow لتطبيق عملية تقديم النماذج (Model Serving)؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'opencv': 'pip install opencv-contrib-python', 'diffusers': 'pip install diffusers transformers accelerate'}, 'example_code': \"import cv2\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nimport numpy as np\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\\nimage = np.array(image)\\nlow_threshold = 100\\nhigh_threshold = 200\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(bird, image, num_inference_steps=20).images[0]\\nimage.save('images/bird_canny_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': '600 GPU-hours with Nvidia A100 80G'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تحتاج شركتنا إلى تطوير نظام لمراقبة التدفق المروري عبر الإشارات الرقمية. كيف يمكن استخدام API TrafficFlow Monitoring من Microsoft Azure لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'opencv': 'pip install opencv-contrib-python', 'diffusers': 'pip install diffusers transformers accelerate'}, 'example_code': \"import cv2\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nimport numpy as np\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\\nimage = np.array(image)\\nlow_threshold = 100\\nhigh_threshold = 200\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(bird, image, num_inference_steps=20).images[0]\\nimage.save('images/bird_canny_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': '600 GPU-hours with Nvidia A100 80G'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] يهدف مشروعنا للتوسع إلى توفير تقنيات تعلم آلي مبتكرة لمحركات البحث. كيف يمكن استخدام API Search Relevance من Amazon Web Services لتحسين كفاءة عملية البحث؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'opencv': 'pip install opencv-contrib-python', 'diffusers': 'pip install diffusers transformers accelerate'}, 'example_code': \"import cv2\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nimport numpy as np\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\\nimage = np.array(image)\\nlow_threshold = 100\\nhigh_threshold = 200\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(bird, image, num_inference_steps=20).images[0]\\nimage.save('images/bird_canny_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': '600 GPU-hours with Nvidia A100 80G'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تريد شركتنا تحسين التحليل المالي من خلال الاستفادة من تقنيات التعلم العميق. كيف يمكن استخدام API Financial Analytics من IBM Watson لتحقيق هذا الهدف؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'opencv': 'pip install opencv-contrib-python', 'diffusers': 'pip install diffusers transformers accelerate'}, 'example_code': \"import cv2\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nimport numpy as np\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\\nimage = np.array(image)\\nlow_threshold = 100\\nhigh_threshold = 200\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(bird, image, num_inference_steps=20).images[0]\\nimage.save('images/bird_canny_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': '600 GPU-hours with Nvidia A100 80G'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تود شركتك تعزيز أمان شبكة الإنترنت الداخلية من خلال تطبيق حلول تشفير متقدمة. كيف يمكن استخدام API Encryption Services من AWS لتنفيذ هذه التدابير الأمنية؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'opencv': 'pip install opencv-contrib-python', 'diffusers': 'pip install diffusers transformers accelerate'}, 'example_code': \"import cv2\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nimport numpy as np\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\\nimage = np.array(image)\\nlow_threshold = 100\\nhigh_threshold = 200\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(bird, image, num_inference_steps=20).images[0]\\nimage.save('images/bird_canny_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': '600 GPU-hours with Nvidia A100 80G'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] يسعى فريقنا لتحسين استراتيجيات التسويق عبر الإنترنت باستخدام تقنيات تعلم الآلة. كيف يمكن استخدام API Marketing Analytics من Google Cloud لدعم هذه الجهود؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'opencv': 'pip install opencv-contrib-python', 'diffusers': 'pip install diffusers transformers accelerate'}, 'example_code': \"import cv2\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nimport numpy as np\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\\nimage = np.array(image)\\nlow_threshold = 100\\nhigh_threshold = 200\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(bird, image, num_inference_steps=20).images[0]\\nimage.save('images/bird_canny_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': '600 GPU-hours with Nvidia A100 80G'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تريد منظمتك تحديث النظام القائم على قاعدة البيانات لتسهيل عمليات الاستعلام وتحليل البيانات. كيف يمكن استخدام API Database Management من Microsoft Azure لتحقيق هذا الهدف؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'opencv': 'pip install opencv-contrib-python', 'diffusers': 'pip install diffusers transformers accelerate'}, 'example_code': \"import cv2\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nimport numpy as np\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\\nimage = np.array(image)\\nlow_threshold = 100\\nhigh_threshold = 200\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(bird, image, num_inference_steps=20).images[0]\\nimage.save('images/bird_canny_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': '600 GPU-hours with Nvidia A100 80G'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تريد شركتك تنفيذ نظام ذكاء اصطناعي لتقديم توصيات شخصية للعملاء. كيف يمكن استخدام API Personalization Services من AWS لتحقيق هذا الهدف؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'opencv': 'pip install opencv-contrib-python', 'diffusers': 'pip install diffusers transformers accelerate'}, 'example_code': \"import cv2\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nimport numpy as np\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\\nimage = np.array(image)\\nlow_threshold = 100\\nhigh_threshold = 200\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(bird, image, num_inference_steps=20).images[0]\\nimage.save('images/bird_canny_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': '600 GPU-hours with Nvidia A100 80G'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST]استخدم هذه الخطوات لاستدعاء واجهة برمجة تطبيقات للتعرف على الصور الطبية في تطبيقك.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Human Pose Estimation', 'api_name': 'lllyasviel/sd-controlnet-openpose', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", 'api_arguments': {'text': 'chef in the kitchen', 'image': 'image', 'num_inference_steps': 20}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import OpenposeDetector\\nfrom diffusers.utils import load_image\\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\\nimage = openpose(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\\nimage.save('images/chef_pose_out.png')\", 'performance': {'dataset': '200k pose-image, caption pairs', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] إنشاء نظام للتحليل الصوتي لنصوص كبيرة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Human Pose Estimation', 'api_name': 'lllyasviel/sd-controlnet-openpose', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", 'api_arguments': {'text': 'chef in the kitchen', 'image': 'image', 'num_inference_steps': 20}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import OpenposeDetector\\nfrom diffusers.utils import load_image\\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\\nimage = openpose(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\\nimage.save('images/chef_pose_out.png')\", 'performance': {'dataset': '200k pose-image, caption pairs', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] قم بإنشاء تطبيق يسمح للمستخدمين بتحويل النصوص إلى لغة برمجية محددة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Human Pose Estimation', 'api_name': 'lllyasviel/sd-controlnet-openpose', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", 'api_arguments': {'text': 'chef in the kitchen', 'image': 'image', 'num_inference_steps': 20}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import OpenposeDetector\\nfrom diffusers.utils import load_image\\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\\nimage = openpose(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\\nimage.save('images/chef_pose_out.png')\", 'performance': {'dataset': '200k pose-image, caption pairs', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] قم بتكامل واجهة برمجة تطبيقات لاستخدام خوارزمية التعلم الآلي في تصنيف النصوص.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Human Pose Estimation', 'api_name': 'lllyasviel/sd-controlnet-openpose', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", 'api_arguments': {'text': 'chef in the kitchen', 'image': 'image', 'num_inference_steps': 20}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import OpenposeDetector\\nfrom diffusers.utils import load_image\\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\\nimage = openpose(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\\nimage.save('images/chef_pose_out.png')\", 'performance': {'dataset': '200k pose-image, caption pairs', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] قم بتدريب نموذج للكشف عن الأشياء في الصور باستخدام واجهة برمجة شبيهة بشركة يوبر.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Human Pose Estimation', 'api_name': 'lllyasviel/sd-controlnet-openpose', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", 'api_arguments': {'text': 'chef in the kitchen', 'image': 'image', 'num_inference_steps': 20}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import OpenposeDetector\\nfrom diffusers.utils import load_image\\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\\nimage = openpose(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\\nimage.save('images/chef_pose_out.png')\", 'performance': {'dataset': '200k pose-image, caption pairs', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] أنشئ تطبيقًا للتعرف على التصنيف الصوتي باستخدام واجهة برمجة API.AI الشهيرة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Human Pose Estimation', 'api_name': 'lllyasviel/sd-controlnet-openpose', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", 'api_arguments': {'text': 'chef in the kitchen', 'image': 'image', 'num_inference_steps': 20}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import OpenposeDetector\\nfrom diffusers.utils import load_image\\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\\nimage = openpose(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\\nimage.save('images/chef_pose_out.png')\", 'performance': {'dataset': '200k pose-image, caption pairs', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] انشئ تطبيق لتوليد نصوص احترافية باستخدام تقنيات التوليف اللغوي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Human Pose Estimation', 'api_name': 'lllyasviel/sd-controlnet-openpose', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", 'api_arguments': {'text': 'chef in the kitchen', 'image': 'image', 'num_inference_steps': 20}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import OpenposeDetector\\nfrom diffusers.utils import load_image\\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\\nimage = openpose(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\\nimage.save('images/chef_pose_out.png')\", 'performance': {'dataset': '200k pose-image, caption pairs', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] قم ببناء نظام لتحويل العبارات إلى عبارات أكثر توضيحًا باستخدام واجهة Natural Language Processing من Google.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Human Pose Estimation', 'api_name': 'lllyasviel/sd-controlnet-openpose', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", 'api_arguments': {'text': 'chef in the kitchen', 'image': 'image', 'num_inference_steps': 20}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import OpenposeDetector\\nfrom diffusers.utils import load_image\\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\\nimage = openpose(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\\nimage.save('images/chef_pose_out.png')\", 'performance': {'dataset': '200k pose-image, caption pairs', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم تقنيات التعرف على اللون لتحديد ألوان الصور.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Human Pose Estimation', 'api_name': 'lllyasviel/sd-controlnet-openpose', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", 'api_arguments': {'text': 'chef in the kitchen', 'image': 'image', 'num_inference_steps': 20}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import OpenposeDetector\\nfrom diffusers.utils import load_image\\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\\nimage = openpose(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\\nimage.save('images/chef_pose_out.png')\", 'performance': {'dataset': '200k pose-image, caption pairs', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] أُنشئ تطبيقًا يستخدم خوارزميات التعلم الآلي للإبلاغ عن محتوى غير ملائم على الإنترنت.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Human Pose Estimation', 'api_name': 'lllyasviel/sd-controlnet-openpose', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", 'api_arguments': {'text': 'chef in the kitchen', 'image': 'image', 'num_inference_steps': 20}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import OpenposeDetector\\nfrom diffusers.utils import load_image\\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\\nimage = openpose(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\\nimage.save('images/chef_pose_out.png')\", 'performance': {'dataset': '200k pose-image, caption pairs', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST]استخدم API التالي لتحليل النصوص الطبية باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-hed', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-hed')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png)\\nimage = hed(image)\\ncontrolnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-hed, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(oil painting of handsome old man, masterpiece, image, num_inference_steps=20).images[0]\\nimage.save('images/man_hed_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] يمكنك الاستعانة بالجملة القائمة على الإبداع لإنشاء تصاميم جذابة للإعلانات. استخدم API التالي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-hed', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-hed')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png)\\nimage = hed(image)\\ncontrolnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-hed, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(oil painting of handsome old man, masterpiece, image, num_inference_steps=20).images[0]\\nimage.save('images/man_hed_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] استخدم API التالي لإنشاء ترجمات للنصوص بين لغات متعددة باستخدام تقنيات الترجمة الآلية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-hed', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-hed')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png)\\nimage = hed(image)\\ncontrolnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-hed, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(oil painting of handsome old man, masterpiece, image, num_inference_steps=20).images[0]\\nimage.save('images/man_hed_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] لتحسين تجربة تصفح موقعك الإلكتروني، يمكنك استخدام API التالي لتوليد صور أصغر بحيث يتم تحميل الموقع بشكل أسرع.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-hed', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-hed')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png)\\nimage = hed(image)\\ncontrolnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-hed, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(oil painting of handsome old man, masterpiece, image, num_inference_steps=20).images[0]\\nimage.save('images/man_hed_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] لتحليل مشاهد الفيديو واستخراج معلومات مفيدة، استخدم API التالي باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-hed', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-hed')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png)\\nimage = hed(image)\\ncontrolnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-hed, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(oil painting of handsome old man, masterpiece, image, num_inference_steps=20).images[0]\\nimage.save('images/man_hed_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] قم بإعداد نظام لتصنيف الصور الطبية باستخدام تقنيات التعلم الآلي من خلال API التالي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-hed', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-hed')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png)\\nimage = hed(image)\\ncontrolnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-hed, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(oil painting of handsome old man, masterpiece, image, num_inference_steps=20).images[0]\\nimage.save('images/man_hed_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] لتحليل الموسيقى واستخراج المعلومات الصوتية، استخدم API التالي الخاص بتقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-hed', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-hed')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png)\\nimage = hed(image)\\ncontrolnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-hed, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(oil painting of handsome old man, masterpiece, image, num_inference_steps=20).images[0]\\nimage.save('images/man_hed_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] لتصميم رسومات ثلاثية الأبعاد بنوعية عالية، يمكنك استخدام API التالي الذي يعتمد على تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-hed', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-hed')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png)\\nimage = hed(image)\\ncontrolnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-hed, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(oil painting of handsome old man, masterpiece, image, num_inference_steps=20).images[0]\\nimage.save('images/man_hed_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] لتحسين تفاعل المستخدمين مع تطبيقك، استخدم API التالي لتوليد نصوص ديناميكية باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-hed', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-hed')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png)\\nimage = hed(image)\\ncontrolnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-hed, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(oil painting of handsome old man, masterpiece, image, num_inference_steps=20).images[0]\\nimage.save('images/man_hed_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] لتحليل المحتوى الصوتي واستخراج المعلومات الهامة، استخدم API التالي المعتمد على تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/sd-controlnet-hed', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-hed')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png)\\nimage = hed(image)\\ncontrolnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-hed, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(oil painting of handsome old man, masterpiece, image, num_inference_steps=20).images[0]\\nimage.save('images/man_hed_out.png')\", 'performance': {'dataset': '3M edge-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST]استخدم ال API لتحليل وتقسيم صورة لمساعدتك في تفحص أخطاء اللغة العربية بطريقة تلقائية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Segmentation', 'api_name': 'lllyasviel/sd-controlnet-seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\", 'api_arguments': ['torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"image = pipe(house, image, num_inference_steps=20).images[0]\\nimage.save('./images/house_seg_out.png')\", 'performance': {'dataset': 'ADE20K', 'accuracy': 'Trained on 164K segmentation-image, caption pairs'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تطبيق ال API لإعداد نظام الكشف التلقائي عن الكائنات في صور الطبيعة لتحسين جودة تصنيف الصور.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Segmentation', 'api_name': 'lllyasviel/sd-controlnet-seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\", 'api_arguments': ['torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"image = pipe(house, image, num_inference_steps=20).images[0]\\nimage.save('./images/house_seg_out.png')\", 'performance': {'dataset': 'ADE20K', 'accuracy': 'Trained on 164K segmentation-image, caption pairs'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] استخدم ال API لتطوير نظام لتحديد الأشخاص في الصور الجماعية بطريقة تلقائية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Segmentation', 'api_name': 'lllyasviel/sd-controlnet-seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\", 'api_arguments': ['torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"image = pipe(house, image, num_inference_steps=20).images[0]\\nimage.save('./images/house_seg_out.png')\", 'performance': {'dataset': 'ADE20K', 'accuracy': 'Trained on 164K segmentation-image, caption pairs'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] قم بتجربة ال API لبناء نظام لتحليل وتصنيف المشاهد الطبيعية في الصور بشكل تلقائي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Segmentation', 'api_name': 'lllyasviel/sd-controlnet-seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\", 'api_arguments': ['torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"image = pipe(house, image, num_inference_steps=20).images[0]\\nimage.save('./images/house_seg_out.png')\", 'performance': {'dataset': 'ADE20K', 'accuracy': 'Trained on 164K segmentation-image, caption pairs'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] عمل تطبيق باستخدام ال API لتصنيف الأطعمة في الصور وتوجيه المستخدمين إلى وجبات غذائية مخصصة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Segmentation', 'api_name': 'lllyasviel/sd-controlnet-seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\", 'api_arguments': ['torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"image = pipe(house, image, num_inference_steps=20).images[0]\\nimage.save('./images/house_seg_out.png')\", 'performance': {'dataset': 'ADE20K', 'accuracy': 'Trained on 164K segmentation-image, caption pairs'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] استخدم ال API لتطوير نظام لاكتشاف الإشارات الطريقية في الصور بشكل آلي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Segmentation', 'api_name': 'lllyasviel/sd-controlnet-seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\", 'api_arguments': ['torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"image = pipe(house, image, num_inference_steps=20).images[0]\\nimage.save('./images/house_seg_out.png')\", 'performance': {'dataset': 'ADE20K', 'accuracy': 'Trained on 164K segmentation-image, caption pairs'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] بناء نظام باستخدام ال API لتحديد الوجوه في الصور الجماعية لأداء العد والتحليل الإحصائي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Segmentation', 'api_name': 'lllyasviel/sd-controlnet-seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\", 'api_arguments': ['torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"image = pipe(house, image, num_inference_steps=20).images[0]\\nimage.save('./images/house_seg_out.png')\", 'performance': {'dataset': 'ADE20K', 'accuracy': 'Trained on 164K segmentation-image, caption pairs'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] استخدام ال API لبناء نظام لتحليل تصاميم الديكور الداخلي في الصور وتوجيه لاختيار الأثاث المناسب.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Segmentation', 'api_name': 'lllyasviel/sd-controlnet-seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\", 'api_arguments': ['torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"image = pipe(house, image, num_inference_steps=20).images[0]\\nimage.save('./images/house_seg_out.png')\", 'performance': {'dataset': 'ADE20K', 'accuracy': 'Trained on 164K segmentation-image, caption pairs'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] إعداد نظام باستخدام ال API لتحديد الحيوانات الأليفة في الصور واقتراح الخدمات البيطرية المناسبة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Segmentation', 'api_name': 'lllyasviel/sd-controlnet-seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\", 'api_arguments': ['torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"image = pipe(house, image, num_inference_steps=20).images[0]\\nimage.save('./images/house_seg_out.png')\", 'performance': {'dataset': 'ADE20K', 'accuracy': 'Trained on 164K segmentation-image, caption pairs'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تجربة ال API لإنشاء نظام للكشف التلقائي عن الأطفال في الصور وتنفيذ إجراءات الحماية المناسبة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Segmentation', 'api_name': 'lllyasviel/sd-controlnet-seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\", 'api_arguments': ['torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"image = pipe(house, image, num_inference_steps=20).images[0]\\nimage.save('./images/house_seg_out.png')\", 'performance': {'dataset': 'ADE20K', 'accuracy': 'Trained on 164K segmentation-image, caption pairs'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST]يُرغب في تطوير نموذج لتحليل مقاطع الفيديو للكشف عن العناصر والأنماط المحددة فيها، يُمكنك استخدام ال API التالي لتحقيق ذلك.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Depth Estimation', 'api_name': 'lllyasviel/sd-controlnet-depth', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'PIL', 'numpy', 'torch'], 'example_code': {'install_packages': 'pip install diffusers transformers accelerate', 'code': ['from transformers import pipeline', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'from PIL import Image', 'import numpy as np', 'import torch', 'from diffusers.utils import load_image', \"depth_estimator = pipeline('depth-estimation')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)', \"image = depth_estimator(image)['depth']\", 'image = np.array(image)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'image = Image.fromarray(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', \"image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]\", \"image.save('./images/stormtrooper_depth_out.png')\"]}, 'performance': {'dataset': '3M depth-image, caption pairs', 'accuracy': '500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] قم بإنشاء تطبيق لتحويل صور الوجوه إلى رسوم مرسومة يدويًا باستخدام خوارزمية توليد الصور الفنية. يمكنك استخدام ال API التالي لتحقيق ذلك.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Depth Estimation', 'api_name': 'lllyasviel/sd-controlnet-depth', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'PIL', 'numpy', 'torch'], 'example_code': {'install_packages': 'pip install diffusers transformers accelerate', 'code': ['from transformers import pipeline', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'from PIL import Image', 'import numpy as np', 'import torch', 'from diffusers.utils import load_image', \"depth_estimator = pipeline('depth-estimation')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)', \"image = depth_estimator(image)['depth']\", 'image = np.array(image)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'image = Image.fromarray(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', \"image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]\", \"image.save('./images/stormtrooper_depth_out.png')\"]}, 'performance': {'dataset': '3M depth-image, caption pairs', 'accuracy': '500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تصميم وتنفيذ نظام لتقديم تقدير حجمي للأشياء على صور مثل السيارات والأشخاص. يُمكنك استخدام ال API التالي لتنفيذ هذه المهمة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Depth Estimation', 'api_name': 'lllyasviel/sd-controlnet-depth', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'PIL', 'numpy', 'torch'], 'example_code': {'install_packages': 'pip install diffusers transformers accelerate', 'code': ['from transformers import pipeline', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'from PIL import Image', 'import numpy as np', 'import torch', 'from diffusers.utils import load_image', \"depth_estimator = pipeline('depth-estimation')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)', \"image = depth_estimator(image)['depth']\", 'image = np.array(image)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'image = Image.fromarray(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', \"image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]\", \"image.save('./images/stormtrooper_depth_out.png')\"]}, 'performance': {'dataset': '3M depth-image, caption pairs', 'accuracy': '500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] يهمنا تطوير نموذج للكشف عن الإشارات المرورية في صور الشوارع. نحن بحاجة إلى استخدام API لتنفيذ عمليات الكشف والتصنيف.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Depth Estimation', 'api_name': 'lllyasviel/sd-controlnet-depth', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'PIL', 'numpy', 'torch'], 'example_code': {'install_packages': 'pip install diffusers transformers accelerate', 'code': ['from transformers import pipeline', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'from PIL import Image', 'import numpy as np', 'import torch', 'from diffusers.utils import load_image', \"depth_estimator = pipeline('depth-estimation')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)', \"image = depth_estimator(image)['depth']\", 'image = np.array(image)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'image = Image.fromarray(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', \"image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]\", \"image.save('./images/stormtrooper_depth_out.png')\"]}, 'performance': {'dataset': '3M depth-image, caption pairs', 'accuracy': '500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تنفيذ نظام لتحليل البيانات الطبية الشعاعية وتحديد الأمراض والشروط الصحية. يُمكنك الاستعانة بال API التالي لتنفيذ مهام تصنيف الصور الطبية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Depth Estimation', 'api_name': 'lllyasviel/sd-controlnet-depth', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'PIL', 'numpy', 'torch'], 'example_code': {'install_packages': 'pip install diffusers transformers accelerate', 'code': ['from transformers import pipeline', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'from PIL import Image', 'import numpy as np', 'import torch', 'from diffusers.utils import load_image', \"depth_estimator = pipeline('depth-estimation')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)', \"image = depth_estimator(image)['depth']\", 'image = np.array(image)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'image = Image.fromarray(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', \"image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]\", \"image.save('./images/stormtrooper_depth_out.png')\"]}, 'performance': {'dataset': '3M depth-image, caption pairs', 'accuracy': '500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] ابنِ تطبيقًا يستخدم تقنية تعلم الآلة لتحديد نسبة السعادة في الوجوه البشرية. يُمكنك استخدام ال API التالي لتحقيق هذا الهدف بدقة عالية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Depth Estimation', 'api_name': 'lllyasviel/sd-controlnet-depth', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'PIL', 'numpy', 'torch'], 'example_code': {'install_packages': 'pip install diffusers transformers accelerate', 'code': ['from transformers import pipeline', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'from PIL import Image', 'import numpy as np', 'import torch', 'from diffusers.utils import load_image', \"depth_estimator = pipeline('depth-estimation')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)', \"image = depth_estimator(image)['depth']\", 'image = np.array(image)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'image = Image.fromarray(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', \"image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]\", \"image.save('./images/stormtrooper_depth_out.png')\"]}, 'performance': {'dataset': '3M depth-image, caption pairs', 'accuracy': '500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تطوير نظام يقوم بتحليل صور الغذاء وتوفير معلومات التغذية والسعرات الحرارية. استخدم ال API التالي لتنفيذ عمليات التحليل والاستخراج الضرورية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Depth Estimation', 'api_name': 'lllyasviel/sd-controlnet-depth', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'PIL', 'numpy', 'torch'], 'example_code': {'install_packages': 'pip install diffusers transformers accelerate', 'code': ['from transformers import pipeline', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'from PIL import Image', 'import numpy as np', 'import torch', 'from diffusers.utils import load_image', \"depth_estimator = pipeline('depth-estimation')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)', \"image = depth_estimator(image)['depth']\", 'image = np.array(image)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'image = Image.fromarray(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', \"image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]\", \"image.save('./images/stormtrooper_depth_out.png')\"]}, 'performance': {'dataset': '3M depth-image, caption pairs', 'accuracy': '500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] يهمنا إنشاء نظام يمكنه تحديد أنواع الحيوانات في الصور البرية. استخدم ال API التالي لتنفيذ هذه المهمة بكفاءة عالية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Depth Estimation', 'api_name': 'lllyasviel/sd-controlnet-depth', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'PIL', 'numpy', 'torch'], 'example_code': {'install_packages': 'pip install diffusers transformers accelerate', 'code': ['from transformers import pipeline', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'from PIL import Image', 'import numpy as np', 'import torch', 'from diffusers.utils import load_image', \"depth_estimator = pipeline('depth-estimation')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)', \"image = depth_estimator(image)['depth']\", 'image = np.array(image)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'image = Image.fromarray(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', \"image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]\", \"image.save('./images/stormtrooper_depth_out.png')\"]}, 'performance': {'dataset': '3M depth-image, caption pairs', 'accuracy': '500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] يُرغب في تنفيذ تطبيق يتمكن من فصل الأشخاص عن الخلفية في صورهم الشخصية. يمكنك استخدام ال API التالي لتنفيذ عملية فصل الخلفية بدقة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Depth Estimation', 'api_name': 'lllyasviel/sd-controlnet-depth', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'PIL', 'numpy', 'torch'], 'example_code': {'install_packages': 'pip install diffusers transformers accelerate', 'code': ['from transformers import pipeline', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'from PIL import Image', 'import numpy as np', 'import torch', 'from diffusers.utils import load_image', \"depth_estimator = pipeline('depth-estimation')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)', \"image = depth_estimator(image)['depth']\", 'image = np.array(image)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'image = Image.fromarray(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', \"image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]\", \"image.save('./images/stormtrooper_depth_out.png')\"]}, 'performance': {'dataset': '3M depth-image, caption pairs', 'accuracy': '500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] يجب علينا تطوير نظام لتحليل مشاهد الفيديو لتتبع الأشخاص والكائنات داخل الفيديو. اسخدم ال API التالي لنجاز هذه المهمة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Depth Estimation', 'api_name': 'lllyasviel/sd-controlnet-depth', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'PIL', 'numpy', 'torch'], 'example_code': {'install_packages': 'pip install diffusers transformers accelerate', 'code': ['from transformers import pipeline', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'from PIL import Image', 'import numpy as np', 'import torch', 'from diffusers.utils import load_image', \"depth_estimator = pipeline('depth-estimation')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)', \"image = depth_estimator(image)['depth']\", 'image = np.array(image)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'image = Image.fromarray(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', \"image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]\", \"image.save('./images/stormtrooper_depth_out.png')\"]}, 'performance': {'dataset': '3M depth-image, caption pairs', 'accuracy': '500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST]تطوير تطبيق لتحليل الصور باستخدام مكتبة OpenCV وتقنيات Computer Vision.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/sd-controlnet-scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png')\\nimage = hed(image, scribble=True)\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble', torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe('bag', image, num_inference_steps=20).images[0]\\nimage.save('images/bag_scribble_out.png')\", 'performance': {'dataset': '500k scribble-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تكامل البحث بالصوت في تطبيق الهاتف الذكي باستخدام API Google Speech-to-Text.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/sd-controlnet-scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png')\\nimage = hed(image, scribble=True)\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble', torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe('bag', image, num_inference_steps=20).images[0]\\nimage.save('images/bag_scribble_out.png')\", 'performance': {'dataset': '500k scribble-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تطوير نظام رؤية آلية لتعقب الأشخاص في مقاطع الفيديو بواسطة API OpenCV.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/sd-controlnet-scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png')\\nimage = hed(image, scribble=True)\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble', torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe('bag', image, num_inference_steps=20).images[0]\\nimage.save('images/bag_scribble_out.png')\", 'performance': {'dataset': '500k scribble-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تنفيذ نظام استشعار ذكاء اصطناعي للكشف عن حركة السيارات في الطريق باستخدام API TensorFlow Object Detection.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/sd-controlnet-scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png')\\nimage = hed(image, scribble=True)\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble', torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe('bag', image, num_inference_steps=20).images[0]\\nimage.save('images/bag_scribble_out.png')\", 'performance': {'dataset': '500k scribble-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] إنشاء نظام التصنيف الآلي لصور الفضاء باستخدام API Clarifai.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/sd-controlnet-scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png')\\nimage = hed(image, scribble=True)\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble', torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe('bag', image, num_inference_steps=20).images[0]\\nimage.save('images/bag_scribble_out.png')\", 'performance': {'dataset': '500k scribble-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تطوير تطبيق لمتابعة تغيرات حجم الكائنات في الفيديو باستخدام واجهة برمجة التطبيقات (API) YOLOv3.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/sd-controlnet-scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png')\\nimage = hed(image, scribble=True)\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble', torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe('bag', image, num_inference_steps=20).images[0]\\nimage.save('images/bag_scribble_out.png')\", 'performance': {'dataset': '500k scribble-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تكامل المدمجات الذكية في تطبيق الهاتف النقال باستخدام API DialogFlow لتحسين تجربة المستخدم.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/sd-controlnet-scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png')\\nimage = hed(image, scribble=True)\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble', torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe('bag', image, num_inference_steps=20).images[0]\\nimage.save('images/bag_scribble_out.png')\", 'performance': {'dataset': '500k scribble-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تنفيذ نظام استشعار ذكاء اصطناعي لتقدير عدد الأشخاص في المنطقة باستخدام مكتبة TensorFlow Object Detection.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/sd-controlnet-scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png')\\nimage = hed(image, scribble=True)\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble', torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe('bag', image, num_inference_steps=20).images[0]\\nimage.save('images/bag_scribble_out.png')\", 'performance': {'dataset': '500k scribble-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تغيير حجم الصور باستخدام خدمة تحسين الصور في سحابة AWS.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/sd-controlnet-scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png')\\nimage = hed(image, scribble=True)\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble', torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe('bag', image, num_inference_steps=20).images[0]\\nimage.save('images/bag_scribble_out.png')\", 'performance': {'dataset': '500k scribble-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تطبيق تقنيات الخوارزميات الجينية في تطوير نظام للتنبؤ بأداء الأسهم باستخدام واجهة برمجة التطبيقات Alpha Vantage.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/sd-controlnet-scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble')\", 'api_arguments': ['image', 'text'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png')\\nimage = hed(image, scribble=True)\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble', torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe('bag', image, num_inference_steps=20).images[0]\\nimage.save('images/bag_scribble_out.png')\", 'performance': {'dataset': '500k scribble-image, caption pairs', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST]تطوير تطبيق يقدم تحليلاً لصور الأقمار الصناعية لمساعدة في تتبع التغيرات البيئية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_canny')\", 'api_arguments': {'text': 'a blue paradise bird in the jungle', 'num_inference_steps': 20, 'generator': 'torch.manual_seed(33)', 'image': 'control_image'}, 'python_environment_requirements': ['pip install opencv-contrib-python', 'pip install diffusers transformers accelerate'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'import numpy as np', 'import cv2', 'from PIL import Image', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15_canny', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/images/input.png', ')', 'image = np.array(image)', 'low_threshold = 100', 'high_threshold = 200', 'image = cv2.Canny(image, low_threshold, high_threshold)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'control_image = Image.fromarray(image)', 'control_image.save(./images/control.png)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(33)', 'image = pipe(a blue paradise bird in the jungle, num_inference_steps=20, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] إنشاء تطبيق يقوم بالتعرف على الموسيقى من الصوت المباشر وإقتراح الأغاني ذات الصلة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_canny')\", 'api_arguments': {'text': 'a blue paradise bird in the jungle', 'num_inference_steps': 20, 'generator': 'torch.manual_seed(33)', 'image': 'control_image'}, 'python_environment_requirements': ['pip install opencv-contrib-python', 'pip install diffusers transformers accelerate'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'import numpy as np', 'import cv2', 'from PIL import Image', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15_canny', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/images/input.png', ')', 'image = np.array(image)', 'low_threshold = 100', 'high_threshold = 200', 'image = cv2.Canny(image, low_threshold, high_threshold)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'control_image = Image.fromarray(image)', 'control_image.save(./images/control.png)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(33)', 'image = pipe(a blue paradise bird in the jungle, num_inference_steps=20, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] تصميم تطبيق يستخدم تحليل اللغة الطبيعية لتلخيص الأخبار العالمية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_canny')\", 'api_arguments': {'text': 'a blue paradise bird in the jungle', 'num_inference_steps': 20, 'generator': 'torch.manual_seed(33)', 'image': 'control_image'}, 'python_environment_requirements': ['pip install opencv-contrib-python', 'pip install diffusers transformers accelerate'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'import numpy as np', 'import cv2', 'from PIL import Image', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15_canny', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/images/input.png', ')', 'image = np.array(image)', 'low_threshold = 100', 'high_threshold = 200', 'image = cv2.Canny(image, low_threshold, high_threshold)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'control_image = Image.fromarray(image)', 'control_image.save(./images/control.png)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(33)', 'image = pipe(a blue paradise bird in the jungle, num_inference_steps=20, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] إنشاء تطبيق يقدم الترجمة الآلية للنصوص بين لغات متعددة باستخدام تقنيات تعلم الآلة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_canny')\", 'api_arguments': {'text': 'a blue paradise bird in the jungle', 'num_inference_steps': 20, 'generator': 'torch.manual_seed(33)', 'image': 'control_image'}, 'python_environment_requirements': ['pip install opencv-contrib-python', 'pip install diffusers transformers accelerate'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'import numpy as np', 'import cv2', 'from PIL import Image', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15_canny', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/images/input.png', ')', 'image = np.array(image)', 'low_threshold = 100', 'high_threshold = 200', 'image = cv2.Canny(image, low_threshold, high_threshold)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'control_image = Image.fromarray(image)', 'control_image.save(./images/control.png)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(33)', 'image = pipe(a blue paradise bird in the jungle, num_inference_steps=20, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] تطوير تطبيق يستخدم تقنيات الرؤية الحاسوبية لتحليل عناصر الوجه وتقديم تقييم معياري للجمال.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_canny')\", 'api_arguments': {'text': 'a blue paradise bird in the jungle', 'num_inference_steps': 20, 'generator': 'torch.manual_seed(33)', 'image': 'control_image'}, 'python_environment_requirements': ['pip install opencv-contrib-python', 'pip install diffusers transformers accelerate'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'import numpy as np', 'import cv2', 'from PIL import Image', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15_canny', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/images/input.png', ')', 'image = np.array(image)', 'low_threshold = 100', 'high_threshold = 200', 'image = cv2.Canny(image, low_threshold, high_threshold)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'control_image = Image.fromarray(image)', 'control_image.save(./images/control.png)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(33)', 'image = pipe(a blue paradise bird in the jungle, num_inference_steps=20, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] بناء تطبيق يقدم نصائح صحية شخصية معتمدة على تحليل بيانات الصحة الشخصية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_canny')\", 'api_arguments': {'text': 'a blue paradise bird in the jungle', 'num_inference_steps': 20, 'generator': 'torch.manual_seed(33)', 'image': 'control_image'}, 'python_environment_requirements': ['pip install opencv-contrib-python', 'pip install diffusers transformers accelerate'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'import numpy as np', 'import cv2', 'from PIL import Image', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15_canny', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/images/input.png', ')', 'image = np.array(image)', 'low_threshold = 100', 'high_threshold = 200', 'image = cv2.Canny(image, low_threshold, high_threshold)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'control_image = Image.fromarray(image)', 'control_image.save(./images/control.png)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(33)', 'image = pipe(a blue paradise bird in the jungle, num_inference_steps=20, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم الذكاء الاصطناعي لتوقع أسعار العقارات بشكل دقيق.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_canny')\", 'api_arguments': {'text': 'a blue paradise bird in the jungle', 'num_inference_steps': 20, 'generator': 'torch.manual_seed(33)', 'image': 'control_image'}, 'python_environment_requirements': ['pip install opencv-contrib-python', 'pip install diffusers transformers accelerate'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'import numpy as np', 'import cv2', 'from PIL import Image', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15_canny', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/images/input.png', ')', 'image = np.array(image)', 'low_threshold = 100', 'high_threshold = 200', 'image = cv2.Canny(image, low_threshold, high_threshold)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'control_image = Image.fromarray(image)', 'control_image.save(./images/control.png)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(33)', 'image = pipe(a blue paradise bird in the jungle, num_inference_steps=20, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] تصميم تطبيق يستخدم تقنيات الذكاء الاصطناعي لتحليل أنماط الشراء وتوفير اقتراحات للمنتجات للعملاء.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_canny')\", 'api_arguments': {'text': 'a blue paradise bird in the jungle', 'num_inference_steps': 20, 'generator': 'torch.manual_seed(33)', 'image': 'control_image'}, 'python_environment_requirements': ['pip install opencv-contrib-python', 'pip install diffusers transformers accelerate'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'import numpy as np', 'import cv2', 'from PIL import Image', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15_canny', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/images/input.png', ')', 'image = np.array(image)', 'low_threshold = 100', 'high_threshold = 200', 'image = cv2.Canny(image, low_threshold, high_threshold)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'control_image = Image.fromarray(image)', 'control_image.save(./images/control.png)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(33)', 'image = pipe(a blue paradise bird in the jungle, num_inference_steps=20, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] تطوير تطبيق يستخدم تحليل الأداء الرياضي لتوفير نصائح تدريبية شخصية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_canny')\", 'api_arguments': {'text': 'a blue paradise bird in the jungle', 'num_inference_steps': 20, 'generator': 'torch.manual_seed(33)', 'image': 'control_image'}, 'python_environment_requirements': ['pip install opencv-contrib-python', 'pip install diffusers transformers accelerate'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'import numpy as np', 'import cv2', 'from PIL import Image', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15_canny', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/images/input.png', ')', 'image = np.array(image)', 'low_threshold = 100', 'high_threshold = 200', 'image = cv2.Canny(image, low_threshold, high_threshold)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'control_image = Image.fromarray(image)', 'control_image.save(./images/control.png)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(33)', 'image = pipe(a blue paradise bird in the jungle, num_inference_steps=20, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] إنشاء تطبيق يعتمد على تحليل البيانات لتقديم توصيات ذكية لتحسين خدمة العملاء.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_canny', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_canny')\", 'api_arguments': {'text': 'a blue paradise bird in the jungle', 'num_inference_steps': 20, 'generator': 'torch.manual_seed(33)', 'image': 'control_image'}, 'python_environment_requirements': ['pip install opencv-contrib-python', 'pip install diffusers transformers accelerate'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'import numpy as np', 'import cv2', 'from PIL import Image', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15_canny', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/images/input.png', ')', 'image = np.array(image)', 'low_threshold = 100', 'high_threshold = 200', 'image = cv2.Canny(image, low_threshold, high_threshold)', 'image = image[:, :, None]', 'image = np.concatenate([image, image, image], axis=2)', 'control_image = Image.fromarray(image)', 'control_image.save(./images/control.png)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(33)', 'image = pipe(a blue paradise bird in the jungle, num_inference_steps=20, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST]تحتاج إلى إنشاء نظام يعتمد على التعلم الآلي لتحليل نمط الكتابة اليدوية في الصور.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'ControlNet - M-LSD Straight Line Version', 'api_name': 'lllyasviel/sd-controlnet-mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': {'import': ['from PIL import Image', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'import torch', 'from controlnet_aux import MLSDdetector', 'from diffusers.utils import load_image'], 'setup': [\"mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)', 'image = mlsd(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)'], 'execution': ['pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', 'image = pipe(room, image, num_inference_steps=20).images[0]', \"image.save('images/room_mlsd_out.png')\"]}, 'performance': {'dataset': '600k edge-image, caption pairs generated from Places2', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] إنشاء نظام قادر على تحليل الصور الطبية لاكتشاف الأورام السرطانية بدقة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'ControlNet - M-LSD Straight Line Version', 'api_name': 'lllyasviel/sd-controlnet-mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': {'import': ['from PIL import Image', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'import torch', 'from controlnet_aux import MLSDdetector', 'from diffusers.utils import load_image'], 'setup': [\"mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)', 'image = mlsd(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)'], 'execution': ['pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', 'image = pipe(room, image, num_inference_steps=20).images[0]', \"image.save('images/room_mlsd_out.png')\"]}, 'performance': {'dataset': '600k edge-image, caption pairs generated from Places2', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تحتاج إلى بناء نظام استشعار يعتمد على الشبكات العصبية العميقة لتتبع حركة الأشخاص في الفيديوهات.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'ControlNet - M-LSD Straight Line Version', 'api_name': 'lllyasviel/sd-controlnet-mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': {'import': ['from PIL import Image', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'import torch', 'from controlnet_aux import MLSDdetector', 'from diffusers.utils import load_image'], 'setup': [\"mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)', 'image = mlsd(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)'], 'execution': ['pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', 'image = pipe(room, image, num_inference_steps=20).images[0]', \"image.save('images/room_mlsd_out.png')\"]}, 'performance': {'dataset': '600k edge-image, caption pairs generated from Places2', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] بناء نظام يعتمد على تقنيات تعلم الآلة لتصنيف الجرائم من خلال تحليل صور المشاهد الجرمية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'ControlNet - M-LSD Straight Line Version', 'api_name': 'lllyasviel/sd-controlnet-mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': {'import': ['from PIL import Image', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'import torch', 'from controlnet_aux import MLSDdetector', 'from diffusers.utils import load_image'], 'setup': [\"mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)', 'image = mlsd(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)'], 'execution': ['pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', 'image = pipe(room, image, num_inference_steps=20).images[0]', \"image.save('images/room_mlsd_out.png')\"]}, 'performance': {'dataset': '600k edge-image, caption pairs generated from Places2', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] إنشاء نظام قادر على تقدير عمر الأشخاص من صورهم باستخدام تقنيات تعلم الآلة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'ControlNet - M-LSD Straight Line Version', 'api_name': 'lllyasviel/sd-controlnet-mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': {'import': ['from PIL import Image', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'import torch', 'from controlnet_aux import MLSDdetector', 'from diffusers.utils import load_image'], 'setup': [\"mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)', 'image = mlsd(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)'], 'execution': ['pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', 'image = pipe(room, image, num_inference_steps=20).images[0]', \"image.save('images/room_mlsd_out.png')\"]}, 'performance': {'dataset': '600k edge-image, caption pairs generated from Places2', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تحتاج إلى بناء نظام يعتمد على التعلم الآلي لتحليل وتصنيف العناصر الغذائية في الصور الغذائية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'ControlNet - M-LSD Straight Line Version', 'api_name': 'lllyasviel/sd-controlnet-mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': {'import': ['from PIL import Image', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'import torch', 'from controlnet_aux import MLSDdetector', 'from diffusers.utils import load_image'], 'setup': [\"mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)', 'image = mlsd(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)'], 'execution': ['pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', 'image = pipe(room, image, num_inference_steps=20).images[0]', \"image.save('images/room_mlsd_out.png')\"]}, 'performance': {'dataset': '600k edge-image, caption pairs generated from Places2', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] بناء نظام يستخدم تقنيات التعلم الآلي لتحليل سلوك المرور في الطرقات من خلال كاميرات المراقبة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'ControlNet - M-LSD Straight Line Version', 'api_name': 'lllyasviel/sd-controlnet-mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': {'import': ['from PIL import Image', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'import torch', 'from controlnet_aux import MLSDdetector', 'from diffusers.utils import load_image'], 'setup': [\"mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)', 'image = mlsd(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)'], 'execution': ['pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', 'image = pipe(room, image, num_inference_steps=20).images[0]', \"image.save('images/room_mlsd_out.png')\"]}, 'performance': {'dataset': '600k edge-image, caption pairs generated from Places2', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] إنشاء نظام قادر على تحليل النصوص الطبية العربية لاستخراج التشخيص والعلاجات المقترحة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'ControlNet - M-LSD Straight Line Version', 'api_name': 'lllyasviel/sd-controlnet-mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': {'import': ['from PIL import Image', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'import torch', 'from controlnet_aux import MLSDdetector', 'from diffusers.utils import load_image'], 'setup': [\"mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)', 'image = mlsd(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)'], 'execution': ['pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', 'image = pipe(room, image, num_inference_steps=20).images[0]', \"image.save('images/room_mlsd_out.png')\"]}, 'performance': {'dataset': '600k edge-image, caption pairs generated from Places2', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تحتاج إلى بناء نظام يستخدم تقنيات التعلم الآلي لتحديد نوع الحيوانات في الصور الطبيعية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'ControlNet - M-LSD Straight Line Version', 'api_name': 'lllyasviel/sd-controlnet-mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': {'import': ['from PIL import Image', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'import torch', 'from controlnet_aux import MLSDdetector', 'from diffusers.utils import load_image'], 'setup': [\"mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)', 'image = mlsd(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)'], 'execution': ['pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', 'image = pipe(room, image, num_inference_steps=20).images[0]', \"image.save('images/room_mlsd_out.png')\"]}, 'performance': {'dataset': '600k edge-image, caption pairs generated from Places2', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] بناء نظام يعتمد على تقنيات التعلم الآلي لتحليل سلوك الموظفين في الشركات من خلال تقييم الأداء والانضباط.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'ControlNet - M-LSD Straight Line Version', 'api_name': 'lllyasviel/sd-controlnet-mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')\", 'api_arguments': {'torch_dtype': 'torch.float16'}, 'python_environment_requirements': {'diffusers': 'pip install diffusers', 'transformers': 'pip install transformers', 'accelerate': 'pip install accelerate', 'controlnet_aux': 'pip install controlnet_aux'}, 'example_code': {'import': ['from PIL import Image', 'from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler', 'import torch', 'from controlnet_aux import MLSDdetector', 'from diffusers.utils import load_image'], 'setup': [\"mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\", 'image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)', 'image = mlsd(image)', 'controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)'], 'execution': ['pipe.enable_xformers_memory_efficient_attention()', 'pipe.enable_model_cpu_offload()', 'image = pipe(room, image, num_inference_steps=20).images[0]', \"image.save('images/room_mlsd_out.png')\"]}, 'performance': {'dataset': '600k edge-image, caption pairs generated from Places2', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST]تطلب شركتنا إنشاء تطبيق يستخدم الـ API للتعرف على أنماط التصوير الفني وتعريف المفاهيم الفنية في الصور. يمكنك استخدام API التالي لتحقيق ذلك:? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'ControlNet', 'api_name': 'lllyasviel/control_v11p_sd15_lineart', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_lineart')\", 'api_arguments': {'checkpoint': 'ControlNet-1-1-preview/control_v11p_sd15_lineart', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate controlnet_aux==0.3.0', 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import LineartDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = ControlNet-1-1-preview/control_v11p_sd15_lineart\\nimage = load_image(\\n https://huggingface.co/ControlNet-1-1-preview/control_v11p_sd15_lineart/resolve/main/images/input.png\\n)\\nimage = image.resize((512, 512))\\nprompt = michael jackson concert\\nprocessor = LineartDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'ControlNet-1-1-preview', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.' </s>"
"<s>[INST] يرغب المستخدم في تطوير تطبيق لتحويل النصوص الأدبية إلى صور فنية. يمكنك استخدام الـ API التالي لتحقيق ذلك:? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'ControlNet', 'api_name': 'lllyasviel/control_v11p_sd15_lineart', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_lineart')\", 'api_arguments': {'checkpoint': 'ControlNet-1-1-preview/control_v11p_sd15_lineart', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate controlnet_aux==0.3.0', 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import LineartDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = ControlNet-1-1-preview/control_v11p_sd15_lineart\\nimage = load_image(\\n https://huggingface.co/ControlNet-1-1-preview/control_v11p_sd15_lineart/resolve/main/images/input.png\\n)\\nimage = image.resize((512, 512))\\nprompt = michael jackson concert\\nprocessor = LineartDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'ControlNet-1-1-preview', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.' </s>"
"<s>[INST] أنشئ تطبيقًا يستند إلى الـ API لاكتشاف الأنماط البصرية والتعابير الفنية في الصور الزهرية. استخدم الـ API التالي لتحقيق هذه الوظيفة:? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'ControlNet', 'api_name': 'lllyasviel/control_v11p_sd15_lineart', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_lineart')\", 'api_arguments': {'checkpoint': 'ControlNet-1-1-preview/control_v11p_sd15_lineart', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate controlnet_aux==0.3.0', 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import LineartDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = ControlNet-1-1-preview/control_v11p_sd15_lineart\\nimage = load_image(\\n https://huggingface.co/ControlNet-1-1-preview/control_v11p_sd15_lineart/resolve/main/images/input.png\\n)\\nimage = image.resize((512, 512))\\nprompt = michael jackson concert\\nprocessor = LineartDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'ControlNet-1-1-preview', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.' </s>"
"<s>[INST]قم بتطوير تطبيق يستخدم تقنية تعلم الآلة لتحديد أنواع الزهور من الصور وعرض توقعات محددة لكل نوع.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Normal Map Estimation', 'api_name': 'lllyasviel/sd-controlnet-normal', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\", 'api_arguments': ['image', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"from PIL import Image\\nfrom transformers import pipeline\\nimport numpy as np\\nimport cv2\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png).convert(RGB)\\ndepth_estimator = pipeline(depth-estimation, model =Intel/dpt-hybrid-midas )\\nimage = depth_estimator(image)['predicted_depth'][0]\\nimage = image.numpy()\\nimage_depth = image.copy()\\nimage_depth -= np.min(image_depth)\\nimage_depth /= np.max(image_depth)\\nbg_threhold = 0.4\\nx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\\nx[image_depth &lt; bg_threhold] = 0\\ny = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\\ny[image_depth &lt; bg_threhold] = 0\\nz = np.ones_like(x) * np.pi * 2.0\\nimage = np.stack([x, y, z], axis=2)\\nimage /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\\nimage = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n fusing/stable-diffusion-v1-5-controlnet-normal, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(cute toy, image, num_inference_steps=20).images[0]\\nimage.save('images/toy_normal_out.png')\", 'performance': {'dataset': 'DIODE', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم تكنولوجيا الذكاء الاصطناعي لتحليل النصوص واستخراج المعلومات الرئيسية منها.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Normal Map Estimation', 'api_name': 'lllyasviel/sd-controlnet-normal', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\", 'api_arguments': ['image', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"from PIL import Image\\nfrom transformers import pipeline\\nimport numpy as np\\nimport cv2\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png).convert(RGB)\\ndepth_estimator = pipeline(depth-estimation, model =Intel/dpt-hybrid-midas )\\nimage = depth_estimator(image)['predicted_depth'][0]\\nimage = image.numpy()\\nimage_depth = image.copy()\\nimage_depth -= np.min(image_depth)\\nimage_depth /= np.max(image_depth)\\nbg_threhold = 0.4\\nx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\\nx[image_depth &lt; bg_threhold] = 0\\ny = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\\ny[image_depth &lt; bg_threhold] = 0\\nz = np.ones_like(x) * np.pi * 2.0\\nimage = np.stack([x, y, z], axis=2)\\nimage /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\\nimage = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n fusing/stable-diffusion-v1-5-controlnet-normal, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(cute toy, image, num_inference_steps=20).images[0]\\nimage.save('images/toy_normal_out.png')\", 'performance': {'dataset': 'DIODE', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] قم بإنشاء نظام يستخدم تقنية التعلم الآلي للتنبؤ بتطورات سوق الأسهم وتقديم توصيات استثمارية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Normal Map Estimation', 'api_name': 'lllyasviel/sd-controlnet-normal', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\", 'api_arguments': ['image', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"from PIL import Image\\nfrom transformers import pipeline\\nimport numpy as np\\nimport cv2\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png).convert(RGB)\\ndepth_estimator = pipeline(depth-estimation, model =Intel/dpt-hybrid-midas )\\nimage = depth_estimator(image)['predicted_depth'][0]\\nimage = image.numpy()\\nimage_depth = image.copy()\\nimage_depth -= np.min(image_depth)\\nimage_depth /= np.max(image_depth)\\nbg_threhold = 0.4\\nx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\\nx[image_depth &lt; bg_threhold] = 0\\ny = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\\ny[image_depth &lt; bg_threhold] = 0\\nz = np.ones_like(x) * np.pi * 2.0\\nimage = np.stack([x, y, z], axis=2)\\nimage /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\\nimage = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n fusing/stable-diffusion-v1-5-controlnet-normal, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(cute toy, image, num_inference_steps=20).images[0]\\nimage.save('images/toy_normal_out.png')\", 'performance': {'dataset': 'DIODE', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم تقنية الذكاء الاصطناعي لتحسين تجربة التسوق عبر الإنترنت من خلال توصيات شخصية للعملاء.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Normal Map Estimation', 'api_name': 'lllyasviel/sd-controlnet-normal', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\", 'api_arguments': ['image', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"from PIL import Image\\nfrom transformers import pipeline\\nimport numpy as np\\nimport cv2\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png).convert(RGB)\\ndepth_estimator = pipeline(depth-estimation, model =Intel/dpt-hybrid-midas )\\nimage = depth_estimator(image)['predicted_depth'][0]\\nimage = image.numpy()\\nimage_depth = image.copy()\\nimage_depth -= np.min(image_depth)\\nimage_depth /= np.max(image_depth)\\nbg_threhold = 0.4\\nx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\\nx[image_depth &lt; bg_threhold] = 0\\ny = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\\ny[image_depth &lt; bg_threhold] = 0\\nz = np.ones_like(x) * np.pi * 2.0\\nimage = np.stack([x, y, z], axis=2)\\nimage /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\\nimage = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n fusing/stable-diffusion-v1-5-controlnet-normal, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(cute toy, image, num_inference_steps=20).images[0]\\nimage.save('images/toy_normal_out.png')\", 'performance': {'dataset': 'DIODE', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تصميم نظام يستخدم تقنية الذكاء الاصطناعي لتحليل سلوك المستخدمين عبر منصة التواصل الاجتماعي وتقديم توقعات دقيقة لاهتماماتهم.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Normal Map Estimation', 'api_name': 'lllyasviel/sd-controlnet-normal', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\", 'api_arguments': ['image', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"from PIL import Image\\nfrom transformers import pipeline\\nimport numpy as np\\nimport cv2\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png).convert(RGB)\\ndepth_estimator = pipeline(depth-estimation, model =Intel/dpt-hybrid-midas )\\nimage = depth_estimator(image)['predicted_depth'][0]\\nimage = image.numpy()\\nimage_depth = image.copy()\\nimage_depth -= np.min(image_depth)\\nimage_depth /= np.max(image_depth)\\nbg_threhold = 0.4\\nx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\\nx[image_depth &lt; bg_threhold] = 0\\ny = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\\ny[image_depth &lt; bg_threhold] = 0\\nz = np.ones_like(x) * np.pi * 2.0\\nimage = np.stack([x, y, z], axis=2)\\nimage /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\\nimage = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n fusing/stable-diffusion-v1-5-controlnet-normal, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(cute toy, image, num_inference_steps=20).images[0]\\nimage.save('images/toy_normal_out.png')\", 'performance': {'dataset': 'DIODE', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم تقنيات الذكاء الاصطناعي لتقييم أداء الطلاب وتقديم توجيهات لتحسين أدائهم الأكاديمي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Normal Map Estimation', 'api_name': 'lllyasviel/sd-controlnet-normal', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\", 'api_arguments': ['image', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"from PIL import Image\\nfrom transformers import pipeline\\nimport numpy as np\\nimport cv2\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png).convert(RGB)\\ndepth_estimator = pipeline(depth-estimation, model =Intel/dpt-hybrid-midas )\\nimage = depth_estimator(image)['predicted_depth'][0]\\nimage = image.numpy()\\nimage_depth = image.copy()\\nimage_depth -= np.min(image_depth)\\nimage_depth /= np.max(image_depth)\\nbg_threhold = 0.4\\nx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\\nx[image_depth &lt; bg_threhold] = 0\\ny = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\\ny[image_depth &lt; bg_threhold] = 0\\nz = np.ones_like(x) * np.pi * 2.0\\nimage = np.stack([x, y, z], axis=2)\\nimage /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\\nimage = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n fusing/stable-diffusion-v1-5-controlnet-normal, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(cute toy, image, num_inference_steps=20).images[0]\\nimage.save('images/toy_normal_out.png')\", 'performance': {'dataset': 'DIODE', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] قم بتطوير نظام يستخدم تقنيات التعلم الآلي لتحليل البيانات الصحية وتقديم توصيات للحفاظ على صحة الفرد.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Normal Map Estimation', 'api_name': 'lllyasviel/sd-controlnet-normal', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\", 'api_arguments': ['image', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"from PIL import Image\\nfrom transformers import pipeline\\nimport numpy as np\\nimport cv2\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png).convert(RGB)\\ndepth_estimator = pipeline(depth-estimation, model =Intel/dpt-hybrid-midas )\\nimage = depth_estimator(image)['predicted_depth'][0]\\nimage = image.numpy()\\nimage_depth = image.copy()\\nimage_depth -= np.min(image_depth)\\nimage_depth /= np.max(image_depth)\\nbg_threhold = 0.4\\nx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\\nx[image_depth &lt; bg_threhold] = 0\\ny = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\\ny[image_depth &lt; bg_threhold] = 0\\nz = np.ones_like(x) * np.pi * 2.0\\nimage = np.stack([x, y, z], axis=2)\\nimage /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\\nimage = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n fusing/stable-diffusion-v1-5-controlnet-normal, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(cute toy, image, num_inference_steps=20).images[0]\\nimage.save('images/toy_normal_out.png')\", 'performance': {'dataset': 'DIODE', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تصميم تطبيق يستخدم تقنيات الذكاء الاصطناعي لتحسين عمليات التنبؤ بالطلبات والإمدادات لشركة تجارية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Normal Map Estimation', 'api_name': 'lllyasviel/sd-controlnet-normal', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\", 'api_arguments': ['image', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"from PIL import Image\\nfrom transformers import pipeline\\nimport numpy as np\\nimport cv2\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png).convert(RGB)\\ndepth_estimator = pipeline(depth-estimation, model =Intel/dpt-hybrid-midas )\\nimage = depth_estimator(image)['predicted_depth'][0]\\nimage = image.numpy()\\nimage_depth = image.copy()\\nimage_depth -= np.min(image_depth)\\nimage_depth /= np.max(image_depth)\\nbg_threhold = 0.4\\nx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\\nx[image_depth &lt; bg_threhold] = 0\\ny = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\\ny[image_depth &lt; bg_threhold] = 0\\nz = np.ones_like(x) * np.pi * 2.0\\nimage = np.stack([x, y, z], axis=2)\\nimage /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\\nimage = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n fusing/stable-diffusion-v1-5-controlnet-normal, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(cute toy, image, num_inference_steps=20).images[0]\\nimage.save('images/toy_normal_out.png')\", 'performance': {'dataset': 'DIODE', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم تقنيات تعلم الآلة لتحليل سلوك العملاء عبر المواقع الإلكترونية وتقديم توصيات تسويقية مخصصة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Normal Map Estimation', 'api_name': 'lllyasviel/sd-controlnet-normal', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\", 'api_arguments': ['image', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"from PIL import Image\\nfrom transformers import pipeline\\nimport numpy as np\\nimport cv2\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png).convert(RGB)\\ndepth_estimator = pipeline(depth-estimation, model =Intel/dpt-hybrid-midas )\\nimage = depth_estimator(image)['predicted_depth'][0]\\nimage = image.numpy()\\nimage_depth = image.copy()\\nimage_depth -= np.min(image_depth)\\nimage_depth /= np.max(image_depth)\\nbg_threhold = 0.4\\nx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\\nx[image_depth &lt; bg_threhold] = 0\\ny = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\\ny[image_depth &lt; bg_threhold] = 0\\nz = np.ones_like(x) * np.pi * 2.0\\nimage = np.stack([x, y, z], axis=2)\\nimage /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\\nimage = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n fusing/stable-diffusion-v1-5-controlnet-normal, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(cute toy, image, num_inference_steps=20).images[0]\\nimage.save('images/toy_normal_out.png')\", 'performance': {'dataset': 'DIODE', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST] تصميم نظام يستخدم تقنيات الذكاء الاصطناعي لتحسين عمليات فحص الجودة في عملية تصنيع معينة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Normal Map Estimation', 'api_name': 'lllyasviel/sd-controlnet-normal', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-normal')\", 'api_arguments': ['image', 'num_inference_steps'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"from PIL import Image\\nfrom transformers import pipeline\\nimport numpy as np\\nimport cv2\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png).convert(RGB)\\ndepth_estimator = pipeline(depth-estimation, model =Intel/dpt-hybrid-midas )\\nimage = depth_estimator(image)['predicted_depth'][0]\\nimage = image.numpy()\\nimage_depth = image.copy()\\nimage_depth -= np.min(image_depth)\\nimage_depth /= np.max(image_depth)\\nbg_threhold = 0.4\\nx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\\nx[image_depth &lt; bg_threhold] = 0\\ny = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\\ny[image_depth &lt; bg_threhold] = 0\\nz = np.ones_like(x) * np.pi * 2.0\\nimage = np.stack([x, y, z], axis=2)\\nimage /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\\nimage = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n fusing/stable-diffusion-v1-5-controlnet-normal, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(cute toy, image, num_inference_steps=20).images[0]\\nimage.save('images/toy_normal_out.png')\", 'performance': {'dataset': 'DIODE', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.' </s>"
"<s>[INST]تطوير تطبيق جديد للتصوير الفوتوغرافي باستخدام واجهة برمجية Camera API للتحكم في إعدادات الكاميرا والتقاط الصور بشكل متقدم.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Diffusers', 'functionality': 'Text-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_scribble', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_scribble\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, scribble=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] تكوين نظام إدارة المحتوى الجديد باستخدام واجهة برمجية Content Management API للإنشاء والنشر وإدارة المحتوى بشكل فعال.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Diffusers', 'functionality': 'Text-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_scribble', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_scribble\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, scribble=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] بناء تطبيق جديد لتحليل البيانات باستخدام واجهة برمجية Data Analytics API لاستعراض واستخراج البيانات المترابطة من مصادر متعددة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Diffusers', 'functionality': 'Text-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_scribble', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_scribble\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, scribble=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] إنشاء تطبيق متطور لرسم الخرائط وعرض المعلومات الجغرافية باستخدام واجهة برمجية Maps API للتفاعل مع الخرائط والمواقع الجغرافية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Diffusers', 'functionality': 'Text-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_scribble', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_scribble\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, scribble=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] تطوير تطبيق متقدم للترجمة الفورية باستخدام واجهة برمجية Translation API لتوفير خدمات الترجمة بين لغات متعددة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Diffusers', 'functionality': 'Text-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_scribble', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_scribble\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, scribble=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] تصميم تطبيق جديد لتحليل الصوت واستخراج المعلومات الصوتية باستخدام واجهة برمجية Audio Processing API لمعالجة وتحليل البيانات الصوتية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Diffusers', 'functionality': 'Text-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_scribble', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_scribble\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, scribble=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] تطوير تطبيق مخصص لتتبع المواقع الجغرافية وإدارة الأسطول باستخدام واجهة برمجية GPS Tracking API للتحكم في تتبع المواقع وعرض البيانات الجغرافية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Diffusers', 'functionality': 'Text-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_scribble', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_scribble\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, scribble=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] إنشاء نظام تحليل الأمان السيبراني باستخدام واجهة برمجية Security Analysis API لفحص وتقييم الثغرات الأمنية في التطبيقات والأنظمة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Diffusers', 'functionality': 'Text-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_scribble', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_scribble\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, scribble=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] تكامل واجهة برمجية Payment Gateway API مع تطبيق التجارة الإلكترونية لتوفير وسائل دفع آمنة وسلسة للمستخدمين.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Diffusers', 'functionality': 'Text-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_scribble', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_scribble\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, scribble=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] تطوير تطبيق عصري للملاحة والتوجيه باستخدام واجهة برمجية Routing API لتوجيه الملاحة وتوفير مسارات محسنة للمستخدمين.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Diffusers', 'functionality': 'Text-to-Image', 'api_name': 'lllyasviel/control_v11p_sd15_scribble', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_scribble', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_scribble\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, scribble=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST]\"تدريب نموذج لتقدير أعمار الأشخاص في الصور باستخدام بيانات التدريب الموجودة.\"، \"استخدم واجهة برمجة التطبيقات لإنشاء تطبيق يحول النص إلى فيديو مع تفسير مرئي للمعلومات النصية.\"، \"تطوير نظام توصيات موسيقية يعتمد على حول اهتمامات المستخدم وتحليل سلوكه.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Diffusers', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15_openpose', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_openpose')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_openpose', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': {'import_libraries': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import OpenposeDetector', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')'], 'load_model': ['checkpoint = lllyasviel/control_v11p_sd15_openpose', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)'], 'example_usage': ['image = load_image(https://huggingface.co/lllyasviel/control_v11p_sd15_openpose/resolve/main/images/input.png)', 'prompt = chef in the kitchen', \"processor = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\", 'control_image = processor(image, hand_and_face=True)', 'control_image.save(./images/control.png)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"]}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.' </s>"
"<s>[INST] استخدم واجهة برمجة التطبيقات المتوفرة لتنفيذ نظام تحليل المشاعر في الوقت الفعلي على منشورات وسائل التواصل الاجتماعي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Diffusers', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15_openpose', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_openpose')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_openpose', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': {'import_libraries': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import OpenposeDetector', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')'], 'load_model': ['checkpoint = lllyasviel/control_v11p_sd15_openpose', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)'], 'example_usage': ['image = load_image(https://huggingface.co/lllyasviel/control_v11p_sd15_openpose/resolve/main/images/input.png)', 'prompt = chef in the kitchen', \"processor = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\", 'control_image = processor(image, hand_and_face=True)', 'control_image.save(./images/control.png)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"]}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.' </s>"
"<s>[INST] إنشاء روبوت محادثة باستخدام واجهة برمجة التطبيقات التي يمكنها مساعدة المستخدمين في الاستفسارات الأساسية وتوفير المعلومات.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Diffusers', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15_openpose', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_openpose')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_openpose', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': {'import_libraries': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import OpenposeDetector', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')'], 'load_model': ['checkpoint = lllyasviel/control_v11p_sd15_openpose', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)'], 'example_usage': ['image = load_image(https://huggingface.co/lllyasviel/control_v11p_sd15_openpose/resolve/main/images/input.png)', 'prompt = chef in the kitchen', \"processor = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\", 'control_image = processor(image, hand_and_face=True)', 'control_image.save(./images/control.png)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"]}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.' </s>"
"<s>[INST] \"إنشاء نظام توصية باستخدام تقنية التصفية التعاونية استنادًا إلى بيانات تفاعلات عناصر المستخدم.\"، \"تطوير نظام تصفية قائم على المحتوى باستخدام واجهة برمجة التطبيقات للتوصية بمنتجات مماثلة للمستخدمين بناءً على تفضيلاتهم.\"، \"تنفيذ نموذج انحدار باستخدام واجهة برمجة التطبيقات\". للتنبؤ بأسعار المنازل بناءً على ميزات مثل الموقع والحجم ووسائل الراحة.، إنشاء نموذج تصنيف باستخدام واجهة برمجة التطبيقات للتنبؤ بتقلبات العملاء استنادًا إلى البيانات التاريخية لسلوك العملاء.، استخدم واجهة برمجة التطبيقات لتطوير نموذج تجزئة الصور لتحديد وتصنيف الكائنات في الصور بدقة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Diffusers', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15_openpose', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_openpose')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_openpose', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': {'import_libraries': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import OpenposeDetector', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')'], 'load_model': ['checkpoint = lllyasviel/control_v11p_sd15_openpose', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)'], 'example_usage': ['image = load_image(https://huggingface.co/lllyasviel/control_v11p_sd15_openpose/resolve/main/images/input.png)', 'prompt = chef in the kitchen', \"processor = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\", 'control_image = processor(image, hand_and_face=True)', 'control_image.save(./images/control.png)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"]}, 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.' </s>"
"<s>[INST]قم ببناء منصة لتحليل توجيهات السفر وتوفير توصيات مخصصة للمستخدمين. يمكنك استخدام خدمة Google Maps API للحصول على معلومات المواقع والاتجاهات.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Super-Resolution', 'api_name': 'caidas/swin2SR-classical-sr-x2-64', 'api_call': \"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\", 'api_arguments': 'image, model, feature_extractor', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation.', 'performance': {'dataset': 'arxiv: 2209.11345', 'accuracy': 'Not provided'}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.' </s>"
"<s>[INST] قم بتطوير تطبيق لفحص النصوص الطبية وتحديد التشخيصات المحتملة باستخدام تقنيات تعلم الآلة. يمكنك استخدام خدمة IBM Watson Health API لتحسين دقة التشخيصات.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Super-Resolution', 'api_name': 'caidas/swin2SR-classical-sr-x2-64', 'api_call': \"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\", 'api_arguments': 'image, model, feature_extractor', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation.', 'performance': {'dataset': 'arxiv: 2209.11345', 'accuracy': 'Not provided'}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.' </s>"
"<s>[INST] ابتكر نظامًا لتحليل مشتريات المستهلكين وتوقع سلوكهم الشرائي باستخدام تقنيات التعلم الآلي. يمكنك الوصول إلى خدمة Amazon Personalize API لتنفيذ هذا.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Super-Resolution', 'api_name': 'caidas/swin2SR-classical-sr-x2-64', 'api_call': \"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\", 'api_arguments': 'image, model, feature_extractor', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation.', 'performance': {'dataset': 'arxiv: 2209.11345', 'accuracy': 'Not provided'}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.' </s>"
"<s>[INST] قم بتطوير تطبيق يقدم ترجمة نصوص بين لغات متعددة باستخدام تقنيات ترجمة اللغات الآلية. يمكنك استخدام Google Cloud Translation API لتنفيذ هذه الوظيفة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Super-Resolution', 'api_name': 'caidas/swin2SR-classical-sr-x2-64', 'api_call': \"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\", 'api_arguments': 'image, model, feature_extractor', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation.', 'performance': {'dataset': 'arxiv: 2209.11345', 'accuracy': 'Not provided'}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.' </s>"
"<s>[INST] قم بتصميم نظام لتحليل البيانات الكبيرة واستخراج الاتجاهات والتوقعات. يمكنك الاستفادة من خدمة Apache Spark API لمعالجة وتحليل البيانات بكفاءة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Super-Resolution', 'api_name': 'caidas/swin2SR-classical-sr-x2-64', 'api_call': \"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\", 'api_arguments': 'image, model, feature_extractor', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation.', 'performance': {'dataset': 'arxiv: 2209.11345', 'accuracy': 'Not provided'}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.' </s>"
"<s>[INST] قم ببناء تطبيق لتحليل سلوك المستخدمين عبر منصات التواصل الاجتماعي باستخدام تقنيات تعلم الآلة. يمكنك استخدام خدمة Twitter API للوصول إلى بيانات التغريدات وتحليلها.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Super-Resolution', 'api_name': 'caidas/swin2SR-classical-sr-x2-64', 'api_call': \"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\", 'api_arguments': 'image, model, feature_extractor', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation.', 'performance': {'dataset': 'arxiv: 2209.11345', 'accuracy': 'Not provided'}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.' </s>"
"<s>[INST] قم بتطوير نظام للتعرف على الكائنات وتصنيفها في الصور باستخدام تقنيات الرؤية الحاسوبية. يمكنك استخدام خدمة OpenCV API لتحقيق هذا الهدف.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Super-Resolution', 'api_name': 'caidas/swin2SR-classical-sr-x2-64', 'api_call': \"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\", 'api_arguments': 'image, model, feature_extractor', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation.', 'performance': {'dataset': 'arxiv: 2209.11345', 'accuracy': 'Not provided'}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.' </s>"
"<s>[INST] قم بإنشاء تطبيق لتحليل نغمات الموسيقى واكتشاف الأنماط والأصوات الموجودة. يمكنك استخدام خدمة Spotify API للوصول إلى بيانات المسارات الصوتية وتحليلها.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Super-Resolution', 'api_name': 'caidas/swin2SR-classical-sr-x2-64', 'api_call': \"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\", 'api_arguments': 'image, model, feature_extractor', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation.', 'performance': {'dataset': 'arxiv: 2209.11345', 'accuracy': 'Not provided'}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.' </s>"
"<s>[INST] قم بتطوير نظام لتحليل البيانات السريرية وتوقع احتياجات المرضى من الرعاية الصحية. يمكنك الوصول إلى خدمة Microsoft Azure Healthcare API لتنفيذ ذلك.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Super-Resolution', 'api_name': 'caidas/swin2SR-classical-sr-x2-64', 'api_call': \"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\", 'api_arguments': 'image, model, feature_extractor', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation.', 'performance': {'dataset': 'arxiv: 2209.11345', 'accuracy': 'Not provided'}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.' </s>"
"<s>[INST] ابتكر نظامًا لتحليل محتوى الفيديو واستخراج مقاطع مفتاحية والتعرف على الأشياء في الأشرطة. يمكنك استخدام خدمة Google Video Intelligence API لتنفيذ هذه المهمة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Super-Resolution', 'api_name': 'caidas/swin2SR-classical-sr-x2-64', 'api_call': \"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\", 'api_arguments': 'image, model, feature_extractor', 'python_environment_requirements': 'transformers', 'example_code': 'Refer to the documentation.', 'performance': {'dataset': 'arxiv: 2209.11345', 'accuracy': 'Not provided'}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.' </s>"
"<s>[INST]تطوير نظام لتصنيف الصور الفوتوغرافية باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11e_sd15_ip2p', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11e_sd15_ip2p')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\\nprompt = make it on fire\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.' </s>"
"<s>[INST] تحسين أداء نظام التقدير الزمني للأحداث الرياضية من خلال استخدام مكتبة معالجة اللغة الطبيعية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11e_sd15_ip2p', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11e_sd15_ip2p')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\\nprompt = make it on fire\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.' </s>"
"<s>[INST] تطوير نظام لتحليل نغمة الصوت واكتشاف الإيقاعات الموسيقية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11e_sd15_ip2p', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11e_sd15_ip2p')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\\nprompt = make it on fire\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.' </s>"
"<s>[INST] تطوير نظام للكشف عن الغش في التقارير الأكاديمية باستخدام تقنيات تحليل النصوص.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11e_sd15_ip2p', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11e_sd15_ip2p')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\\nprompt = make it on fire\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.' </s>"
"<s>[INST] بناء نظام لتحليل المشاهد من مقاطع الفيديو واكتشاف الأشخاص والأشياء الرئيسية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11e_sd15_ip2p', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11e_sd15_ip2p')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\\nprompt = make it on fire\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.' </s>"
"<s>[INST] تطوير نظام لتقدير عمر الأشخاص من صورهم الشخصية باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11e_sd15_ip2p', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11e_sd15_ip2p')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\\nprompt = make it on fire\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.' </s>"
"<s>[INST] إنشاء نظام لتحليل المشاهد وتقديم تفاصيل دقيقة حول المواقع الجغرافية في الصور.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11e_sd15_ip2p', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11e_sd15_ip2p')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\\nprompt = make it on fire\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.' </s>"
"<s>[INST] تطوير نظام لتحليل المحتوى النصي لمقاطع الفيديو وتوليد ملخصات تلقائية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11e_sd15_ip2p', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11e_sd15_ip2p')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\\nprompt = make it on fire\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.' </s>"
"<s>[INST] بناء نظام لفهم وترجمة النصوص من الصور الضوئية باستخدام تكنولوجيا التعلم الآلي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11e_sd15_ip2p', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11e_sd15_ip2p')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\\nprompt = make it on fire\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.' </s>"
"<s>[INST] تطوير نظام لاكتشاف الأشكال الهندسية في الصور وتحديد خصائصها.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11e_sd15_ip2p', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11e_sd15_ip2p')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\\nprompt = make it on fire\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not provided'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.' </s>"
"<s>[INST]\"كيف يمكننا تجميع بيانات المرضى من مصادر متعددة وتحليلها باستخدام نماذج التعلم الآلي لتحسين تشخيص الأمراض؟\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_seg'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] \"كيف يمكن إنشاء نظام يستخدم تقنيات التحليل النصي لمراقبة وتحليل التغريدات الاجتماعية لقياس معدل السعادة في المجتمع؟\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_seg'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] \"كيف يمكن تنفيذ نظام تصنيف للصور يعتمد على تعلم الآلة لتحديد محتوى الصور وتصنيفها بشكل تلقائي؟\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_seg'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] \"كيف يمكن إعداد نظام تعرف على النصوص لتحليل وتصنيف النصوص وفقًا لمواضيعها وتحديد المفردات الرئيسية؟\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_seg'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] \"كيف يمكن استخدام تقنيات التعلم العميق لتطوير نموذج توصية يوفر إقتراحات شخصية ومخصصة لتحسين تجربة التسوق عبر الإنترنت؟\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_seg'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] \"كيف يمكن أن تُستخدم تقنيات التعلم الآلي ومعالجة اللغات الطبيعية لتطوير نظام لتحليل وتصنيف المقاطع الصوتية؟\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_seg'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] \"كيف يمكن تنفيذ نظام للاستدلال الآلي لتوليد توصيات عن الأفلام بناءً على تفضيلات المستخدمين وتقييماتهم السابقة؟\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_seg'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] \"كيف يمكن إنشاء نظام دعم قرار يستخدم تقنيات التعلم الآلي لمساعدة المديرين في اتخاذ القرارات الاستراتيجية؟\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_seg'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] \"كيف يمكن تطبيق التعلم الآلي وتقنيات الذكاء الاصطناعي لتطوير نظام تشخيص ذكي لتحديد الأمراض بدقة وسرعة؟\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_seg', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_seg'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate'], 'example_code': \"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'COCO', 'accuracy': 'Not specified'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST]تطوير تطبيق لتحليل المشاعر في التغريدات باستخدام API تحليل اللغة الطبيعية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_softedge', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_softedge')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_softedge', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_softedge\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_softedge/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\nprocessor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, safe=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'ControlNet', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] إنشاء تطبيق يقوم بتحويل النصوص إلى كلام مسموع باستخدام واجهة تحويل النص إلى كلام.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_softedge', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_softedge')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_softedge', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_softedge\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_softedge/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\nprocessor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, safe=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'ControlNet', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] تطوير تطبيق لتحليل الصور واستخراج المعلومات الكتابية منها باستخدام واجهة تحليل الصور.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_softedge', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_softedge')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_softedge', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_softedge\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_softedge/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\nprocessor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, safe=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'ControlNet', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] تطوير تطبيق لتحليل المستندات واستخراج المعلومات الرئيسية منها باستخدام واجهة تحليل المستندات.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_softedge', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_softedge')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_softedge', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_softedge\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_softedge/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\nprocessor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, safe=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'ControlNet', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم واجهة تحليل التواصل اللغوي لتحليل طريقة التواصل في النصوص.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_softedge', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_softedge')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_softedge', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_softedge\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_softedge/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\nprocessor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, safe=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'ControlNet', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] تطوير تطبيق يستخدم API الترجمة الآلية لترجمة النصوص بين لغات مختلفة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_softedge', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_softedge')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_softedge', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_softedge\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_softedge/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\nprocessor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, safe=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'ControlNet', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] بناء تطبيق يستخدم واجهة التعرف على الصوت لتحويل المدخلات الصوتية إلى نصوص مكتوبة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_softedge', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_softedge')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_softedge', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_softedge\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_softedge/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\nprocessor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, safe=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'ControlNet', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] تطوير تطبيق يستخدم واجهة تعرف على الكائنات لتحديد وصف محتوى الصور بواسطة التعرف على الكائنات.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_softedge', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_softedge')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_softedge', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_softedge\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_softedge/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\nprocessor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, safe=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'ControlNet', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] إنشاء تطبيق يستفيد من واجهة تعرف الشخصيات للكشف عن الشخصيات الموجودة في الصور.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_softedge', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_softedge')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_softedge', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_softedge\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_softedge/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\nprocessor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, safe=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'ControlNet', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] بناء تطبيق يستفيد من واجهة التعرف على النصوص للكشف عن النصوص والمعلومات المهمة في الصور.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation', 'api_name': 'lllyasviel/control_v11p_sd15_softedge', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_softedge')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_softedge', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux==0.3.0'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_softedge\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_softedge/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\nprocessor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, safe=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'ControlNet', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST]كتابة تطبيق يستخدم تقنيات Computer Vision لتحديد الأشكال الهندسية في الصور. هل يتوفر API مناسب لهذه الغرض؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-lightweight-x2-64', 'api_call': \"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\", 'api_arguments': 'feature_extractor, model', 'python_environment_requirements': 'transformers, torch', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.' </s>"
"<s>[INST] بناء نظام يستخدم الـNatural Language Processing لتوليد نصوص افتراضية بناء على بيانات الإدخال. أي API ممكن استخدامه لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-lightweight-x2-64', 'api_call': \"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\", 'api_arguments': 'feature_extractor, model', 'python_environment_requirements': 'transformers, torch', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.' </s>"
"<s>[INST] تطوير تطبيق يستخدم تقنيات Image Segmentation لتقسيم الصور إلى أقسام مختلفة. هل يوجد API مناسب لهذه المهمة؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-lightweight-x2-64', 'api_call': \"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\", 'api_arguments': 'feature_extractor, model', 'python_environment_requirements': 'transformers, torch', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.' </s>"
"<s>[INST] تكوين نظام يستخدم تقنيات التعلم الآلي لتحليل معنى النصوص الطبية. هل يتوفر API مناسب لهذه المهمة؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-lightweight-x2-64', 'api_call': \"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\", 'api_arguments': 'feature_extractor, model', 'python_environment_requirements': 'transformers, torch', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم تقنيات Computer Vision لتعلم وتحليل نمط الألوان في الصور. هل هناك API مناسب لهذا النوع من المهام؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-lightweight-x2-64', 'api_call': \"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\", 'api_arguments': 'feature_extractor, model', 'python_environment_requirements': 'transformers, torch', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.' </s>"
"<s>[INST] بناء نظام يستخدم تقنيات Image Recognition لمعرفة وتصنيف الأطعمة المعروضة في الصور. هل تتوفر API مناسب لهذا الهدف؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-lightweight-x2-64', 'api_call': \"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\", 'api_arguments': 'feature_extractor, model', 'python_environment_requirements': 'transformers, torch', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.' </s>"
"<s>[INST] تطوير تطبيق يستخدم تقنيات التعلم العميق لتحديد الأشياء والكائنات في الصور بدقة. هل يمكن الاعتماد على API محدد لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-lightweight-x2-64', 'api_call': \"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\", 'api_arguments': 'feature_extractor, model', 'python_environment_requirements': 'transformers, torch', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.' </s>"
"<s>[INST] تجربة تطبيق يستخدم تقنيات التعلم العميق لتحليل سلوك المستخدمين على الإنترنت. هل يوجد API ملائم لهذا الهدف؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-lightweight-x2-64', 'api_call': \"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\", 'api_arguments': 'feature_extractor, model', 'python_environment_requirements': 'transformers, torch', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.' </s>"
"<s>[INST] إنشاء نظام يستخدم تقنيات Natural Language Processing لترجمة النصوص إلى لغات مختلفة. هل يوجد API مناسب يمكن استخدامه لهذه الغرض؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-lightweight-x2-64', 'api_call': \"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\", 'api_arguments': 'feature_extractor, model', 'python_environment_requirements': 'transformers, torch', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.' </s>"
"<s>[INST] بناء تطبيق يستخدم تقنيات Computer Vision لتحليل وفهم تركيبة الوجوه في الصور. هل يتوفر API مناسب لهذا النوع من المهام؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-lightweight-x2-64', 'api_call': \"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\", 'api_arguments': 'feature_extractor, model', 'python_environment_requirements': 'transformers, torch', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.' </s>"
"<s>[INST]استخدم واجهة برمجة لإنشاء تطبيق يقدم تطبيقاً لتحليل النصوص العربية واستخراج المعلومات الرئيسية منها.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15_mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import MLSDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_mlsd\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'MLSD', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.' </s>"
"<s>[INST] انشئ خدمة ويب تستخدم واجهة برمجة لعرض الصور بشكل آلي مع توفير خيارات للتحكم والتعديل.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15_mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import MLSDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_mlsd\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'MLSD', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.' </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم واجهة برمجة لتحليل الأداء الرياضي للاعبين واقتراح تدابير لتحسين الأداء.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15_mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import MLSDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_mlsd\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'MLSD', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم واجهة برمجة لتحليل مقاطع الفيديو واستخراج المعلومات الهامة منها.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15_mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import MLSDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_mlsd\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'MLSD', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.' </s>"
"<s>[INST] اكتب تطبيقاً يستخدم واجهة برمجة لتوليد نصوص إبداعية بشكل آلي بناءً على مدخلات المستخدم.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15_mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import MLSDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_mlsd\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'MLSD', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.' </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم واجهة برمجة لتحليل السلوك الاستهلاكي للمستخدمين وتقديم توصيات شخصية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15_mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import MLSDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_mlsd\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'MLSD', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.' </s>"
"<s>[INST] أنشئ تطبيقا يستخدم واجهة برمجة لتحليل بيانات صحية وتقديم نصائح طبية شخصية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15_mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import MLSDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_mlsd\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'MLSD', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.' </s>"
"<s>[INST] كتابة برنامج يستخدم واجهة برمجة لتحليل البيانات الجغرافية وتقديم توصيات فيما يتعلق بالطقس.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15_mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import MLSDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_mlsd\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'MLSD', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.' </s>"
"<s>[INST] إعداد تطبيق يستخدم واجهة برمجة لتقديم نصائح تغذية صحية مبنية على تفاصيل المستخدم.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15_mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import MLSDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_mlsd\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'MLSD', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.' </s>"
"<s>[INST] ابتكر تطبيقا يستخدم واجهة برمجة لتحليل مواقع الويب وتلميحات لتحسين تصميمها وأدائها.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15_mlsd', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import MLSDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_mlsd\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'MLSD', 'accuracy': 'Not provided'}, 'description': 'Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.' </s>"
"<s>[INST]تحتاج شركة لتطوير نظام يستطيع تحليل تسجيلات الملاحظات الصوتية وتحويلها إلى نصوص كتابية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11p_sd15_normalbae', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import NormalBaeDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_normalbae\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_normalbae/resolve/main/images/input.png\\n)\\nprompt = A head full of roses\\nprocessor = NormalBaeDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(33)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] قاعدة بيانات ترغب في تطوير نظام للاستعلام عن البيانات بواجهة برمجية توفر نتائج دقيقة وسريعة. تقدم واجهة برمجية DuckDB API المناسبة لهذا الغرض.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11p_sd15_normalbae', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import NormalBaeDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_normalbae\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_normalbae/resolve/main/images/input.png\\n)\\nprompt = A head full of roses\\nprocessor = NormalBaeDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(33)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] تطبيق تهتم بتحسين تجربة المستخدم يحتاج لتكامل مع واجهة برمجية توفر مقترحات ذكية لإكمال الكلمات أثناء كتابة النصوص. يمكن استخدام واجهة برمجية GPT-3 API لتحقيق هذه الوظيفة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11p_sd15_normalbae', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import NormalBaeDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_normalbae\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_normalbae/resolve/main/images/input.png\\n)\\nprompt = A head full of roses\\nprocessor = NormalBaeDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(33)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] ترغب منصة التعليم الإلكتروني في تطوير نظام يحفظ ويحلل بيانات تقييم الطلاب لتقديم توجيهات تعليمية شخصية. يمكنك استخدام واجهة برمجية AI Tutoring System API لتحقيق هذا الهدف بفعالية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11p_sd15_normalbae', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import NormalBaeDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_normalbae\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_normalbae/resolve/main/images/input.png\\n)\\nprompt = A head full of roses\\nprocessor = NormalBaeDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(33)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] شركة ترغب في تحسين خدمة العملاء تود تطوير نظام للتعرف الصوتي على أوامر العملاء وتحويلها إلى إجراءات محددة. يمكنك اللجوء إلى واجهة برمجية Speech Recognition API لتنفيذ هذا المشروع بنجاح.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11p_sd15_normalbae', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import NormalBaeDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_normalbae\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_normalbae/resolve/main/images/input.png\\n)\\nprompt = A head full of roses\\nprocessor = NormalBaeDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(33)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] تطبيق تواصل اجتماعي يود تحسين خوارزمية العرض لتقديم منشورات أكثر ارتباطًا بميول المستخدمين. يمكنك استعمال واجهة برمجية Recommendation System API لتطوير هذه الخاصية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11p_sd15_normalbae', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import NormalBaeDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_normalbae\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_normalbae/resolve/main/images/input.png\\n)\\nprompt = A head full of roses\\nprocessor = NormalBaeDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(33)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] منظمة ترغب في تحسين عمليات إدارة المخاطر ترغب في تطوير نظام لتحليل البيانات وتوقع السلوكيات الاحتمالية. يمكنك استخدام واجهة برمجية Risk Analysis API لتحقيق هذا الهدف بفاعلية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11p_sd15_normalbae', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import NormalBaeDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_normalbae\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_normalbae/resolve/main/images/input.png\\n)\\nprompt = A head full of roses\\nprocessor = NormalBaeDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(33)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] شركة ترغب في تحسين استراتيجيتها التسويقية تريد تطوير نظام لتحليل تفاعل المستخدمين مع إعلاناتها عبر الإنترنت. يمكنك الاستفادة من واجهة برمجية Marketing Analytics API لهذا الغرض.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11p_sd15_normalbae', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import NormalBaeDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_normalbae\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_normalbae/resolve/main/images/input.png\\n)\\nprompt = A head full of roses\\nprocessor = NormalBaeDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(33)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] نريد تطوير تطبيق تجاري يحتوي على ميزة التعرف على الأشياء والأمور من الصور لتحقيق تجربة تسوق متقدمة. يمكنك اللجوء إلى واجهة برمجية Object Recognition API لتنفيذ هذا المشروع بكفاءة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11p_sd15_normalbae', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import NormalBaeDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_normalbae\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_normalbae/resolve/main/images/input.png\\n)\\nprompt = A head full of roses\\nprocessor = NormalBaeDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(33)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST] منظمة بحثية ترغب في تحسين عملية تصنيف وتحليل المقالات العلمية ترغب في تطوير نظام لتصنيف المقالات العلمية تلقائيا. يمكنك الاعتماد على واجهة برمجية Scientific Paper Classification API لتحقيق هذا الهدف بكفاءة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Diffusion-based text-to-image generation model', 'api_name': 'lllyasviel/control_v11p_sd15_normalbae', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae')\", 'api_arguments': ['checkpoint', 'torch_dtype'], 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'controlnet_aux'], 'example_code': \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import NormalBaeDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_normalbae\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_normalbae/resolve/main/images/input.png\\n)\\nprompt = A head full of roses\\nprocessor = NormalBaeDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(33)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", 'performance': {'dataset': 'N/A', 'accuracy': 'N/A'}, 'description': 'ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.' </s>"
"<s>[INST]\"نريد تطبيق نموذج تحليل النصوص mbien/recipetext2image لتوليد صور توضيحية للوصف الغذائي.\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-classical-sr-x4-64', 'api_call': \"pipeline('image-super-resolution', model='caidas/swin2SR-classical-sr-x4-64')\", 'api_arguments': ['input_image'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.' </s>"
"<s>[INST] \"يحتاج فريقنا إلى استخدام API قوي لتحسين صور الويب لموقعنا الإلكتروني. يمكنك استخدام API التالي لتحقيق هذا الهدف:\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-classical-sr-x4-64', 'api_call': \"pipeline('image-super-resolution', model='caidas/swin2SR-classical-sr-x4-64')\", 'api_arguments': ['input_image'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.' </s>"
"<s>[INST] \"تهدف شركتنا إلى تطوير نظام يمكنه تصنيف الصور بالتلقائي. يمكنك استخدام الAPI التالي لتحقيق ذلك:\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-classical-sr-x4-64', 'api_call': \"pipeline('image-super-resolution', model='caidas/swin2SR-classical-sr-x4-64')\", 'api_arguments': ['input_image'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.' </s>"
"<s>[INST] \"لدينا حاجة ملحة إلى تطبيق نموذج ترجمة النصوص الفورية للعمل مع عملائنا العالميين. يمكنك استخدام API التالي لهذا الغرض:\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-classical-sr-x4-64', 'api_call': \"pipeline('image-super-resolution', model='caidas/swin2SR-classical-sr-x4-64')\", 'api_arguments': ['input_image'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.' </s>"
"<s>[INST] \"نريد تطبيق نموذج توليد النصوص لإنشاء وصفات طبخ مبتكرة. يمكنك استخدام الAPI التالي لإنجاز هذه المهمة:\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-classical-sr-x4-64', 'api_call': \"pipeline('image-super-resolution', model='caidas/swin2SR-classical-sr-x4-64')\", 'api_arguments': ['input_image'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.' </s>"
"<s>[INST] \"ترغب شركتنا في تصميم نموذج للكشف التلقائي عن الأشياء في صور الغابة. يمكنك استخدام الAPI التالي لبناء هذا النموذج:\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-classical-sr-x4-64', 'api_call': \"pipeline('image-super-resolution', model='caidas/swin2SR-classical-sr-x4-64')\", 'api_arguments': ['input_image'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.' </s>"
"<s>[INST] \"عميلنا يبحث عن حل يمكنه تحسين جودة صورهم الطبية الرقمية. يمكنك استخدام الAPI التالي لتحقيق هذا الهدف:\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-classical-sr-x4-64', 'api_call': \"pipeline('image-super-resolution', model='caidas/swin2SR-classical-sr-x4-64')\", 'api_arguments': ['input_image'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.' </s>"
"<s>[INST] \"تريد شركتنا تطبيق نموذج لتحليل الملامح الوجهية في الصور. يمكنك استخدام الAPI التالي لإنشاء هذا التطبيق:\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-classical-sr-x4-64', 'api_call': \"pipeline('image-super-resolution', model='caidas/swin2SR-classical-sr-x4-64')\", 'api_arguments': ['input_image'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.' </s>"
"<s>[INST] \"لدينا حاجة لنظام يمكنه تحويل النصوص إلى صور للتوضيح. يمكنك استخدام الAPI التالي لتنفيذ هذه المهمة:\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-classical-sr-x4-64', 'api_call': \"pipeline('image-super-resolution', model='caidas/swin2SR-classical-sr-x4-64')\", 'api_arguments': ['input_image'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.' </s>"
"<s>[INST] \"عميلنا يرغب في تطبيق نموذج لاكتشاف الأشياء في الطبيعة من خلال صور الغابات. يمكنك استخدام الAPI التالي لهذا الغرض:\"? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'swin2SR-classical-sr-x4-64', 'api_call': \"pipeline('image-super-resolution', model='caidas/swin2SR-classical-sr-x4-64')\", 'api_arguments': ['input_image'], 'python_environment_requirements': ['transformers'], 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.' </s>"
"<s>[INST]يرغب المستخدم في تنمية تطبيق تعليمي لتعليم الأطفال الرياضيات الأساسية. ما هي الخطوات اللازمة لاستخدام TensorFlow.js لإنشاء نموذج تعلم آلي لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'GreeneryScenery/SheepsControlV3', 'api_call': \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", 'api_arguments': {'image': 'Path to image file', 'text_guidance': 'Optional text guidance for the model'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': ['from transformers import pipeline', \"model = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", \"result = model({'image': 'path/to/image.jpg', 'text_guidance': 'Optional text guidance'})\"], 'performance': {'dataset': 'GreeneryScenery/SheepsControlV3', 'accuracy': 'Not provided'}, 'description': 'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.' </s>"
"<s>[INST] تود تطوير تطبيق يمكنه تحليل محتوى النصوص باللغة العربية لاكتشاف الأخبار الكاذبة. كيف يمكن استخدام API TensorFlow NLP لهذا الهدف؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'GreeneryScenery/SheepsControlV3', 'api_call': \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", 'api_arguments': {'image': 'Path to image file', 'text_guidance': 'Optional text guidance for the model'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': ['from transformers import pipeline', \"model = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", \"result = model({'image': 'path/to/image.jpg', 'text_guidance': 'Optional text guidance'})\"], 'performance': {'dataset': 'GreeneryScenery/SheepsControlV3', 'accuracy': 'Not provided'}, 'description': 'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.' </s>"
"<s>[INST] تبحث عن طريقة لتحسين أداء نموذج التعلم العميق الحالي الذي تستخدمه في التعرف على الأشخاص من الصور. كيف يمكن استخدام API PyTorch لتحسين دقة التعرف على الوجوه؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'GreeneryScenery/SheepsControlV3', 'api_call': \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", 'api_arguments': {'image': 'Path to image file', 'text_guidance': 'Optional text guidance for the model'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': ['from transformers import pipeline', \"model = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", \"result = model({'image': 'path/to/image.jpg', 'text_guidance': 'Optional text guidance'})\"], 'performance': {'dataset': 'GreeneryScenery/SheepsControlV3', 'accuracy': 'Not provided'}, 'description': 'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.' </s>"
"<s>[INST] تسعى لتطوير تطبيق لتحليل المشاهد الفيديو للكشف عن السلوكيات غير المألوفة، كيف يمكن استخدام تقنيات تعلم الآلة لهذا الغرض بواسطة TensorFlow Object Detection API؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'GreeneryScenery/SheepsControlV3', 'api_call': \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", 'api_arguments': {'image': 'Path to image file', 'text_guidance': 'Optional text guidance for the model'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': ['from transformers import pipeline', \"model = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", \"result = model({'image': 'path/to/image.jpg', 'text_guidance': 'Optional text guidance'})\"], 'performance': {'dataset': 'GreeneryScenery/SheepsControlV3', 'accuracy': 'Not provided'}, 'description': 'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.' </s>"
"<s>[INST] تريد تطوير تطبيق لإجراء ترجمة فورية للنصوص من اللغة العربية إلى الإنكليزية. كيف يمكن استخدام Google Cloud Translation API لتحقيق هذا الهدف؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'GreeneryScenery/SheepsControlV3', 'api_call': \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", 'api_arguments': {'image': 'Path to image file', 'text_guidance': 'Optional text guidance for the model'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': ['from transformers import pipeline', \"model = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", \"result = model({'image': 'path/to/image.jpg', 'text_guidance': 'Optional text guidance'})\"], 'performance': {'dataset': 'GreeneryScenery/SheepsControlV3', 'accuracy': 'Not provided'}, 'description': 'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.' </s>"
"<s>[INST] تود تنفيذ تطبيق يستخدم تقنيات تعلم الآلة لتحليل سلوك المستخدمين في المواقع الإلكترونية. كيف يمكن استخدام TensorFlow Recommender API لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'GreeneryScenery/SheepsControlV3', 'api_call': \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", 'api_arguments': {'image': 'Path to image file', 'text_guidance': 'Optional text guidance for the model'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': ['from transformers import pipeline', \"model = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", \"result = model({'image': 'path/to/image.jpg', 'text_guidance': 'Optional text guidance'})\"], 'performance': {'dataset': 'GreeneryScenery/SheepsControlV3', 'accuracy': 'Not provided'}, 'description': 'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.' </s>"
"<s>[INST] تريد تطوير تطبيق لتحليل وتصنيف صور الطعام. كيف يمكن استخدام API TensorFlow Image Classification لتحقيق هذا الغرض؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'GreeneryScenery/SheepsControlV3', 'api_call': \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", 'api_arguments': {'image': 'Path to image file', 'text_guidance': 'Optional text guidance for the model'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': ['from transformers import pipeline', \"model = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", \"result = model({'image': 'path/to/image.jpg', 'text_guidance': 'Optional text guidance'})\"], 'performance': {'dataset': 'GreeneryScenery/SheepsControlV3', 'accuracy': 'Not provided'}, 'description': 'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.' </s>"
"<s>[INST] يرغب المستخدم في تنمية تطبيق يدعم التعرف على الصوت. ما هي الخطوات الضرورية لاستخدام API SpeechRecognition من Google Cloud لهذا الغرض؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'GreeneryScenery/SheepsControlV3', 'api_call': \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", 'api_arguments': {'image': 'Path to image file', 'text_guidance': 'Optional text guidance for the model'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': ['from transformers import pipeline', \"model = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", \"result = model({'image': 'path/to/image.jpg', 'text_guidance': 'Optional text guidance'})\"], 'performance': {'dataset': 'GreeneryScenery/SheepsControlV3', 'accuracy': 'Not provided'}, 'description': 'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.' </s>"
"<s>[INST] تبحث عن طريقة لتمكين تحليل النصوص الطبيعية لاكتشاف الإشاعات على وسائل التواصل الاجتماعي. كيف يمكن استخدام API BERT من Hugging Face لتحقيق هذا الهدف؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'GreeneryScenery/SheepsControlV3', 'api_call': \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", 'api_arguments': {'image': 'Path to image file', 'text_guidance': 'Optional text guidance for the model'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': ['from transformers import pipeline', \"model = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", \"result = model({'image': 'path/to/image.jpg', 'text_guidance': 'Optional text guidance'})\"], 'performance': {'dataset': 'GreeneryScenery/SheepsControlV3', 'accuracy': 'Not provided'}, 'description': 'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.' </s>"
"<s>[INST] تخطط لتطوير تطبيق لتحليل وتصنيف سلوكيات السائقين من مقاطع الفيديو. كيف يمكن استخدام TensorFlow Object Tracking API لتحقيق هذا الغرض؟? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'GreeneryScenery/SheepsControlV3', 'api_call': \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", 'api_arguments': {'image': 'Path to image file', 'text_guidance': 'Optional text guidance for the model'}, 'python_environment_requirements': {'transformers': 'latest', 'torch': 'latest'}, 'example_code': ['from transformers import pipeline', \"model = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", \"result = model({'image': 'path/to/image.jpg', 'text_guidance': 'Optional text guidance'})\"], 'performance': {'dataset': 'GreeneryScenery/SheepsControlV3', 'accuracy': 'Not provided'}, 'description': 'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.' </s>"
"<s>[INST]قم بتطبيق تحليل نصوص مكتوبة بخط اليد بواسطة نموذج التعلم الآلي الذي يعمل على تحسين دقة التعرف على الحروف والكلمات.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'GreeneryScenery/SheepsControlV5', 'api_call': \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV5')\", 'api_arguments': {'input_image': 'path/to/image/file'}, 'python_environment_requirements': {'huggingface_hub': '>=0.0.17', 'transformers': '>=4.13.0', 'torch': '>=1.10.0'}, 'example_code': '', 'performance': {'dataset': 'poloclub/diffusiondb', 'accuracy': 'Not provided'}, 'description': 'SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.' </s>"
"<s>[INST] استخدم نموذج GreeneryScenery/SheepsControlV5 من Hugging Face لتحويل صور تحتوي على نصوص يدوية إلى نصوص رقمية بدقة عالية. كتب كود Python لتطبيق هذا التحويل.\\n   \\n3. instruction: قم بتطبيق نظام سجلات مركزية داخل تطبيقك لتتبع عمليات الوصول إلى البيانات. استخدم GreeneryScenery/SheepsControlV5 من Hugging Face لتحويل الصور إلى تمثيلات أخرى تساعد في هذا العمل.\\n\\n4. instruction: يهدف فريق التطوير إلى تنفيذ نظام سجلات مركزية لتتبع الوصول إلى البيانات داخل التطبيق. قم باستخدام نموذج GreeneryScenery/SheepsControlV5 من Hugging Face لتحويل الصور وتحسين عمليات التعرف على البيانات.\\n\\n5. instruction: ابدأ بتحليل النصوص المكتوبة بخط اليد عبر استخدام GreeneryScenery/SheepsControlV5 التابع لـ Hugging Face. اكتب جزء الكود الضروري لتطبيق هذه التحليلات.\\n\\n6. instruction: يرغب الفريق في تحويل النصوص اليدوية في الصور إلى نصوص رقمية دقيقة. يمكنك استخدام GreeneryScenery/SheepsControlV5 من Hugging Face لتحقيق ذلك. اكتب الجزء البرمجي المطلوب.\\n\\n7. instruction: قم بتنفيذ نظام سجلات مركزي لتتبع الوصول إلى البيانات داخل التطبيق باستخدام GreeneryScenery/SheepsControlV5 من Hugging Face. كتب جزء الكود لتطبيق هذه الدورية.\\n\\n8. instruction: استخدم GreeneryScenery/SheepsControlV5 من Hugging Face لتحويل النصوص اليدوية في الصور إلى نصوص رقمية. كتب كود Python لتنفيذ هذه العملية بكفاءة.\\n\\n9. instruction: قم بتطبيق نظام سجلات مركزية داخل التطبيق لتتبع عمليات الوصول إلى البيانات باستخدام GreeneryScenery/SheepsControlV5 من Hugging Face. كتب الجزء البرمجي المطلوب.\\n\\n10. instruction: يحتاج فريق التطوير إلى تنفيذ نظام سجلات يساعد في تتبع عمليات الوصول إلى البيانات داخل التطبيق. استخدم GreeneryScenery/SheepsControlV5 من Hugging Face لتحويل الصور وتطوير العمليات المطلوبة. كتب الكود الضروري لتحقيق هذا الهدف.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image-to-Image', 'api_name': 'GreeneryScenery/SheepsControlV5', 'api_call': \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV5')\", 'api_arguments': {'input_image': 'path/to/image/file'}, 'python_environment_requirements': {'huggingface_hub': '>=0.0.17', 'transformers': '>=4.13.0', 'torch': '>=1.10.0'}, 'example_code': '', 'performance': {'dataset': 'poloclub/diffusiondb', 'accuracy': 'Not provided'}, 'description': 'SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.' </s>"
"<s>[INST]لتطوير تطبيق يقدم تفاصيل ومعلومات عن الفنادق والمنتجعات القريبة من موقع المستخدم.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Keras', 'functionality': 'Image Deblurring', 'api_name': 'google/maxim-s3-deblurring-gopro', 'api_call': \"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\", 'api_arguments': ['image'], 'python_environment_requirements': ['huggingface_hub', 'PIL', 'tensorflow', 'numpy', 'requests'], 'example_code': 'from huggingface_hub import from_pretrained_keras\\nfrom PIL import Image\\nimport tensorflow as tf\\nimport numpy as np\\nimport requests\\nurl = https://github.com/sayakpaul/maxim-tf/raw/main/images/Deblurring/input/1fromGOPR0950.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage = np.array(image)\\nimage = tf.convert_to_tensor(image)\\nimage = tf.image.resize(image, (256, 256))\\nmodel = from_pretrained_keras(google/maxim-s3-deblurring-gopro)\\npredictions = model.predict(tf.expand_dims(image, 0))', 'performance': {'dataset': 'GoPro', 'accuracy': {'PSNR': 32.86, 'SSIM': 0.961}}, 'description': 'MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.' </s>"
"<s>[INST] لبناء نظام يوفر تحليل تفصيلي لمركبات المرضى وإدارتها في المستشفيات.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Keras', 'functionality': 'Image Deblurring', 'api_name': 'google/maxim-s3-deblurring-gopro', 'api_call': \"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\", 'api_arguments': ['image'], 'python_environment_requirements': ['huggingface_hub', 'PIL', 'tensorflow', 'numpy', 'requests'], 'example_code': 'from huggingface_hub import from_pretrained_keras\\nfrom PIL import Image\\nimport tensorflow as tf\\nimport numpy as np\\nimport requests\\nurl = https://github.com/sayakpaul/maxim-tf/raw/main/images/Deblurring/input/1fromGOPR0950.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage = np.array(image)\\nimage = tf.convert_to_tensor(image)\\nimage = tf.image.resize(image, (256, 256))\\nmodel = from_pretrained_keras(google/maxim-s3-deblurring-gopro)\\npredictions = model.predict(tf.expand_dims(image, 0))', 'performance': {'dataset': 'GoPro', 'accuracy': {'PSNR': 32.86, 'SSIM': 0.961}}, 'description': 'MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.' </s>"
"<s>[INST] لتطوير تطبيق يوفر تنبؤات دقيقة حول حركة الأسهم وتوجيهات الاستثمار.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Keras', 'functionality': 'Image Deblurring', 'api_name': 'google/maxim-s3-deblurring-gopro', 'api_call': \"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\", 'api_arguments': ['image'], 'python_environment_requirements': ['huggingface_hub', 'PIL', 'tensorflow', 'numpy', 'requests'], 'example_code': 'from huggingface_hub import from_pretrained_keras\\nfrom PIL import Image\\nimport tensorflow as tf\\nimport numpy as np\\nimport requests\\nurl = https://github.com/sayakpaul/maxim-tf/raw/main/images/Deblurring/input/1fromGOPR0950.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage = np.array(image)\\nimage = tf.convert_to_tensor(image)\\nimage = tf.image.resize(image, (256, 256))\\nmodel = from_pretrained_keras(google/maxim-s3-deblurring-gopro)\\npredictions = model.predict(tf.expand_dims(image, 0))', 'performance': {'dataset': 'GoPro', 'accuracy': {'PSNR': 32.86, 'SSIM': 0.961}}, 'description': 'MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.' </s>"
"<s>[INST] لتطوير تطبيق يتيح للمستخدمين حجز تذاكر السفر وتنظيم رحلاتهم بسهولة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Keras', 'functionality': 'Image Deblurring', 'api_name': 'google/maxim-s3-deblurring-gopro', 'api_call': \"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\", 'api_arguments': ['image'], 'python_environment_requirements': ['huggingface_hub', 'PIL', 'tensorflow', 'numpy', 'requests'], 'example_code': 'from huggingface_hub import from_pretrained_keras\\nfrom PIL import Image\\nimport tensorflow as tf\\nimport numpy as np\\nimport requests\\nurl = https://github.com/sayakpaul/maxim-tf/raw/main/images/Deblurring/input/1fromGOPR0950.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage = np.array(image)\\nimage = tf.convert_to_tensor(image)\\nimage = tf.image.resize(image, (256, 256))\\nmodel = from_pretrained_keras(google/maxim-s3-deblurring-gopro)\\npredictions = model.predict(tf.expand_dims(image, 0))', 'performance': {'dataset': 'GoPro', 'accuracy': {'PSNR': 32.86, 'SSIM': 0.961}}, 'description': 'MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.' </s>"
"<s>[INST] لبناء نظام يستخدم الذكاء الاصطناعي في تحليل البيانات الكبيرة لفهم سلوك المستخدمين على منصة التواصل الاجتماعي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Keras', 'functionality': 'Image Deblurring', 'api_name': 'google/maxim-s3-deblurring-gopro', 'api_call': \"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\", 'api_arguments': ['image'], 'python_environment_requirements': ['huggingface_hub', 'PIL', 'tensorflow', 'numpy', 'requests'], 'example_code': 'from huggingface_hub import from_pretrained_keras\\nfrom PIL import Image\\nimport tensorflow as tf\\nimport numpy as np\\nimport requests\\nurl = https://github.com/sayakpaul/maxim-tf/raw/main/images/Deblurring/input/1fromGOPR0950.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage = np.array(image)\\nimage = tf.convert_to_tensor(image)\\nimage = tf.image.resize(image, (256, 256))\\nmodel = from_pretrained_keras(google/maxim-s3-deblurring-gopro)\\npredictions = model.predict(tf.expand_dims(image, 0))', 'performance': {'dataset': 'GoPro', 'accuracy': {'PSNR': 32.86, 'SSIM': 0.961}}, 'description': 'MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.' </s>"
"<s>[INST] لتطوير تطبيق يستخدم تكنولوجيا تعلم الآلة للتعرف على أنماط الإنترنت السلوكية لتوجيه المحتوى الرقمي للمستخدمين.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Keras', 'functionality': 'Image Deblurring', 'api_name': 'google/maxim-s3-deblurring-gopro', 'api_call': \"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\", 'api_arguments': ['image'], 'python_environment_requirements': ['huggingface_hub', 'PIL', 'tensorflow', 'numpy', 'requests'], 'example_code': 'from huggingface_hub import from_pretrained_keras\\nfrom PIL import Image\\nimport tensorflow as tf\\nimport numpy as np\\nimport requests\\nurl = https://github.com/sayakpaul/maxim-tf/raw/main/images/Deblurring/input/1fromGOPR0950.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage = np.array(image)\\nimage = tf.convert_to_tensor(image)\\nimage = tf.image.resize(image, (256, 256))\\nmodel = from_pretrained_keras(google/maxim-s3-deblurring-gopro)\\npredictions = model.predict(tf.expand_dims(image, 0))', 'performance': {'dataset': 'GoPro', 'accuracy': {'PSNR': 32.86, 'SSIM': 0.961}}, 'description': 'MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.' </s>"
"<s>[INST] لتطوير تطبيق يستخدم تقنيات تحليل البيانات لفهم أنماط التسوق عبر الإنترنت وتوجيه المستخدمين لمشتريات أفضل.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Keras', 'functionality': 'Image Deblurring', 'api_name': 'google/maxim-s3-deblurring-gopro', 'api_call': \"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\", 'api_arguments': ['image'], 'python_environment_requirements': ['huggingface_hub', 'PIL', 'tensorflow', 'numpy', 'requests'], 'example_code': 'from huggingface_hub import from_pretrained_keras\\nfrom PIL import Image\\nimport tensorflow as tf\\nimport numpy as np\\nimport requests\\nurl = https://github.com/sayakpaul/maxim-tf/raw/main/images/Deblurring/input/1fromGOPR0950.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage = np.array(image)\\nimage = tf.convert_to_tensor(image)\\nimage = tf.image.resize(image, (256, 256))\\nmodel = from_pretrained_keras(google/maxim-s3-deblurring-gopro)\\npredictions = model.predict(tf.expand_dims(image, 0))', 'performance': {'dataset': 'GoPro', 'accuracy': {'PSNR': 32.86, 'SSIM': 0.961}}, 'description': 'MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.' </s>"
"<s>[INST] لبناء نظام يقدم توصيات شخصية للمستخدمين استنادًا إلى تحليل سلوكهم عبر تطبيق التواصل الاجتماعي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Keras', 'functionality': 'Image Deblurring', 'api_name': 'google/maxim-s3-deblurring-gopro', 'api_call': \"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\", 'api_arguments': ['image'], 'python_environment_requirements': ['huggingface_hub', 'PIL', 'tensorflow', 'numpy', 'requests'], 'example_code': 'from huggingface_hub import from_pretrained_keras\\nfrom PIL import Image\\nimport tensorflow as tf\\nimport numpy as np\\nimport requests\\nurl = https://github.com/sayakpaul/maxim-tf/raw/main/images/Deblurring/input/1fromGOPR0950.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage = np.array(image)\\nimage = tf.convert_to_tensor(image)\\nimage = tf.image.resize(image, (256, 256))\\nmodel = from_pretrained_keras(google/maxim-s3-deblurring-gopro)\\npredictions = model.predict(tf.expand_dims(image, 0))', 'performance': {'dataset': 'GoPro', 'accuracy': {'PSNR': 32.86, 'SSIM': 0.961}}, 'description': 'MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.' </s>"
"<s>[INST] لتطوير تطبيق يستخدم تقنيات التعلم العميق لتحليل تصرفات المشترين عبر متجر التجزئة الإلكتروني وتحسين تجربتهم.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Keras', 'functionality': 'Image Deblurring', 'api_name': 'google/maxim-s3-deblurring-gopro', 'api_call': \"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\", 'api_arguments': ['image'], 'python_environment_requirements': ['huggingface_hub', 'PIL', 'tensorflow', 'numpy', 'requests'], 'example_code': 'from huggingface_hub import from_pretrained_keras\\nfrom PIL import Image\\nimport tensorflow as tf\\nimport numpy as np\\nimport requests\\nurl = https://github.com/sayakpaul/maxim-tf/raw/main/images/Deblurring/input/1fromGOPR0950.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage = np.array(image)\\nimage = tf.convert_to_tensor(image)\\nimage = tf.image.resize(image, (256, 256))\\nmodel = from_pretrained_keras(google/maxim-s3-deblurring-gopro)\\npredictions = model.predict(tf.expand_dims(image, 0))', 'performance': {'dataset': 'GoPro', 'accuracy': {'PSNR': 32.86, 'SSIM': 0.961}}, 'description': 'MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.' </s>"
"<s>[INST] لبناء نظام يستخدم تقنيات التعلم الآلي في تحليل بيانات السيارات المتوفرة للبيع لتقديم توصيات للمستخدمين.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Keras', 'functionality': 'Image Deblurring', 'api_name': 'google/maxim-s3-deblurring-gopro', 'api_call': \"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\", 'api_arguments': ['image'], 'python_environment_requirements': ['huggingface_hub', 'PIL', 'tensorflow', 'numpy', 'requests'], 'example_code': 'from huggingface_hub import from_pretrained_keras\\nfrom PIL import Image\\nimport tensorflow as tf\\nimport numpy as np\\nimport requests\\nurl = https://github.com/sayakpaul/maxim-tf/raw/main/images/Deblurring/input/1fromGOPR0950.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage = np.array(image)\\nimage = tf.convert_to_tensor(image)\\nimage = tf.image.resize(image, (256, 256))\\nmodel = from_pretrained_keras(google/maxim-s3-deblurring-gopro)\\npredictions = model.predict(tf.expand_dims(image, 0))', 'performance': {'dataset': 'GoPro', 'accuracy': {'PSNR': 32.86, 'SSIM': 0.961}}, 'description': 'MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.' </s>"
"<s>[INST]استخدم API Google Maps للعثور على أقرب محطة وقود لموقعك وحساب المسافة بينهما.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['pip install diffusers transformers accelerate', 'pip install controlnet_aux==0.3.0'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import LineartAnimeDetector', 'from transformers import CLIPTextModel', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15s2_lineart_anime', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15s2_lineart_anime/resolve/main/images/input.png', ')', 'image = image.resize((512, 512))', 'prompt = A warrior girl in the jungle', 'processor = LineartAnimeDetector.from_pretrained(lllyasviel/Annotators)', 'control_image = processor(image)', 'control_image.save(./images/control.png)', 'text_encoder = CLIPTextModel.from_pretrained(runwayml/stable-diffusion-v1-5, subfolder=text_encoder, num_hidden_layers=11, torch_dtype=torch.float16)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.' </s>"
"<s>[INST] قم بتطوير تطبيق يسمح للمستخدمين بتحميل الصور وتحويلها إلى رسومات كاريكاتورية باستخدام API DeepArt.io.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['pip install diffusers transformers accelerate', 'pip install controlnet_aux==0.3.0'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import LineartAnimeDetector', 'from transformers import CLIPTextModel', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15s2_lineart_anime', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15s2_lineart_anime/resolve/main/images/input.png', ')', 'image = image.resize((512, 512))', 'prompt = A warrior girl in the jungle', 'processor = LineartAnimeDetector.from_pretrained(lllyasviel/Annotators)', 'control_image = processor(image)', 'control_image.save(./images/control.png)', 'text_encoder = CLIPTextModel.from_pretrained(runwayml/stable-diffusion-v1-5, subfolder=text_encoder, num_hidden_layers=11, torch_dtype=torch.float16)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم API Spotify للحصول على قوائم التشغيل الشهيرة وتشغيل الأغاني.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['pip install diffusers transformers accelerate', 'pip install controlnet_aux==0.3.0'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import LineartAnimeDetector', 'from transformers import CLIPTextModel', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15s2_lineart_anime', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15s2_lineart_anime/resolve/main/images/input.png', ')', 'image = image.resize((512, 512))', 'prompt = A warrior girl in the jungle', 'processor = LineartAnimeDetector.from_pretrained(lllyasviel/Annotators)', 'control_image = processor(image)', 'control_image.save(./images/control.png)', 'text_encoder = CLIPTextModel.from_pretrained(runwayml/stable-diffusion-v1-5, subfolder=text_encoder, num_hidden_layers=11, torch_dtype=torch.float16)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.' </s>"
"<s>[INST] تصميم نظام يستخدم API GitHub للوصول إلى معلومات المستودعات والمساهمين وإدارة المشروعات.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['pip install diffusers transformers accelerate', 'pip install controlnet_aux==0.3.0'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import LineartAnimeDetector', 'from transformers import CLIPTextModel', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15s2_lineart_anime', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15s2_lineart_anime/resolve/main/images/input.png', ')', 'image = image.resize((512, 512))', 'prompt = A warrior girl in the jungle', 'processor = LineartAnimeDetector.from_pretrained(lllyasviel/Annotators)', 'control_image = processor(image)', 'control_image.save(./images/control.png)', 'text_encoder = CLIPTextModel.from_pretrained(runwayml/stable-diffusion-v1-5, subfolder=text_encoder, num_hidden_layers=11, torch_dtype=torch.float16)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.' </s>"
"<s>[INST] إنشاء تطبيق يعتمد على API Twitter للبحث عن التغريدات الحديثة وتحليل الاتجاهات.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['pip install diffusers transformers accelerate', 'pip install controlnet_aux==0.3.0'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import LineartAnimeDetector', 'from transformers import CLIPTextModel', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15s2_lineart_anime', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15s2_lineart_anime/resolve/main/images/input.png', ')', 'image = image.resize((512, 512))', 'prompt = A warrior girl in the jungle', 'processor = LineartAnimeDetector.from_pretrained(lllyasviel/Annotators)', 'control_image = processor(image)', 'control_image.save(./images/control.png)', 'text_encoder = CLIPTextModel.from_pretrained(runwayml/stable-diffusion-v1-5, subfolder=text_encoder, num_hidden_layers=11, torch_dtype=torch.float16)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.' </s>"
"<s>[INST] أنشئ نظام يستخدم API Amazon Web Services (AWS) لتخزين وإدارة البيانات بشكل آمن وفعال.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['pip install diffusers transformers accelerate', 'pip install controlnet_aux==0.3.0'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import LineartAnimeDetector', 'from transformers import CLIPTextModel', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15s2_lineart_anime', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15s2_lineart_anime/resolve/main/images/input.png', ')', 'image = image.resize((512, 512))', 'prompt = A warrior girl in the jungle', 'processor = LineartAnimeDetector.from_pretrained(lllyasviel/Annotators)', 'control_image = processor(image)', 'control_image.save(./images/control.png)', 'text_encoder = CLIPTextModel.from_pretrained(runwayml/stable-diffusion-v1-5, subfolder=text_encoder, num_hidden_layers=11, torch_dtype=torch.float16)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم API Yelp للبحث عن المطاعم القريبة وقراءة التقييمات والتوصيات.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['pip install diffusers transformers accelerate', 'pip install controlnet_aux==0.3.0'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import LineartAnimeDetector', 'from transformers import CLIPTextModel', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15s2_lineart_anime', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15s2_lineart_anime/resolve/main/images/input.png', ')', 'image = image.resize((512, 512))', 'prompt = A warrior girl in the jungle', 'processor = LineartAnimeDetector.from_pretrained(lllyasviel/Annotators)', 'control_image = processor(image)', 'control_image.save(./images/control.png)', 'text_encoder = CLIPTextModel.from_pretrained(runwayml/stable-diffusion-v1-5, subfolder=text_encoder, num_hidden_layers=11, torch_dtype=torch.float16)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.' </s>"
"<s>[INST] استخدم API Spotify لإنشاء قائمة التشغيل المتناسبة مع مزاج المستخدم وتشغيل الأغاني.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['pip install diffusers transformers accelerate', 'pip install controlnet_aux==0.3.0'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import LineartAnimeDetector', 'from transformers import CLIPTextModel', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15s2_lineart_anime', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15s2_lineart_anime/resolve/main/images/input.png', ')', 'image = image.resize((512, 512))', 'prompt = A warrior girl in the jungle', 'processor = LineartAnimeDetector.from_pretrained(lllyasviel/Annotators)', 'control_image = processor(image)', 'control_image.save(./images/control.png)', 'text_encoder = CLIPTextModel.from_pretrained(runwayml/stable-diffusion-v1-5, subfolder=text_encoder, num_hidden_layers=11, torch_dtype=torch.float16)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.' </s>"
"<s>[INST] إنشاء تطبيق يعرض الأخبار الحالية باستخدام API NewsAPI ويسمح للمستخدمين بقراءة المقالات وتحديد المواضيع المفضلة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['pip install diffusers transformers accelerate', 'pip install controlnet_aux==0.3.0'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import LineartAnimeDetector', 'from transformers import CLIPTextModel', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15s2_lineart_anime', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15s2_lineart_anime/resolve/main/images/input.png', ')', 'image = image.resize((512, 512))', 'prompt = A warrior girl in the jungle', 'processor = LineartAnimeDetector.from_pretrained(lllyasviel/Annotators)', 'control_image = processor(image)', 'control_image.save(./images/control.png)', 'text_encoder = CLIPTextModel.from_pretrained(runwayml/stable-diffusion-v1-5, subfolder=text_encoder, num_hidden_layers=11, torch_dtype=torch.float16)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.' </s>"
"<s>[INST] أنشىء نظام يستخدم API YouTube للبحث عن مقاطع الفيديو الشائعة وتشغيلها في التطبيق.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Diffusion Models', 'api_name': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15s2_lineart_anime')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15s2_lineart_anime', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['pip install diffusers transformers accelerate', 'pip install controlnet_aux==0.3.0'], 'example_code': ['import torch', 'import os', 'from huggingface_hub import HfApi', 'from pathlib import Path', 'from diffusers.utils import load_image', 'from PIL import Image', 'import numpy as np', 'from controlnet_aux import LineartAnimeDetector', 'from transformers import CLIPTextModel', 'from diffusers import (', ' ControlNetModel,', ' StableDiffusionControlNetPipeline,', ' UniPCMultistepScheduler,', ')', 'checkpoint = lllyasviel/control_v11p_sd15s2_lineart_anime', 'image = load_image(', ' https://huggingface.co/lllyasviel/control_v11p_sd15s2_lineart_anime/resolve/main/images/input.png', ')', 'image = image.resize((512, 512))', 'prompt = A warrior girl in the jungle', 'processor = LineartAnimeDetector.from_pretrained(lllyasviel/Annotators)', 'control_image = processor(image)', 'control_image.save(./images/control.png)', 'text_encoder = CLIPTextModel.from_pretrained(runwayml/stable-diffusion-v1-5, subfolder=text_encoder, num_hidden_layers=11, torch_dtype=torch.float16)', 'controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)', 'pipe = StableDiffusionControlNetPipeline.from_pretrained(', ' runwayml/stable-diffusion-v1-5, text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16', ')', 'pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)', 'pipe.enable_model_cpu_offload()', 'generator = torch.manual_seed(0)', 'image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]', \"image.save('images/image_out.png')\"], 'performance': {'dataset': 'Not specified', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.' </s>"
"<s>[INST]تصميم تطبيق يساعد العلماء البيولوجيين في تحليل الصور الخلوية بصورة دقيقة وفعّالة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Inpainting', 'api_name': 'lllyasviel/control_v11p_sd15_inpaint', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_inpaint')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_inpaint', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': \"import torch\\nimport os\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_inpaint\\noriginal_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/original.png\\n)\\nmask_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/mask.png\\n)\\ndef make_inpaint_condition(image, image_mask):\\n image = np.array(image.convert(RGB)).astype(np.float32) / 255.0\\n image_mask = np.array(image_mask.convert(L))\\n assert image.shape[0:1] == image_mask.shape[0:1], image and image_mask must have the same image size\\n image[image_mask < 128] = -1.0 # set as masked pixel \\n image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\\n image = torch.from_numpy(image)\\n return image\\ncontrol_image = make_inpaint_condition(original_image, mask_image)\\nprompt = best quality\\nnegative_prompt=lowres, bad anatomy, bad hands, cropped, worst quality\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(2)\\nimage = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, \\n generator=generator, image=control_image).images[0]\\nimage.save('images/output.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.' </s>"
"<s>[INST] قم بتطوير أداة لتحويل النصوص إلى تنسيق بيانات قابل للتحليل والمعالجة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Inpainting', 'api_name': 'lllyasviel/control_v11p_sd15_inpaint', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_inpaint')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_inpaint', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': \"import torch\\nimport os\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_inpaint\\noriginal_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/original.png\\n)\\nmask_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/mask.png\\n)\\ndef make_inpaint_condition(image, image_mask):\\n image = np.array(image.convert(RGB)).astype(np.float32) / 255.0\\n image_mask = np.array(image_mask.convert(L))\\n assert image.shape[0:1] == image_mask.shape[0:1], image and image_mask must have the same image size\\n image[image_mask < 128] = -1.0 # set as masked pixel \\n image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\\n image = torch.from_numpy(image)\\n return image\\ncontrol_image = make_inpaint_condition(original_image, mask_image)\\nprompt = best quality\\nnegative_prompt=lowres, bad anatomy, bad hands, cropped, worst quality\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(2)\\nimage = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, \\n generator=generator, image=control_image).images[0]\\nimage.save('images/output.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.' </s>"
"<s>[INST] انشئ تطبيقًا يقوم بتشخيص الأمراض الجلدية عن طريق تحليل الصور الجلدية بشكل دقيق.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Inpainting', 'api_name': 'lllyasviel/control_v11p_sd15_inpaint', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_inpaint')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_inpaint', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': \"import torch\\nimport os\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_inpaint\\noriginal_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/original.png\\n)\\nmask_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/mask.png\\n)\\ndef make_inpaint_condition(image, image_mask):\\n image = np.array(image.convert(RGB)).astype(np.float32) / 255.0\\n image_mask = np.array(image_mask.convert(L))\\n assert image.shape[0:1] == image_mask.shape[0:1], image and image_mask must have the same image size\\n image[image_mask < 128] = -1.0 # set as masked pixel \\n image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\\n image = torch.from_numpy(image)\\n return image\\ncontrol_image = make_inpaint_condition(original_image, mask_image)\\nprompt = best quality\\nnegative_prompt=lowres, bad anatomy, bad hands, cropped, worst quality\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(2)\\nimage = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, \\n generator=generator, image=control_image).images[0]\\nimage.save('images/output.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.' </s>"
"<s>[INST] قم بتطوير أداة لتحليل النصوص القانونية واستخراج المعلومات الهامة منها.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Inpainting', 'api_name': 'lllyasviel/control_v11p_sd15_inpaint', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_inpaint')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_inpaint', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': \"import torch\\nimport os\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_inpaint\\noriginal_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/original.png\\n)\\nmask_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/mask.png\\n)\\ndef make_inpaint_condition(image, image_mask):\\n image = np.array(image.convert(RGB)).astype(np.float32) / 255.0\\n image_mask = np.array(image_mask.convert(L))\\n assert image.shape[0:1] == image_mask.shape[0:1], image and image_mask must have the same image size\\n image[image_mask < 128] = -1.0 # set as masked pixel \\n image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\\n image = torch.from_numpy(image)\\n return image\\ncontrol_image = make_inpaint_condition(original_image, mask_image)\\nprompt = best quality\\nnegative_prompt=lowres, bad anatomy, bad hands, cropped, worst quality\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(2)\\nimage = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, \\n generator=generator, image=control_image).images[0]\\nimage.save('images/output.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.' </s>"
"<s>[INST] ابتكر تطبيقًا يقوم بتحليل النصوص الإخبارية وتصنيفها حسب الموضوعات والتوجهات السياسية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Inpainting', 'api_name': 'lllyasviel/control_v11p_sd15_inpaint', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_inpaint')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_inpaint', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': \"import torch\\nimport os\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_inpaint\\noriginal_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/original.png\\n)\\nmask_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/mask.png\\n)\\ndef make_inpaint_condition(image, image_mask):\\n image = np.array(image.convert(RGB)).astype(np.float32) / 255.0\\n image_mask = np.array(image_mask.convert(L))\\n assert image.shape[0:1] == image_mask.shape[0:1], image and image_mask must have the same image size\\n image[image_mask < 128] = -1.0 # set as masked pixel \\n image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\\n image = torch.from_numpy(image)\\n return image\\ncontrol_image = make_inpaint_condition(original_image, mask_image)\\nprompt = best quality\\nnegative_prompt=lowres, bad anatomy, bad hands, cropped, worst quality\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(2)\\nimage = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, \\n generator=generator, image=control_image).images[0]\\nimage.save('images/output.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.' </s>"
"<s>[INST] أنشئ أداة لتحليل نغمات صوت الموسيقى وتصنيف الأغاني حسب النمط الموسيقي.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Inpainting', 'api_name': 'lllyasviel/control_v11p_sd15_inpaint', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_inpaint')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_inpaint', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': \"import torch\\nimport os\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_inpaint\\noriginal_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/original.png\\n)\\nmask_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/mask.png\\n)\\ndef make_inpaint_condition(image, image_mask):\\n image = np.array(image.convert(RGB)).astype(np.float32) / 255.0\\n image_mask = np.array(image_mask.convert(L))\\n assert image.shape[0:1] == image_mask.shape[0:1], image and image_mask must have the same image size\\n image[image_mask < 128] = -1.0 # set as masked pixel \\n image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\\n image = torch.from_numpy(image)\\n return image\\ncontrol_image = make_inpaint_condition(original_image, mask_image)\\nprompt = best quality\\nnegative_prompt=lowres, bad anatomy, bad hands, cropped, worst quality\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(2)\\nimage = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, \\n generator=generator, image=control_image).images[0]\\nimage.save('images/output.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.' </s>"
"<s>[INST] طور تطبيقًا يقوم بتحليل توجهات السوق المالية من خلال قراءة تقارير وأخبار البورصة.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Inpainting', 'api_name': 'lllyasviel/control_v11p_sd15_inpaint', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_inpaint')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_inpaint', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': \"import torch\\nimport os\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_inpaint\\noriginal_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/original.png\\n)\\nmask_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/mask.png\\n)\\ndef make_inpaint_condition(image, image_mask):\\n image = np.array(image.convert(RGB)).astype(np.float32) / 255.0\\n image_mask = np.array(image_mask.convert(L))\\n assert image.shape[0:1] == image_mask.shape[0:1], image and image_mask must have the same image size\\n image[image_mask < 128] = -1.0 # set as masked pixel \\n image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\\n image = torch.from_numpy(image)\\n return image\\ncontrol_image = make_inpaint_condition(original_image, mask_image)\\nprompt = best quality\\nnegative_prompt=lowres, bad anatomy, bad hands, cropped, worst quality\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(2)\\nimage = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, \\n generator=generator, image=control_image).images[0]\\nimage.save('images/output.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.' </s>"
"<s>[INST] ابتكر أداة تساعد على تحليل بيانات المرضى وتقديم توقعات دقيقة حول حالتهم الصحية.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Inpainting', 'api_name': 'lllyasviel/control_v11p_sd15_inpaint', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_inpaint')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_inpaint', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': \"import torch\\nimport os\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_inpaint\\noriginal_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/original.png\\n)\\nmask_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/mask.png\\n)\\ndef make_inpaint_condition(image, image_mask):\\n image = np.array(image.convert(RGB)).astype(np.float32) / 255.0\\n image_mask = np.array(image_mask.convert(L))\\n assert image.shape[0:1] == image_mask.shape[0:1], image and image_mask must have the same image size\\n image[image_mask < 128] = -1.0 # set as masked pixel \\n image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\\n image = torch.from_numpy(image)\\n return image\\ncontrol_image = make_inpaint_condition(original_image, mask_image)\\nprompt = best quality\\nnegative_prompt=lowres, bad anatomy, bad hands, cropped, worst quality\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(2)\\nimage = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, \\n generator=generator, image=control_image).images[0]\\nimage.save('images/output.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.' </s>"
"<s>[INST] قم بتطوير أداة لتوليد النصوص الإبداعية والروائية بشكل تلقائي ومبتكر.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Inpainting', 'api_name': 'lllyasviel/control_v11p_sd15_inpaint', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_inpaint')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_inpaint', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': \"import torch\\nimport os\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_inpaint\\noriginal_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/original.png\\n)\\nmask_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/mask.png\\n)\\ndef make_inpaint_condition(image, image_mask):\\n image = np.array(image.convert(RGB)).astype(np.float32) / 255.0\\n image_mask = np.array(image_mask.convert(L))\\n assert image.shape[0:1] == image_mask.shape[0:1], image and image_mask must have the same image size\\n image[image_mask < 128] = -1.0 # set as masked pixel \\n image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\\n image = torch.from_numpy(image)\\n return image\\ncontrol_image = make_inpaint_condition(original_image, mask_image)\\nprompt = best quality\\nnegative_prompt=lowres, bad anatomy, bad hands, cropped, worst quality\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(2)\\nimage = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, \\n generator=generator, image=control_image).images[0]\\nimage.save('images/output.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.' </s>"
"<s>[INST] أنشئ تطبيقًا يستخدم الذكاء الاصطناعي لتحليل الأفلام وتوصية المستخدمين بالأفلام المناسبة لهم.? [/INST]'domain': 'Computer Vision Image-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Inpainting', 'api_name': 'lllyasviel/control_v11p_sd15_inpaint', 'api_call': \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_inpaint')\", 'api_arguments': {'checkpoint': 'lllyasviel/control_v11p_sd15_inpaint', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': 'pip install diffusers transformers accelerate', 'example_code': \"import torch\\nimport os\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_inpaint\\noriginal_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/original.png\\n)\\nmask_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/mask.png\\n)\\ndef make_inpaint_condition(image, image_mask):\\n image = np.array(image.convert(RGB)).astype(np.float32) / 255.0\\n image_mask = np.array(image_mask.convert(L))\\n assert image.shape[0:1] == image_mask.shape[0:1], image and image_mask must have the same image size\\n image[image_mask < 128] = -1.0 # set as masked pixel \\n image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\\n image = torch.from_numpy(image)\\n return image\\ncontrol_image = make_inpaint_condition(original_image, mask_image)\\nprompt = best quality\\nnegative_prompt=lowres, bad anatomy, bad hands, cropped, worst quality\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(2)\\nimage = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, \\n generator=generator, image=control_image).images[0]\\nimage.save('images/output.png')\", 'performance': {'dataset': 'Stable Diffusion v1-5', 'accuracy': 'Not specified'}, 'description': 'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.' </s>"
"<s>[INST]إنشاء نموذج يتعرف على أنواع مختلفة من السيارات من الصور.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Synthesis', 'api_name': 'google/ddpm-cifar10-32', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cifar10-32\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. The model supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.' </s>"
"<s>[INST] تطوير نظام يمكنه تصنيف الملابس الشتوية من الصور.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Synthesis', 'api_name': 'google/ddpm-cifar10-32', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cifar10-32\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. The model supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.' </s>"
"<s>[INST] أريد نظامًا يتمكن من اكتشاف عناصر البناء في الصور الهندسية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Synthesis', 'api_name': 'google/ddpm-cifar10-32', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cifar10-32\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. The model supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.' </s>"
"<s>[INST] قم بتطوير نموذج يستطيع تحديد أنواع مختلفة من الحيوانات من الصور الطبيعية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Synthesis', 'api_name': 'google/ddpm-cifar10-32', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cifar10-32\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. The model supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.' </s>"
"<s>[INST] تطوير نظام يقوم بتحليل الوجوه في الصور لتقييم التعابير الوجهية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Synthesis', 'api_name': 'google/ddpm-cifar10-32', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cifar10-32\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. The model supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.' </s>"
"<s>[INST] قم بتطوير نظام يستخدم تعلم الآلة لتصنيف أنماط الأزياء في الصور.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Synthesis', 'api_name': 'google/ddpm-cifar10-32', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cifar10-32\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. The model supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.' </s>"
"<s>[INST] أريد إنشاء نموذج يمكنه التعرف على العناصر المختلفة في الطبيعة من الصور الخارجية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Synthesis', 'api_name': 'google/ddpm-cifar10-32', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cifar10-32\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. The model supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.' </s>"
"<s>[INST] قم بتطوير نظام لتصنيف الأطعمة في الصور لتحديد الوجبات الغذائية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Synthesis', 'api_name': 'google/ddpm-cifar10-32', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cifar10-32\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. The model supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.' </s>"
"<s>[INST] أقوم ببناء نظام يستطيع تحديد الأشخاص في الصور لتطبيقات التعرف على الوجوه.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Synthesis', 'api_name': 'google/ddpm-cifar10-32', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cifar10-32\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. The model supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.' </s>"
"<s>[INST] تطوير نظام لاكتشاف الأشياء المفقودة في الصور لمساعدة في البحث والإنقاذ.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Image Synthesis', 'api_name': 'google/ddpm-cifar10-32', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cifar10-32\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. The model supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.' </s>"
"<s>[INST]تطوير نظام استشعار ذكي لتحليل سلوك المركبات ورصد السلامة على الطرق باستخدام واجهة برمجة التطبيقات.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('ddpm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm()[sample]\\nimage[0].save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46.' </s>"
"<s>[INST] إنشاء تطبيق للتحقق من الهوية الثنائية بواسطة التعرف الحيوي باستخدام واجهة برمجة التطبيقات.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('ddpm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm()[sample]\\nimage[0].save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46.' </s>"
"<s>[INST] تصميم نظام للتعرف على الأصوات الضارة في البيئة باستخدام تقنية معالجة اللغة الطبيعية وواجهة برمجة التطبيقات.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('ddpm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm()[sample]\\nimage[0].save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46.' </s>"
"<s>[INST] بناء نظام توصيفي لتصنيف النصوص الطبية بدقة من خلال استخدام واجهة برمجة التطبيقات.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('ddpm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm()[sample]\\nimage[0].save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46.' </s>"
"<s>[INST] تنفيذ نظام توصيفي لتحليل بيانات العملاء وتوقع نمط الشراء باستخدام واجهة برمجة التطبيقات المحددة.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('ddpm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm()[sample]\\nimage[0].save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46.' </s>"
"<s>[INST] إعداد نظام لتجميع وتحليل بيانات السوق المالية واستخدامها لتوقع الاتجاهات الاقتصادية باستخدام واجهة برمجة التطبيقات.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('ddpm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm()[sample]\\nimage[0].save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46.' </s>"
"<s>[INST] تصميم وتطوير نظام استشعار لتحليل بيانات الطقس الحالي وإصدار تنبيهات للظواهر الجوية القريبة باستخدام واجهة برمجة التطبيقات.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('ddpm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm()[sample]\\nimage[0].save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46.' </s>"
"<s>[INST] تطوير تطبيق لمراقبة الصحة وتحليل البيانات الحيوية للمستخدم بشكل دوري باستخدام واجهة برمجة التطبيقات.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('ddpm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm()[sample]\\nimage[0].save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46.' </s>"
"<s>[INST] بناء نظام لتحليل واكتشاف الإحصاءات الأساسية للبيانات العملية باستخدام واجهة برمجة التطبيقات.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('ddpm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm()[sample]\\nimage[0].save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46.' </s>"
"<s>[INST] تنفيذ تطبيق للتحقق من المستندات والتحقق من صحتها باستخدام تقنيات البصر الحاسوبي وواجهة برمجة التطبيقات.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('ddpm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm()[sample]\\nimage[0].save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46.' </s>"
"<s>[INST]تطوير API لتحليل الصوت وتحويله إلى نصوص بلغات متعددة مثل الإنجليزية والفرنسية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-cat-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cat-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cat-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.' </s>"
"<s>[INST] إنشاء API لتحليل صور الأقمار الصناعية وتأكيد الأنماط الجيولوجية والزراعية بواسطة تقنيات تعلم الآلة.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-cat-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cat-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cat-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.' </s>"
"<s>[INST] تجهيز واجهة API لتحويل النصوص العربية إلى لهجات مختلفة بدقة عالية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-cat-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cat-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cat-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.' </s>"
"<s>[INST] تكوين API لتقديم استفسارات مرخصة عن الأفلام واسترداد معلومات حول الممثلين والمخرجين والتقييمات.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-cat-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cat-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cat-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.' </s>"
"<s>[INST] بناء API لتحويل النص إلى صوت بمخرجات صوتية طبيعية واقعية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-cat-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cat-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cat-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.' </s>"
"<s>[INST] تنفيذ API لتصنيف الصور الطبية والكشف عن الأمراض باستخدام تقنيات التعلم العميق.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-cat-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cat-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cat-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.' </s>"
"<s>[INST] إنشاء واجهة API للتحقق من هوية الأشخاص باستخدام تقنيات التعرف على الوجوه.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-cat-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cat-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cat-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.' </s>"
"<s>[INST] تنفيذ واجهة API لمعالجة الملفات الصوتية وتحويلها إلى تنسيقات مختلفة مثل MP3 و WAV.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-cat-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cat-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cat-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.' </s>"
"<s>[INST] تكوين API للتواصل مع خدمة الويب لاسترداد بيانات الموقع الجغرافي والمسارات.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-cat-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cat-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cat-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.' </s>"
"<s>[INST] بناء واجهة API لتخزين واستعادة بيانات الصور بشكل آمن وفعال.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-cat-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-cat-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cat-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.' </s>"
"<s>[INST]تصميم واجهة جرافيكية جذابة لتطبيق الهاتف الجوال باستخدام تقنيات تحسين الصور.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-ema-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-celebahq-256')\", 'api_arguments': {'model_id': 'google/ddpm-ema-celebahq-256'}, 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': {'CIFAR10': {'Inception_score': 9.46, 'FID_score': 3.17}, 'LSUN': {'sample_quality': 'similar to ProgressiveGAN'}}}, 'description': 'High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.' </s>"
"<s>[INST] بناء نظام تعرف على الوجوه البشرية وتصنيفها في شروح عن طريق تدفق الميزات البصرية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-ema-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-celebahq-256')\", 'api_arguments': {'model_id': 'google/ddpm-ema-celebahq-256'}, 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': {'CIFAR10': {'Inception_score': 9.46, 'FID_score': 3.17}, 'LSUN': {'sample_quality': 'similar to ProgressiveGAN'}}}, 'description': 'High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.' </s>"
"<s>[INST] استخدام تقنيات توليد النص التلقائي لتطوير نظام توليد قصص قصيرة تفاعلي.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-ema-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-celebahq-256')\", 'api_arguments': {'model_id': 'google/ddpm-ema-celebahq-256'}, 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': {'CIFAR10': {'Inception_score': 9.46, 'FID_score': 3.17}, 'LSUN': {'sample_quality': 'similar to ProgressiveGAN'}}}, 'description': 'High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.' </s>"
"<s>[INST] تطبيق تقنيات تعرف الكائنات لتطوير نظام كشف عن الأجسام في الفيديوهات.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-ema-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-celebahq-256')\", 'api_arguments': {'model_id': 'google/ddpm-ema-celebahq-256'}, 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': {'CIFAR10': {'Inception_score': 9.46, 'FID_score': 3.17}, 'LSUN': {'sample_quality': 'similar to ProgressiveGAN'}}}, 'description': 'High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.' </s>"
"<s>[INST] بناء نظام تصنيف تلقائي للصور الطبية باستخدام تقنيات التعلم العميق.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-ema-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-celebahq-256')\", 'api_arguments': {'model_id': 'google/ddpm-ema-celebahq-256'}, 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': {'CIFAR10': {'Inception_score': 9.46, 'FID_score': 3.17}, 'LSUN': {'sample_quality': 'similar to ProgressiveGAN'}}}, 'description': 'High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.' </s>"
"<s>[INST] تحليل بيانات الزبائن وتوليد تقارير توجيهية لتحسين تجربة التسوق عبر الإنترنت.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-ema-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-celebahq-256')\", 'api_arguments': {'model_id': 'google/ddpm-ema-celebahq-256'}, 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': {'CIFAR10': {'Inception_score': 9.46, 'FID_score': 3.17}, 'LSUN': {'sample_quality': 'similar to ProgressiveGAN'}}}, 'description': 'High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.' </s>"
"<s>[INST] تطوير نظام توليد التوصيات الشخصية للمستخدمين باستخدام تقنيات التعلم الآلي.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-ema-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-celebahq-256')\", 'api_arguments': {'model_id': 'google/ddpm-ema-celebahq-256'}, 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': {'CIFAR10': {'Inception_score': 9.46, 'FID_score': 3.17}, 'LSUN': {'sample_quality': 'similar to ProgressiveGAN'}}}, 'description': 'High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.' </s>"
"<s>[INST] تصميم وتطوير نظام لتحليل المشهد وكشف الحركة في مقاطع الفيديو الأمنية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-ema-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-celebahq-256')\", 'api_arguments': {'model_id': 'google/ddpm-ema-celebahq-256'}, 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': {'CIFAR10': {'Inception_score': 9.46, 'FID_score': 3.17}, 'LSUN': {'sample_quality': 'similar to ProgressiveGAN'}}}, 'description': 'High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.' </s>"
"<s>[INST] تنفيذ تصنيف تلقائي للأخبار باستخدام تقنيات معالجة اللغة الطبيعية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-ema-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-celebahq-256')\", 'api_arguments': {'model_id': 'google/ddpm-ema-celebahq-256'}, 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': {'CIFAR10': {'Inception_score': 9.46, 'FID_score': 3.17}, 'LSUN': {'sample_quality': 'similar to ProgressiveGAN'}}}, 'description': 'High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.' </s>"
"<s>[INST] تطوير نظام لتحليل النصوص واستخراج المعلومات الهامة من التقارير الطبية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-ema-celebahq-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-celebahq-256')\", 'api_arguments': {'model_id': 'google/ddpm-ema-celebahq-256'}, 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': {'CIFAR10': {'Inception_score': 9.46, 'FID_score': 3.17}, 'LSUN': {'sample_quality': 'similar to ProgressiveGAN'}}}, 'description': 'High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.' </s>"
"<s>[INST]قم بتشغيل نموذج تحويل النصوص العميقة لتوليد قصائد باللغة العربية باستخدام Hugging Face Transformers.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-ema-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. DDPM models can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. The model can be used with different pipelines for faster inference and better trade-off between quality and speed.' </s>"
"<s>[INST] استخدم الواقع الافتراضي لتصميم واجهة مستخدم ثلاثية الأبعاد لنظام تفاعلي لعرض معلومات سياحية عن المعالم السياحية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-ema-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. DDPM models can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. The model can be used with different pipelines for faster inference and better trade-off between quality and speed.' </s>"
"<s>[INST] قم بإنشاء نظام يستخدم تقنيات تعلم الآلة لتحليل وتقييم ترجمات النصوص بين العربية والإنجليزية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-ema-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. DDPM models can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. The model can be used with different pipelines for faster inference and better trade-off between quality and speed.' </s>"
"<s>[INST] قم بتحليل ومقارنة أداء نماذج تحليل العواطف على النصوص العربية بواسطة تقنيات تعلم الآلة.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-ema-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. DDPM models can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. The model can be used with different pipelines for faster inference and better trade-off between quality and speed.' </s>"
"<s>[INST] قم بتصميم نظام يستخدم تقنيات تعلم الآلة لتوليد تصاميم جديدة للواجهات الرقمية للتطبيقات الجوال.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-ema-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. DDPM models can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. The model can be used with different pipelines for faster inference and better trade-off between quality and speed.' </s>"
"<s>[INST] استخدم الذكاء الاصطناعي لتحسين أداء نظام توصيات للسياح بناء على تفضيلاتهم واهتماماتهم.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-ema-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. DDPM models can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. The model can be used with different pipelines for faster inference and better trade-off between quality and speed.' </s>"
"<s>[INST] قم بتطوير نظام يستخدم تقنيات تعلم الآلة لتحليل صور الأقمار الصناعية وتقديم معلومات دقيقة حول استخدام الأراضي.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-ema-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. DDPM models can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. The model can be used with different pipelines for faster inference and better trade-off between quality and speed.' </s>"
"<s>[INST] استخدام تقنيات الذكاء الاصطناعي لتشخيص الأمراض النباتية والحشرية بناءً على الصور المقدمة.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-ema-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. DDPM models can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. The model can be used with different pipelines for faster inference and better trade-off between quality and speed.' </s>"
"<s>[INST] قم بتطوير نظام استشعار ذكي يعتمد على تعلم الآلة لتحليل بيانات الاهتزاز والتنبؤ بأعطال المعدات الصناعية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-ema-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. DDPM models can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. The model can be used with different pipelines for faster inference and better trade-off between quality and speed.' </s>"
"<s>[INST] قم بتنفيذ نظام تصنيف للنصوص العربية باستخدام تقنيات تعلم الآلة وتقييم أدائه بمقاييس الدقة والانحراف المعياري.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Diffusers', 'api_name': 'google/ddpm-ema-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-ema-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. DDPM models can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. The model can be used with different pipelines for faster inference and better trade-off between quality and speed.' </s>"
"<s>[INST]لبناء تطبيق يمكنه تحليل وتصنيف صور الأزياء والملابس باستخدام API التعرف على الصور.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'CompVis/ldm-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = CompVis/ldm-celebahq-256\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\nimage = pipeline(num_inference_steps=200)[sample]\\nimage[0].save(ldm_generated_image.png)', 'performance': {'dataset': 'CelebA-HQ', 'accuracy': 'N/A'}, 'description': 'Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.' </s>"
"<s>[INST] قم بتصميم نظام للكشف عن الأشياء المفقودة في الصور باستخدام تقنيات API للكشف على الصور.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'CompVis/ldm-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = CompVis/ldm-celebahq-256\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\nimage = pipeline(num_inference_steps=200)[sample]\\nimage[0].save(ldm_generated_image.png)', 'performance': {'dataset': 'CelebA-HQ', 'accuracy': 'N/A'}, 'description': 'Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.' </s>"
"<s>[INST] إنشاء تطبيق يستخدم تكنولوجيا تعرف الوجه لتحديد الهوية باستخدام API التعرف على الوجوه.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'CompVis/ldm-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = CompVis/ldm-celebahq-256\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\nimage = pipeline(num_inference_steps=200)[sample]\\nimage[0].save(ldm_generated_image.png)', 'performance': {'dataset': 'CelebA-HQ', 'accuracy': 'N/A'}, 'description': 'Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.' </s>"
"<s>[INST] لإنشاء نظام لتحليل تفاعل المستخدمين مع الصور على وسائل التواصل الاجتماعي باستخدام تقنيات API تعرف الصور.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'CompVis/ldm-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = CompVis/ldm-celebahq-256\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\nimage = pipeline(num_inference_steps=200)[sample]\\nimage[0].save(ldm_generated_image.png)', 'performance': {'dataset': 'CelebA-HQ', 'accuracy': 'N/A'}, 'description': 'Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.' </s>"
"<s>[INST] بناء تطبيق للكشف عن المنتجات المشابهة في الصور للمساعدة في عمليات التسوق باستخدام تقنيات API التعرف على الصور.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'CompVis/ldm-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = CompVis/ldm-celebahq-256\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\nimage = pipeline(num_inference_steps=200)[sample]\\nimage[0].save(ldm_generated_image.png)', 'performance': {'dataset': 'CelebA-HQ', 'accuracy': 'N/A'}, 'description': 'Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.' </s>"
"<s>[INST] تطوير نظام لتحليل الصور الطبية واكتشاف التغييرات المرضية باستخدام API لتحليل الصور الطبية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'CompVis/ldm-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = CompVis/ldm-celebahq-256\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\nimage = pipeline(num_inference_steps=200)[sample]\\nimage[0].save(ldm_generated_image.png)', 'performance': {'dataset': 'CelebA-HQ', 'accuracy': 'N/A'}, 'description': 'Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.' </s>"
"<s>[INST] إنشاء تطبيق للتعرف على النصوص في الصور واستخراجها باستخدام تقنيات API لتعرف النصوص في الصور.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'CompVis/ldm-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = CompVis/ldm-celebahq-256\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\nimage = pipeline(num_inference_steps=200)[sample]\\nimage[0].save(ldm_generated_image.png)', 'performance': {'dataset': 'CelebA-HQ', 'accuracy': 'N/A'}, 'description': 'Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.' </s>"
"<s>[INST] لبناء نظام لتحسين جودة الصور وتقديم صور عالية الدقة باستخدام API لتحسين الصور.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'CompVis/ldm-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = CompVis/ldm-celebahq-256\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\nimage = pipeline(num_inference_steps=200)[sample]\\nimage[0].save(ldm_generated_image.png)', 'performance': {'dataset': 'CelebA-HQ', 'accuracy': 'N/A'}, 'description': 'Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.' </s>"
"<s>[INST] إنشاء تطبيق لتحليل الصور الفضائية واستخراج المعلومات الجغرافية باستخدام API تحليل الصور الفضائية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'CompVis/ldm-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = CompVis/ldm-celebahq-256\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\nimage = pipeline(num_inference_steps=200)[sample]\\nimage[0].save(ldm_generated_image.png)', 'performance': {'dataset': 'CelebA-HQ', 'accuracy': 'N/A'}, 'description': 'Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.' </s>"
"<s>[INST] بناء نظام يستخدم تقنيات تعرف الصور لتصنيف الحيوانات في الصور باستخدام API تعرف الصور.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'CompVis/ldm-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = CompVis/ldm-celebahq-256\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\nimage = pipeline(num_inference_steps=200)[sample]\\nimage[0].save(ldm_generated_image.png)', 'performance': {'dataset': 'CelebA-HQ', 'accuracy': 'N/A'}, 'description': 'Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.' </s>"
"<s>[INST]\"استخدم تقنية التعلم العميق باستخدام API \\openai/clip\\ لتحليل محتوى الصور وتوصيفه بوصف نصي.\"? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.' </s>"
"<s>[INST] \"نفذ نظام تصنيف النصوص باستخدام API \\facebook/bart\\ لتحليل ومعالجة النصوص باللغة العربية.\"? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.' </s>"
"<s>[INST] \"قم بتحديد أشكال الوجوه البشرية باستخدام API \\microsoft/face-detection\\ لتطبيق تقنيات معالجة الصور على صور الوجوه.\"? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.' </s>"
"<s>[INST] \"استخدم API \\ibm/watson-nlu\\ لتحليل وتصنيف محتوى النصوص الطبية بدقة عالية.\"? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.' </s>"
"<s>[INST] \"نفذ نظام لتحيين اللهجات اللغوية باستخدام API \\google/t5\\ لتحليل ومعالجة النصوص باللغة العربية.\"? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.' </s>"
"<s>[INST] \"نفذ نظام لإضافة تأثيرات بصرية على الصور باستخدام API \\adobe/photoshop-effect\\ لتحويل الصور بطرق إبداعية.\"? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.' </s>"
"<s>[INST] \"استخدم API \\amazon/comprehend\\ لتحليل المشاعر في النصوص وفهم تعبيرات المشاعر بدقة.\"? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.' </s>"
"<s>[INST] \"استخدم API \\github/spaCy\\ لتنقيح وتحسين النصوص بتطبيق تقنيات متقدمة لمعالجة اللغة.\"? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.' </s>"
"<s>[INST] \"نفذ نظام لتقريب العملات باستخدام API \\exchangeratesapi/io\\ للحصول على معدلات صرف العملات الحالية.\"? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.' </s>"
"<s>[INST] \"نفذ نظام لتحليل التغريدات باستخدام API \\twitter/tweet-sentiment\\ لفهم مشاعر المستخدمين وتصنيف التغريدات.\"? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-church-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-church-256')\", 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.' </s>"
"<s>[INST]قم بتحليل صورة طبية لمريض باستخدام تقنية الرؤية الحاسوبية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ncsnpp-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\", 'api_arguments': {'model_id': 'google/ncsnpp-celebahq-256'}, 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-celebahq-256\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.' </s>"
"<s>[INST] قم بتوليد نص بصري موجه لذوي الاحتياجات الخاصة من خلال تحويل النص إلى صورة.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ncsnpp-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\", 'api_arguments': {'model_id': 'google/ncsnpp-celebahq-256'}, 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-celebahq-256\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.' </s>"
"<s>[INST] أنشىء نموذج لتحديد الكائنات في الصور باستخدام الشبكات العصبية العميقة.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ncsnpp-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\", 'api_arguments': {'model_id': 'google/ncsnpp-celebahq-256'}, 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-celebahq-256\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.' </s>"
"<s>[INST] قم بتجسيد إدخال المستخدم في شكل صورة لتعزيز تجربة تسجيل الدخول.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ncsnpp-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\", 'api_arguments': {'model_id': 'google/ncsnpp-celebahq-256'}, 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-celebahq-256\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.' </s>"
"<s>[INST] أنشىء تطبيقًا يحول الرسومات البيانية إلى صور ملوّنة بشكل زاهي.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ncsnpp-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\", 'api_arguments': {'model_id': 'google/ncsnpp-celebahq-256'}, 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-celebahq-256\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.' </s>"
"<s>[INST] قم بتدريب نموذج لتوليد صور مذهلة بدقة عالية من مجموعة CIFAR-10.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ncsnpp-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\", 'api_arguments': {'model_id': 'google/ncsnpp-celebahq-256'}, 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-celebahq-256\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.' </s>"
"<s>[INST] أضف ميزة التعرف التلقائي على النصوص إلى تطبيقك لتمييز المحتوى بشكل فعال.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ncsnpp-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\", 'api_arguments': {'model_id': 'google/ncsnpp-celebahq-256'}, 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-celebahq-256\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.' </s>"
"<s>[INST] أنشىء تطبيق لتحليل المشهد في الوقت الحقيقي من خلال تقنيات مرئية متطورة.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ncsnpp-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\", 'api_arguments': {'model_id': 'google/ncsnpp-celebahq-256'}, 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-celebahq-256\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.' </s>"
"<s>[INST] استخدم التعلم العميق لتصميم تصنيفات ملصقية للصور الفوتوغرافية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ncsnpp-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\", 'api_arguments': {'model_id': 'google/ncsnpp-celebahq-256'}, 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-celebahq-256\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.' </s>"
"<s>[INST] قم بتحليل المشهد في الصورة لتحديد المواقع والأجسام البارزة باستخدام تقنيات التعرف على الكائنات.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ncsnpp-celebahq-256', 'api_call': \"DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\", 'api_arguments': {'model_id': 'google/ncsnpp-celebahq-256'}, 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-celebahq-256\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)', 'performance': {'dataset': 'CIFAR-10', 'accuracy': {'Inception_score': 9.89, 'FID': 2.2, 'likelihood': 2.99}}, 'description': 'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.' </s>"
"<s>[INST]\"إنشاء نظام لتتبع الأشخاص عن بعد من خلال تحليل تدفقات الفيديو باستخدام مكتبة YOLO (أنت تنظر مرة واحدة فقط) لاكتشاف الأشخاص.\"، \"تنفيذ واجهة برمجة التطبيقات لتحليل بيانات نص الوسائط الاجتماعية لتحليل المشاعر باستخدام BERT (تمثيلات التشفير ثنائية الاتجاه من المحولات) ) نموذج.، قم بتطوير نظام لإنشاء التسميات التوضيحية للصور تلقائيًا باستخدام نموذج العرض والحضور والإخبار في مجال رؤية الكمبيوتر.، قم بإنشاء نظام للتنبؤ بالكلمة التالية في الجملة باستخدام GPT-3 (التوليدي نموذج محول 3) تم تدريبه مسبقًا في مجال معالجة اللغة الطبيعية.، قم بتطوير واجهة برمجة التطبيقات لاكتشاف الكائنات في الصور باستخدام خوارزمية أسرع R-CNN (الشبكة العصبية التلافيفية القائمة على المنطقة) في مجال رؤية الكمبيوتر.، إنشاء نظام لتصنيف الصور باستخدام نموذج ResNet (الشبكة العصبية المتبقية) في مجال رؤية الكمبيوتر.، تنفيذ واجهة برمجة التطبيقات لتحويل الكلام إلى نص باستخدام نموذج DeepSpeech في مجال معالجة اللغة الطبيعية.، تطوير نظام. لتلخيص المستندات باستخدام نموذج BART (المحولات ثنائية الاتجاه والانحدار التلقائي) في مجال معالجة اللغة الطبيعية.، إنشاء واجهة برمجة تطبيقات لتنفيذ ترجمة نصية باستخدام نموذج T5 (محول نقل النص إلى نص) في معالجة اللغة الطبيعية المجال.، تطوير نظام لتجزئة الصور باستخدام نموذج U-Net في مجال رؤية الكمبيوتر.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'ceyda/butterfly_cropped_uniq1K_512', 'api_call': \"LightweightGAN.from_pretrained('ceyda/butterfly_cropped_uniq1K_512')\", 'api_arguments': ['pretrained_model_name_or_path'], 'python_environment_requirements': ['torch', 'huggan.pytorch.lightweight_gan.lightweight_gan'], 'example_code': 'import torch\\nfrom huggan.pytorch.lightweight_gan.lightweight_gan import LightweightGAN\\ngan = LightweightGAN.from_pretrained(ceyda/butterfly_cropped_uniq1K_512)\\ngan.eval()\\nbatch_size = 1\\nwith torch.no_grad():\\n ims = gan.G(torch.randn(batch_size, gan.latent_dim)).clamp_(0., 1.)*255\\n ims = ims.permute(0,2,3,1).detach().cpu().numpy().astype(np.uint8)\\n # ims is [BxWxHxC] call Image.fromarray(ims[0])', 'performance': {'dataset': 'huggan/smithsonian_butterflies_subset', 'accuracy': 'FID score on 100 images'}, 'description': \"Butterfly GAN model based on the paper 'Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis'. The model is intended for fun and learning purposes. It was trained on 1000 images from the huggan/smithsonian_butterflies_subset dataset, with a focus on low data training as mentioned in the paper. The model generates high-quality butterfly images.\" </s>"
"<s>[INST]قم بتطوير تطبيق يستخدم تقنيات توليد الصور بدون شروط لتحويل النصوص إلى صور واقعية.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-bedroom-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-bedroom-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.' </s>"
"<s>[INST] قم بتكامل API يستخدم تقنيات تحسين الصور لتحسين جودة الصور الطبية الناتجة من أجهزة الرنين المغناطيسي.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-bedroom-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-bedroom-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.' </s>"
"<s>[INST] قم بتطوير تطبيق يستخدم تقنيات تعزيز الصور لتحسين دقة ووضوح الصور القديمة والتالفة.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-bedroom-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-bedroom-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.' </s>"
"<s>[INST] قم بإنشاء واجهة برمجية تستخدم تقنيات تحسين الصور لتحسين وضوح الصور الفوتوغرافية بخلفيات معقدة.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-bedroom-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-bedroom-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.' </s>"
"<s>[INST] قم بدمج API يستخدم تقنيات تحسين الصور مع تقنيات توليد الصور بدون شروط لتحويل الصور القديمة إلى صور واقعية عالية الوضوح.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-bedroom-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-bedroom-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.' </s>"
"<s>[INST] قم بتطوير API يستخدم تقنيات تحسين الصور لتنقية الصور الطبية ورفع جودتها لتحسين تشخيص الأمراض.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-bedroom-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-bedroom-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.' </s>"
"<s>[INST] إنشاء واجهة برمجية باستخدام تقنيات تحسين الصور لتحسين جودة الصور الساتلية وزيادة وضوح التفاصيل بها.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-bedroom-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-bedroom-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.' </s>"
"<s>[INST] قم بدمج API يستخدم تقنيات توليد الصور بدون شروط وتقنيات تعزيز الصور لتحويل مقاطع الفيديو إلى مقاطع فيديو عالية الجودة.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-bedroom-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-bedroom-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.' </s>"
"<s>[INST] قم بإنشاء تطبيق يستخدم تقنيات تعزيز الصور لتحسين رؤية الكاميرات الأمنية ورفع وضوح التفاصيل بها.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-bedroom-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-bedroom-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.' </s>"
"<s>[INST] قم بدمج واجهة برمجية تستخدم تقنيات تحسين الصور وتقنيات توليد الصور بدون شروط لتحويل الرسومات البيانية إلى رسوم يمكن تحليلها بسهولة.? [/INST]'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-bedroom-256', 'api_call': \"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\", 'api_arguments': 'None', 'python_environment_requirements': 'diffusers', 'example_code': '!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-bedroom-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception score': 9.46, 'FID score': 3.17}}, 'description': 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.' </s>"
